{"2023-11-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.07575v1","updated":"2023-11-13T18:59:47Z","published":"2023-11-13T18:59:47Z","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for\n  Multi-modal Large Language Models","summary":"  We present SPHINX, a versatile multi-modal large language model (MLLM) with a\njoint mixing of model weights, tuning tasks, and visual embeddings. First, for\nstronger vision-language alignment, we unfreeze the large language model (LLM)\nduring pre-training, and introduce a weight mix strategy between LLMs trained\nby real-world and synthetic data. By directly integrating the weights from two\ndomains, the mixed LLM can efficiently incorporate diverse semantics with\nfavorable robustness. Then, to enable multi-purpose capabilities, we mix a\nvariety of tasks for joint visual instruction tuning, and design task-specific\ninstructions to avoid inter-task conflict. In addition to the basic visual\nquestion answering, we include more challenging tasks such as region-level\nunderstanding, caption grounding, document layout detection, and human pose\nestimation, contributing to mutual enhancement over different scenarios.\nAdditionally, we propose to extract comprehensive visual embeddings from\nvarious network architectures, pre-training paradigms, and information\ngranularity, providing language models with more robust image representations.\nBased on our proposed joint mixing, SPHINX exhibits superior multi-modal\nunderstanding capabilities on a wide range of applications. On top of this, we\nfurther propose an efficient strategy aiming to better capture fine-grained\nappearances of high-resolution images. With a mixing of different scales and\nhigh-resolution sub-images, SPHINX attains exceptional visual parsing and\nreasoning performance on existing evaluation benchmarks. We hope our work may\ncast a light on the exploration of joint mixing in future MLLM research. Code\nis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.\n","authors":["Ziyi Lin","Chris Liu","Renrui Zhang","Peng Gao","Longtian Qiu","Han Xiao","Han Qiu","Chen Lin","Wenqi Shao","Keqin Chen","Jiaming Han","Siyuan Huang","Yichi Zhang","Xuming He","Hongsheng Li","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2311.07575v1.pdf","comment":"Work in progress. Code and demos are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"id":"http://arxiv.org/abs/2311.07564v1","updated":"2023-11-13T18:54:17Z","published":"2023-11-13T18:54:17Z","title":"Can Authorship Attribution Models Distinguish Speakers in Speech\n  Transcripts?","summary":"  Authorship verification is the problem of determining if two distinct writing\nsamples share the same author and is typically concerned with the attribution\nof written text. In this paper, we explore the attribution of transcribed\nspeech, which poses novel challenges. The main challenge is that many stylistic\nfeatures, such as punctuation and capitalization, are not available or\nreliable. Therefore, we expect a priori that transcribed speech is a more\nchallenging domain for attribution. On the other hand, other stylistic\nfeatures, such as speech disfluencies, may enable more successful attribution\nbut, being specific to speech, require special purpose models. To better\nunderstand the challenges of this setting, we contribute the first systematic\nstudy of speaker attribution based solely on transcribed speech. Specifically,\nwe propose a new benchmark for speaker attribution focused on conversational\nspeech transcripts. To control for spurious associations of speakers with\ntopic, we employ both conversation prompts and speakers' participating in the\nsame conversation to construct challenging verification trials of varying\ndifficulties. We establish the state of the art on this new benchmark by\ncomparing a suite of neural and non-neural baselines, finding that although\nwritten text attribution models achieve surprisingly good performance in\ncertain settings, they struggle in the hardest settings we consider.\n","authors":["Cristina Aggazzotti","Nicholas Andrews","Elizabeth Allyn Smith"],"pdf_url":"https://arxiv.org/pdf/2311.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11816v2","updated":"2023-11-13T18:51:42Z","published":"2023-06-20T18:19:17Z","title":"Learning to Generate Better Than Your LLM","summary":"  Reinforcement learning (RL) has emerged as a powerful paradigm for\nfine-tuning Large Language Models (LLMs) for text generation. In particular,\nrecent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with\nusers after finetuning with RL. Capitalizing on key properties of text\ngeneration, we seek to investigate RL algorithms beyond general purpose\nalgorithms like Proximal Policy Optimization (PPO). In particular, we extend RL\nalgorithms to allow them to interact with a dynamic black-box guide LLM and\npropose RL with guided feedback (RLGF), a suite of RL algorithms for LLM\nfine-tuning. We provide two ways for the guide LLM to interact with the LLM to\nbe optimized for maximizing rewards. The guide LLM can generate text which\nserves as additional starting states for the RL optimization procedure. The\nguide LLM can also be used to complete the partial sentences generated by the\nLLM that is being optimized, treating the guide LLM as an expert to imitate and\nsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,\nand TL;DR summarization tasks. We show that our RL algorithms achieve higher\nperformance than supervised learning (SL) and the RL baseline PPO,\ndemonstrating the benefit of interaction with the guide LLM. On both CommonGen\nand TL;DR, we not only outperform our SL baselines but also improve upon PPO\nacross a variety of metrics beyond the one we optimized for. Our code can be\nfound at https://github.com/Cornell-RL/tril.\n","authors":["Jonathan D. Chang","Kiante Brantley","Rajkumar Ramamurthy","Dipendra Misra","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2306.11816v2.pdf","comment":"23 pages, 5 figures, 7 tables, 4 algorithms"},{"id":"http://arxiv.org/abs/2311.07556v1","updated":"2023-11-13T18:49:13Z","published":"2023-11-13T18:49:13Z","title":"Using Natural Language Explanations to Improve Robustness of In-context\n  Learning for Natural Language Inference","summary":"  Recent studies have demonstrated that large language models (LLMs) excel in\ndiverse tasks through in-context learning (ICL) facilitated by task-specific\nprompts and examples. However, the existing literature shows that ICL\nencounters performance deterioration when exposed to adversarial inputs.\nEnhanced performance has been observed when ICL is augmented with natural\nlanguage explanations (NLEs) (we refer to it as X-ICL). Thus, this work\ninvestigates whether X-ICL can improve the robustness of LLMs on a suite of\nseven adversarial and challenging natural language inference datasets.\nMoreover, we introduce a new approach to X-ICL by prompting an LLM (ChatGPT in\nour case) with few human-generated NLEs to produce further NLEs (we call it\nChatGPT few-shot), which we show superior to both ChatGPT zero-shot and\nhuman-generated NLEs alone. We evaluate five popular LLMs (GPT3.5-turbo,\nLLaMa2, Vicuna, Zephyr, Mistral) and show that X-ICL with ChatGPT few-shot\nyields over 6% improvement over ICL. Furthermore, while prompt selection\nstrategies were previously shown to significantly improve ICL on\nin-distribution test sets, we show that these strategies do not match the\nefficacy of the X-ICL paradigm in robustness-oriented evaluations.\n","authors":["Xuanli He","Yuxiang Wu","Oana-Maria Camburu","Pasquale Minervini","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2311.07556v1.pdf","comment":"pre-print"},{"id":"http://arxiv.org/abs/2305.15047v2","updated":"2023-11-13T18:43:11Z","published":"2023-05-24T11:37:10Z","title":"Ghostbuster: Detecting Text Ghostwritten by Large Language Models","summary":"  We introduce Ghostbuster, a state-of-the-art system for detecting\nAI-generated text. Our method works by passing documents through a series of\nweaker language models, running a structured search over possible combinations\nof their features, and then training a classifier on the selected features to\npredict whether documents are AI-generated. Crucially, Ghostbuster does not\nrequire access to token probabilities from the target model, making it useful\nfor detecting text generated by black-box models or unknown model versions. In\nconjunction with our model, we release three new datasets of human- and\nAI-generated text as detection benchmarks in the domains of student essays,\ncreative writing, and news articles. We compare Ghostbuster to a variety of\nexisting detectors, including DetectGPT and GPTZero, as well as a new RoBERTa\nbaseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is\n5.9 F1 higher than the best preexisting model. It also outperforms all previous\napproaches in generalization across writing domains (+7.5 F1), prompting\nstrategies (+2.1 F1), and language models (+4.4 F1). We also analyze the\nrobustness of our system to a variety of perturbations and paraphrasing attacks\nand evaluate its performance on documents written by non-native English\nspeakers.\n","authors":["Vivek Verma","Eve Fleisig","Nicholas Tomlin","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2305.15047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07547v1","updated":"2023-11-13T18:36:50Z","published":"2023-11-13T18:36:50Z","title":"GPT-4V(ision) as A Social Media Analysis Engine","summary":"  Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.\n","authors":["Hanjia Lyu","Jinfa Huang","Daoan Zhang","Yongsheng Yu","Xinyi Mou","Jinsheng Pan","Zhengyuan Yang","Zhongyu Wei","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2311.07547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07538v1","updated":"2023-11-13T18:28:25Z","published":"2023-11-13T18:28:25Z","title":"Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided\n  Classifiers","summary":"  Recent approaches have explored language-guided classifiers capable of\nclassifying examples from novel tasks when provided with task-specific natural\nlanguage explanations, instructions or prompts (Sanh et al., 2022; R. Menon et\nal., 2022). While these classifiers can generalize in zero-shot settings, their\ntask performance often varies substantially between different language\nexplanations in unpredictable ways (Lu et al., 2022; Gonen et al., 2022). Also,\ncurrent approaches fail to leverage unlabeled examples that may be available in\nmany scenarios. Here, we introduce TALC, a framework that uses data programming\nto adapt a language-guided classifier for a new task during inference when\nprovided with explanations from multiple teachers and unlabeled test examples.\nOur results show that TALC consistently outperforms a competitive baseline from\nprior work by an impressive 9.3% (relative improvement). Further, we\ndemonstrate the robustness of TALC to variations in the quality and quantity of\nprovided explanations, highlighting its potential in scenarios where learning\nfrom multiple teachers or a crowd is involved. Our code is available at:\nhttps://github.com/WeiKangda/TALC.git.\n","authors":["Kangda Wei","Sayan Ghosh","Rakesh R. Menon","Shashank Srivastava"],"pdf_url":"https://arxiv.org/pdf/2311.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13749v2","updated":"2023-11-13T18:27:21Z","published":"2023-05-23T07:05:50Z","title":"Goal-Driven Explainable Clustering via Language Descriptions","summary":"  Unsupervised clustering is widely used to explore large corpora, but existing\nformulations neither consider the users' goals nor explain clusters' meanings.\nWe propose a new task formulation, \"Goal-Driven Clustering with Explanations\"\n(GoalEx), which represents both the goal and the explanations as free-form\nlanguage descriptions. For example, to categorize the errors made by a\nsummarization system, the input to GoalEx is a corpus of annotator-written\ncomments for system-generated summaries and a goal description \"cluster the\ncomments based on why the annotators think the summary is imperfect.''; the\noutputs are text clusters each with an explanation (\"this cluster mentions that\nthe summary misses important context information.\"), which relates to the goal\nand precisely explain which comments should (not) belong to a cluster. To\ntackle GoalEx, we prompt a language model with \"[corpus subset] + [goal] +\nBrainstorm a list of explanations each representing a cluster.\"; then we\nclassify whether each sample belongs to a cluster based on its explanation;\nfinally, we use integer linear programming to select a subset of candidate\nclusters to cover most samples while minimizing overlaps. Under both automatic\nand human evaluation on corpora with or without labels, our method produces\nmore accurate and goal-related explanations than prior methods. We release our\ndata and implementation at https://github.com/ZihanWangKi/GoalEx.\n","authors":["Zihan Wang","Jingbo Shang","Ruiqi Zhong"],"pdf_url":"https://arxiv.org/pdf/2305.13749v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2211.13308v4","updated":"2023-11-13T18:25:27Z","published":"2022-11-23T21:25:39Z","title":"SciRepEval: A Multi-Format Benchmark for Scientific Document\n  Representations","summary":"  Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks without further fine-tuning. However, existing\nbenchmarks for evaluating these representations fail to capture the diversity\nof relevant tasks. In response, we introduce SciRepEval, the first\ncomprehensive benchmark for training and evaluating scientific document\nrepresentations. It includes 24 challenging and realistic tasks, 8 of which are\nnew, across four formats: classification, regression, ranking and search. We\nthen use this benchmark to study and improve the generalization ability of\nscientific document representation models. We show how state-of-the-art models\nlike SPECTER and SciNCL struggle to generalize across the task formats, and\nthat simple multi-task training fails to improve them. However, a new approach\nthat learns multiple embeddings per document, each tailored to a different\nformat, can improve performance. We experiment with task-format-specific\ncontrol codes and adapters and find they outperform the existing\nsingle-embedding state-of-the-art by over 2 points absolute. We release the\nresulting family of multi-format models, called SPECTER2, for the community to\nuse and build on.\n","authors":["Amanpreet Singh","Mike D'Arcy","Arman Cohan","Doug Downey","Sergey Feldman"],"pdf_url":"https://arxiv.org/pdf/2211.13308v4.pdf","comment":"19 pages, 2 figures, 11 tables. Accepted in EMNLP 2023 Main\n  Conference"},{"id":"http://arxiv.org/abs/2311.07536v1","updated":"2023-11-13T18:22:32Z","published":"2023-11-13T18:22:32Z","title":"A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual\n  Question Answering","summary":"  The emergence of multimodal large models (MLMs) has significantly advanced\nthe field of visual understanding, offering remarkable capabilities in the\nrealm of visual question answering (VQA). Yet, the true challenge lies in the\ndomain of knowledge-intensive VQA tasks, which necessitate not just recognition\nof visual elements, but also a deep comprehension of the visual information in\nconjunction with a vast repository of learned knowledge. To uncover such\ncapabilities of MLMs, particularly the newly introduced GPT-4V, we provide an\nin-depth evaluation from three perspectives: 1) Commonsense Knowledge, which\nassesses how well models can understand visual cues and connect to general\nknowledge; 2) Fine-grained World Knowledge, which tests the model's skill in\nreasoning out specific knowledge from images, showcasing their proficiency\nacross various specialized fields; 3) Comprehensive Knowledge with\nDecision-making Rationales, which examines model's capability to provide\nlogical explanations for its inference, facilitating a deeper analysis from the\ninterpretability perspective. Extensive experiments indicate that GPT-4V\nachieves SOTA performance on above three tasks. Interestingly, we find that: a)\nGPT-4V demonstrates enhanced reasoning and explanation when using composite\nimages as few-shot; b) GPT-4V produces severe hallucinations when dealing with\nworld knowledge, highlighting the future need for advancements in this research\ndirection.\n","authors":["Yunxin Li","Longyue Wang","Baotian Hu","Xinyu Chen","Wanqi Zhong","Chenyang Lyu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07536v1.pdf","comment":"18 pages, 13pages; working in progress"},{"id":"http://arxiv.org/abs/2311.01012v2","updated":"2023-11-13T18:19:44Z","published":"2023-11-02T06:14:41Z","title":"COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances","summary":"  We present publicly available COPAL-ID, a novel Indonesian language common\nsense reasoning dataset. Unlike the previous Indonesian COPA dataset\n(XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and\ntherefore, provides a more natural portrayal of day-to-day causal reasoning\nwithin the Indonesian cultural sphere. Professionally written by natives from\nscratch, COPAL-ID is more fluent and free from awkward phrases, unlike the\ntranslated XCOPA-ID. In addition, we present COPAL-ID in both standard\nIndonesian and in Jakartan Indonesian--a dialect commonly used in daily\nconversation. COPAL-ID poses a greater challenge for existing open-sourced and\nclosed state-of-the-art multilingual language models, yet is trivially easy for\nhumans. Our findings suggest that even the current best open-source,\nmultilingual model struggles to perform well, achieving 65.47% accuracy on\nCOPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%).\nDespite GPT-4's impressive score, it suffers the same performance degradation\ncompared to its XCOPA-ID score, and it still falls short of human performance.\nThis shows that these language models are still way behind in comprehending the\nlocal nuances of Indonesian.\n","authors":["Haryo Akbarianto Wibowo","Erland Hilman Fuadi","Made Nindyatama Nityasya","Radityo Eko Prasojo","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2311.01012v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.07532v1","updated":"2023-11-13T18:18:22Z","published":"2023-11-13T18:18:22Z","title":"It's Not Easy Being Wrong: Evaluating Process of Elimination Reasoning\n  in Large Language Models","summary":"  Chain-of-thought (COT) prompting can help large language models (LLMs) reason\ntoward correct answers, but its efficacy in reasoning toward incorrect answers\nis unexplored. This strategy of process of elimination (PoE), when used with\nCOT, has the potential to enhance interpretability in tasks like medical\ndiagnoses of exclusion. Thus, we propose PoE with COT, a new task where LLMs\nmust reason toward incorrect options on multiple-choice questions. We evaluate\nthe ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on 2-choice\ncommonsense and scientific reasoning datasets. We show that PoE consistently\nunderperforms directly choosing the correct answer. The agreement of these\nstrategies is also lower than the self-consistency of each strategy. To study\nthese issues further, we conduct an error analysis and give suggestions for\nfuture work.\n","authors":["Nishant Balepur","Shramay Palta","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2311.07532v1.pdf","comment":"In progress preprint"},{"id":"http://arxiv.org/abs/2302.13439v2","updated":"2023-11-13T18:10:16Z","published":"2023-02-26T23:46:29Z","title":"Navigating the Grey Area: How Expressions of Uncertainty and\n  Overconfidence Affect Language Models","summary":"  The increased deployment of LMs for real-world tasks involving knowledge and\nfacts makes it important to understand model epistemology: what LMs think they\nknow, and how their attitudes toward that knowledge are affected by language\nuse in their inputs. Here, we study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or evidentiality like \"I'm sure\nit's\", \"I think it's\", or \"Wikipedia says it's\" affect models, and whether they\ncontribute to model failures. We develop a typology of epistemic markers and\ninject 50 markers into prompts for question answering. We find that LMs are\nhighly sensitive to epistemic markers in prompts, with accuracies varying more\nthan 80%. Surprisingly, we find that expressions of high certainty result in a\n7% decrease in accuracy as compared to low certainty expressions; similarly,\nfactive verbs hurt performance, while evidentials benefit performance. Our\nanalysis of a popular pretraining dataset shows that these markers of\nuncertainty are associated with answers on question-answering websites, while\nmarkers of certainty are associated with questions. These associations may\nsuggest that the behavior of LMs is based on mimicking observed language use,\nrather than truly reflecting epistemic uncertainty.\n","authors":["Kaitlyn Zhou","Dan Jurafsky","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2302.13439v2.pdf","comment":"EMNLP 2023 (Oral)"},{"id":"http://arxiv.org/abs/2302.08624v6","updated":"2023-11-13T17:56:19Z","published":"2023-02-16T23:29:22Z","title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis","summary":"  We introduce InstructABSA, an instruction learning paradigm for Aspect-Based\nSentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,\nand neutral examples to each training sample, and instruction tune the model\n(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.\nExperimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that\nInstructABSA outperforms the previous state-of-the-art (SOTA) approaches on\nTerm Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair\nExtraction (ASPE) subtasks. In particular, InstructABSA outperforms the\nprevious state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the\nRest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%\npoints, surpassing 7x larger models. We also get competitive results on AOOE,\nAOPE, and AOSTE subtasks indicating strong generalization ability to all\nsubtasks. Exploring sample efficiency reveals that just 50% train data is\nrequired to get competitive results with other instruction tuning approaches.\nLastly, we assess the quality of instructions and observe that InstructABSA's\nperformance experiences a decline of ~10% when adding misleading examples.\n","authors":["Kevin Scaria","Himanshu Gupta","Siddharth Goyal","Saurabh Arjun Sawant","Swaroop Mishra","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2302.08624v6.pdf","comment":"4 pages, 3 figures, 9 tables, 9 appendix pages"},{"id":"http://arxiv.org/abs/2311.07509v1","updated":"2023-11-13T17:54:50Z","published":"2023-11-13T17:54:50Z","title":"A Benchmark to Understand the Role of Knowledge Graphs on Large Language\n  Model's Accuracy for Question Answering on Enterprise SQL Databases","summary":"  Enterprise applications of Large Language Models (LLMs) hold promise for\nquestion answering on enterprise SQL databases. However, the extent to which\nLLMs can accurately respond to enterprise questions in such databases remains\nunclear, given the absence of suitable Text-to-SQL benchmarks tailored to\nenterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to\nenhance LLM-based question answering by providing business context is not well\nunderstood. This study aims to evaluate the accuracy of LLM-powered question\nanswering systems in the context of enterprise questions and SQL databases,\nwhile also exploring the role of knowledge graphs in improving accuracy. To\nachieve this, we introduce a benchmark comprising an enterprise SQL schema in\nthe insurance domain, a range of enterprise queries encompassing reporting to\nmetrics, and a contextual layer incorporating an ontology and mappings that\ndefine a knowledge graph. Our primary finding reveals that question answering\nusing GPT-4, with zero-shot prompts directly on SQL databases, achieves an\naccuracy of 16%. Notably, this accuracy increases to 54% when questions are\nposed over a Knowledge Graph representation of the enterprise SQL database.\nTherefore, investing in Knowledge Graph provides higher accuracy for LLM\npowered question answering systems.\n","authors":["Juan Sequeda","Dean Allemang","Bryon Jacob"],"pdf_url":"https://arxiv.org/pdf/2311.07509v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2304.03738v3","updated":"2023-11-13T17:50:22Z","published":"2023-04-07T17:14:00Z","title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language\n  Models","summary":"  As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.\n","authors":["Emilio Ferrara"],"pdf_url":"https://arxiv.org/pdf/2304.03738v3.pdf","comment":"Published on First Monday\n  https://firstmonday.org/ojs/index.php/fm/article/view/13346/11365"},{"id":"http://arxiv.org/abs/2311.07497v1","updated":"2023-11-13T17:36:58Z","published":"2023-11-13T17:36:58Z","title":"Multilingual Nonce Dependency Treebanks: Understanding how LLMs\n  represent and process syntactic structure","summary":"  We introduce SPUD (Semantically Perturbed Universal Dependencies), a\nframework for creating nonce treebanks for the multilingual Universal\nDependencies (UD) corpora. SPUD data satisfies syntactic argument structure,\nprovides syntactic annotations, and ensures grammaticality via\nlanguage-specific rules. We create nonce data in Arabic, English, French,\nGerman, and Russian, and demonstrate two use cases of SPUD treebanks. First, we\ninvestigate the effect of nonce data on word co-occurrence statistics, as\nmeasured by perplexity scores of autoregressive (ALM) and masked language\nmodels (MLM). We find that ALM scores are significantly more affected by nonce\ndata than MLM scores. Second, we show how nonce data affects the performance of\nsyntactic dependency probes. We replicate the findings of M\\\"uller-Eberstein et\nal. (2022) on nonce test data and show that the performance declines on both\nMLMs and ALMs wrt. original test data. However, a majority of the performance\nis kept, suggesting that the probe indeed learns syntax independently from\nsemantics.\n","authors":["David Arps","Laura Kallmeyer","Younes Samih","Hassan Sajjad"],"pdf_url":"https://arxiv.org/pdf/2311.07497v1.pdf","comment":"Our software is available at https://github.com/davidarps/spud"},{"id":"http://arxiv.org/abs/2305.19915v4","updated":"2023-11-13T17:34:53Z","published":"2023-05-31T14:47:44Z","title":"Source Code Data Augmentation for Deep Learning: A Survey","summary":"  The increasingly popular adoption of deep learning models in many critical\nsource code tasks motivates the development of data augmentation (DA)\ntechniques to enhance training data and improve various capabilities (e.g.,\nrobustness and generalizability) of these models. Although a series of DA\nmethods have been proposed and tailored for source code models, there lacks a\ncomprehensive survey and examination to understand their effectiveness and\nimplications. This paper fills this gap by conducting a comprehensive and\nintegrative survey of data augmentation for source code, wherein we\nsystematically compile and encapsulate existing literature to provide a\ncomprehensive overview of the field. We start with an introduction of data\naugmentation in source code and then provide a discussion on major\nrepresentative approaches. Next, we highlight the general strategies and\ntechniques to optimize the DA quality. Subsequently, we underscore techniques\nuseful in real-world source code scenarios and downstream tasks. Finally, we\noutline the prevailing challenges and potential opportunities for future\nresearch. In essence, we aim to demystify the corpus of existing literature on\nsource code DA for deep learning, and foster further exploration in this\nsphere. Complementing this, we present a continually updated GitHub repository\nthat hosts a list of update-to-date papers on DA for source code modeling,\naccessible at \\url{https://github.com/terryyz/DataAug4Code}.\n","authors":["Terry Yue Zhuo","Zhou Yang","Zhensu Sun","Yufei Wang","Li Li","Xiaoning Du","Zhenchang Xing","David Lo"],"pdf_url":"https://arxiv.org/pdf/2305.19915v4.pdf","comment":"ongoing work; 89 publications"},{"id":"http://arxiv.org/abs/2311.07491v1","updated":"2023-11-13T17:28:03Z","published":"2023-11-13T17:28:03Z","title":"A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question\n  Decomposition with Large Language Models","summary":"  While large language models exhibit remarkable performance in the Question\nAnswering task, they are susceptible to hallucinations. Challenges arise when\nthese models grapple with understanding multi-hop relations in complex\nquestions or lack the necessary knowledge for a comprehensive response. To\naddress this issue, we introduce the \"Decompose-and-Query\" framework (D&Q).\nThis framework guides the model to think and utilize external knowledge similar\nto ReAct, while also restricting its thinking to reliable information,\neffectively mitigating the risk of hallucinations. Experiments confirm the\neffectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT\nin 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1\nscore of 59.6%. Our code is available at\nhttps://github.com/alkaidpku/DQ-ToolQA.\n","authors":["Hejing Cao","Zhenwei An","Jiazhan Feng","Kun Xu","Liwei Chen","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.07491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07484v1","updated":"2023-11-13T17:19:14Z","published":"2023-11-13T17:19:14Z","title":"Psychometric Predictive Power of Large Language Models","summary":"  Next-word probabilities from language models have been shown to successfully\nsimulate human reading behavior. Building on this, we show that, interestingly,\ninstruction-tuned large language models (LLMs) yield worse psychometric\npredictive power (PPP) for human reading behavior than base LLMs with\nequivalent perplexities. In other words, instruction tuning, which helps LLMs\nprovide human-preferred responses, does not always make them human-like from\nthe computational psycholinguistics perspective. In addition, we explore\nprompting methodologies in simulating human reading behavior with LLMs, showing\nthat prompts reflecting a particular linguistic hypothesis lead LLMs to exhibit\nbetter PPP but are still worse than base LLMs. These highlight that recent\ninstruction tuning and prompting do not offer better estimates than direct\nprobability measurements from base LLMs in cognitive modeling.\n","authors":["Tatsuki Kuribayashi","Yohei Oseki","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2311.07484v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.07470v1","updated":"2023-11-13T17:03:02Z","published":"2023-11-13T17:03:02Z","title":"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer","summary":"  Multi-modal large language models (LLM) have achieved powerful capabilities\nfor visual semantic understanding in recent years. However, little is known\nabout how LLMs comprehend visual information and interpret different modalities\nof features. In this paper, we propose a new method for identifying multi-modal\nneurons in transformer-based multi-modal LLMs. Through a series of experiments,\nWe highlight three critical properties of multi-modal neurons by four\nwell-designed quantitative evaluation metrics. Furthermore, we introduce a\nknowledge editing method based on the identified multi-modal neurons, for\nmodifying a specific token to another designative token. We hope our findings\ncan inspire further explanatory researches on understanding mechanisms of\nmulti-modal LLMs.\n","authors":["Haowen Pan","Yixin Cao","Xiaozhi Wang","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2311.07470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07469v1","updated":"2023-11-13T17:02:06Z","published":"2023-11-13T17:02:06Z","title":"InCA: Rethinking In-Car Conversational System Assessment Leveraging\n  Large Language Models","summary":"  The assessment of advanced generative large language models (LLMs) poses a\nsignificant challenge, given their heightened complexity in recent\ndevelopments. Furthermore, evaluating the performance of LLM-based applications\nin various industries, as indicated by Key Performance Indicators (KPIs), is a\ncomplex undertaking. This task necessitates a profound understanding of\nindustry use cases and the anticipated system behavior. Within the context of\nthe automotive industry, existing evaluation metrics prove inadequate for\nassessing in-car conversational question answering (ConvQA) systems. The unique\ndemands of these systems, where answers may relate to driver or car safety and\nare confined within the car domain, highlight the limitations of current\nmetrics. To address these challenges, this paper introduces a set of KPIs\ntailored for evaluating the performance of in-car ConvQA systems, along with\ndatasets specifically designed for these KPIs. A preliminary and comprehensive\nempirical evaluation substantiates the efficacy of our proposed approach.\nFurthermore, we investigate the impact of employing varied personas in prompts\nand found that it enhances the model's capacity to simulate diverse viewpoints\nin assessments, mirroring how individuals with different backgrounds perceive a\ntopic.\n","authors":["Ken E. Friedl","Abbas Goher Khan","Soumya Ranjan Sahoo","Md Rashad Al Hasan Rony","Jana Germies","Christian Süß"],"pdf_url":"https://arxiv.org/pdf/2311.07469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07468v1","updated":"2023-11-13T17:01:12Z","published":"2023-11-13T17:01:12Z","title":"Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation\n  of the Reversal Curse","summary":"  Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B. However, it may encounter\nconfusion when presented with questions concerning B. We contend that the\nreversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.\n","authors":["Ang Lv","Kaiyi Zhang","Shufang Xie","Quan Tu","Yuhan Chen","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2311.07468v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.06237v2","updated":"2023-11-13T17:00:40Z","published":"2023-11-10T18:52:58Z","title":"Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the\n  Wild","summary":"  Engaging in the deliberate generation of abnormal outputs from large language\nmodels (LLMs) by attacking them is a novel human activity. This paper presents\na thorough exposition of how and why people perform such attacks. Using a\nformal qualitative methodology, we interviewed dozens of practitioners from a\nbroad range of backgrounds, all contributors to this novel work of attempting\nto cause LLMs to fail. We relate and connect this activity between its\npractitioners' motivations and goals; the strategies and techniques they\ndeploy; and the crucial role the community plays. As a result, this paper\npresents a grounded theory of how and why people attack large language models:\nLLM red teaming in the wild.\n","authors":["Nanna Inie","Jonathan Stray","Leon Derczynski"],"pdf_url":"https://arxiv.org/pdf/2311.06237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07466v1","updated":"2023-11-13T16:53:51Z","published":"2023-11-13T16:53:51Z","title":"On Measuring Faithfulness of Natural Language Explanations","summary":"  Large language models (LLMs) can explain their own predictions, through\npost-hoc or Chain-of-Thought (CoT) explanations. However the LLM could make up\nreasonably sounding explanations that are unfaithful to its underlying\nreasoning. Recent work has designed tests that aim to judge the faithfulness of\neither post-hoc or CoT explanations. In this paper we argue that existing\nfaithfulness tests are not actually measuring faithfulness in terms of the\nmodels' inner workings, but only evaluate their self-consistency on the output\nlevel. The aims of our work are two-fold. i) We aim to clarify the status of\nexisting faithfulness tests in terms of model explainability, characterising\nthem as self-consistency tests instead. This assessment we underline by\nconstructing a Comparative Consistency Bank for self-consistency tests that for\nthe first time compares existing tests on a common suite of 11 open-source LLMs\nand 5 datasets -- including ii) our own proposed self-consistency measure\nCC-SHAP. CC-SHAP is a new fine-grained measure (not test) of LLM\nself-consistency that compares a model's input contributions to answer\nprediction and generated explanation. With CC-SHAP, we aim to take a step\nfurther towards measuring faithfulness with a more interpretable and\nfine-grained method. Code available at\n\\url{https://github.com/Heidelberg-NLP/CC-SHAP}\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2311.07466v1.pdf","comment":"10 main paper pages, 17 appendix pages"},{"id":"http://arxiv.org/abs/2311.07463v1","updated":"2023-11-13T16:45:37Z","published":"2023-11-13T16:45:37Z","title":"MEGAVERSE: Benchmarking Large Language Models Across Languages,\n  Modalities, Models and Tasks","summary":"  Recently, there has been a rapid advancement in research on Large Language\nModels (LLMs), resulting in significant progress in several Natural Language\nProcessing (NLP) tasks. Consequently, there has been a surge in LLM evaluation\nresearch to comprehend the models' capabilities and limitations. However, much\nof this research has been confined to the English language, leaving LLM\nbuilding and evaluation for non-English languages relatively unexplored. There\nhas been an introduction of several new LLMs, necessitating their evaluation on\nnon-English languages. This study aims to expand our MEGA benchmarking suite by\nincluding six new datasets to form the MEGAVERSE benchmark. The benchmark\ncomprises 22 datasets covering 81 languages, including low-resource African\nlanguages. We evaluate several state-of-the-art LLMs like GPT-3.5-Turbo, GPT4,\nPaLM2, and Llama2 on the MEGAVERSE datasets. Additionally, we include two\nmultimodal datasets in the benchmark and assess the performance of the\nLLaVa-v1.5 model. Our experiments suggest that GPT4 and PaLM2 outperform the\nLlama models on various tasks, notably on low-resource languages, with GPT4\noutperforming PaLM2 on more datasets than vice versa. However, issues such as\ndata contamination must be addressed to obtain an accurate assessment of LLM\nperformance on non-English languages.\n","authors":["Sanchit Ahuja","Divyanshu Aggarwal","Varun Gumma","Ishaan Watts","Ashutosh Sathe","Millicent Ochieng","Rishav Hada","Prachi Jain","Maxamed Axmed","Kalika Bali","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2311.07463v1.pdf","comment":"23 pages, 30 figures and 1 table"},{"id":"http://arxiv.org/abs/2203.03691v3","updated":"2023-11-13T16:39:55Z","published":"2022-03-07T20:23:46Z","title":"HyperMixer: An MLP-based Low Cost Alternative to Transformers","summary":"  Transformer-based architectures are the model of choice for natural language\nunderstanding, but they come at a significant cost, as they have quadratic\ncomplexity in the input length, require a lot of training data, and can be\ndifficult to tune. In the pursuit of lower costs, we investigate simple\nMLP-based architectures. We find that existing architectures such as MLPMixer,\nwhich achieves token mixing through a static MLP applied to each feature\nindependently, are too detached from the inductive biases required for natural\nlanguage understanding. In this paper, we propose a simple variant, HyperMixer,\nwhich forms the token mixing MLP dynamically using hypernetworks. Empirically,\nwe demonstrate that our model performs better than alternative MLP-based\nmodels, and on par with Transformers. In contrast to Transformers, HyperMixer\nachieves these results at substantially lower costs in terms of processing\ntime, training data, and hyperparameter tuning.\n","authors":["Florian Mai","Arnaud Pannatier","Fabio Fehr","Haolin Chen","Francois Marelli","Francois Fleuret","James Henderson"],"pdf_url":"https://arxiv.org/pdf/2203.03691v3.pdf","comment":"Published at ACL 2023"},{"id":"http://arxiv.org/abs/2311.07453v1","updated":"2023-11-13T16:35:29Z","published":"2023-11-13T16:35:29Z","title":"ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World\n  Chart Images","summary":"  Data visualizations are common in the real-world. We often use them in data\nsources such as scientific documents, news articles, textbooks, and social\nmedia to summarize key information in a visual form. Charts can also mislead\nits audience by communicating false information or biasing them towards a\nspecific agenda. Verifying claims against charts is not a straightforward\nprocess. It requires analyzing both the text and visual components of the\nchart, considering characteristics such as colors, positions, and orientations.\nMoreover, to determine if a claim is supported by the chart content often\nrequires different types of reasoning. To address this challenge, we introduce\nChartCheck, a novel dataset for fact-checking against chart images. ChartCheck\nis the first large-scale dataset with 1.7k real-world charts and 10.5k\nhuman-written claims and explanations. We evaluated the dataset on\nstate-of-the-art models and achieved an accuracy of 73.9 in the finetuned\nsetting. Additionally, we identified chart characteristics and reasoning types\nthat challenge the models.\n","authors":["Mubashara Akhtar","Nikesh Subedi","Vivek Gupta","Sahar Tahmasebi","Oana Cocarascu","Elena Simperl"],"pdf_url":"https://arxiv.org/pdf/2311.07453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07445v1","updated":"2023-11-13T16:19:42Z","published":"2023-11-13T16:19:42Z","title":"Think Before You Speak: Cultivating Communication Skills of Large\n  Language Models via Inner Monologue","summary":"  The emergence of large language models (LLMs) further improves the\ncapabilities of open-domain dialogue systems and can generate fluent, coherent,\nand diverse responses. However, LLMs still lack an important ability:\ncommunication skills, which makes them more like information seeking tools than\nanthropomorphic chatbots. To make LLMs more anthropomorphic and proactive\nduring the conversation, we add five communication skills to the response\ngeneration process: topic transition, proactively asking questions, concept\nguidance, empathy, and summarising often. The addition of communication skills\nincreases the interest of users in the conversation and attracts them to chat\nfor longer. To enable LLMs better understand and use communication skills, we\ndesign and add the inner monologue to LLMs. The complete process is achieved\nthrough prompt engineering and in-context learning. To evaluate communication\nskills, we construct a benchmark named Cskills for evaluating various\ncommunication skills, which can also more comprehensively evaluate the dialogue\ngeneration ability of the model. Experimental results show that the proposed\nCSIM strategy improves the backbone models and outperforms the baselines in\nboth automatic and human evaluations.\n","authors":["Junkai Zhou","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2311.07445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07439v1","updated":"2023-11-13T16:15:20Z","published":"2023-11-13T16:15:20Z","title":"Investigating Multi-Pivot Ensembling with Massively Multilingual Machine\n  Translation Models","summary":"  Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. As an alternative, we propose MaxEns, a\ncombination strategy that is biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n","authors":["Alireza Mohammadshahi","Jannis Vamvas","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2311.07439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19680v2","updated":"2023-11-13T16:12:02Z","published":"2023-10-30T16:00:13Z","title":"Integrating Pre-trained Language Model into Neural Machine Translation","summary":"  Neural Machine Translation (NMT) has become a significant technology in\nnatural language processing through extensive research and development.\nHowever, the deficiency of high-quality bilingual language pair data still\nposes a major challenge to improving NMT performance. Recent studies are\nexploring the use of contextual information from pre-trained language model\n(PLM) to address this problem. Yet, the issue of incompatibility between PLM\nand NMT model remains unresolved. This study proposes a PLM-integrated NMT\n(PiNMT) model to overcome the identified problems. The PiNMT model consists of\nthree critical components, PLM Multi Layer Converter, Embedding Fusion, and\nCosine Alignment, each playing a vital role in providing effective PLM\ninformation to NMT. Furthermore, two training strategies, Separate Learning\nRates and Dual Step Training, are also introduced in this paper. By\nimplementing the proposed PiNMT model and training strategy, we achieved\nstate-of-the-art performance on the IWSLT'14 En$\\leftrightarrow$De dataset.\nThis study's outcomes are noteworthy as they demonstrate a novel approach for\nefficiently integrating PLM with NMT to overcome incompatibility and enhance\nperformance.\n","authors":["Soon-Jae Hwang","Chang-Sung Jeong"],"pdf_url":"https://arxiv.org/pdf/2310.19680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07430v1","updated":"2023-11-13T16:03:23Z","published":"2023-11-13T16:03:23Z","title":"Controlled Text Generation for Black-box Language Models via Score-based\n  Progressive Editor","summary":"  Despite recent progress in language models, generating constrained text for\nspecific domains remains a challenge, particularly when utilizing black-box\nmodels that lack domain-specific knowledge. In this paper, we introduce ScoPE\n(Score-based Progressive Editor) generation, a novel approach for controlled\ntext generation for black-box language models. We employ ScoPE to facilitate\ntext generation in the target domain by integrating it with language models\nthrough a cascading approach. Trained to enhance the target domain score of the\nedited text, ScoPE progressively edits intermediate output discrete tokens to\nalign with the target attributes throughout the auto-regressive generation\nprocess of the language model. This iterative process guides subsequent steps\nto produce desired output texts for the target domain. Our experimental results\non diverse controlled generations demonstrate that ScoPE effectively\nfacilitates controlled text generation for black-box language models in both\nin-domain and out-of-domain conditions, which is challenging for existing\nmethods.\n","authors":["Sangwon Yu","Changmin Lee","Hojin Lee","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2311.07430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02775v2","updated":"2023-11-13T16:03:15Z","published":"2023-11-05T21:43:02Z","title":"ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using\n  Open-Source LLMs","summary":"  Responding to the thousands of student questions on online QA platforms each\nsemester has a considerable human cost, particularly in computing courses with\nrapidly growing enrollments. To address the challenges of scalable and\nintelligent question-answering (QA), we introduce an innovative solution that\nleverages open-source Large Language Models (LLMs) from the LLaMA-2 family to\nensure data privacy. Our approach combines augmentation techniques such as\nretrieval augmented generation (RAG), supervised fine-tuning (SFT), and\nlearning from human preferences data using Direct Preference Optimization\n(DPO). Through extensive experimentation on a Piazza dataset from an\nintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs of\npreference data, we demonstrate a significant 30% improvement in the quality of\nanswers, with RAG being a particularly impactful addition. Our contributions\ninclude the development of a novel architecture for educational QA, extensive\nevaluations of LLM performance utilizing both human assessments and LLM-based\nmetrics, and insights into the challenges and future directions of educational\ndata processing. This work paves the way for the development of CHATA, an\nintelligent QA assistant customizable for courses with an online QA platform\n","authors":["Yann Hicke","Anmol Agarwal","Qianou Ma","Paul Denny"],"pdf_url":"https://arxiv.org/pdf/2311.02775v2.pdf","comment":"Updates for camera-ready submission"},{"id":"http://arxiv.org/abs/2311.07424v1","updated":"2023-11-13T15:58:18Z","published":"2023-11-13T15:58:18Z","title":"Hallucination Augmented Recitations for Language Models","summary":"  Attribution is a key concept in large language models (LLMs) as it enables\ncontrol over information sources and enhances the factuality of LLMs. While\nexisting approaches utilize open book question answering to improve\nattribution, factual datasets may reward language models to recall facts that\nthey already know from their pretraining data, not attribution. In contrast,\ncounterfactual open book QA datasets would further improve attribution because\nthe answer could only be grounded in the given text. We propose Hallucination\nAugmented Recitations (HAR) for creating counterfactual datasets by utilizing\nhallucination in LLMs to improve attribution. For open book QA as a case study,\nwe demonstrate that models finetuned with our counterfactual datasets improve\ntext grounding, leading to better open book QA performance, with up to an 8.0%\nincrease in F1 score. Our counterfactual dataset leads to significantly better\nperformance than using humanannotated factual datasets, even with 4x smaller\ndatasets and 4x smaller models. We observe that improvements are consistent\nacross various model sizes and datasets, including multi-hop, biomedical, and\nadversarial QA datasets.\n","authors":["Abdullatif Köksal","Renat Aksitov","Chung-Ching Chang"],"pdf_url":"https://arxiv.org/pdf/2311.07424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07418v1","updated":"2023-11-13T15:54:30Z","published":"2023-11-13T15:54:30Z","title":"Speech-based Slot Filling using Large Language Models","summary":"  Recently, advancements in large language models (LLMs) have shown an\nunprecedented ability across various language tasks. This paper investigates\nthe potential application of LLMs to slot filling with noisy ASR\ntranscriptions, via both in-context learning and task-specific fine-tuning.\nDedicated prompt designs and fine-tuning approaches are proposed to improve the\nrobustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a\nlinearised knowledge injection (LKI) scheme is also proposed to integrate\ndynamic external knowledge into LLMs. Experiments were performed on SLURP to\nquantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and\nVicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the\nproposed fine-tuning together with the LKI scheme for LLaMA-13B achieved an\n8.3% absolute SLU-F1 improvement compared to the strong Flan-T5-base baseline\nsystem on a limited data setup.\n","authors":["Guangzhi Sun","Shutong Feng","Dongcheng Jiang","Chao Zhang","Milica Gašić","Philip C. Woodland"],"pdf_url":"https://arxiv.org/pdf/2311.07418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01544v2","updated":"2023-11-13T15:33:35Z","published":"2023-11-02T18:55:53Z","title":"Divergent Token Metrics: Measuring degradation to prune away LLM\n  components -- and optimize quantization","summary":"  Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. Their ever-increasing size, however, raised\nconcerns about their effective deployment and the need for LLM compressions.\nThis study introduces the Divergent Token metrics (DTMs), a novel approach for\nassessing compressed LLMs, addressing the limitations of traditional perplexity\nor accuracy measures that fail to accurately reflect text generation quality.\nDTMs focus on token divergence, that allow deeper insights into the subtleties\nof model compression, i.p. when evaluating component's impacts individually.\nUtilizing the First Divergent Token metric (FDTM) in model sparsification\nreveals that a quarter of all attention components can be pruned beyond 90% on\nthe Llama-2 model family, still keeping SOTA performance. For quantization FDTM\nsuggests that over 80% of parameters can naively be transformed to int8 without\nspecial outlier management. These evaluations indicate the necessity of\nchoosing appropriate compressions for parameters individually-and that FDTM can\nidentify those-while standard metrics result in deteriorated outcomes.\n","authors":["Björn Deiseroth","Max Meuer","Nikolas Gritsch","Constantin Eichenberg","Patrick Schramowski","Matthias Aßenmacher","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2311.01544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07397v1","updated":"2023-11-13T15:25:42Z","published":"2023-11-13T15:25:42Z","title":"An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination\n  Evaluation","summary":"  Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucination, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of hallucination and task). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including object\nexistence, object attribute and object relation hallucination. Based on AMBER,\nwe design a low-cost and efficient evaluation pipeline. Additionally, we\nconduct a comprehensive evaluation and detailed analysis of mainstream MLLMs\nincluding GPT-4V(ision), and also give guideline suggestions for mitigating\nhallucinations. The data and code of AMBER are available at\nhttps://github.com/junyangwang0410/AMBER.\n","authors":["Junyang Wang","Yuhang Wang","Guohai Xu","Jing Zhang","Yukai Gu","Haitao Jia","Ming Yan","Ji Zhang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2311.07397v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.16607v2","updated":"2023-11-13T15:20:43Z","published":"2023-10-25T12:59:51Z","title":"On the Interplay between Fairness and Explainability","summary":"  In order to build reliable and trustworthy NLP applications, models need to\nbe both fair across different demographics and explainable. Usually these two\nobjectives, fairness and explainability, are optimized and/or examined\nindependently of each other. Instead, we argue that forthcoming, trustworthy\nNLP systems should consider both. In this work, we perform a first study to\nunderstand how they influence each other: do fair(er) models rely on more\nplausible rationales? and vice versa. To this end, we conduct experiments on\ntwo English multi-class text classification datasets, BIOS and ECtHR, that\nprovide information on gender and nationality, respectively, as well as\nhuman-annotated rationales. We fine-tune pre-trained language models with\nseveral methods for (i) bias mitigation, which aims to improve fairness; (ii)\nrationale extraction, which aims to produce plausible explanations. We find\nthat bias mitigation algorithms do not always lead to fairer models. Moreover,\nwe discover that empirical fairness and explainability are orthogonal.\n","authors":["Stephanie Brandl","Emanuele Bugliarello","Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2310.16607v2.pdf","comment":"15 pages (incl Appendix), 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2311.07387v1","updated":"2023-11-13T15:11:26Z","published":"2023-11-13T15:11:26Z","title":"Assessing Logical Puzzle Solving in Large Language Models: Insights from\n  a Minesweeper Case Study","summary":"  Large Language Models (LLMs) have shown remarkable proficiency in language\nunderstanding and have been successfully applied to a variety of real-world\ntasks through task-specific fine-tuning or prompt engineering. Despite these\nadvancements, it remains an open question whether LLMs are fundamentally\ncapable of reasoning and planning, or if they primarily rely on recalling and\nsynthesizing information from their training data. In our research, we\nintroduce a novel task -- Minesweeper -- specifically designed in a format\nunfamiliar to LLMs and absent from their training datasets. This task\nchallenges LLMs to identify the locations of mines based on numerical clues\nprovided by adjacent opened cells. Successfully completing this task requires\nan understanding of each cell's state, discerning spatial relationships between\nthe clues and mines, and strategizing actions based on logical deductions drawn\nfrom the arrangement of the cells. Our experiments, including trials with the\nadvanced GPT-4 model, indicate that while LLMs possess the foundational\nabilities required for this task, they struggle to integrate these into a\ncoherent, multi-step logical reasoning process needed to solve Minesweeper.\nThese findings highlight the need for further research to understand and nature\nof reasoning capabilities in LLMs under similar circumstances, and to explore\npathways towards more sophisticated AI reasoning and planning models.\n","authors":["Yinghao Li","Haorui Wang","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07387v1.pdf","comment":"24 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.19056v2","updated":"2023-11-13T15:09:01Z","published":"2023-10-29T16:04:10Z","title":"MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion","summary":"  Query expansion is a commonly-used technique in many search systems to better\nrepresent users' information needs with additional query terms. Existing\nstudies for this task usually propose to expand a query with retrieved or\ngenerated contextual documents. However, both types of methods have clear\nlimitations. For retrieval-based methods, the documents retrieved with the\noriginal query might not be accurate enough to reveal the search intent,\nespecially when the query is brief or ambiguous. For generation-based methods,\nexisting models can hardly be trained or aligned on a particular corpus, due to\nthe lack of corpus-specific labeled data. In this paper, we propose a novel\nLarge Language Model (LLM) based mutual verification framework for query\nexpansion, which alleviates the aforementioned limitations. Specifically, we\nfirst design a query-query-document generation pipeline, which can effectively\nleverage the contextual knowledge encoded in LLMs to generate sub-queries and\ncorresponding documents from multiple perspectives. Next, we employ a mutual\nverification method for both generated and retrieved contextual documents,\nwhere 1) retrieved documents are filtered with the external contextual\nknowledge in generated documents, and 2) generated documents are filtered with\nthe corpus-specific knowledge in retrieved documents. Overall, the proposed\nmethod allows retrieved and generated documents to complement each other to\nfinalize a better query expansion. We conduct extensive experiments on three\ninformation retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.\nThe results demonstrate that our method outperforms other baselines\nsignificantly.\n","authors":["Pengyue Jia","Yiding Liu","Xiangyu Zhao","Xiaopeng Li","Changying Hao","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.19056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07383v1","updated":"2023-11-13T15:08:59Z","published":"2023-11-13T15:08:59Z","title":"LM-Polygraph: Uncertainty Estimation for Language Models","summary":"  Recent advancements in the capabilities of large language models (LLMs) have\npaved the way for a myriad of groundbreaking applications in various fields.\nHowever, a significant challenge arises as these models often \"hallucinate\",\ni.e., fabricate facts without providing users an apparent means to discern the\nveracity of their statements. Uncertainty estimation (UE) methods are one path\nto safer, more responsible, and more effective use of LLMs. However, to date,\nresearch on UE methods for LLMs has been focused primarily on theoretical\nrather than engineering contributions. In this work, we tackle this issue by\nintroducing LM-Polygraph, a framework with implementations of a battery of\nstate-of-the-art UE methods for LLMs in text generation tasks, with unified\nprogram interfaces in Python. Additionally, it introduces an extendable\nbenchmark for consistent evaluation of UE techniques by researchers, and a demo\nweb application that enriches the standard chat dialog with confidence scores,\nempowering end-users to discern unreliable responses. LM-Polygraph is\ncompatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and\nGPT-4, and is designed to support future releases of similarly-styled LMs.\n","authors":["Ekaterina Fadeeva","Roman Vashurin","Akim Tsvigun","Artem Vazhentsev","Sergey Petrakov","Kirill Fedyanin","Daniil Vasilev","Elizaveta Goncharova","Alexander Panchenko","Maxim Panov","Timothy Baldwin","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2311.07383v1.pdf","comment":"Accepted at EMNLP-2023"},{"id":"http://arxiv.org/abs/2305.13169v2","updated":"2023-11-13T14:50:06Z","published":"2023-05-22T15:57:53Z","title":"A Pretrainer's Guide to Training Data: Measuring the Effects of Data\n  Age, Domain Coverage, Quality, & Toxicity","summary":"  Pretraining is the preliminary and fundamental step in developing capable\nlanguage models (LM). Despite this, pretraining data design is critically\nunder-documented and often guided by empirically unsupported intuitions. To\naddress this, we pretrain 28 1.5B parameter decoder-only models, training on\ndata curated (1) at different times, (2) with varying toxicity and quality\nfilters, and (3) with different domain compositions. First, we quantify the\neffect of pretraining data age. A temporal shift between evaluation data and\npretraining data leads to performance degradation, which is not overcome by\nfinetuning. Second, we explore the effect of quality and toxicity filters,\nshowing a trade-off between performance on standard benchmarks and risk of\ntoxic generations. Our findings indicate there does not exist a\none-size-fits-all solution to filtering training data. We also find that the\neffects of different types of filtering are not predictable from text domain\ncharacteristics. Lastly, we empirically validate that the inclusion of\nheterogeneous data sources, like books and web, is broadly beneficial and\nwarrants greater prioritization. These findings constitute the largest set of\nexperiments to validate, quantify, and expose many undocumented intuitions\nabout text pretraining, which we hope will help support more informed\ndata-centric decisions in LM development.\n","authors":["Shayne Longpre","Gregory Yauney","Emily Reif","Katherine Lee","Adam Roberts","Barret Zoph","Denny Zhou","Jason Wei","Kevin Robinson","David Mimno","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2305.13169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05317v5","updated":"2023-11-13T14:35:22Z","published":"2023-10-09T00:20:59Z","title":"Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy\n  in Mental Health and Beyond","summary":"  We propose task-adaptive tokenization as a way to adapt the generation\npipeline to the specifics of a downstream task and enhance long-form generation\nin mental health. Inspired by insights from cognitive science, our\ntask-adaptive tokenizer samples variable segmentations from multiple outcomes,\nwith sampling probabilities optimized based on task-specific data. We introduce\na strategy for building a specialized vocabulary and introduce a vocabulary\nmerging protocol that allows for the integration of task-specific tokens into\nthe pre-trained model's tokenization step. Through extensive experiments on\npsychological question-answering tasks in both Chinese and English, we find\nthat our task-adaptive tokenization approach brings a significant improvement\nin generation performance while using up to 60% fewer tokens. Preliminary\nexperiments point to promising results when using our tokenization approach\nwith very large language models.\n","authors":["Siyang Liu","Naihao Deng","Sahand Sabour","Yilin Jia","Minlie Huang","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2310.05317v5.pdf","comment":"Accepted at the main conference of The 2023 Conference on Empirical\n  Methods in Natural Language Processing; 8 pages"},{"id":"http://arxiv.org/abs/2307.06440v3","updated":"2023-11-13T14:33:01Z","published":"2023-07-12T20:10:14Z","title":"No Train No Gain: Revisiting Efficient Training Algorithms For\n  Transformer-based Language Models","summary":"  The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n","authors":["Jean Kaddour","Oscar Key","Piotr Nawrot","Pasquale Minervini","Matt J. Kusner"],"pdf_url":"https://arxiv.org/pdf/2307.06440v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07362v1","updated":"2023-11-13T14:26:24Z","published":"2023-11-13T14:26:24Z","title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback\n  Guided Revision","summary":"  Large multimodal models (LMMs) suffer from multimodal hallucination, where\nthey provide incorrect responses misaligned with the given visual information.\nRecent works have conjectured that one of the reasons behind multimodal\nhallucination might be due to the vision encoder failing to ground on the image\nproperly. To mitigate this issue, we propose a novel approach that leverages\nself-feedback as visual cues. Building on this approach, we introduce Volcano,\na multimodal self-feedback guided revision model. Volcano generates natural\nlanguage feedback to its initial response based on the provided visual\ninformation and utilizes this feedback to self-revise its initial response.\nVolcano effectively reduces multimodal hallucination and achieves\nstate-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general\nmultimodal abilities and outperforms previous models on MM-Vet and MMBench.\nThrough a qualitative analysis, we show that Volcano's feedback is properly\ngrounded on the image than the initial response. This indicates that Volcano\ncan provide itself with richer visual information, helping alleviate multimodal\nhallucination. We publicly release Volcano models of 7B and 13B sizes along\nwith the data and code at https://github.com/kaistAI/Volcano.\n","authors":["Seongyun Lee","Sue Hyun Park","Yongrae Jo","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.07362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07361v1","updated":"2023-11-13T14:26:12Z","published":"2023-11-13T14:26:12Z","title":"The Impact of Large Language Models on Scientific Discovery: a\n  Preliminary Study using GPT-4","summary":"  In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.\n","authors":["Microsoft Research AI4Science","Microsoft Azure Quantum"],"pdf_url":"https://arxiv.org/pdf/2311.07361v1.pdf","comment":"230 pages report; 181 pages for main contents"},{"id":"http://arxiv.org/abs/2308.10248v3","updated":"2023-11-13T14:05:13Z","published":"2023-08-20T12:21:05Z","title":"Activation Addition: Steering Language Models Without Optimization","summary":"  Reliably controlling the behavior of large language models is a pressing open\nproblem. Existing methods include supervised finetuning, reinforcement learning\nfrom human feedback, prompt engineering and guided decoding. We instead\ninvestigate activation engineering: modifying activations at inference-time to\npredictably alter model behavior. We bias the forward pass with a 'steering\nvector' implicitly specified through natural language. Past work learned these\nsteering vectors; our Activation Addition (ActAdd) method instead computes them\nby taking the activation differences which result from pairs of prompts.\n  We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate\nthe effect on Llama-13B and GPT-J-6B. Our approach yields inference-time\ncontrol over high-level properties of output & preserves performance on\noff-target topics. The method requires far less compute and implementation\neffort than finetuning and RLHF, allows for natural language specification by\nusers, and its overhead scales naturally with model size.\n","authors":["Alexander Matt Turner","Lisa Thiergart","David Udell","Gavin Leech","Ulisse Mini","Monte MacDiarmid"],"pdf_url":"https://arxiv.org/pdf/2308.10248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14338v2","updated":"2023-11-13T13:23:44Z","published":"2023-10-22T16:07:06Z","title":"From Chaos to Clarity: Claim Normalization to Empower Fact-Checking","summary":"  With the rise of social media, users are exposed to many misleading claims.\nHowever, the pervasive noise inherent in these posts presents a challenge in\nidentifying precise and prominent claims that require verification. Extracting\nthe important claims from such posts is arduous and time-consuming, yet it is\nan underexplored problem. Here, we aim to bridge this gap. We introduce a novel\ntask, Claim Normalization (aka ClaimNorm), which aims to decompose complex and\nnoisy social media posts into more straightforward and understandable forms,\ntermed normalized claims. We propose CACN, a pioneering approach that leverages\nchain-of-thought and claim check-worthiness estimation, mimicking human\nreasoning processes, to comprehend intricate claims. Moreover, we capitalize on\nthe in-context learning capabilities of large language models to provide\nguidance and to improve claim normalization. To evaluate the effectiveness of\nour proposed model, we meticulously compile a comprehensive real-world dataset,\nCLAN, comprising more than 6k instances of social media posts alongside their\nrespective normalized claims. Our experiments demonstrate that CACN outperforms\nseveral baselines across various evaluation measures. Finally, our rigorous\nerror analysis validates CACN's capabilities and pitfalls.\n","authors":["Megha Sundriyal","Tanmoy Chakraborty","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2310.14338v2.pdf","comment":"Accepted at Findings EMNLP2023"},{"id":"http://arxiv.org/abs/2310.11715v2","updated":"2023-11-13T13:18:58Z","published":"2023-10-18T05:13:34Z","title":"Enhancing Low-resource Fine-grained Named Entity Recognition by\n  Leveraging Coarse-grained Datasets","summary":"  Named Entity Recognition (NER) frequently suffers from the problem of\ninsufficient labeled data, particularly in fine-grained NER scenarios. Although\n$K$-shot learning techniques can be applied, their performance tends to\nsaturate when the number of annotations exceeds several tens of labels. To\novercome this problem, we utilize existing coarse-grained datasets that offer a\nlarge number of annotations. A straightforward approach to address this problem\nis pre-finetuning, which employs coarse-grained data for representation\nlearning. However, it cannot directly utilize the relationships between\nfine-grained and coarse-grained entities, although a fine-grained entity type\nis likely to be a subcategory of a coarse-grained entity type. We propose a\nfine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage\nthe hierarchical structure explicitly. In addition, we present an inconsistency\nfiltering method to eliminate coarse-grained entities that are inconsistent\nwith fine-grained entity types to avoid performance degradation. Our\nexperimental results show that our method outperforms both $K$-shot learning\nand supervised learning methods when dealing with a small number of\nfine-grained annotations.\n","authors":["Su Ah Lee","Seokjin Oh","Woohwan Jung"],"pdf_url":"https://arxiv.org/pdf/2310.11715v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2307.16833v2","updated":"2023-11-13T13:17:03Z","published":"2023-07-26T02:12:58Z","title":"Data Augmentation for Neural Machine Translation using Generative\n  Language Model","summary":"  Despite the rapid growth in model architecture, the scarcity of large\nparallel corpora remains the main bottleneck in Neural Machine Translation.\nData augmentation is a technique that enhances the performance of data-hungry\nmodels by generating synthetic data instead of collecting new ones. We explore\nprompt-based data augmentation approaches that leverage large-scale language\nmodels such as ChatGPT. To create a synthetic parallel corpus, we compare 3\nmethods using different prompts. We employ two assessment metrics to measure\nthe diversity of the generated synthetic data. This approach requires no\nfurther model training cost, which is mandatory in other augmentation methods\nlike back-translation. The proposed method improves the unaugmented baseline by\n0.68 BLEU score.\n","authors":["Seokjin Oh","Su Ah Lee","Woohwan Jung"],"pdf_url":"https://arxiv.org/pdf/2307.16833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07314v1","updated":"2023-11-13T13:10:44Z","published":"2023-11-13T13:10:44Z","title":"Semi-automatic Data Enhancement for Document-Level Relation Extraction\n  with Distant Supervision from Large Language Models","summary":"  Document-level Relation Extraction (DocRE), which aims to extract relations\nfrom a long context, is a critical challenge in achieving fine-grained\nstructural comprehension and generating interpretable document representations.\nInspired by recent advances in in-context learning capabilities emergent from\nlarge language models (LLMs), such as ChatGPT, we aim to design an automated\nannotation method for DocRE with minimum human effort. Unfortunately, vanilla\nin-context learning is infeasible for document-level relation extraction due to\nthe plenty of predefined fine-grained relation types and the uncontrolled\ngenerations of LLMs. To tackle this issue, we propose a method integrating a\nlarge language model (LLM) and a natural language inference (NLI) module to\ngenerate relation triples, thereby augmenting document-level relation datasets.\nWe demonstrate the effectiveness of our approach by introducing an enhanced\ndataset known as DocGNRE, which excels in re-annotating numerous long-tail\nrelation types. We are confident that our method holds the potential for\nbroader applications in domain-specific relation type definitions and offers\ntangible benefits in advancing generalized language semantic comprehension.\n","authors":["Junpeng Li","Zixia Jia","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2311.07314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07311v1","updated":"2023-11-13T13:05:15Z","published":"2023-11-13T13:05:15Z","title":"Do large language models and humans have similar behaviors in causal\n  inference with script knowledge?","summary":"  Recently, large pre-trained language models (LLMs) have demonstrated superior\nlanguage understanding abilities, including zero-shot causal reasoning.\nHowever, it is unclear to what extent their capabilities are similar to human\nones. We here study the processing of an event $B$ in a script-based story,\nwhich causally depends on a previous event $A$. In our manipulation, event $A$\nis stated, negated, or omitted in an earlier section of the text. We first\nconducted a self-paced reading experiment, which showed that humans exhibit\nsignificantly longer reading times when causal conflicts exist ($\\neg A\n\\rightarrow B$) than under logical conditions ($A \\rightarrow B$). However,\nreading times remain similar when cause A is not explicitly mentioned,\nindicating that humans can easily infer event B from their script knowledge. We\nthen tested a variety of LLMs on the same data to check to what extent the\nmodels replicate human behavior. Our experiments show that 1) only recent LLMs,\nlike GPT-3 or Vicuna, correlate with human behavior in the $\\neg A \\rightarrow\nB$ condition. 2) Despite this correlation, all models still fail to predict\nthat $nil \\rightarrow B$ is less surprising than $\\neg A \\rightarrow B$,\nindicating that LLMs still have difficulties integrating script knowledge. Our\ncode and collected data set are available at\nhttps://github.com/tony-hong/causal-script.\n","authors":["Xudong Hong","Margarita Ryzhova","Daniel Adrian Biondi","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2311.07311v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.06552v3","updated":"2023-11-13T12:38:00Z","published":"2023-10-10T11:56:48Z","title":"Automated clinical coding using off-the-shelf large language models","summary":"  The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment, with no\nneed for further task-specific training. Unsupervised pre-training alone does\nnot guarantee precise knowledge of the ICD ontology and specialist clinical\ncoding task, therefore we frame the task as information extraction, providing a\ndescription of each coded concept and asking the model to retrieve related\nmentions. For efficiency, rather than iterating over all codes, we leverage the\nhierarchical nature of the ICD ontology to sparsely search for relevant codes.\n","authors":["Joseph S. Boyle","Antanas Kascenas","Pat Lok","Maria Liakata","Alison Q. O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.06552v3.pdf","comment":"Accepted to the NeurIPS 2023 workshop Deep Generative Models For\n  Health (DGM4H). 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.07296v1","updated":"2023-11-13T12:36:53Z","published":"2023-11-13T12:36:53Z","title":"BIDRN: A Method of Bidirectional Recurrent Neural Network for Sentiment\n  Analysis","summary":"  Text mining research has grown in importance in recent years due to the\ntremendous increase in the volume of unstructured textual data. This has\nresulted in immense potential as well as obstacles in the sector, which may be\nefficiently addressed with adequate analytical and study methods. Deep\nBidirectional Recurrent Neural Networks are used in this study to analyze\nsentiment. The method is categorized as sentiment polarity analysis because it\nmay generate a dataset with sentiment labels. This dataset can be used to train\nand evaluate sentiment analysis models capable of extracting impartial\nopinions. This paper describes the Sentiment Analysis-Deep Bidirectional\nRecurrent Neural Networks (SA-BDRNN) Scheme, which seeks to overcome the\nchallenges and maximize the potential of text mining in the context of Big\nData. The current study proposes a SA-DBRNN Scheme that attempts to give a\nsystematic framework for sentiment analysis in the context of student input on\ninstitution choice. The purpose of this study is to compare the effectiveness\nof the proposed SA- DBRNN Scheme to existing frameworks to establish a robust\ndeep neural network that might serve as an adequate classification model in the\nfield of sentiment analysis.\n","authors":["Dr. D Muthusankar","Dr. P Kaladevi","Dr. V R Sadasivam","R Praveen"],"pdf_url":"https://arxiv.org/pdf/2311.07296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07277v1","updated":"2023-11-13T12:20:48Z","published":"2023-11-13T12:20:48Z","title":"AdaCCD: Adaptive Semantic Contrasts Discovery based Cross Lingual\n  Adaptation for Code Clone Detection","summary":"  Code Clone Detection, which aims to retrieve functionally similar programs\nfrom large code bases, has been attracting increasing attention. Modern\nsoftware often involves a diverse range of programming languages. However,\ncurrent code clone detection methods are generally limited to only a few\npopular programming languages due to insufficient annotated data as well as\ntheir own model design constraints. To address these issues, we present AdaCCD,\na novel cross-lingual adaptation method that can detect cloned codes in a new\nlanguage without any annotations in that language. AdaCCD leverages\nlanguage-agnostic code representations from pre-trained programming language\nmodels and propose an Adaptively Refined Contrastive Learning framework to\ntransfer knowledge from resource-rich languages to resource-poor languages. We\nevaluate the cross-lingual adaptation results of AdaCCD by constructing a\nmultilingual code clone detection benchmark consisting of 5 programming\nlanguages. AdaCCD achieves significant improvements over other baselines, and\nit is even comparable to supervised fine-tuning.\n","authors":["Yangkai Du","Tengfei Ma","Lingfei Wu","Xuhong Zhang","Shouling Ji"],"pdf_url":"https://arxiv.org/pdf/2311.07277v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.07264v1","updated":"2023-11-13T12:03:52Z","published":"2023-11-13T12:03:52Z","title":"Danish Foundation Models","summary":"  Large language models, sometimes referred to as foundation models, have\ntransformed multiple fields of research. However, smaller languages risk\nfalling behind due to high training costs and small incentives for large\ncompanies to train these models. To combat this, the Danish Foundation Models\nproject seeks to provide and maintain open, well-documented, and high-quality\nfoundation models for the Danish language. This is achieved through broad\ncooperation with public and private institutions, to ensure high data quality\nand applicability of the trained models. We present the motivation of the\nproject, the current status, and future perspectives.\n","authors":["Kenneth Enevoldsen","Lasse Hansen","Dan S. Nielsen","Rasmus A. F. Egebæk","Søren V. Holm","Martin C. Nielsen","Martin Bernstorff","Rasmus Larsen","Peter B. Jørgensen","Malte Højmark-Bertelsen","Peter B. Vahlstrup","Per Møldrup-Dalum","Kristoffer Nielbo"],"pdf_url":"https://arxiv.org/pdf/2311.07264v1.pdf","comment":"4 pages, 2 tables"},{"id":"http://arxiv.org/abs/2311.07237v1","updated":"2023-11-13T10:56:59Z","published":"2023-11-13T10:56:59Z","title":"In Search of the Long-Tail: Systematic Generation of Long-Tail Knowledge\n  via Logical Rule Guided Search","summary":"  Since large language models have approached human-level performance on many\ntasks, it has become increasingly harder for researchers to find tasks that are\nstill challenging to the models. Failure cases usually come from the long-tail\ndistribution - data that an oracle language model could assign a probability on\nthe lower end of its distribution. Current methodology such as prompt\nengineering or crowdsourcing are insufficient for creating long-tail examples\nbecause humans are constrained by cognitive bias. We propose a\nLogic-Induced-Knowledge-Search (LINK) framework for systematically generating\nlong-tail knowledge statements. Grounded by a symbolic rule, we search for\nlong-tail values for each variable of the rule by first prompting a LLM, then\nverifying the correctness of the values with a critic, and lastly pushing for\nthe long-tail distribution with a reranker. With this framework we construct a\ndataset, Logic-Induced-Long-Tail (LINT), consisting of 200 symbolic rules and\n50K knowledge statements spanning across four domains. Human annotations find\nthat 84% of the statements in LINT are factually correct. In contrast, ChatGPT\nand GPT4 struggle with directly generating long-tail statements under the\nguidance of logic rules, each only getting 56% and 78% of their statements\ncorrect. Moreover, their \"long-tail\" generations in fact fall into the higher\nlikelihood range, and thus are not really long-tail. Our findings suggest that\nLINK is effective for generating data in the long-tail distribution while\nenforcing quality. LINT can be useful for systematically evaluating LLMs'\ncapabilities in the long-tail distribution. We challenge the models with a\nsimple entailment classification task using samples from LINT. We find that\nChatGPT and GPT4's capability in identifying incorrect knowledge drop by ~3% in\nthe long-tail distribution compared to head distribution.\n","authors":["Huihan Li","Yuting Ning","Zeyi Liao","Siyuan Wang","Xiang Lorraine Li","Ximing Lu","Faeze Brahman","Wenting Zhao","Yejin Choi","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2311.07237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11772v3","updated":"2023-11-13T10:56:22Z","published":"2023-07-18T04:43:24Z","title":"AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment\n  enabled by Large Language Models","summary":"  The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named AutoAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nAutoAlign constructs a predicate-proximity-graph with the help of large\nlanguage models to automatically capture the similarity between predicates\nacross two KGs. For entity embeddings, AutoAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nAutoAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that AutoAlign improves the performance of entity\nalignment significantly compared to state-of-the-art methods.\n","authors":["Rui Zhang","Yixin Su","Bayu Distiawan Trisedya","Xiaoyan Zhao","Min Yang","Hong Cheng","Jianzhong Qi"],"pdf_url":"https://arxiv.org/pdf/2307.11772v3.pdf","comment":"14 pages, 5 figures, 4 tables, IEEE Transactions on Knowledge and\n  Data Engineering"},{"id":"http://arxiv.org/abs/2311.07230v1","updated":"2023-11-13T10:52:01Z","published":"2023-11-13T10:52:01Z","title":"How are Prompts Different in Terms of Sensitivity?","summary":"  In-context learning (ICL) has become one of the most popular learning\nparadigms. While there is a growing body of literature focusing on prompt\nengineering, there is a lack of systematic analysis comparing the effects of\nprompts across different models and tasks. To address this gap, we present a\ncomprehensive prompt analysis based on the sensitivity of a function. Our\nanalysis reveals that sensitivity is an unsupervised proxy for model\nperformance, as it exhibits a strong negative correlation with accuracy. We use\ngradient-based saliency scores to empirically demonstrate how different prompts\naffect the relevance of input tokens to the output, resulting in different\nlevels of sensitivity. Furthermore, we introduce sensitivity-aware decoding\nwhich incorporates sensitivity estimation as a penalty term in the standard\ngreedy decoding. We show that this approach is particularly helpful when\ninformation in the input is scarce. Our work provides a fresh perspective on\nthe analysis of prompts, and contributes to a better understanding of the\nmechanism of ICL.\n","authors":["Sheng Lu","Hendrik Schuff","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2311.07230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07217v1","updated":"2023-11-13T10:24:51Z","published":"2023-11-13T10:24:51Z","title":"Troubles and Failures in Interactional Language. Towards a\n  Linguistically Informed Taxonomy","summary":"  The goal of this talk is to introduce a systematic research agenda which aims\nto understand the nature of interaction between humans and artificial\nconversational agents (CA) (henceforth humanmachine interaction, HMI).\nSpecifically, we shall take an explicit linguistic perspective focusing on\nlinguistically defined variables that are known to influence the flow of\nconversations among humans (henceforth human-human interaction, HHI).\n","authors":["Martina Wiltschko"],"pdf_url":"https://arxiv.org/pdf/2311.07217v1.pdf","comment":"3 pages, 3 figures, Part of WTF 23 workshop proceedings"},{"id":"http://arxiv.org/abs/2311.07215v1","updated":"2023-11-13T10:15:19Z","published":"2023-11-13T10:15:19Z","title":"Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback","summary":"  Code editing is an essential step towards reliable program synthesis to\nautomatically correct critical errors generated from code LLMs. Recent studies\nhave demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable\nof generating corrective feedback to edit erroneous inputs. However, it remains\nchallenging for open-source code LLMs to generate feedback for code editing,\nsince these models tend to adhere to the superficial formats of feedback and\nprovide feedback with misleading information. Hence, the focus of our work is\nto leverage open-source code LLMs to generate helpful feedback with correct\nguidance for code editing. To this end, we present Coffee, a collected dataset\nspecifically designed for code fixing with feedback. Using this dataset, we\nconstruct CoffeePots, a framework for COde Fixing with FEEdback via\nPreference-Optimized Tuning and Selection. The proposed framework aims to\nautomatically generate helpful feedback for code editing while minimizing the\npotential risk of superficial feedback. The combination of Coffee and\nCoffeePots marks a significant advancement, achieving state-of-the-art\nperformance on HumanEvalFix benchmark. Codes and model checkpoints are publicly\navailable at https://github.com/Lune-Blue/COFFEE.\n","authors":["Seungjun Moon","Yongho Song","Hyungjoo Chae","Dongjin Kang","Taeyoon Kwon","Kai Tzu-iunn Ong","Seung-won Hwang","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2311.07215v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.07194v1","updated":"2023-11-13T09:32:12Z","published":"2023-11-13T09:32:12Z","title":"Exploring the Dialogue Comprehension Ability of Large Language Models","summary":"  The recent emergence of large language models (LLMs) have attracted\nconsiderable attention. LLMs may interact with users in the form of dialogue\nand generate responses following their instructions, which naturally require\ndialogue comprehension abilities. Without correct comprehension of the\ndialogue, the model may inevitably generate incorrect responses. However,\ndialogue comprehension is a general language ability which is hard to be\nevaluated directly. In this work, we propose to perform the evaluation with the\nhelp of the dialogue summarization task. Beside evaluating and analyzing the\ndialogue summarization performance (DIAC-Sum), we also derive factual questions\nfrom the generated summaries and use them as a more flexible measurement of\ndialogue comprehension (DIAC-FactQA). Our evaluation shows that, on average,\n27% of the summaries generated by LLMs contain factual inconsistency. Even\nChatGPT, the strongest evaluated model, has such errors in 16% of its\nsummaries. For answering the factual questions, which is more challenging, the\naverage accuracy of all evaluated LLMs is only 62.8%. Both results indicate\nserious deficiencies. Detailed analysis shows that the understanding of\nsubject/object of the conversation is still the most challenging problem for\nLLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability\nof LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task\ndata. The experimental results demonstrate that our method achieved an accuracy\nimprovement of 8.9% on DIAC-FactQA.\n","authors":["Shuaijie She","Shujian Huang","Xingyun Wang","Yanke Zhou","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07172v1","updated":"2023-11-13T09:06:58Z","published":"2023-11-13T09:06:58Z","title":"VerityMath: Advancing Mathematical Reasoning by Self-Verification\n  Through Unit Consistency","summary":"  Large Language Models (LLMs) combined with program-based solving techniques\nare increasingly demonstrating proficiency in mathematical reasoning. However,\nsuch progress is mostly demonstrated in closed-source models such as\nOpenAI-GPT4 and Claude. In this paper, we seek to study the performance of\nstrong open-source LLMs. Specifically, we analyze the outputs of Code Llama\n(7B) when applied to math word problems. We identify a category of problems\nthat pose a challenge for the model, particularly those involving quantities\nthat span multiple types or units. To address this issue, we propose a\nsystematic approach by defining units for each quantity and ensuring the\nconsistency of these units during mathematical operations. We developed Unit\nConsistency Programs (UCPs), an annotated dataset of math word problems, each\npaired with programs that contain unit specifications and unit verification\nroutines. Finally, we finetune the Code Llama (7B) model with UCPs to produce\nVerityMath and present our preliminary findings.\n","authors":["Vernon Toh","Ratish Puduppully","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07172v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2311.07171v1","updated":"2023-11-13T09:06:43Z","published":"2023-11-13T09:06:43Z","title":"calamanCy: A Tagalog Natural Language Processing Toolkit","summary":"  We introduce calamanCy, an open-source toolkit for constructing natural\nlanguage processing (NLP) pipelines for Tagalog. It is built on top of spaCy,\nenabling easy experimentation and integration with other frameworks. calamanCy\naddresses the development gap by providing a consistent API for building NLP\napplications and offering general-purpose multitask models with out-of-the-box\nsupport for dependency parsing, parts-of-speech (POS) tagging, and named entity\nrecognition (NER). calamanCy aims to accelerate the progress of Tagalog NLP by\nconsolidating disjointed resources in a unified framework. The calamanCy\ntoolkit is available on GitHub: https://github.com/ljvmiranda921/calamanCy.\n","authors":["Lester James V. Miranda"],"pdf_url":"https://arxiv.org/pdf/2311.07171v1.pdf","comment":"To be published in The Third Workshop for NLP-OSS at EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.07167v1","updated":"2023-11-13T09:02:30Z","published":"2023-11-13T09:02:30Z","title":"STEER: Unified Style Transfer with Expert Reinforcement","summary":"  While text style transfer has many applications across natural language\nprocessing, the core premise of transferring from a single source style is\nunrealistic in a real-world setting. In this work, we focus on arbitrary style\ntransfer: rewriting a text from an arbitrary, unknown style to a target style.\n  We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified\nframe-work developed to overcome the challenge of limited parallel data for\nstyle transfer. STEER involves automatically generating a corpus of\nstyle-transfer pairs using a product of experts during decoding. The generated\noffline data is then used to pre-train an initial policy before switching to\nonline, off-policy reinforcement learning for further improvements via\nfine-grained reward signals. STEER is unified and can transfer to multiple\ntarget styles from an arbitrary, unknown source style, making it particularly\nflexible and efficient.\n  Experimental results on a challenging dataset with text from a diverse set of\nstyles demonstrate state-of-the-art results compared to competitive baselines.\nRemarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on\noverall style transfer quality, despite being 226 times smaller in size. We\nalso show STEER is robust, maintaining its style transfer capabilities on\nout-of-domain data, and surpassing nearly all baselines across various styles.\nThe success of our method highlights the potential of RL algorithms when\naugmented with controllable decoding to overcome the challenge of limited data\nsupervision.\n","authors":["Skyler Hallinan","Faeze Brahman","Ximing Lu","Jaehun Jung","Sean Welleck","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2311.07167v1.pdf","comment":"for associated code, see\n  https://github.com/shallinan1/STEERStyleTransfer"},{"id":"http://arxiv.org/abs/2311.07161v1","updated":"2023-11-13T08:56:47Z","published":"2023-11-13T08:56:47Z","title":"Developing a Named Entity Recognition Dataset for Tagalog","summary":"  We present the development of a Named Entity Recognition (NER) dataset for\nTagalog. This corpus helps fill the resource gap present in Philippine\nlanguages today, where NER resources are scarce. The texts were obtained from a\npretraining corpora containing news reports, and were labeled by native\nspeakers in an iterative fashion. The resulting dataset contains ~7.8k\ndocuments across three entity types: Person, Organization, and Location. The\ninter-annotator agreement, as measured by Cohen's $\\kappa$, is 0.81. We also\nconducted extensive empirical evaluation of state-of-the-art methods across\nsupervised and transfer learning settings. Finally, we released the data and\nprocessing code publicly to inspire future work on Tagalog NLP.\n","authors":["Lester James V. Miranda"],"pdf_url":"https://arxiv.org/pdf/2311.07161v1.pdf","comment":"To be published in The First Workshop for Southeast Asian Language\n  Processing 2023 at IJCNLP-AACL"},{"id":"http://arxiv.org/abs/2311.02408v3","updated":"2023-11-13T08:40:04Z","published":"2023-11-04T14:08:15Z","title":"Citance-Contextualized Summarization of Scientific Papers","summary":"  Current approaches to automatic summarization of scientific papers generate\ninformative summaries in the form of abstracts. However, abstracts are not\nintended to show the relationship between a paper and the references cited in\nit. We propose a new contextualized summarization approach that can generate an\ninformative summary conditioned on a given sentence containing the citation of\na reference (a so-called \"citance\"). This summary outlines the content of the\ncited paper relevant to the citation location. Thus, our approach extracts and\nmodels the citances of a paper, retrieves relevant passages from cited papers,\nand generates abstractive summaries tailored to each citance. We evaluate our\napproach using $\\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing\n540K~computer science papers and 4.6M~citances therein.\n","authors":["Shahbaz Syed","Ahmad Dawar Hakimi","Khalid Al-Khatib","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2311.02408v3.pdf","comment":"Accepted at EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2311.07150v1","updated":"2023-11-13T08:39:06Z","published":"2023-11-13T08:39:06Z","title":"Interaction is all You Need? A Study of Robots Ability to Understand and\n  Execute","summary":"  This paper aims to address a critical challenge in robotics, which is\nenabling them to operate seamlessly in human environments through natural\nlanguage interactions. Our primary focus is to equip robots with the ability to\nunderstand and execute complex instructions in coherent dialogs to facilitate\nintricate task-solving scenarios. To explore this, we build upon the Execution\nfrom Dialog History (EDH) task from the Teach benchmark. We employ a\nmulti-transformer model with BART LM. We observe that our best configuration\noutperforms the baseline with a success rate score of 8.85 and a\ngoal-conditioned success rate score of 14.02. In addition, we suggest an\nalternative methodology for completing this task. Moreover, we introduce a new\ntask by expanding the EDH task and making predictions about game plans instead\nof individual actions. We have evaluated multiple BART models and an LLaMA2\nLLM, which has achieved a ROGUE-L score of 46.77 for this task.\n","authors":["Kushal Koshti","Nidhir Bhavsar"],"pdf_url":"https://arxiv.org/pdf/2311.07150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07138v1","updated":"2023-11-13T08:09:01Z","published":"2023-11-13T08:09:01Z","title":"WaterBench: Towards Holistic Evaluation of Watermarks for Large Language\n  Models","summary":"  To mitigate the potential misuse of large language models (LLMs), recent\nresearch has developed watermarking algorithms, which restrict the generation\nprocess to leave an invisible trace for watermark detection. Due to the\ntwo-stage nature of the task, most studies evaluate the generation and\ndetection separately, thereby presenting a challenge in unbiased, thorough, and\napplicable evaluations. In this paper, we introduce WaterBench, the first\ncomprehensive benchmark for LLM watermarks, in which we design three crucial\nfactors: (1) For \\textbf{benchmarking procedure}, to ensure an apples-to-apples\ncomparison, we first adjust each watermarking method's hyper-parameter to reach\nthe same watermarking strength, then jointly evaluate their generation and\ndetection performance. (2) For \\textbf{task selection}, we diversify the input\nand output length to form a five-category taxonomy, covering $9$ tasks. (3) For\n\\textbf{evaluation metric}, we adopt the GPT4-Judge for automatically\nevaluating the decline of instruction-following abilities after watermarking.\nWe evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking\nstrengths and observe the common struggles for current methods on maintaining\nthe generation quality. The code and data are available at\n\\url{https://github.com/THU-KEG/WaterBench}.\n","authors":["Shangqing Tu","Yuliang Sun","Yushi Bai","Jifan Yu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2311.07138v1.pdf","comment":"22pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.05163v3","updated":"2023-11-13T07:24:14Z","published":"2023-10-08T13:45:05Z","title":"An Investigation of LLMs' Inefficacy in Understanding Converse Relations","summary":"  Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.\n","authors":["Chengwen Qi","Bowen Li","Binyuan Hui","Bailin Wang","Jinyang Li","Jinwang Wu","Yuanjun Laili"],"pdf_url":"https://arxiv.org/pdf/2310.05163v3.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.07115v1","updated":"2023-11-13T07:12:57Z","published":"2023-11-13T07:12:57Z","title":"Gen-Z: Generative Zero-Shot Text Classification with Contextualized\n  Label Descriptions","summary":"  Language model (LM) prompting--a popular paradigm for solving NLP tasks--has\nbeen shown to be susceptible to miscalibration and brittleness to slight prompt\nvariations, caused by its discriminative prompting approach, i.e., predicting\nthe label given the input. To address these issues, we propose Gen-Z--a\ngenerative prompting framework for zero-shot text classification. GEN-Z is\ngenerative, as it measures the LM likelihood of input text, conditioned on\nnatural language descriptions of labels. The framework is multivariate, as\nlabel descriptions allow us to seamlessly integrate additional contextual\ninformation about the labels to improve task performance. On various standard\nclassification benchmarks, with six open-source LM families, we show that\nzero-shot classification with simple contextualization of the data source of\nthe evaluation set consistently outperforms both zero-shot and few-shot\nbaselines while improving robustness to prompt variations. Further, our\napproach enables personalizing classification in a zero-shot manner by\nincorporating author, subject, or reader information in the label descriptions.\n","authors":["Sachin Kumar","Chan Young Park","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2311.07115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17743v2","updated":"2023-11-13T06:38:53Z","published":"2023-10-26T19:31:22Z","title":"StyleBART: Decorate Pretrained Model with Style Adapters for\n  Unsupervised Stylistic Headline Generation","summary":"  Stylistic headline generation is the task to generate a headline that not\nonly summarizes the content of an article, but also reflects a desired style\nthat attracts users. As style-specific article-headline pairs are scarce,\nprevious researches focus on unsupervised approaches with a standard headline\ngeneration dataset and mono-style corpora. In this work, we follow this line\nand propose StyleBART, an unsupervised approach for stylistic headline\ngeneration. Our method decorates the pretrained BART model with adapters that\nare responsible for different styles and allows the generation of headlines\nwith diverse styles by simply switching the adapters. Different from previous\nworks, StyleBART separates the task of style learning and headline generation,\nmaking it possible to freely combine the base model and the style adapters\nduring inference. We further propose an inverse paraphrasing task to enhance\nthe style adapters. Extensive automatic and human evaluations show that\nStyleBART achieves new state-of-the-art performance in the unsupervised\nstylistic headline generation task, producing high-quality headlines with the\ndesired style.\n","authors":["Hanqing Wang","Yajing Luo","Boya Xiong","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.17743v2.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.07102v1","updated":"2023-11-13T06:24:27Z","published":"2023-11-13T06:24:27Z","title":"Fovea Transformer: Efficient Long-Context Modeling with Structured\n  Fine-to-Coarse Attention","summary":"  The quadratic complexity of self-attention in Transformers has hindered the\nprocessing of long text. To alleviate this problem, previous works have\nproposed to sparsify the attention matrix, taking advantage of the observation\nthat crucial information about a token can be derived from its neighbors. These\nmethods typically combine one or another form of local attention and global\nattention. Such combinations introduce abrupt changes in contextual granularity\nwhen going from local to global, which may be undesirable. We believe that a\nsmoother transition could potentially enhance model's ability to capture\nlong-context dependencies. In this study, we introduce Fovea Transformer, a\nlong-context focused transformer that addresses the challenges of capturing\nglobal dependencies while maintaining computational efficiency. To achieve\nthis, we construct a multi-scale tree from the input sequence, and use\nrepresentations of context tokens with a progressively coarser granularity in\nthe tree, as their distance to the query token increases. We evaluate our model\non three long-context summarization tasks\\footnote{Our code is publicly\navailable at: \\textit{https://github.com/ZiweiHe/Fovea-Transformer}}. It\nachieves state-of-the-art performance on two of them, and competitive results\non the third with mixed improvement and setback of the evaluation metrics.\n","authors":["Ziwei He","Jian Yuan","Le Zhou","Jingwen Leng","Bo Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.07102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07099v1","updated":"2023-11-13T06:13:38Z","published":"2023-11-13T06:13:38Z","title":"Explanation-aware Soft Ensemble Empowers Large Language Model In-context\n  Learning","summary":"  Large language models (LLMs) have shown remarkable capabilities in various\nnatural language understanding tasks. With only a few demonstration examples,\nthese LLMs can quickly adapt to target tasks without expensive gradient\nupdates. Common strategies to boost such 'in-context' learning ability are to\nensemble multiple model decoded results and require the model to generate an\nexplanation along with the prediction. However, these models often treat\ndifferent class predictions equally and neglect the potential discrepancy\nbetween the explanations and predictions. To fully unleash the power of\nexplanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to\nempower in-context learning with LLMs. We design two techniques,\nexplanation-guided ensemble, and soft probability aggregation, to mitigate the\neffect of unreliable explanations and improve the consistency between\nexplanations and final predictions. Experiments on seven natural language\nunderstanding tasks and four varying-size LLMs demonstrate the effectiveness of\nour proposed framework.\n","authors":["Yue Yu","Jiaming Shen","Tianqi Liu","Zhen Qin","Jing Nathan Yan","Jialu Liu","Chao Zhang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2311.07099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07093v1","updated":"2023-11-13T05:45:55Z","published":"2023-11-13T05:45:55Z","title":"On the Effectiveness of ASR Representations in Real-world Noisy Speech\n  Emotion Recognition","summary":"  This paper proposes an efficient attempt to noisy speech emotion recognition\n(NSER). Conventional NSER approaches have proven effective in mitigating the\nimpact of artificial noise sources, such as white Gaussian noise, but are\nlimited to non-stationary noises in real-world environments due to their\ncomplexity and uncertainty. To overcome this limitation, we introduce a new\nmethod for NSER by adopting the automatic speech recognition (ASR) model as a\nnoise-robust feature extractor to eliminate non-vocal information in noisy\nspeech. We first obtain intermediate layer information from the ASR model as a\nfeature representation for emotional speech and then apply this representation\nfor the downstream NSER task. Our experimental results show that 1) the\nproposed method achieves better NSER performance compared with the conventional\nnoise reduction method, 2) outperforms self-supervised learning approaches, and\n3) even outperforms text-based approaches using ASR transcription or the ground\ntruth transcription of noisy speech.\n","authors":["Xiaohan Shi","Jiajun He","Xingfeng Li","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2311.07093v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2311.07092v1","updated":"2023-11-13T05:40:11Z","published":"2023-11-13T05:40:11Z","title":"To Tell The Truth: Language of Deception and Language Models","summary":"  Text-based misinformation permeates online discourses, yet evidence of\npeople's ability to discern truth from such deceptive textual content is\nscarce. We analyze a novel TV game show data where conversations in a\nhigh-stake environment between individuals with conflicting objectives result\nin lies. We investigate the manifestation of potentially verifiable language\ncues of deception in the presence of objective truth, a distinguishing feature\nabsent in previous text-based deception datasets. We show that there exists a\nclass of detectors (algorithms) that have similar truth detection performance\ncompared to human subjects, even when the former accesses only the language\ncues while the latter engages in conversations with complete access to all\npotential sources of cues (language and audio-visual). Our model, built on a\nlarge language model, employs a bottleneck framework to learn discernible cues\nto determine truth, an act of reasoning in which human subjects often perform\npoorly, even with incentives. Our model detects novel but accurate language\ncues in many cases where humans failed to detect deception, opening up the\npossibility of humans collaborating with algorithms and ameliorating their\nability to detect the truth.\n","authors":["Bodhisattwa Prasad Majumder","Sanchaita Hazra"],"pdf_url":"https://arxiv.org/pdf/2311.07092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14303v2","updated":"2023-11-13T05:28:47Z","published":"2023-10-22T13:55:46Z","title":"Language Model Unalignment: Parametric Red-Teaming to Expose Hidden\n  Harms and Biases","summary":"  Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.\n","authors":["Rishabh Bhardwaj","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2310.14303v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.12816v3","updated":"2023-11-13T05:07:41Z","published":"2023-03-22T07:34:33Z","title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient\n  Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream applications. Conventional KGE\nmethods require high-dimensional representations to learn the complex structure\nof knowledge graph, but lead to oversized model parameters. Recent advances\nreduce parameters by low-dimensional entity representations, while developing\ntechniques (e.g., knowledge distillation or reinvented representation forms) to\ncompensate for reduced dimension. However, such operations introduce\ncomplicated computations and model designs that may not benefit large knowledge\ngraphs. To seek a simple strategy to improve the parameter efficiency of\nconventional KGE models, we take inspiration from that deeper neural networks\nrequire exponentially fewer parameters to achieve expressiveness comparable to\nwider networks for compositional structures. We view all entity representations\nas a single-layer embedding network, and conventional KGE methods that adopt\nhigh-dimensional entity representations equal widening the embedding network to\ngain expressiveness. To achieve parameter efficiency, we instead propose a\ndeeper embedding network for entity representations, i.e., a narrow entity\nembedding layer plus a multi-layer dimension lifting network (LiftNet).\nExperiments on three public datasets show that by integrating LiftNet, four\nconventional KGE methods with 16-dimensional representations achieve comparable\nlink prediction accuracy as original models that adopt 512-dimensional\nrepresentations, saving 68.4% to 96.9% parameters.\n","authors":["Borui Cai","Yong Xiang","Longxiang Gao","Di Wu","He Zhang","Jiong Jin","Tom Luan"],"pdf_url":"https://arxiv.org/pdf/2303.12816v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07076v1","updated":"2023-11-13T04:56:48Z","published":"2023-11-13T04:56:48Z","title":"On the Discussion of Large Language Models: Symmetry of Agents and\n  Interplay with Prompts","summary":"  Two ways has been discussed to unlock the reasoning capability of a large\nlanguage model. The first one is prompt engineering and the second one is to\ncombine the multiple inferences of large language models, or the multi-agent\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\nmechanisms from the symmetry of agents. Empirically, this paper reports the\nempirical results of the interplay of prompts and discussion mechanisms,\nrevealing the empirical state-of-the-art performance of complex multi-agent\nmechanisms can be approached by carefully developed prompt engineering. This\npaper also proposes a scalable discussion mechanism based on conquer and merge,\nproviding a simple multi-agent discussion solution with simple prompts but\nstate-of-the-art performance.\n","authors":["Qineng Wang","Zihao Wang","Ying Su","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2311.07076v1.pdf","comment":"Working in progress, and code will be released soon"},{"id":"http://arxiv.org/abs/2311.07070v1","updated":"2023-11-13T04:28:49Z","published":"2023-11-13T04:28:49Z","title":"Explain-then-Translate: An Analysis on Improving Program Translation\n  with Self-generated Explanations","summary":"  This work explores the use of self-generated natural language explanations as\nan intermediate step for code-to-code translation with language models. Across\nthree types of explanations and 19 programming languages constructed from the\nMultiPL-E dataset, we find the explanations to be particularly effective in the\nzero-shot case, improving performance by 12% on average. Improvements with\nnatural language explanations are particularly pronounced on difficult\nprograms. We release our dataset, code, and canonical solutions in all 19\nlanguages.\n","authors":["Zilu Tang","Mayank Agarwal","Alex Shypula","Bailin Wang","Derry Wijaya","Jie Chen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.07070v1.pdf","comment":"9 pages, 4 figures, 5 tables, 48 pages total. To be published in\n  EMNLP Findings 2023"},{"id":"http://arxiv.org/abs/2310.12467v2","updated":"2023-11-13T04:18:58Z","published":"2023-10-19T04:49:36Z","title":"Contrastive Learning for Inference in Dialogue","summary":"  Inference, especially those derived from inductive processes, is a crucial\ncomponent in our conversation to complement the information implicitly or\nexplicitly conveyed by a speaker. While recent large language models show\nremarkable advances in inference tasks, their performance in inductive\nreasoning, where not all information is present in the context, is far behind\ndeductive reasoning. In this paper, we analyze the behavior of the models based\non the task difficulty defined by the semantic information gap -- which\ndistinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).\nOur analysis reveals that the disparity in information between dialogue\ncontexts and desired inferences poses a significant challenge to the inductive\ninference process. To mitigate this information gap, we investigate a\ncontrastive learning approach by feeding negative samples. Our experiments\nsuggest negative samples help models understand what is wrong and improve their\ninference generations.\n","authors":["Etsuko Ishii","Yan Xu","Bryan Wilie","Ziwei Ji","Holy Lovenia","Willy Chung","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2310.12467v2.pdf","comment":"Accepted to EMNLP2023"},{"id":"http://arxiv.org/abs/2311.07066v1","updated":"2023-11-13T04:11:32Z","published":"2023-11-13T04:11:32Z","title":"Context Consistency between Training and Testing in Simultaneous Machine\n  Translation","summary":"  Simultaneous Machine Translation (SiMT) aims to yield a real-time partial\ntranslation with a monotonically growing the source-side context. However,\nthere is a counterintuitive phenomenon about the context usage between training\nand testing: e.g., the wait-k testing model consistently trained with wait-k is\nmuch worse than that model inconsistently trained with wait-k' (k' is not equal\nto k) in terms of translation quality. To this end, we first investigate the\nunderlying reasons behind this phenomenon and uncover the following two\nfactors: 1) the limited correlation between translation quality and training\n(cross-entropy) loss; 2) exposure bias between training and testing. Based on\nboth reasons, we then propose an effective training approach called context\nconsistency training accordingly, which makes consistent the context usage\nbetween training and testing by optimizing translation quality and latency as\nbi-objectives and exposing the predictions to the model during the training.\nThe experiments on three language pairs demonstrate our intuition: our system\nencouraging context consistency outperforms that existing systems with context\ninconsistency for the first time, with the help of our context consistency\ntraining approach.\n","authors":["Meizhi Zhong","Lemao Liu","Kehai Chen","Mingming Yang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07064v1","updated":"2023-11-13T04:08:49Z","published":"2023-11-13T04:08:49Z","title":"PROPANE: Prompt design as an inverse problem","summary":"  Carefully-designed prompts are key to inducing desired behavior in Large\nLanguage Models (LLMs). As a result, great effort has been dedicated to\nengineering prompts that guide LLMs toward particular behaviors. In this work,\nwe propose an automatic prompt optimization framework, PROPANE, which aims to\nfind a prompt that induces semantically similar outputs to a fixed set of\nexamples without user intervention. We further demonstrate that PROPANE can be\nused to (a) improve existing prompts, and (b) discover semantically obfuscated\nprompts that transfer between models.\n","authors":["Rimon Melamed","Lucas H. McCabe","Tanay Wakhare","Yejin Kim","H. Howie Huang","Enric Boix-Adsera"],"pdf_url":"https://arxiv.org/pdf/2311.07064v1.pdf","comment":"27 pages, 11 figures, preprint"},{"id":"http://arxiv.org/abs/2307.04349v2","updated":"2023-11-13T03:49:27Z","published":"2023-07-10T05:18:18Z","title":"RLTF: Reinforcement Learning from Unit Test Feedback","summary":"  The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, current representative works\neither rely solely on offline frameworks, limiting the exploration of new\nsample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these\nissues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,\na novel online RL framework with unit test feedback of multi-granularity for\nrefining code LLMs. Our approach generates data in real-time during training\nand simultaneously utilizes fine-grained feedback signals to guide the model\ntowards producing higher-quality code. Extensive experiments show that RLTF\nachieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.\n","authors":["Jiate Liu","Yiqin Zhu","Kaiwen Xiao","Qiang Fu","Xiao Han","Wei Yang","Deheng Ye"],"pdf_url":"https://arxiv.org/pdf/2307.04349v2.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2303.16445v2","updated":"2023-11-13T03:41:06Z","published":"2023-03-29T04:00:53Z","title":"Larger Probes Tell a Different Story: Extending Psycholinguistic\n  Datasets Via In-Context Learning","summary":"  Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.\n","authors":["Namrata Shivagunde","Vladislav Lialin","Anna Rumshisky"],"pdf_url":"https://arxiv.org/pdf/2303.16445v2.pdf","comment":"14 pages, 6 figures. Published as a conference paper at EMNLP 20223\n  (short). The datasets and code are available on this\n  $\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{URL}$"},{"id":"http://arxiv.org/abs/2311.07052v1","updated":"2023-11-13T03:36:18Z","published":"2023-11-13T03:36:18Z","title":"Towards the Law of Capacity Gap in Distilling Language Models","summary":"  Language model (LM) distillation is a trending area that aims to distil the\nknowledge resided in a large teacher LM to a small student one. While various\nmethods have been proposed to push the distillation to its limits, it is still\na pain distilling LMs when a large capacity gap is exhibited between the\nteacher and the student LMs. The pain is mainly resulted by the curse of\ncapacity gap, which describes that a larger teacher LM cannot always lead to a\nbetter student LM than one distilled from a smaller teacher LM due to the\naffect of capacity gap increment. That is, there is likely an optimal point\nyielding the best student LM along the scaling course of the teacher LM. Even\nworse, the curse of capacity gap can be only partly yet not fully lifted as\nindicated in previous studies.\n  However, the tale is not ever one-sided. Although a larger teacher LM has\nbetter performance than a smaller teacher LM, it is much more\nresource-demanding especially in the context of recent large LMs (LLMs).\nConsequently, instead of sticking to lifting the curse, leaving the curse as is\nshould be arguably fine. Even better, in this paper, we reveal that the optimal\ncapacity gap is almost consistent across different student scales and\narchitectures, fortunately turning the curse into the law of capacity gap. The\nlaw later guides us to distil a 3B student LM (termed MiniMA) from a 7B teacher\nLM (adapted LLaMA2-7B). MiniMA is demonstrated to yield a new\ncompute-performance pareto frontier among existing 3B LMs on commonly used\nbenchmarks, and its instruction-tuned version (termed MiniChat) outperforms a\nwide range of 3B competitors in GPT4 evaluation and could even compete with\nseveral 7B chat models.\n","authors":["Chen Zhang","Dawei Song","Zheyu Ye","Yan Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07052v1.pdf","comment":"22 pages, 8 figures, 12 tables, work in progress. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA"},{"id":"http://arxiv.org/abs/2311.04498v3","updated":"2023-11-13T03:35:23Z","published":"2023-11-08T07:15:05Z","title":"NExT-Chat: An LMM for Chat, Detection and Segmentation","summary":"  The development of large language models (LLMs) has greatly advanced the\nfield of multimodal understanding, leading to the emergence of large multimodal\nmodels (LMMs). In order to enhance the level of visual comprehension, recent\nstudies have equipped LMMs with region-level understanding capabilities by\nrepresenting object bounding box coordinates as a series of text sequences\n(pixel2seq). In this paper, we introduce a novel paradigm for object location\nmodeling called pixel2emb method, where we ask the LMM to output the location\nembeddings and then decoded by different decoders. This paradigm allows for\ndifferent location formats (such as bounding boxes and masks) to be used in\nmultimodal conversations Furthermore, this kind of embedding based location\nmodeling enables the utilization of existing practices in localization tasks,\nsuch as detection and segmentation. In scenarios with limited resources, our\npixel2emb demonstrates superior performance compared to existing\nstate-of-the-art (SOTA) approaches in both the location input and output tasks\nunder fair comparison. Leveraging the proposed pixel2emb method, we train an\nLMM named NExT-Chat and demonstrate its capability of handling multiple tasks\nlike visual grounding, region caption, and grounded reasoning.\n","authors":["Ao Zhang","Wei Ji","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.04498v3.pdf","comment":"Technical Report (project page: https://next-chatv.github.io/)"},{"id":"http://arxiv.org/abs/2310.14450v2","updated":"2023-11-13T03:22:32Z","published":"2023-10-22T23:23:44Z","title":"TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings","summary":"  Stance detection is important for understanding different attitudes and\nbeliefs on the Internet. However, given that a passage's stance toward a given\ntopic is often highly dependent on that topic, building a stance detection\nmodel that generalizes to unseen topics is difficult. In this work, we propose\nusing contrastive learning as well as an unlabeled dataset of news articles\nthat cover a variety of different topics to train topic-agnostic/TAG and\ntopic-aware/TAW embeddings for use in downstream stance detection. Combining\nthese embeddings in our full TATA model, we achieve state-of-the-art\nperformance across several public stance detection datasets (0.771 $F_1$-score\non the Zero-shot VAST dataset). We release our code and data at\nhttps://github.com/hanshanley/tata.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2310.14450v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.07037v1","updated":"2023-11-13T02:41:41Z","published":"2023-11-13T02:41:41Z","title":"Phonological Level wav2vec2-based Mispronunciation Detection and\n  Diagnosis Method","summary":"  The automatic identification and analysis of pronunciation errors, known as\nMispronunciation Detection and Diagnosis (MDD) plays a crucial role in Computer\nAided Pronunciation Learning (CAPL) tools such as Second-Language (L2) learning\nor speech therapy applications. Existing MDD methods relying on analysing\nphonemes can only detect categorical errors of phonemes that have an adequate\namount of training data to be modelled. With the unpredictable nature of the\npronunciation errors of non-native or disordered speakers and the scarcity of\ntraining datasets, it is unfeasible to model all types of mispronunciations.\nMoreover, phoneme-level MDD approaches have a limited ability to provide\ndetailed diagnostic information about the error made. In this paper, we propose\na low-level MDD approach based on the detection of speech attribute features.\nSpeech attribute features break down phoneme production into elementary\ncomponents that are directly related to the articulatory system leading to more\nformative feedback to the learner. We further propose a multi-label variant of\nthe Connectionist Temporal Classification (CTC) approach to jointly model the\nnon-mutually exclusive speech attributes using a single model. The pre-trained\nwav2vec2 model was employed as a core model for the speech attribute detector.\nThe proposed method was applied to L2 speech corpora collected from English\nlearners from different native languages. The proposed speech attribute MDD\nmethod was further compared to the traditional phoneme-level MDD and achieved a\nsignificantly lower False Acceptance Rate (FAR), False Rejection Rate (FRR),\nand Diagnostic Error Rate (DER) over all speech attributes compared to the\nphoneme-level equivalent.\n","authors":["Mostafa Shahin","Julien Epps","Beena Ahmed"],"pdf_url":"https://arxiv.org/pdf/2311.07037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10072v2","updated":"2023-11-13T02:38:51Z","published":"2023-10-16T05:09:16Z","title":"Fine-tuning ChatGPT for Automatic Scoring","summary":"  This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for\nautomatically scoring student written constructed responses using example\nassessment tasks in science education. Recent studies on OpenAI's generative\nmodel GPT-3.5 proved its superiority in predicting the natural language with\nhigh accuracy and human-like responses. GPT-3.5 has been trained over enormous\nonline language materials such as journals and Wikipedia; therefore, more than\ndirect usage of pre-trained GPT-3.5 is required for automatic scoring as\nstudents utilize a different language than trained material. These imply that a\ndomain-specific model, fine-tuned over data for specific tasks, can enhance\nmodel performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks\nwith a diverse dataset of middle-school and high-school student responses and\nexpert scoring. The six tasks comprise two multi-label and four multi-class\nassessment tasks. We compare the performance of fine-tuned GPT-3.5 with the\nfine-tuned state-of-the-art Google's generated language model, BERT. The\nresults show that in-domain training corpora constructed from science questions\nand responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5\nshows a remarkable average increase (9.1%) in automatic scoring accuracy (mean\n= 9.15, SD = 0.042) for the six tasks, p =0.001 < 0.05. Specifically, for\nmulti-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5\nachieved significantly higher scoring accuracy than BERT across all the labels,\nwith the second item achieving a 7.1% increase. The average scoring increase\nfor the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our\nstudy confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring\nof student responses on domain-specific data in education with high accuracy.\nWe have released fine-tuned models for public use and community engagement.\n","authors":["Ehsan Latif","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2310.10072v2.pdf","comment":"Submitted to Computers and Education: Artificial Intelligence"},{"id":"http://arxiv.org/abs/2311.07032v1","updated":"2023-11-13T02:31:16Z","published":"2023-11-13T02:31:16Z","title":"ExpNote: Black-box Large Language Models are Better Task Solvers with\n  Experience Notebook","summary":"  Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote\n","authors":["Wangtao Sun","Xuanqing Yu","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.07032v1.pdf","comment":"EMNLP 2023 findings"},{"id":"http://arxiv.org/abs/2311.07022v1","updated":"2023-11-13T02:13:13Z","published":"2023-11-13T02:13:13Z","title":"ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in\n  Video-Language Models","summary":"  With the ever-increasing popularity of pretrained Video-Language Models\n(VidLMs), there is a pressing need to develop robust evaluation methodologies\nthat delve deeper into their visio-linguistic capabilities. To address this\nchallenge, we present ViLMA (Video Language Model Assessment), a task-agnostic\nbenchmark that places the assessment of fine-grained capabilities of these\nmodels on a firm footing. Task-based evaluations, while valuable, fail to\ncapture the complexities and specific temporal aspects of moving images that\nVidLMs need to process. Through carefully curated counterfactuals, ViLMA offers\na controlled evaluation suite that sheds light on the true potential of these\nmodels, as well as their performance gaps compared to human-level\nunderstanding. ViLMA also includes proficiency tests, which assess basic\ncapabilities deemed essential to solving the main counterfactual tests. We show\nthat current VidLMs' grounding abilities are no better than those of\nvision-language models which use static images. This is especially striking\nonce the performance on proficiency tests is factored in. Our benchmark serves\nas a catalyst for future research on VidLMs, helping to highlight areas that\nstill need to be explored.\n","authors":["Ilker Kesen","Andrea Pedrotti","Mustafa Dogan","Michele Cafagna","Emre Can Acikgoz","Letitia Parcalabescu","Iacer Calixto","Anette Frank","Albert Gatt","Aykut Erdem","Erkut Erdem"],"pdf_url":"https://arxiv.org/pdf/2311.07022v1.pdf","comment":"Preprint. 48 pages, 22 figures, 10 tables"},{"id":"http://arxiv.org/abs/2311.07014v1","updated":"2023-11-13T01:53:12Z","published":"2023-11-13T01:53:12Z","title":"Teach me with a Whisper: Enhancing Large Language Models for Analyzing\n  Spoken Transcripts using Speech Embeddings","summary":"  Speech data has rich acoustic and paralinguistic information with important\ncues for understanding a speaker's tone, emotion, and intent, yet traditional\nlarge language models such as BERT do not incorporate this information. There\nhas been an increased interest in multi-modal language models leveraging audio\nand/or visual information and text. However, current multi-modal language\nmodels require both text and audio/visual data streams during inference/test\ntime. In this work, we propose a methodology for training language models\nleveraging spoken language audio data but without requiring the audio stream\nduring prediction time. This leads to an improved language model for analyzing\nspoken transcripts while avoiding an audio processing overhead at test time. We\nachieve this via an audio-language knowledge distillation framework, where we\ntransfer acoustic and paralinguistic information from a pre-trained speech\nembedding (OpenAI Whisper) teacher model to help train a student language model\non an audio-text dataset. In our experiments, the student model achieves\nconsistent improvement over traditional language models on tasks analyzing\nspoken transcripts.\n","authors":["Fatema Hasan","Yulong Li","James Foulds","Shimei Pan","Bishwaranjan Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2311.07014v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2305.14291v2","updated":"2023-11-13T01:41:43Z","published":"2023-05-23T17:34:37Z","title":"Evaluation of African American Language Bias in Natural Language\n  Generation","summary":"  We evaluate how well LLMs understand African American Language (AAL) in\ncomparison to their performance on White Mainstream English (WME), the\nencouraged \"standard\" form of English taught in American classrooms. We measure\nLLM performance using automatic metrics and human judgments for two tasks: a\ncounterpart generation task, where a model generates AAL (or WME) given WME (or\nAAL), and a masked span prediction (MSP) task, where models predict a phrase\nthat was removed from their input. Our contributions include: (1) evaluation of\nsix pre-trained, large language models on the two language generation tasks;\n(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop\nlyrics, focus groups, and linguistic interviews) with human-annotated\ncounterparts in WME; and (3) documentation of model performance gaps that\nsuggest bias and identification of trends in lack of understanding of AAL\nfeatures.\n","authors":["Nicholas Deas","Jessi Grieser","Shana Kleiner","Desmond Patton","Elsbeth Turcan","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2305.14291v2.pdf","comment":"EMNLP 2023 Camera-Ready"},{"id":"http://arxiv.org/abs/2311.07006v1","updated":"2023-11-13T01:25:30Z","published":"2023-11-13T01:25:30Z","title":"Context-dependent Instruction Tuning for Dialogue Response Generation","summary":"  Recent language models have achieved impressive performance in natural\nlanguage tasks by incorporating instructions with task input during\nfine-tuning. Since all samples in the same natural language task can be\nexplained with the same task instructions, many instruction datasets only\nprovide a few instructions for the entire task, without considering the input\nof each example in the task. However, this approach becomes ineffective in\ncomplex multi-turn dialogue generation tasks, where the input varies highly\nwith each turn as the dialogue context changes, so that simple task\ninstructions cannot improve the generation performance. To address this\nlimitation, we introduce a context-based instruction fine-tuning framework for\neach multi-turn dialogue which generates both responses and instructions based\non the previous context as input. During the evaluation, the model generates\ninstructions based on the previous context to self-guide the response. The\nproposed framework produces comparable or even outstanding results compared to\nthe baselines by aligning instructions to the input during fine-tuning with the\ninstructions in quantitative evaluations on dialogue benchmark datasets with\nreduced computation budget.\n","authors":["Jin Myung Kwak","Minseon Kim","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2311.07006v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2305.13583v4","updated":"2023-11-13T00:09:47Z","published":"2023-05-23T01:24:15Z","title":"Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical\n  Fusion for Multimodal Affect Recognition","summary":"  Fusing multiple modalities has proven effective for multimodal information\nprocessing. However, the incongruity between modalities poses a challenge for\nmultimodal fusion, especially in affect recognition. In this study, we first\nanalyze how the salient affective information in one modality can be affected\nby the other, and demonstrate that inter-modal incongruity exists latently in\ncrossmodal attention. Based on this finding, we propose the Hierarchical\nCrossmodal Transformer with Dynamic Modality Gating (HCT-DMG), a lightweight\nincongruity-aware model, which dynamically chooses the primary modality in each\ntraining batch and reduces fusion times by leveraging the learned hierarchy in\nthe latent space to alleviate incongruity. The experimental evaluation on five\nbenchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP (sentiment and emotion),\nwhere incongruity implicitly lies in hard samples, as well as UR-FUNNY (humour)\nand MUStaRD (sarcasm), where incongruity is common, verifies the efficacy of\nour approach, showing that HCT-DMG: 1) outperforms previous multimodal models\nwith a reduced size of approximately 0.8M parameters; 2) recognizes hard\nsamples where incongruity makes affect recognition difficult; 3) mitigates the\nincongruity at the latent level in crossmodal attention.\n","authors":["Yaoting Wang","Yuanchao Li","Paul Pu Liang","Louis-Philippe Morency","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2305.13583v4.pdf","comment":"*First two authors contributed equally"},{"id":"http://arxiv.org/abs/2311.07811v1","updated":"2023-11-13T23:52:43Z","published":"2023-11-13T23:52:43Z","title":"In-context Learning Generalizes, But Not Always Robustly: The Case of\n  Syntax","summary":"  In-context learning (ICL) is now a common method for supervising large\nlanguage models (LLMs): given labeled examples in the input context, the LLM\nlearns to perform the task without weight updates. Despite ICL's prevalence and\nutility, we understand little about whether models supervised in this manner\nrepresent the underlying structure of their tasks, rather than superficial\nheuristics that only generalize to identically distributed examples. In this\nstudy, we investigate the robustness of LLMs supervised via ICL using the test\ncase of sensitivity to syntax, which is a prerequisite for robust language\nunderstanding. Our experiments are based on two simple and well-controlled\nsyntactic transformations tasks, where correct out-of-distribution\ngeneralization requires an accurate syntactic analysis of the input. We further\ninvestigate whether out-of-distribution generalization can be improved via\nchain-of-thought prompting, where the model is provided with a sequence of\nintermediate computation steps that illustrate how the task ought to be\nperformed. In experiments with models from the GPT, PaLM, and Llama 2 families,\nwe find large variance across LMs on this fundamental linguistic phenomenon,\nand that the variance is explained more by the composition of the pre-training\ncorpus and supervision methods than by model size. In particular, we find\nevidence that models pre-trained on code generalize better, and benefit to a\ngreater extent from chain-of-thought prompting.\n","authors":["Aaron Mueller","Albert Webson","Jackson Petty","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2311.07811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07804v1","updated":"2023-11-13T23:36:35Z","published":"2023-11-13T23:36:35Z","title":"IruMozhi: Automatically classifying diglossia in Tamil","summary":"  Tamil, a Dravidian language of South Asia, is a highly diglossic language\nwith two very different registers in everyday use: Literary Tamil (preferred in\nwriting and formal communication) and Spoken Tamil (confined to speech and\ninformal media). Spoken Tamil is under-supported in modern NLP systems. In this\npaper, we release IruMozhi, a human-annotated dataset of parallel text in\nLiterary and Spoken Tamil. We train classifiers on the task of identifying\nwhich variety a text belongs to. We use these models to gauge the availability\nof pretraining data in Spoken Tamil, to audit the composition of existing\nlabelled datasets for Tamil, and to encourage future work on the variety.\n","authors":["Kabilan Prasanna","Aryaman Arora"],"pdf_url":"https://arxiv.org/pdf/2311.07804v1.pdf","comment":"4 pages main text, 7 total"},{"id":"http://arxiv.org/abs/2211.13854v3","updated":"2023-11-13T23:10:24Z","published":"2022-11-25T01:37:48Z","title":"ComCLIP: Training-Free Compositional Image and Text Matching","summary":"  Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.\n","authors":["Kenan Jiang","Xuehai He","Ruize Xu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.13854v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00176v2","updated":"2023-11-13T23:07:55Z","published":"2023-10-31T22:35:58Z","title":"ChipNeMo: Domain-Adapted LLMs for Chip Design","summary":"  ChipNeMo aims to explore the applications of large language models (LLMs) for\nindustrial chip design. Instead of directly deploying off-the-shelf commercial\nor open-source LLMs, we instead adopt the following domain adaptation\ntechniques: custom tokenizers, domain-adaptive continued pretraining,\nsupervised fine-tuning (SFT) with domain-specific instructions, and\ndomain-adapted retrieval models. We evaluate these methods on three selected\nLLM applications for chip design: an engineering assistant chatbot, EDA script\ngeneration, and bug summarization and analysis. Our results show that these\ndomain adaptation techniques enable significant LLM performance improvements\nover general-purpose base models across the three evaluated applications,\nenabling up to 5x model size reduction with similar or better performance on a\nrange of design tasks. Our findings also indicate that there's still room for\nimprovement between our current results and ideal outcomes. We believe that\nfurther investigation of domain-adapted LLM approaches will help close this gap\nin the future.\n","authors":["Mingjie Liu","Teodor-Dumitru Ene","Robert Kirby","Chris Cheng","Nathaniel Pinckney","Rongjian Liang","Jonah Alben","Himyanshu Anand","Sanmitra Banerjee","Ismet Bayraktaroglu","Bonita Bhaskaran","Bryan Catanzaro","Arjun Chaudhuri","Sharon Clay","Bill Dally","Laura Dang","Parikshit Deshpande","Siddhanth Dhodhi","Sameer Halepete","Eric Hill","Jiashang Hu","Sumit Jain","Brucek Khailany","Kishor Kunal","Xiaowei Li","Hao Liu","Stuart Oberman","Sujeet Omar","Sreedhar Pratty","Jonathan Raiman","Ambar Sarkar","Zhengjiang Shao","Hanfei Sun","Pratik P Suthar","Varun Tej","Kaizhe Xu","Haoxing Ren"],"pdf_url":"https://arxiv.org/pdf/2311.00176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07772v1","updated":"2023-11-13T21:42:38Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n","authors":["Tomer Bar Nathan","Gilad Deutch","Nadav Magar","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07767v1","updated":"2023-11-13T21:33:12Z","published":"2023-11-13T21:33:12Z","title":"GreekT5: A Series of Greek Sequence-to-Sequence Models for News\n  Summarization","summary":"  Text summarization (TS) is a natural language processing (NLP) subtask\npertaining to the automatic formulation of a concise and coherent summary that\ncovers the major concepts and topics from one or multiple documents. Recent\nadvancements in deep learning have led to the development of abstractive\nsummarization transformer-based models, which outperform classical approaches.\nIn any case, research in this field focuses on high resource languages such as\nEnglish, while the corresponding work for low resource languages is still\nunderdeveloped. Taking the above into account, this paper proposes a series of\nnovel TS models for Greek news articles. The proposed models were thoroughly\nevaluated on the same dataset against GreekBART, which is the state-of-the-art\nmodel in Greek abstractive news summarization. Our evaluation results reveal\nthat most of the proposed models significantly outperform GreekBART on various\nevaluation metrics. We make our evaluation code public, aiming to increase the\nreproducibility of this work and facilitate future research in the field.\n","authors":["Nikolaos Giarelis","Charalampos Mastrokostas","Nikos Karacapilidis"],"pdf_url":"https://arxiv.org/pdf/2311.07767v1.pdf","comment":"26 pages, 0 figures"},{"id":"http://arxiv.org/abs/2311.07766v1","updated":"2023-11-13T21:32:37Z","published":"2023-11-13T21:32:37Z","title":"Vision-Language Integration in Multimodal Video Transformers (Partially)\n  Aligns with the Brain","summary":"  Integrating information from multiple modalities is arguably one of the\nessential prerequisites for grounding artificial intelligence systems with an\nunderstanding of the real world. Recent advances in video transformers that\njointly learn from vision, text, and sound over time have made some progress\ntoward this goal, but the degree to which these models integrate information\nfrom modalities still remains unclear. In this work, we present a promising\napproach for probing a pre-trained multimodal video transformer model by\nleveraging neuroscientific evidence of multimodal information processing in the\nbrain. Using brain recordings of participants watching a popular TV show, we\nanalyze the effects of multi-modal connections and interactions in a\npre-trained multi-modal video transformer on the alignment with uni- and\nmulti-modal brain regions. We find evidence that vision enhances masked\nprediction performance during language processing, providing support that\ncross-modal representations in models can benefit individual modalities.\nHowever, we don't find evidence of brain-relevant information captured by the\njoint multi-modal transformer representations beyond that captured by all of\nthe individual modalities. We finally show that the brain alignment of the\npre-trained joint representation can be improved by fine-tuning using a task\nthat requires vision-language inferences. Overall, our results paint an\noptimistic picture of the ability of multi-modal transformers to integrate\nvision and language in partially brain-relevant ways but also show that\nimproving the brain alignment of these models may require new approaches.\n","authors":["Dota Tianai Dong","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2311.07766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07723v1","updated":"2023-11-13T20:07:36Z","published":"2023-11-13T20:07:36Z","title":"Generalization Analogies (GENIES): A Testbed for Generalizing AI\n  Oversight to Hard-To-Measure Domains","summary":"  As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENaralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n","authors":["Joshua Clymer","Garrett Baker","Rohan Subramani","Sam Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07723v1.pdf","comment":"Code: https://github.com/Joshuaclymer/GENIES Website:\n  https://joshuaclymer.github.io/generalization-analogies-website/"},{"id":"http://arxiv.org/abs/2311.07715v1","updated":"2023-11-13T19:56:18Z","published":"2023-11-13T19:56:18Z","title":"PolyIE: A Dataset of Information Extraction from Polymer Material\n  Scientific Literature","summary":"  Scientific information extraction (SciIE), which aims to automatically\nextract information from scientific literature, is becoming more important than\never. However, there are no existing SciIE datasets for polymer materials,\nwhich is an important class of materials used ubiquitously in our daily lives.\nTo bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer\nmaterials. POLYIE is curated from 146 full-length polymer scholarly articles,\nwhich are annotated with different named entities (i.e., materials, properties,\nvalues, conditions) as well as their N-ary relations by domain experts. POLYIE\npresents several unique challenges due to diverse lexical formats of entities,\nambiguity between entities, and variable-length relations. We evaluate\nstate-of-the-art named entity extraction and relation extraction models on\nPOLYIE, analyze their strengths and weaknesses, and highlight some difficult\ncases for these models. To the best of our knowledge, POLYIE is the first SciIE\nbenchmark for polymer materials, and we hope it will lead to more research\nefforts from the community on this challenging task. Our code and data are\navailable on: https://github.com/jerry3027/PolyIE.\n","authors":["Jerry Junyang Cheung","Yuchen Zhuang","Yinghao Li","Pranav Shetty","Wantian Zhao","Sanjeev Grampurohit","Rampi Ramprasad","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07715v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2305.14647v2","updated":"2023-11-13T19:47:35Z","published":"2023-05-24T02:33:35Z","title":"Scientific Opinion Summarization: Meta-review Generation with\n  Checklist-guided Iterative Introspection","summary":"  Opinions in the scientific domain can be divergent, leading to controversy or\nconsensus among reviewers. However, current opinion summarization datasets\nmostly focus on product review domains, which do not account for this\nvariability under the assumption that the input opinions are non-controversial.\nTo address this gap, we propose the task of scientific opinion summarization,\nwhere research paper reviews are synthesized into meta-reviews. To facilitate\nthis task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews\nand 40,903 paper reviews from 39 conferences. Furthermore, we propose the\nChecklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down\nthe task into several stages and iteratively refines the summary under the\nguidance of questions from a checklist. We conclude that (1) human-written\nsummaries are not always reliable since many do not follow the guidelines, and\n(2) the combination of task decomposition and iterative self-refinement shows\npromising discussion involvement ability and can be applied to other complex\ntext generation using black-box LLM.\n","authors":["Qi Zeng","Mankeerat Sidhu","Hou Pong Chan","Lu Wang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2305.14647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07703v1","updated":"2023-11-13T19:41:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that interlocutors who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans by answering the\nfollowing questions: 1) Do patterns of written and spoken entrainment in\nmonolingual settings generalize to code-switched settings? 2) Do patterns of\nentrainment on code-switching in generated text generalize to spontaneous\ncode-switched speech? We find evidence of affirmative answers to both of these\nquestions, with important implications for the potentially \"universal\" nature\nof entrainment as a communication phenomenon, and potential applications in\ninclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07700v1","updated":"2023-11-13T19:36:54Z","published":"2023-11-13T19:36:54Z","title":"AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language\n  Models Denoising","summary":"  Large language models (LLMs) have opened up enormous opportunities while\nsimultaneously posing ethical dilemmas. One of the major concerns is their\nability to create text that closely mimics human writing, which can lead to\npotential misuse, such as academic misconduct, disinformation, and fraud. To\naddress this problem, we present AuthentiGPT, an efficient classifier that\ndistinguishes between machine-generated and human-written texts. Under the\nassumption that human-written text resides outside the distribution of\nmachine-generated text, AuthentiGPT leverages a black-box LLM to denoise input\ntext with artificially added noise, and then semantically compares the denoised\ntext with the original to determine if the content is machine-generated. With\nonly one trainable parameter, AuthentiGPT eliminates the need for a large\ntraining dataset, watermarking the LLM's output, or computing the\nlog-likelihood. Importantly, the detection capability of AuthentiGPT can be\neasily adapted to any generative language model. With a 0.918 AUROC score on a\ndomain-specific dataset, AuthentiGPT demonstrates its effectiveness over other\ncommercial algorithms, highlighting its potential for detecting\nmachine-generated text in academic settings.\n","authors":["Zhen Guo","Shangdi Yu"],"pdf_url":"https://arxiv.org/pdf/2311.07700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07692v1","updated":"2023-11-13T19:21:25Z","published":"2023-11-13T19:21:25Z","title":"On The Truthfulness of 'Surprisingly Likely' Responses of Large Language\n  Models","summary":"  The surprisingly likely criterion in the seminal work of Prelec (the Bayesian\nTruth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,\nby rewarding rational agents to maximise the expected information gain with\ntheir answers w.r.t. their probabilistic beliefs. We investigate the relevance\nof a similar criterion for responses of LLMs. We hypothesize that if the\nsurprisingly likely criterion works in LLMs, under certain conditions, the\nresponses that maximize the reward under this criterion should be more accurate\nthan the responses that only maximize the posterior probability. Using\nbenchmarks including the TruthfulQA benchmark and using openly available LLMs:\nGPT-2 and LLaMA-2, we show that the method indeed improves the accuracy\nsignificantly (for example, upto 24 percentage points aggregate improvement on\nTruthfulQA and upto 70 percentage points improvement on individual categories\nof questions).\n","authors":["Naman Goel"],"pdf_url":"https://arxiv.org/pdf/2311.07692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07689v1","updated":"2023-11-13T19:13:29Z","published":"2023-11-13T19:13:29Z","title":"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming","summary":"  Red-teaming is a common practice for mitigating unsafe behaviors in Large\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\npotential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic\nred-teaming typically discovers safety risks without addressing them. In this\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\nincorporates both automatic adversarial prompt writing and safe response\ngeneration, significantly increasing red-teaming scalability and the safety of\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\nwith each other in an iterative manner, where the adversarial LLM aims to\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\nwhile the target LLM is fine-tuned with safety aligned data on these\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\non the updated target LLM, while the target LLM also improves itself through\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\nachieving comparable performance to LLMs with extensive adversarial prompt\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\nthroughout iterations, indicating the target LLM maintains strong performance\non instruction following.\n","authors":["Suyu Ge","Chunting Zhou","Rui Hou","Madian Khabsa","Yi-Chia Wang","Qifan Wang","Jiawei Han","Yuning Mao"],"pdf_url":"https://arxiv.org/pdf/2311.07689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07687v1","updated":"2023-11-13T19:12:49Z","published":"2023-11-13T19:12:49Z","title":"Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend\n  Actions in Text Games","summary":"  Large Language Models (LLMs) have demonstrated superior performance in\nlanguage understanding benchmarks. CALM, a popular approach, leverages\nlinguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to\nimprove the performance in text games in Jericho without environment-provided\nactions. However, CALM adapts GPT-2 with annotated human gameplays and keeps\nthe LLM fixed during the learning of the text based games. In this work, we\nexplore and evaluate updating LLM used for candidate recommendation during the\nlearning of the text based game as well to mitigate the reliance on the human\nannotated gameplays, which are costly to acquire. We observe that by updating\nthe LLM during learning using carefully selected in-game transitions, we can\nreduce the dependency on using human annotated game plays for fine-tuning the\nLLMs. We conducted further analysis to study the transferability of the updated\nLLMs and observed that transferring in-game trained models to other games did\nnot result in a consistent transfer.\n","authors":["Arjun Vaithilingam Sudhakar","Prasanna Parthasarathi","Janarthanan Rajendran","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2311.07687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07682v1","updated":"2023-11-13T19:02:56Z","published":"2023-11-13T19:02:56Z","title":"Fuse to Forget: Bias Reduction and Selective Memorization through Model\n  Fusion","summary":"  Model fusion research aims to aggregate the knowledge of multiple models to\nenhance performance by combining their weights. In this work, we study the\ninverse, investigating whether and how can model fusion interfere and reduce\nunwanted knowledge. We delve into the effects of model fusion on the evolution\nof learned shortcuts, social biases, and memorization capabilities in\nfine-tuned language models. Through several experiments covering text\nclassification and generation tasks, our analysis highlights that shared\nknowledge among models is usually enhanced during model fusion, while unshared\nknowledge is usually lost or forgotten. Based on this observation, we\ndemonstrate the potential of model fusion as a debiasing tool and showcase its\nefficacy in addressing privacy concerns associated with language models.\n","authors":["Kerem Zaman","Leshem Choshen","Shashank Srivastava"],"pdf_url":"https://arxiv.org/pdf/2311.07682v1.pdf","comment":"16 pages, 9 figures, 6 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2311.07575v1","updated":"2023-11-13T18:59:47Z","published":"2023-11-13T18:59:47Z","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for\n  Multi-modal Large Language Models","summary":"  We present SPHINX, a versatile multi-modal large language model (MLLM) with a\njoint mixing of model weights, tuning tasks, and visual embeddings. First, for\nstronger vision-language alignment, we unfreeze the large language model (LLM)\nduring pre-training, and introduce a weight mix strategy between LLMs trained\nby real-world and synthetic data. By directly integrating the weights from two\ndomains, the mixed LLM can efficiently incorporate diverse semantics with\nfavorable robustness. Then, to enable multi-purpose capabilities, we mix a\nvariety of tasks for joint visual instruction tuning, and design task-specific\ninstructions to avoid inter-task conflict. In addition to the basic visual\nquestion answering, we include more challenging tasks such as region-level\nunderstanding, caption grounding, document layout detection, and human pose\nestimation, contributing to mutual enhancement over different scenarios.\nAdditionally, we propose to extract comprehensive visual embeddings from\nvarious network architectures, pre-training paradigms, and information\ngranularity, providing language models with more robust image representations.\nBased on our proposed joint mixing, SPHINX exhibits superior multi-modal\nunderstanding capabilities on a wide range of applications. On top of this, we\nfurther propose an efficient strategy aiming to better capture fine-grained\nappearances of high-resolution images. With a mixing of different scales and\nhigh-resolution sub-images, SPHINX attains exceptional visual parsing and\nreasoning performance on existing evaluation benchmarks. We hope our work may\ncast a light on the exploration of joint mixing in future MLLM research. Code\nis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.\n","authors":["Ziyi Lin","Chris Liu","Renrui Zhang","Peng Gao","Longtian Qiu","Han Xiao","Han Qiu","Chen Lin","Wenqi Shao","Keqin Chen","Jiaming Han","Siyuan Huang","Yichi Zhang","Xuming He","Hongsheng Li","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2311.07575v1.pdf","comment":"Work in progress. Code and demos are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"id":"http://arxiv.org/abs/2311.07574v1","updated":"2023-11-13T18:59:31Z","published":"2023-11-13T18:59:31Z","title":"To See is to Believe: Prompting GPT-4V for Better Visual Instruction\n  Tuning","summary":"  Existing visual instruction tuning methods typically prompt large language\nmodels with textual descriptions to generate instruction-following data.\nDespite the promising performance achieved, these descriptions are derived from\nimage annotations, which are oftentimes coarse-grained. Furthermore, the\ninstructions might even contradict the visual content without observing the\nentire visual context. To address this challenge, we introduce a fine-grained\nvisual instruction dataset, LVIS-Instruct4V, which contains 220K visually\naligned and context-aware instructions produced by prompting the powerful\nGPT-4V with images from LVIS. Through experimental validation and case studies,\nwe demonstrate that high-quality visual instructional data could improve the\nperformance of LLaVA-1.5, a state-of-the-art large multimodal model, across a\nwide spectrum of benchmarks by clear margins. Notably, by simply replacing the\nLLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA\non most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet\n(40.2 vs. 35.4). We release our data and model at\nhttps://github.com/X2FD/LVIS-INSTRUCT4V.\n","authors":["Junke Wang","Lingchen Meng","Zejia Weng","Bo He","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.07574v1.pdf","comment":"techical report; work in progress"},{"id":"http://arxiv.org/abs/2308.13488v2","updated":"2023-11-13T18:56:23Z","published":"2023-08-25T16:55:30Z","title":"Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis\n  of Dynamic Contrast-enhanced Cardiac MRI Datasets","summary":"  Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is\na widely used modality for diagnosing myocardial blood flow (perfusion)\nabnormalities. During a typical free-breathing DCE-CMRI scan, close to 300\ntime-resolved images of myocardial perfusion are acquired at various contrast\n\"wash in/out\" phases. Manual segmentation of myocardial contours in each\ntime-frame of a DCE image series can be tedious and time-consuming,\nparticularly when non-rigid motion correction has failed or is unavailable.\nWhile deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI\ndatasets, a \"dynamic quality control\" (dQC) technique for reliably detecting\nfailed segmentations is lacking. Here we propose a new space-time uncertainty\nmetric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI\ndatasets by validating the proposed metric on an external dataset and\nestablishing a human-in-the-loop framework to improve the segmentation results.\nIn the proposed approach, we referred the top 10% most uncertain segmentations\nas detected by our dQC tool to the human expert for refinement. This approach\nresulted in a significant increase in the Dice score (p<0.001) and a notable\ndecrease in the number of images with failed segmentation (16.2% to 11.3%)\nwhereas the alternative approach of randomly selecting the same number of\nsegmentations for human referral did not achieve any significant improvement.\nOur results suggest that the proposed dQC framework has the potential to\naccurately identify poor-quality segmentations and may enable efficient\nDNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinical\ninterpretation and reporting of dynamic CMRI datasets.\n","authors":["Dilek M. Yalcinkaya","Khalid Youssef","Bobak Heydari","Orlando Simonetti","Rohan Dharmakumar","Subha Raman","Behzad Sharif"],"pdf_url":"https://arxiv.org/pdf/2308.13488v2.pdf","comment":"Accepted for publication in MICCAI 2023"},{"id":"http://arxiv.org/abs/2311.07562v1","updated":"2023-11-13T18:53:37Z","published":"2023-11-13T18:53:37Z","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone\n  GUI Navigation","summary":"  We present MM-Navigator, a GPT-4V-based agent for the smartphone graphical\nuser interface (GUI) navigation task. MM-Navigator can interact with a\nsmartphone screen as human users, and determine subsequent actions to fulfill\ngiven instructions. Our findings demonstrate that large multimodal models\n(LMMs), specifically GPT-4V, excel in zero-shot GUI navigation through its\nadvanced screen interpretation, action reasoning, and precise action\nlocalization capabilities. We first benchmark MM-Navigator on our collected iOS\nscreen dataset. According to human assessments, the system exhibited a 91\\%\naccuracy rate in generating reasonable action descriptions and a 75\\% accuracy\nrate in executing the correct actions for single-step instructions on iOS.\nAdditionally, we evaluate the model on a subset of an Android screen navigation\ndataset, where the model outperforms previous GUI navigators in a zero-shot\nfashion. Our benchmark and detailed analyses aim to lay a robust groundwork for\nfuture research into the GUI navigation task. The project page is at\nhttps://github.com/zzxslp/MM-Navigator.\n","authors":["An Yan","Zhengyuan Yang","Wanrong Zhu","Kevin Lin","Linjie Li","Jianfeng Wang","Jianwei Yang","Yiwu Zhong","Julian McAuley","Jianfeng Gao","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07562v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.07561v1","updated":"2023-11-13T18:53:30Z","published":"2023-11-13T18:53:30Z","title":"Fast Normalized Cross-Correlation for Template Matching with Rotations","summary":"  Normalized cross-correlation is the reference approach to carry out template\nmatching on images. When it is computed in Fourier space, it can handle\nefficiently template translations but it cannot do so with template rotations.\nIncluding rotations requires sampling the whole space of rotations, repeating\nthe computation of the correlation each time.\n  This article develops an alternative mathematical theory to handle\nefficiently, at the same time, rotations and translations. Our proposal has a\nreduced computational complexity because it does not require to repeatedly\nsample the space of rotations. To do so, we integrate the information relative\nto all rotated versions of the template into a unique symmetric tensor template\n-which is computed only once per template-. Afterward, we demonstrate that the\ncorrelation between the image to be processed with the independent tensor\ncomponents of the tensorial template contains enough information to recover\ntemplate instance positions and rotations.\n  Our proposed method has the potential to speed up conventional template\nmatching computations by a factor of several magnitude orders for the case of\n3D images.\n","authors":["José María Almira","Harold Phelippeau","Antonio Martinez-Sanchez"],"pdf_url":"https://arxiv.org/pdf/2311.07561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01455v2","updated":"2023-11-13T18:40:10Z","published":"2023-11-02T17:59:21Z","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning\n  via Generative Simulation","summary":"  We present RoboGen, a generative robotic agent that automatically learns\ndiverse robotic skills at scale via generative simulation. RoboGen leverages\nthe latest advancements in foundation and generative models. Instead of\ndirectly using or adapting these models to produce policies or low-level\nactions, we advocate for a generative scheme, which uses these models to\nautomatically generate diversified tasks, scenes, and training supervisions,\nthereby scaling up robotic skill learning with minimal human supervision. Our\napproach equips a robotic agent with a self-guided propose-generate-learn\ncycle: the agent first proposes interesting tasks and skills to develop, and\nthen generates corresponding simulation environments by populating pertinent\nobjects and assets with proper spatial configurations. Afterwards, the agent\ndecomposes the proposed high-level task into sub-tasks, selects the optimal\nlearning approach (reinforcement learning, motion planning, or trajectory\noptimization), generates required training supervision, and then learns\npolicies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried\nrepeatedly, producing an endless stream of skill demonstrations associated with\ndiverse tasks and environments.\n","authors":["Yufei Wang","Zhou Xian","Feng Chen","Tsun-Hsuan Wang","Yian Wang","Zackory Erickson","David Held","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2311.01455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07547v1","updated":"2023-11-13T18:36:50Z","published":"2023-11-13T18:36:50Z","title":"GPT-4V(ision) as A Social Media Analysis Engine","summary":"  Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.\n","authors":["Hanjia Lyu","Jinfa Huang","Daoan Zhang","Yongsheng Yu","Xinyi Mou","Jinsheng Pan","Zhengyuan Yang","Zhongyu Wei","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2311.07547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1902.11122v4","updated":"2023-11-13T18:25:42Z","published":"2019-02-22T10:09:11Z","title":"Deep Learning in Cardiology","summary":"  The medical field is creating large amount of data that physicians are unable\nto decipher and use efficiently. Moreover, rule-based expert systems are\ninefficient in solving complicated medical tasks or for creating insights using\nbig data. Deep learning has emerged as a more accurate and effective technology\nin a wide range of medical problems such as diagnosis, prediction and\nintervention. Deep learning is a representation learning method that consists\nof layers that transform the data non-linearly, thus, revealing hierarchical\nrelationships and structures. In this review we survey deep learning\napplication papers that use structured data, signal and imaging modalities from\ncardiology. We discuss the advantages and limitations of applying deep learning\nin cardiology that also apply in medicine in general, while proposing certain\ndirections as the most viable for clinical use.\n","authors":["Paschalis Bizopoulos","Dimitrios Koutsouris"],"pdf_url":"https://arxiv.org/pdf/1902.11122v4.pdf","comment":"27 pages, 2 figures, 10 tables"},{"id":"http://arxiv.org/abs/2307.10455v3","updated":"2023-11-13T18:10:23Z","published":"2023-07-19T20:54:08Z","title":"A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect\n  Dataset","summary":"  In an effort to catalog insect biodiversity, we propose a new large dataset\nof hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is\ntaxonomically classified by an expert, and also has associated genetic\ninformation including raw nucleotide barcode sequences and assigned barcode\nindex numbers, which are genetically-based proxies for species classification.\nThis paper presents a curated million-image dataset, primarily to train\ncomputer-vision models capable of providing image-based taxonomic assessment,\nhowever, the dataset also presents compelling characteristics, the study of\nwhich would be of interest to the broader machine learning community. Driven by\nthe biological nature inherent to the dataset, a characteristic long-tailed\nclass-imbalance distribution is exhibited. Furthermore, taxonomic labelling is\na hierarchical classification scheme, presenting a highly fine-grained\nclassification problem at lower levels. Beyond spurring interest in\nbiodiversity research within the machine learning community, progress on\ncreating an image-based taxonomic classifier will also further the ultimate\ngoal of all BIOSCAN research: to lay the foundation for a comprehensive survey\nof global biodiversity. This paper introduces the dataset and explores the\nclassification task through the implementation and analysis of a baseline\nclassifier.\n","authors":["Zahra Gharaee","ZeMing Gong","Nicholas Pellegrino","Iuliia Zarubiieva","Joakim Bruslund Haurum","Scott C. Lowe","Jaclyn T. A. McKeown","Chris C. Y. Ho","Joschka McLeod","Yi-Yun C Wei","Jireh Agda","Sujeevan Ratnasingham","Dirk Steinke","Angel X. Chang","Graham W. Taylor","Paul Fieguth"],"pdf_url":"https://arxiv.org/pdf/2307.10455v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07514v1","updated":"2023-11-13T17:56:54Z","published":"2023-11-13T17:56:54Z","title":"VGSG: Vision-Guided Semantic-Group Network for Text-based Person Search","summary":"  Text-based Person Search (TBPS) aims to retrieve images of target pedestrian\nindicated by textual descriptions. It is essential for TBPS to extract\nfine-grained local features and align them crossing modality. Existing methods\nutilize external tools or heavy cross-modal interaction to achieve explicit\nalignment of cross-modal fine-grained features, which is inefficient and\ntime-consuming. In this work, we propose a Vision-Guided Semantic-Group Network\n(VGSG) for text-based person search to extract well-aligned fine-grained visual\nand textual features. In the proposed VGSG, we develop a Semantic-Group Textual\nLearning (SGTL) module and a Vision-guided Knowledge Transfer (VGKT) module to\nextract textual local features under the guidance of visual local clues. In\nSGTL, in order to obtain the local textual representation, we group textual\nfeatures from the channel dimension based on the semantic cues of language\nexpression, which encourages similar semantic patterns to be grouped implicitly\nwithout external tools. In VGKT, a vision-guided attention is employed to\nextract visual-related textual features, which are inherently aligned with\nvisual cues and termed vision-guided textual features. Furthermore, we design a\nrelational knowledge transfer, including a vision-language similarity transfer\nand a class probability transfer, to adaptively propagate information of the\nvision-guided textual features to semantic-group textual features. With the\nhelp of relational knowledge transfer, VGKT is capable of aligning\nsemantic-group textual features with corresponding visual features without\nexternal tools and complex pairwise interaction. Experimental results on two\nchallenging benchmarks demonstrate its superiority over state-of-the-art\nmethods.\n","authors":["Shuting He","Hao Luo","Wei Jiang","Xudong Jiang","Henghui Ding"],"pdf_url":"https://arxiv.org/pdf/2311.07514v1.pdf","comment":"Accepted to IEEE TIP"},{"id":"http://arxiv.org/abs/2311.07485v1","updated":"2023-11-13T17:25:06Z","published":"2023-11-13T17:25:06Z","title":"EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient\n  Federated Learning","summary":"  Federated Learning (FL) is a decentralized machine learning paradigm that\nenables collaborative model training across dispersed nodes without having to\nforce individual nodes to share data. However, its broad adoption is hindered\nby the high communication costs of transmitting a large number of model\nparameters. This paper presents EvoFed, a novel approach that integrates\nEvolutionary Strategies (ES) with FL to address these challenges. EvoFed\nemploys a concept of 'fitness-based information sharing', deviating\nsignificantly from the conventional model-based FL. Rather than exchanging the\nactual updated model parameters, each node transmits a distance-based\nsimilarity measure between the locally updated model and each member of the\nnoise-perturbed model population. Each node, as well as the server, generates\nan identical population set of perturbed models in a completely synchronized\nfashion using the same random seeds. With properly chosen noise variance and\npopulation size, perturbed models can be combined to closely reflect the actual\nmodel updated using the local dataset, allowing the transmitted similarity\nmeasures (or fitness values) to carry nearly the complete information about the\nmodel parameters. As the population size is typically much smaller than the\nnumber of model parameters, the savings in communication load is large. The\nserver aggregates these fitness values and is able to update the global model.\nThis global fitness vector is then disseminated back to the nodes, each of\nwhich applies the same update to be synchronized to the global model. Our\nanalysis shows that EvoFed converges, and our experimental results validate\nthat at the cost of increased local processing loads, EvoFed achieves\nperformance comparable to FedAvg while reducing overall communication\nrequirements drastically in various practical settings.\n","authors":["Mohammad Mahdi Rahimi","Hasnain Irshad Bhatti","Younghyun Park","Humaira Kousar","Jaekyun Moon"],"pdf_url":"https://arxiv.org/pdf/2311.07485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16963v2","updated":"2023-11-13T17:21:27Z","published":"2023-05-26T14:19:17Z","title":"Semantic segmentation of sparse irregular point clouds for leaf/wood\n  discrimination","summary":"  LiDAR (Light Detection and Ranging) has become an essential part of the\nremote sensing toolbox used for biosphere monitoring. In particular, LiDAR\nprovides the opportunity to map forest leaf area with unprecedented accuracy,\nwhile leaf area has remained an important source of uncertainty affecting\nmodels of gas exchanges between the vegetation and the atmosphere. Unmanned\nAerial Vehicles (UAV) are easy to mobilize and therefore allow frequent\nrevisits to track the response of vegetation to climate change. However,\nminiature sensors embarked on UAVs usually provide point clouds of limited\ndensity, which are further affected by a strong decrease in density from top to\nbottom of the canopy due to progressively stronger occlusion. In such a\ncontext, discriminating leaf points from wood points presents a significant\nchallenge due in particular to strong class imbalance and spatially irregular\nsampling intensity. Here we introduce a neural network model based on the\nPointnet ++ architecture which makes use of point geometry only (excluding any\nspectral information). To cope with local data sparsity, we propose an\ninnovative sampling scheme which strives to preserve local important geometric\ninformation. We also propose a loss function adapted to the severe class\nimbalance. We show that our model outperforms state-of-the-art alternatives on\nUAV point clouds. We discuss future possible improvements, particularly\nregarding much denser point clouds acquired from below the canopy.\n","authors":["Yuchen Bai","Jean-Baptiste Durand","Florence Forbes","Grégoire Vincent"],"pdf_url":"https://arxiv.org/pdf/2305.16963v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07477v1","updated":"2023-11-13T17:11:35Z","published":"2023-11-13T17:11:35Z","title":"Temporal Performance Prediction for Deep Convolutional Long Short-Term\n  Memory Networks","summary":"  Quantifying predictive uncertainty of deep semantic segmentation networks is\nessential in safety-critical tasks. In applications like autonomous driving,\nwhere video data is available, convolutional long short-term memory networks\nare capable of not only providing semantic segmentations but also predicting\nthe segmentations of the next timesteps. These models use cell states to\nbroadcast information from previous data by taking a time series of inputs to\npredict one or even further steps into the future. We present a temporal\npostprocessing method which estimates the prediction performance of\nconvolutional long short-term memory networks by either predicting the\nintersection over union of predicted and ground truth segments or classifying\nbetween intersection over union being equal to zero or greater than zero. To\nthis end, we create temporal cell state-based input metrics per segment and\ninvestigate different models for the estimation of the predictive quality based\non these metrics. We further study the influence of the number of considered\ncell states for the proposed metrics.\n","authors":["Laura Fieback","Bidya Dash","Jakob Spiegelberg","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2311.07477v1.pdf","comment":"14 pages, 4 figures, this work is related to arXiv:1811.00648 and\n  arXiv:1911.05075"},{"id":"http://arxiv.org/abs/2311.07475v1","updated":"2023-11-13T17:09:57Z","published":"2023-11-13T17:09:57Z","title":"Masked Face Dataset Generation and Masked Face Recognition","summary":"  In the post-pandemic era, wearing face masks has posed great challenge to the\nordinary face recognition. In the previous study, researchers has applied\npretrained VGG16, and ResNet50 to extract features on the elaborate curated\nexisting masked face recognition (MFR) datasets, RMFRD and SMFRD. To make the\nmodel more adaptable to the real world situation where the sample size is\nsmaller and the camera environment has greater changes, we created a more\nchallenging masked face dataset ourselves, by selecting 50 identities with 1702\nimages from Labelled Faces in the Wild (LFW) Dataset, and simulated face masks\nthrough key point detection. The another part of our study is to solve the\nmasked face recognition problem, and we chose models by referring to the former\nstate of the art results, instead of directly using pretrained models, we fine\ntuned the model on our new dataset and use the last linear layer to do the\nclassification directly. Furthermore, we proposed using data augmentation\nstrategy to further increase the test accuracy, and fine tuned a new networks\nbeyond the former study, one of the most SOTA networks, Inception ResNet v1.\nThe best test accuracy on 50 identity MFR has achieved 95%.\n","authors":["Rui Cai","Xuying Ning","Peter N. Belhumeur"],"pdf_url":"https://arxiv.org/pdf/2311.07475v1.pdf","comment":"A good demonstration of masked face dataset generation method and\n  masked face recognition method"},{"id":"http://arxiv.org/abs/2309.17105v2","updated":"2023-11-13T16:37:36Z","published":"2023-09-29T10:06:28Z","title":"Continual Action Assessment via Task-Consistent Score-Discriminative\n  Feature Distribution Modeling","summary":"  Action Quality Assessment (AQA) is a task that tries to answer how well an\naction is carried out. While remarkable progress has been achieved, existing\nworks on AQA assume that all the training data are visible for training in one\ntime, but do not enable continual learning on assessing new technical actions.\nIn this work, we address such a Continual Learning problem in AQA\n(Continual-AQA), which urges a unified model to learn AQA tasks sequentially\nwithout forgetting. Our idea for modeling Continual-AQA is to sequentially\nlearn a task-consistent score-discriminative feature distribution, in which the\nlatent features express a strong correlation with the score labels regardless\nof the task or action types. From this perspective, we aim to mitigate the\nforgetting in Continual-AQA from two aspects. Firstly, to fuse the features of\nnew and previous data into a score-discriminative distribution, a novel\nFeature-Score Correlation-Aware Rehearsal is proposed to store and reuse data\nfrom previous tasks with limited memory size. Secondly, an Action\nGeneral-Specific Graph is developed to learn and decouple the action-general\nand action-specific knowledge so that the task-consistent score-discriminative\nfeatures can be better extracted across various tasks. Extensive experiments\nare conducted to evaluate the contributions of proposed components. The\ncomparisons with the existing continual learning methods additionally verify\nthe effectiveness and versatility of our approach.\n","authors":["Yuan-Ming Li","Ling-An Zeng","Jing-Ke Meng","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2309.17105v2.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.07453v1","updated":"2023-11-13T16:35:29Z","published":"2023-11-13T16:35:29Z","title":"ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World\n  Chart Images","summary":"  Data visualizations are common in the real-world. We often use them in data\nsources such as scientific documents, news articles, textbooks, and social\nmedia to summarize key information in a visual form. Charts can also mislead\nits audience by communicating false information or biasing them towards a\nspecific agenda. Verifying claims against charts is not a straightforward\nprocess. It requires analyzing both the text and visual components of the\nchart, considering characteristics such as colors, positions, and orientations.\nMoreover, to determine if a claim is supported by the chart content often\nrequires different types of reasoning. To address this challenge, we introduce\nChartCheck, a novel dataset for fact-checking against chart images. ChartCheck\nis the first large-scale dataset with 1.7k real-world charts and 10.5k\nhuman-written claims and explanations. We evaluated the dataset on\nstate-of-the-art models and achieved an accuracy of 73.9 in the finetuned\nsetting. Additionally, we identified chart characteristics and reasoning types\nthat challenge the models.\n","authors":["Mubashara Akhtar","Nikesh Subedi","Vivek Gupta","Sahar Tahmasebi","Oana Cocarascu","Elena Simperl"],"pdf_url":"https://arxiv.org/pdf/2311.07453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07449v1","updated":"2023-11-13T16:30:49Z","published":"2023-11-13T16:30:49Z","title":"Language Grounded QFormer for Efficient Vision Language Understanding","summary":"  Large-scale pretraining and instruction tuning have been successful for\ntraining general-purpose language models with broad competencies. However,\nextending to general-purpose vision-language models is challenging due to the\ndistributional diversity in visual inputs. A recent line of work explores\nvision-language instruction tuning, taking inspiration from the Query\nTransformer (QFormer) approach proposed in BLIP-2 models for bridging frozen\nmodalities. However, these approaches rely heavily on large-scale multi-modal\npretraining for representation learning before eventual finetuning, incurring a\nhuge computational overhead, poor scaling, and limited accessibility. To that\nend, we propose a more efficient method for QFormer-based vision-language\nalignment and demonstrate the effectiveness of our strategy compared to\nexisting baselines in improving the efficiency of vision-language pretraining.\n","authors":["Moulik Choraria","Nitesh Sekhar","Yue Wu","Xu Zhang","Prateek Singhal","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2311.07449v1.pdf","comment":"Preprint Under Review"},{"id":"http://arxiv.org/abs/2311.07446v1","updated":"2023-11-13T16:22:38Z","published":"2023-11-13T16:22:38Z","title":"Story-to-Motion: Synthesizing Infinite and Controllable Character\n  Animation from Long Text","summary":"  Generating natural human motion from a story has the potential to transform\nthe landscape of animation, gaming, and film industries. A new and challenging\ntask, Story-to-Motion, arises when characters are required to move to various\nlocations and perform specific motions based on a long text description. This\ntask demands a fusion of low-level control (trajectories) and high-level\ncontrol (motion semantics). Previous works in character control and\ntext-to-motion have addressed related aspects, yet a comprehensive solution\nremains elusive: character control methods do not handle text description,\nwhereas text-to-motion methods lack position constraints and often produce\nunstable motions. In light of these limitations, we propose a novel system that\ngenerates controllable, infinitely long motions and trajectories aligned with\nthe input text. (1) We leverage contemporary Large Language Models to act as a\ntext-driven motion scheduler to extract a series of (text, position, duration)\npairs from long text. (2) We develop a text-driven motion retrieval scheme that\nincorporates motion matching with motion semantic and trajectory constraints.\n(3) We design a progressive mask transformer that addresses common artifacts in\nthe transition motion such as unnatural pose and foot sliding. Beyond its\npioneering role as the first comprehensive solution for Story-to-Motion, our\nsystem undergoes evaluation across three distinct sub-tasks: trajectory\nfollowing, temporal action composition, and motion blending, where it\noutperforms previous state-of-the-art motion synthesis methods across the\nboard. Homepage: https://story2motion.github.io/.\n","authors":["Zhongfei Qing","Zhongang Cai","Zhitao Yang","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2311.07446v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.17010v3","updated":"2023-11-13T16:06:33Z","published":"2023-06-29T15:06:21Z","title":"milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human\n  Motion Sensing","summary":"  Approaching the era of ubiquitous computing, human motion sensing plays a\ncrucial role in smart systems for decision making, user interaction, and\npersonalized services. Extensive research has been conducted on human tracking,\npose estimation, gesture recognition, and activity recognition, which are\npredominantly based on cameras in traditional methods. However, the intrusive\nnature of cameras limits their use in smart home applications. To address this,\nmmWave radars have gained popularity due to their privacy-friendly features. In\nthis work, we propose milliFlow, a novel deep learning method for scene flow\nestimation as a complementary motion information for mmWave point cloud,\nserving as an intermediate level of features and directly benefiting downstream\nhuman motion sensing tasks. Experimental results demonstrate the superior\nperformance of our method with an average 3D endpoint error of 4.6cm,\nsignificantly surpassing the competing approaches. Furthermore, by\nincorporating scene flow information, we achieve remarkable improvements in\nhuman activity recognition, human parsing, and human body part tracking. To\nfoster further research in this area, we will provide our codebase and dataset\nfor open access upon acceptance.\n","authors":["Fangqiang Ding","Zhen Luo","Peijun Zhao","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2306.17010v3.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.07432v1","updated":"2023-11-13T16:04:41Z","published":"2023-11-13T16:04:41Z","title":"Supersampling of Data from Structured-light Scanner with Deep Learning","summary":"  This paper focuses on increasing the resolution of depth maps obtained from\n3D cameras using structured light technology. Two deep learning models FDSR and\nDKN are modified to work with high-resolution data, and data pre-processing\ntechniques are implemented for stable training. The models are trained on our\ncustom dataset of 1200 3D scans. The resulting high-resolution depth maps are\nevaluated using qualitative and quantitative metrics. The approach for depth\nmap upsampling offers benefits such as reducing the processing time of a\npipeline by first downsampling a high-resolution depth map, performing various\nprocessing steps at the lower resolution and upsampling the resulting depth map\nor increasing the resolution of a point cloud captured in lower resolution by a\ncheaper device. The experiments demonstrate that the FDSR model excels in terms\nof faster processing time, making it a suitable choice for applications where\nspeed is crucial. On the other hand, the DKN model provides results with higher\nprecision, making it more suitable for applications that prioritize accuracy.\n","authors":["Martin Melicherčík","Lukáš Gajdošech","Viktor Kocur","Martin Madaras"],"pdf_url":"https://arxiv.org/pdf/2311.07432v1.pdf","comment":"Pubslished in 2023 World Symposium on Digital Intelligence for\n  Systems and Machines (DISA) Proceedings. Published version copyrighted by\n  IEEE, pre-print released in accordance with the copyright agreement"},{"id":"http://arxiv.org/abs/2311.07426v1","updated":"2023-11-13T16:00:16Z","published":"2023-11-13T16:00:16Z","title":"Optimising Human-AI Collaboration by Learning Convincing Explanations","summary":"  Machine learning models are being increasingly deployed to take, or assist in\ntaking, complicated and high-impact decisions, from quasi-autonomous vehicles\nto clinical decision support systems. This poses challenges, particularly when\nmodels have hard-to-detect failure modes and are able to take actions without\noversight. In order to handle this challenge, we propose a method for a\ncollaborative system that remains safe by having a human ultimately making\ndecisions, while giving the model the best opportunity to convince and debate\nthem with interpretable explanations. However, the most helpful explanation\nvaries among individuals and may be inconsistent across stated preferences. To\nthis end we develop an algorithm, Ardent, to efficiently learn a ranking\nthrough interaction and best assist humans complete a task. By utilising a\ncollaborative approach, we can ensure safety and improve performance while\naddressing transparency and accountability concerns. Ardent enables efficient\nand effective decision-making by adapting to individual preferences for\nexplanations, which we validate through extensive simulations alongside a user\nstudy involving a challenging image classification task, demonstrating\nconsistent improvement over competing systems.\n","authors":["Alex J. Chan","Alihan Huyuk","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2311.07426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07421v1","updated":"2023-11-13T15:57:17Z","published":"2023-11-13T15:57:17Z","title":"Robust semi-supervised segmentation with timestep ensembling diffusion\n  models","summary":"  Medical image segmentation is a challenging task, made more difficult by many\ndatasets' limited size and annotations. Denoising diffusion probabilistic\nmodels (DDPM) have recently shown promise in modelling the distribution of\nnatural images and were successfully applied to various medical imaging tasks.\nThis work focuses on semi-supervised image segmentation using diffusion models,\nparticularly addressing domain generalisation. Firstly, we demonstrate that\nsmaller diffusion steps generate latent representations that are more robust\nfor downstream tasks than larger steps. Secondly, we use this insight to\npropose an improved esembling scheme that leverages information-dense small\nsteps and the regularising effect of larger steps to generate predictions. Our\nmodel shows significantly better performance in domain-shifted settings while\nretaining competitive performance in-domain. Overall, this work highlights the\npotential of DDPMs for semi-supervised medical image segmentation and provides\ninsights into optimising their performance under domain shift.\n","authors":["Margherita Rosnati","Melanie Roschewitz","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2311.07421v1.pdf","comment":"Published at Machine Learning for Health (ML4H) 2023, presented at\n  Medical Imaging meets NeurIPS 2023 and Deep Generative Models for Health\n  Workshop NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07417v1","updated":"2023-11-13T15:54:27Z","published":"2023-11-13T15:54:27Z","title":"Mitigating Backdoors within Deep Neural Networks in Data-limited\n  Configuration","summary":"  As the capacity of deep neural networks (DNNs) increases, their need for huge\namounts of data significantly grows. A common practice is to outsource the\ntraining process or collect more data over the Internet, which introduces the\nrisks of a backdoored DNN. A backdoored DNN shows normal behavior on clean data\nwhile behaving maliciously once a trigger is injected into a sample at the test\ntime. In such cases, the defender faces multiple difficulties. First, the\navailable clean dataset may not be sufficient for fine-tuning and recovering\nthe backdoored DNN. Second, it is impossible to recover the trigger in many\nreal-world applications without information about it. In this paper, we\nformulate some characteristics of poisoned neurons. This backdoor\nsuspiciousness score can rank network neurons according to their activation\nvalues, weights, and their relationship with other neurons in the same layer.\nOur experiments indicate the proposed method decreases the chance of attacks\nbeing successful by more than 50% with a tiny clean dataset, i.e., ten clean\nsamples for the CIFAR-10 dataset, without significantly deteriorating the\nmodel's performance. Moreover, the proposed method runs three times as fast as\nbaselines.\n","authors":["Soroush Hashemifar","Saeed Parsa","Morteza Zakeri-Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2311.07417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07414v1","updated":"2023-11-13T15:50:25Z","published":"2023-11-13T15:50:25Z","title":"FIRST: A Million-Entry Dataset for Text-Driven Fashion Synthesis and\n  Design","summary":"  Text-driven fashion synthesis and design is an extremely valuable part of\nartificial intelligence generative content(AIGC), which has the potential to\npropel a tremendous revolution in the traditional fashion industry. To advance\nthe research on text-driven fashion synthesis and design, we introduce a new\ndataset comprising a million high-resolution fashion images with rich\nstructured textual(FIRST) descriptions. In the FIRST, there is a wide range of\nattire categories and each image-paired textual description is organized at\nmultiple hierarchical levels. Experiments on prevalent generative models\ntrained over FISRT show the necessity of FIRST. We invite the community to\nfurther develop more intelligent fashion synthesis and design systems that make\nfashion design more creative and imaginative based on our dataset. The dataset\nwill be released soon.\n","authors":["Zhen Huang","Yihao Li","Dong Pei","Jiapeng Zhou","Xuliang Ning","Jianlin Han","Xiaoguang Han","Xuejun Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07414v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.07407v1","updated":"2023-11-13T15:41:25Z","published":"2023-11-13T15:41:25Z","title":"Towards Automatic Honey Bee Flower-Patch Assays with Paint Marking\n  Re-Identification","summary":"  In this paper, we show that paint markings are a feasible approach to\nautomatize the analysis of behavioral assays involving honey bees in the field\nwhere marking has to be as lightweight as possible. We contribute a novel\ndataset for bees re-identification with paint-markings with 4392 images and 27\nidentities. Contrastive learning with a ResNet backbone and triplet loss led to\nidentity representation features with almost perfect recognition in closed\nsetting where identities are known in advance. Diverse experiments evaluate the\ncapability to generalize to separate IDs, and show the impact of using\ndifferent body parts for identification, such as using the unmarked abdomen\nonly. In addition, we show the potential to fully automate the visit detection\nand provide preliminary results of compute time for future real-time deployment\nin the field on an edge device.\n","authors":["Luke Meyers","Josué Rodríguez Cordero","Carlos Corrada Bravo","Fanfan Noel","José Agosto-Rivera","Tugrul Giray","Rémi Mégret"],"pdf_url":"https://arxiv.org/pdf/2311.07407v1.pdf","comment":"Paper 17, workshop \"CV4Animals: Computer Vision for Animal Behavior\n  Tracking and Modeling\", in conjunction with Computer Vision and Pattern\n  Recognition (CVPR 2023), June 18, 2023, Vancouver, Canada"},{"id":"http://arxiv.org/abs/2311.07398v1","updated":"2023-11-13T15:25:55Z","published":"2023-11-13T15:25:55Z","title":"Processing and Segmentation of Human Teeth from 2D Images using Weakly\n  Supervised Learning","summary":"  Teeth segmentation is an essential task in dental image analysis for accurate\ndiagnosis and treatment planning. While supervised deep learning methods can be\nutilized for teeth segmentation, they often require extensive manual annotation\nof segmentation masks, which is time-consuming and costly. In this research, we\npropose a weakly supervised approach for teeth segmentation that reduces the\nneed for manual annotation. Our method utilizes the output heatmaps and\nintermediate feature maps from a keypoint detection network to guide the\nsegmentation process. We introduce the TriDental dataset, consisting of 3000\noral cavity images annotated with teeth keypoints, to train a teeth keypoint\ndetection network. We combine feature maps from different layers of the\nkeypoint detection network, enabling accurate teeth segmentation without\nexplicit segmentation annotations. The detected keypoints are also used for\nfurther refinement of the segmentation masks. Experimental results on the\nTriDental dataset demonstrate the superiority of our approach in terms of\naccuracy and robustness compared to state-of-the-art segmentation methods. Our\nmethod offers a cost-effective and efficient solution for teeth segmentation in\nreal-world dental applications, eliminating the need for extensive manual\nannotation efforts.\n","authors":["Tomáš Kunzo","Viktor Kocur","Lukáš Gajdošech","Martin Madaras"],"pdf_url":"https://arxiv.org/pdf/2311.07398v1.pdf","comment":"Pubslished in 2023 World Symposium on Digital Intelligence for\n  Systems and Machines (DISA) Proceedings. Published version copyrighted by\n  IEEE, pre-print released in accordance with the copyright agreement"},{"id":"http://arxiv.org/abs/2311.07397v1","updated":"2023-11-13T15:25:42Z","published":"2023-11-13T15:25:42Z","title":"An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination\n  Evaluation","summary":"  Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucination, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of hallucination and task). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including object\nexistence, object attribute and object relation hallucination. Based on AMBER,\nwe design a low-cost and efficient evaluation pipeline. Additionally, we\nconduct a comprehensive evaluation and detailed analysis of mainstream MLLMs\nincluding GPT-4V(ision), and also give guideline suggestions for mitigating\nhallucinations. The data and code of AMBER are available at\nhttps://github.com/junyangwang0410/AMBER.\n","authors":["Junyang Wang","Yuhang Wang","Guohai Xu","Jing Zhang","Yukai Gu","Haitao Jia","Ming Yan","Ji Zhang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2311.07397v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.07390v1","updated":"2023-11-13T15:14:53Z","published":"2023-11-13T15:14:53Z","title":"Evaluating the Significance of Outdoor Advertising from Driver's\n  Perspective Using Computer Vision","summary":"  Outdoor advertising, such as roadside billboards, plays a significant role in\nmarketing campaigns but can also be a distraction for drivers, potentially\nleading to accidents. In this study, we propose a pipeline for evaluating the\nsignificance of roadside billboards in videos captured from a driver's\nperspective. We have collected and annotated a new BillboardLamac dataset,\ncomprising eight videos captured by drivers driving through a predefined path\nwearing eye-tracking devices. The dataset includes annotations of billboards,\nincluding 154 unique IDs and 155 thousand bounding boxes, as well as eye\nfixation data. We evaluate various object tracking methods in combination with\na YOLOv8 detector to identify billboard advertisements with the best approach\nachieving 38.5 HOTA on BillboardLamac. Additionally, we train a random forest\nclassifier to classify billboards into three classes based on the length of\ndriver fixations achieving 75.8% test accuracy. An analysis of the trained\nclassifier reveals that the duration of billboard visibility, its saliency, and\nsize are the most influential features when assessing billboard significance.\n","authors":["Zuzana Černeková","Zuzana Berger Haladová","Ján Špirka","Viktor Kocur"],"pdf_url":"https://arxiv.org/pdf/2311.07390v1.pdf","comment":"Pubslished in 2023 World Symposium on Digital Intelligence for\n  Systems and Machines (DISA) Proceedings. Published version copyrighted by\n  IEEE, pre-print released in accordance with the copyright agreement"},{"id":"http://arxiv.org/abs/2311.05698v2","updated":"2023-11-13T14:53:10Z","published":"2023-11-09T19:15:12Z","title":"Mirasol3B: A Multimodal Autoregressive model for time-aligned and\n  contextual modalities","summary":"  One of the main challenges of multimodal learning is the need to combine\nheterogeneous modalities (e.g., video, audio, text). For example, video and\naudio are obtained at much higher rates than text and are roughly aligned in\ntime. They are often not synchronized with text, which comes as a global\ncontext, e.g., a title, or a description. Furthermore, video and audio inputs\nare of much larger volumes, and grow as the video length increases, which\nnaturally requires more compute dedicated to these modalities and makes\nmodeling of long-range dependencies harder.\n  We here decouple the multimodal modeling, dividing it into separate, focused\nautoregressive models, processing the inputs according to the characteristics\nof the modalities. We propose a multimodal model, called Mirasol3B, consisting\nof an autoregressive component for the time-synchronized modalities (audio and\nvideo), and an autoregressive component for the context modalities which are\nnot necessarily aligned in time but are still sequential. To address the\nlong-sequences of the video-audio inputs, we propose to further partition the\nvideo and audio sequences in consecutive snippets and autoregressively process\ntheir representations. To that end, we propose a Combiner mechanism, which\nmodels the audio-video information jointly within a timeframe. The Combiner\nlearns to extract audio and video features from raw spatio-temporal signals,\nand then learns to fuse these features producing compact but expressive\nrepresentations per snippet.\n  Our approach achieves the state-of-the-art on well established multimodal\nbenchmarks, outperforming much larger models. It effectively addresses the high\ncomputational demand of media inputs by both learning compact representations,\ncontrolling the sequence length of the audio-video feature representations, and\nmodeling their dependencies in time.\n","authors":["AJ Piergiovanni","Isaac Noble","Dahun Kim","Michael S. Ryoo","Victor Gomes","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2311.05698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07370v1","updated":"2023-11-13T14:36:29Z","published":"2023-11-13T14:36:29Z","title":"Classification of developmental and brain disorders via graph\n  convolutional aggregation","summary":"  While graph convolution based methods have become the de-facto standard for\ngraph representation learning, their applications to disease prediction tasks\nremain quite limited, particularly in the classification of neurodevelopmental\nand neurodegenerative brain disorders. In this paper, we introduce an\naggregator normalization graph convolutional network by leveraging aggregation\nin graph sampling, as well as skip connections and identity mapping. The\nproposed model learns discriminative graph node representations by\nincorporating both imaging and non-imaging features into the graph nodes and\nedges, respectively, with the aim of augmenting predictive capabilities and\nproviding a holistic perspective on the underlying mechanisms of brain\ndisorders. Skip connections enable the direct flow of information from the\ninput features to later layers of the network, while identity mapping helps\nmaintain the structural information of the graph during feature learning. We\nbenchmark our model against several recent baseline methods on two large\ndatasets, Autism Brain Imaging Data Exchange (ABIDE) and Alzheimer's Disease\nNeuroimaging Initiative (ADNI), for the prediction of autism spectrum disorder\nand Alzheimer's disease, respectively. Experimental results demonstrate the\ncompetitive performance of our approach in comparison with recent baselines in\nterms of several evaluation metrics, achieving relative improvements of 50% and\n13.56% in classification accuracy over graph convolutional networks on ABIDE\nand ADNI, respectively.\n","authors":["Ibrahim Salim","A. Ben Hamza"],"pdf_url":"https://arxiv.org/pdf/2311.07370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01368v2","updated":"2023-11-13T14:31:18Z","published":"2022-12-02T18:51:10Z","title":"Fast Non-Rigid Radiance Fields from Monocularized Data","summary":"  The reconstruction and novel view synthesis of dynamic scenes recently gained\nincreased attention. As reconstruction from large-scale multi-view data\ninvolves immense memory and computational requirements, recent benchmark\ndatasets provide collections of single monocular views per timestamp sampled\nfrom multiple (virtual) cameras. We refer to this form of inputs as\n\"monocularized\" data. Existing work shows impressive results for synthetic\nsetups and forward-facing real-world data, but is often limited in the training\nspeed and angular range for generating novel views. This paper addresses these\nlimitations and proposes a new method for full 360{\\deg} inward-facing novel\nview synthesis of non-rigidly deforming scenes. At the core of our method are:\n1) An efficient deformation module that decouples the processing of spatial and\ntemporal information for accelerated training and inference; and 2) A static\nmodule representing the canonical scene as a fast hash-encoded neural radiance\nfield. In addition to existing synthetic monocularized data, we systematically\nanalyze the performance on real-world inward-facing scenes using a newly\nrecorded challenging dataset sampled from a synchronized large-scale multi-view\nrig. In both cases, our method is significantly faster than previous methods,\nconverging in less than 7 minutes and achieving real-time framerates at 1K\nresolution, while obtaining a higher visual accuracy for generated novel views.\nOur source code and data is available at our project page\nhttps://graphics.tu-bs.de/publications/kappel2022fast.\n","authors":["Moritz Kappel","Vladislav Golyanik","Susana Castillo","Christian Theobalt","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2212.01368v2.pdf","comment":"18 pages, 14 figures; project page:\n  https://graphics.tu-bs.de/publications/kappel2022fast"},{"id":"http://arxiv.org/abs/2311.07362v1","updated":"2023-11-13T14:26:24Z","published":"2023-11-13T14:26:24Z","title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback\n  Guided Revision","summary":"  Large multimodal models (LMMs) suffer from multimodal hallucination, where\nthey provide incorrect responses misaligned with the given visual information.\nRecent works have conjectured that one of the reasons behind multimodal\nhallucination might be due to the vision encoder failing to ground on the image\nproperly. To mitigate this issue, we propose a novel approach that leverages\nself-feedback as visual cues. Building on this approach, we introduce Volcano,\na multimodal self-feedback guided revision model. Volcano generates natural\nlanguage feedback to its initial response based on the provided visual\ninformation and utilizes this feedback to self-revise its initial response.\nVolcano effectively reduces multimodal hallucination and achieves\nstate-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general\nmultimodal abilities and outperforms previous models on MM-Vet and MMBench.\nThrough a qualitative analysis, we show that Volcano's feedback is properly\ngrounded on the image than the initial response. This indicates that Volcano\ncan provide itself with richer visual information, helping alleviate multimodal\nhallucination. We publicly release Volcano models of 7B and 13B sizes along\nwith the data and code at https://github.com/kaistAI/Volcano.\n","authors":["Seongyun Lee","Sue Hyun Park","Yongrae Jo","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.07362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07357v1","updated":"2023-11-13T14:21:55Z","published":"2023-11-13T14:21:55Z","title":"Registered and Segmented Deformable Object Reconstruction from a Single\n  View Point Cloud","summary":"  In deformable object manipulation, we often want to interact with specific\nsegments of an object that are only defined in non-deformed models of the\nobject. We thus require a system that can recognize and locate these segments\nin sensor data of deformed real world objects. This is normally done using\ndeformable object registration, which is problem specific and complex to tune.\nRecent methods utilize neural occupancy functions to improve deformable object\nregistration by registering to an object reconstruction. Going one step\nfurther, we propose a system that in addition to reconstruction learns\nsegmentation of the reconstructed object. As the resulting output already\ncontains the information about the segments, we can skip the registration\nprocess. Tested on a variety of deformable objects in simulation and the real\nworld, we demonstrate that our method learns to robustly find these segments.\nWe also introduce a simple sampling algorithm to generate better training data\nfor occupancy learning.\n","authors":["Pit Henrich","Balázs Gyenes","Paul Maria Scheikl","Gerhard Neumann","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2311.07357v1.pdf","comment":"Accepted at WACV 2024"},{"id":"http://arxiv.org/abs/2311.00562v2","updated":"2023-11-13T14:21:49Z","published":"2023-11-01T14:59:41Z","title":"MNN: Mixed Nearest-Neighbors for Self-Supervised Learning","summary":"  In contrastive self-supervised learning, positive samples are typically drawn\nfrom the same image but in different augmented views, resulting in a relatively\nlimited source of positive samples. An effective way to alleviate this problem\nis to incorporate the relationship between samples, which involves including\nthe top-K nearest neighbors of positive samples. However, the problem of false\nneighbors (i.e., neighbors that do not belong to the same category as the\npositive sample) is an objective but often overlooked challenge due to the\nquery of neighbor samples without supervision information. In this paper, we\npresent a simple self-supervised learning framework called Mixed\nNearest-Neighbors for Self-Supervised Learning (MNN). MNN optimizes the\ninfluence of neighbor samples on the semantics of positive samples through an\nintuitive weighting approach and image mixture operations. The results\ndemonstrate that MNN exhibits exceptional generalization performance and\ntraining efficiency on four benchmark datasets.\n","authors":["Xianzhong Long","Chen Peng","Yun Li"],"pdf_url":"https://arxiv.org/pdf/2311.00562v2.pdf","comment":"31 pages, 7 figures, source code and pretrained models are available\n  https://github.com/pc-cp/MNN"},{"id":"http://arxiv.org/abs/2311.07348v1","updated":"2023-11-13T14:06:44Z","published":"2023-11-13T14:06:44Z","title":"Deformable Groupwise Registration Using a Locally Low-Rank Dissimilarity\n  Metric for Myocardial Strain Estimation from Cardiac Cine MRI Images","summary":"  Objective: Cardiovascular magnetic resonance-feature tracking (CMR-FT)\nrepresents a group of methods for myocardial strain estimation from cardiac\ncine MRI images. Established CMR-FT methods are mainly based on optical flow or\npairwise registration. However, these methods suffer from either inaccurate\nestimation of large motion or drift effect caused by accumulative tracking\nerrors. In this work, we propose a deformable groupwise registration method\nusing a locally low-rank (LLR) dissimilarity metric for CMR-FT. Methods: The\nproposed method (Groupwise-LLR) tracks the feature points by a groupwise\nregistration-based two-step strategy. Unlike the globally low-rank (GLR)\ndissimilarity metric, the proposed LLR metric imposes low-rankness on local\nimage patches rather than the whole image. We quantitatively compared\nGroupwise-LLR with the Farneback optical flow, a pairwise registration method,\nand a GLR-based groupwise registration method on simulated and in vivo\ndatasets. Results: Results from the simulated dataset showed that Groupwise-LLR\nachieved more accurate tracking and strain estimation compared with the other\nmethods. Results from the in vivo dataset showed that Groupwise-LLR achieved\nmore accurate tracking and elimination of the drift effect in late-diastole.\nInter-observer reproducibility of strain estimates was similar between all\nstudied methods. Conclusion: The proposed method estimates myocardial strains\nmore accurately due to the application of a groupwise registration-based\ntracking strategy and an LLR-based dissimilarity metric. Significance: The\nproposed CMR-FT method may facilitate more accurate estimation of myocardial\nstrains, especially in diastole, for clinical assessments of cardiac\ndysfunction.\n","authors":["Haiyang Chen","Juan Gao","Chenxi Hu"],"pdf_url":"https://arxiv.org/pdf/2311.07348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03062v3","updated":"2023-11-13T14:00:24Z","published":"2022-06-07T07:27:28Z","title":"Object Scan Context: Object-centric Spatial Descriptor for Place\n  Recognition within 3D Point Cloud Map","summary":"  The integration of a SLAM algorithm with place recognition technology\nempowers it with the ability to mitigate accumulated errors and to relocalize\nitself. However, existing methods for point cloud-based place recognition\npredominantly rely on the matching of descriptors, which are mostly\nlidar-centric. These methods suffer from two major drawbacks: first, they\ncannot perform place recognition when the distance between two point clouds is\nsignificant, and second, they can only calculate the rotation angle without\nconsidering the offset in the X and Y directions. To overcome these\nlimitations, we propose a novel local descriptor that is constructed around the\nMain Object. By using a geometric method, we can accurately calculate the\nrelative pose. We have provided a theoretical analysis to demonstrate that this\nmethod can overcome the aforementioned limitations. Furthermore, we conducted\nextensive experiments on KITTI Odometry and KITTI360, which indicate that our\nproposed method has significant advantages over state-of-the-art methods.\n","authors":["Haodong Yuan","Yudong Zhang","Shengyin Fan","Xue Li","Jian Wang"],"pdf_url":"https://arxiv.org/pdf/2206.03062v3.pdf","comment":"9 pages, 11 figures"},{"id":"http://arxiv.org/abs/2311.07321v1","updated":"2023-11-13T13:20:54Z","published":"2023-11-13T13:20:54Z","title":"Connecting the Dots: Graph Neural Network Powered Ensemble and\n  Classification of Medical Images","summary":"  Deep learning models have demonstrated remarkable results for various\ncomputer vision tasks, including the realm of medical imaging. However, their\napplication in the medical domain is limited due to the requirement for large\namounts of training data, which can be both challenging and expensive to\nobtain. To mitigate this, pre-trained models have been fine-tuned on\ndomain-specific data, but such an approach can suffer from inductive biases.\nFurthermore, deep learning models struggle to learn the relationship between\nspatially distant features and their importance, as convolution operations\ntreat all pixels equally. Pioneering a novel solution to this challenge, we\nemploy the Image Foresting Transform to optimally segment images into\nsuperpixels. These superpixels are subsequently transformed into\ngraph-structured data, enabling the proficient extraction of features and\nmodeling of relationships using Graph Neural Networks (GNNs). Our method\nharnesses an ensemble of three distinct GNN architectures to boost its\nrobustness. In our evaluations targeting pneumonia classification, our\nmethodology surpassed prevailing Deep Neural Networks (DNNs) in performance,\nall while drastically cutting down on the parameter count. This not only trims\ndown the expenses tied to data but also accelerates training and minimizes\nbias. Consequently, our proposition offers a sturdy, economically viable, and\nscalable strategy for medical image classification, significantly diminishing\ndependency on extensive training data sets.\n","authors":["Aryan Singh","Pepijn Van de Ven","Ciarán Eising","Patrick Denny"],"pdf_url":"https://arxiv.org/pdf/2311.07321v1.pdf","comment":"Our code is available at\n  https://github.com/aryan-at-ul/AICS_2023_submission"},{"id":"http://arxiv.org/abs/2310.05375v2","updated":"2023-11-13T13:14:50Z","published":"2023-10-09T03:11:08Z","title":"IPDreamer: Appearance-Controllable 3D Object Generation with Image\n  Prompts","summary":"  Recent advances in text-to-3D generation have been remarkable, with methods\nsuch as DreamFusion leveraging large-scale text-to-image diffusion-based models\nto supervise 3D generation. These methods, including the variational score\ndistillation proposed by ProlificDreamer, enable the synthesis of detailed and\nphotorealistic textured meshes. However, the appearance of 3D objects generated\nby these methods is often random and uncontrollable, posing a challenge in\nachieving appearance-controllable 3D objects. To address this challenge, we\nintroduce IPDreamer, a novel approach that incorporates image prompts to\nprovide specific and comprehensive appearance information for 3D object\ngeneration. Our results demonstrate that IPDreamer effectively generates\nhigh-quality 3D objects that are consistent with both the provided text and\nimage prompts, demonstrating its promising capability in\nappearance-controllable 3D object generation.\n","authors":["Bohan Zeng","Shanglin Li","Yutang Feng","Hong Li","Sicheng Gao","Jiaming Liu","Huaxia Li","Xu Tang","Jianzhuang Liu","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05375v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.07306v1","updated":"2023-11-13T12:52:29Z","published":"2023-11-13T12:52:29Z","title":"What Large Language Models Bring to Text-rich VQA?","summary":"  Text-rich VQA, namely Visual Question Answering based on text recognition in\nthe images, is a cross-modal task that requires both image comprehension and\ntext recognition. In this work, we focus on investigating the advantages and\nbottlenecks of LLM-based approaches in addressing this problem. To address the\nabove concern, we separate the vision and language modules, where we leverage\nexternal OCR models to recognize texts in the image and Large Language Models\n(LLMs) to answer the question given texts. The whole framework is training-free\nbenefiting from the in-context ability of LLMs. This pipeline achieved superior\nperformance compared to the majority of existing Multimodal Large Language\nModels (MLLM) on four text-rich VQA datasets. Besides, based on the ablation\nstudy, we find that LLM brings stronger comprehension ability and may introduce\nhelpful knowledge for the VQA problem. The bottleneck for LLM to address\ntext-rich VQA problems may primarily lie in visual part. We also combine the\nOCR module with MLLMs and pleasantly find that the combination of OCR module\nwith MLLM also works. It's worth noting that not all MLLMs can comprehend the\nOCR information, which provides insights into how to train an MLLM that\npreserves the abilities of LLM.\n","authors":["Xuejing Liu","Wei Tang","Xinzhe Ni","Jinghui Lu","Rui Zhao","Zechao Li","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2311.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07301v1","updated":"2023-11-13T12:44:14Z","published":"2023-11-13T12:44:14Z","title":"Dynamically Weighted Factor-Graph for Feature-based Geo-localization","summary":"  Feature-based geo-localization relies on associating features extracted from\naerial imagery with those detected by the vehicle's sensors. This requires that\nthe type of landmarks must be observable from both sources. This no-variety of\nfeature types generates poor representations that lead to outliers and\ndeviations, produced by ambiguities and lack of detections respectively. To\nmitigate these drawbacks, in this paper, we present a dynamically weighted\nfactor graph model for the vehicle's trajectory estimation. The weight\nadjustment in this implementation depends on information quantification in the\ndetections performed using a LiDAR sensor. Also, a prior (GNSS-based) error\nestimation is included in the model. Then, when the representation becomes\nambiguous or sparse, the weights are dynamically adjusted to rely on the\ncorrected prior trajectory, mitigating in this way outliers and deviations. We\ncompare our method against state-of-the-art geo-localization ones in a\nchallenging ambiguous environment, where we also cause detection losses. We\ndemonstrate mitigation of the mentioned drawbacks where the other methods fail.\n","authors":["Miguel Ángel Muñoz-Bañón","Alejandro Olivas","Edison Velasco-Sánchez","Francisco A. Candelas","Fernando Torres"],"pdf_url":"https://arxiv.org/pdf/2311.07301v1.pdf","comment":"This paper is under review at the journal \"IEEE Robotics and\n  Automation Letters\""},{"id":"http://arxiv.org/abs/2311.07285v1","updated":"2023-11-13T12:27:06Z","published":"2023-11-13T12:27:06Z","title":"Multi Sentence Description of Complex Manipulation Action Videos","summary":"  Automatic video description requires the generation of natural language\nstatements about the actions, events, and objects in the video. An important\nhuman trait, when we describe a video, is that we are able to do this with\nvariable levels of detail. Different from this, existing approaches for\nautomatic video descriptions are mostly focused on single sentence generation\nat a fixed level of detail. Instead, here we address video description of\nmanipulation actions where different levels of detail are required for being\nable to convey information about the hierarchical structure of these actions\nrelevant also for modern approaches of robot learning. We propose one hybrid\nstatistical and one end-to-end framework to address this problem. The hybrid\nmethod needs much less data for training, because it models statistically\nuncertainties within the video clips, while in the end-to-end method, which is\nmore data-heavy, we are directly connecting the visual encoder to the language\ndecoder without any intermediate (statistical) processing step. Both frameworks\nuse LSTM stacks to allow for different levels of description granularity and\nvideos can be described by simple single-sentences or complex multiple-sentence\ndescriptions. In addition, quantitative results demonstrate that these methods\nproduce more realistic descriptions than other competing approaches.\n","authors":["Fatemeh Ziaeetabar","Reza Safabakhsh","Saeedeh Momtazi","Minija Tamosiunaite","Florentin Wörgötter"],"pdf_url":"https://arxiv.org/pdf/2311.07285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07263v1","updated":"2023-11-13T12:02:46Z","published":"2023-11-13T12:02:46Z","title":"LT-ViT: A Vision Transformer for multi-label Chest X-ray classification","summary":"  Vision Transformers (ViTs) are widely adopted in medical imaging tasks, and\nsome existing efforts have been directed towards vision-language training for\nChest X-rays (CXRs). However, we envision that there still exists a potential\nfor improvement in vision-only training for CXRs using ViTs, by aggregating\ninformation from multiple scales, which has been proven beneficial for\nnon-transformer networks. Hence, we have developed LT-ViT, a transformer that\nutilizes combined attention between image tokens and randomly initialized\nauxiliary tokens that represent labels. Our experiments demonstrate that LT-ViT\n(1) surpasses the state-of-the-art performance using pure ViTs on two publicly\navailable CXR datasets, (2) is generalizable to other pre-training methods and\ntherefore is agnostic to model initialization, and (3) enables model\ninterpretability without grad-cam and its variants.\n","authors":["Umar Marikkar","Sara Atito","Muhammad Awais","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2311.07263v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07271v4","updated":"2023-11-13T11:58:00Z","published":"2023-03-09T20:09:15Z","title":"Provably Convergent Plug-and-Play Quasi-Newton Methods","summary":"  Plug-and-Play (PnP) methods are a class of efficient iterative methods that\naim to combine data fidelity terms and deep denoisers using classical\noptimization algorithms, such as ISTA or ADMM, with applications in inverse\nproblems and imaging. Provable PnP methods are a subclass of PnP methods with\nconvergence guarantees, such as fixed point convergence or convergence to\ncritical points of some energy function. Many existing provable PnP methods\nimpose heavy restrictions on the denoiser or fidelity function, such as\nnon-expansiveness or strict convexity, respectively. In this work, we propose a\nnovel algorithmic approach incorporating quasi-Newton steps into a provable PnP\nframework based on proximal denoisers, resulting in greatly accelerated\nconvergence while retaining light assumptions on the denoiser. By\ncharacterizing the denoiser as the proximal operator of a weakly convex\nfunction, we show that the fixed points of the proposed quasi-Newton PnP\nalgorithm are critical points of a weakly convex function. Numerical\nexperiments on image deblurring and super-resolution demonstrate 2--8x faster\nconvergence as compared to other provable PnP methods with similar\nreconstruction quality.\n","authors":["Hong Ye Tan","Subhadip Mukherjee","Junqi Tang","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2303.07271v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07261v1","updated":"2023-11-13T11:53:49Z","published":"2023-11-13T11:53:49Z","title":"Sketch-based Video Object Segmentation: Benchmark and Analysis","summary":"  Reference-based video object segmentation is an emerging topic which aims to\nsegment the corresponding target object in each video frame referred by a given\nreference, such as a language expression or a photo mask. However, language\nexpressions can sometimes be vague in conveying an intended concept and\nambiguous when similar objects in one frame are hard to distinguish by\nlanguage. Meanwhile, photo masks are costly to annotate and less practical to\nprovide in a real application. This paper introduces a new task of sketch-based\nvideo object segmentation, an associated benchmark, and a strong baseline. Our\nbenchmark includes three datasets, Sketch-DAVIS16, Sketch-DAVIS17 and\nSketch-YouTube-VOS, which exploit human-drawn sketches as an informative yet\nlow-cost reference for video object segmentation. We take advantage of STCN, a\npopular baseline of semi-supervised VOS task, and evaluate what the most\neffective design for incorporating a sketch reference is. Experimental results\nshow sketch is more effective yet annotation-efficient than other references,\nsuch as photo masks, language and scribble.\n","authors":["Ruolin Yang","Da Li","Conghui Hu","Timothy Hospedales","Honggang Zhang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2311.07261v1.pdf","comment":"BMVC 2023"},{"id":"http://arxiv.org/abs/2311.07247v1","updated":"2023-11-13T11:29:38Z","published":"2023-11-13T11:29:38Z","title":"Simultaneous Clutter Detection and Semantic Segmentation of Moving\n  Objects for Automotive Radar Data","summary":"  The unique properties of radar sensors, such as their robustness to adverse\nweather conditions, make them an important part of the environment perception\nsystem of autonomous vehicles. One of the first steps during the processing of\nradar point clouds is often the detection of clutter, i.e. erroneous points\nthat do not correspond to real objects. Another common objective is the\nsemantic segmentation of moving road users. These two problems are handled\nstrictly separate from each other in literature. The employed neural networks\nare always focused entirely on only one of the tasks. In contrast to this, we\nexamine ways to solve both tasks at the same time with a single jointly used\nmodel. In addition to a new augmented multi-head architecture, we also devise a\nmethod to represent a network's predictions for the two tasks with only one\noutput value. This novel approach allows us to solve the tasks simultaneously\nwith the same inference time as a conventional task-specific model. In an\nextensive evaluation, we show that our setup is highly effective and\noutperforms every existing network for semantic segmentation on the RadarScenes\ndataset.\n","authors":["Johannes Kopp","Dominik Kellner","Aldi Piroli","Vinzenz Dallabetta","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2311.07247v1.pdf","comment":"Published at IEEE International Conference of Intelligent\n  Transportation Systems (ITSC), Bilbao, ESP, 2023"},{"id":"http://arxiv.org/abs/2212.04155v3","updated":"2023-11-13T11:29:37Z","published":"2022-12-08T09:21:09Z","title":"Latent Graph Representations for Critical View of Safety Assessment","summary":"  Assessing the critical view of safety in laparoscopic cholecystectomy\nrequires accurate identification and localization of key anatomical structures,\nreasoning about their geometric relationships to one another, and determining\nthe quality of their exposure. Prior works have approached this task by\nincluding semantic segmentation as an intermediate step, using predicted\nsegmentation masks to then predict the CVS. While these methods are effective,\nthey rely on extremely expensive ground-truth segmentation annotations and tend\nto fail when the predicted segmentation is incorrect, limiting generalization.\nIn this work, we propose a method for CVS prediction wherein we first represent\na surgical image using a disentangled latent scene graph, then process this\nrepresentation using a graph neural network. Our graph representations\nexplicitly encode semantic information - object location, class information,\ngeometric relations - to improve anatomy-driven reasoning, as well as visual\nfeatures to retain differentiability and thereby provide robustness to semantic\nerrors. Finally, to address annotation cost, we propose to train our method\nusing only bounding box annotations, incorporating an auxiliary image\nreconstruction objective to learn fine-grained object boundaries. We show that\nour method not only outperforms several baseline methods when trained with\nbounding box annotations, but also scales effectively when trained with\nsegmentation masks, maintaining state-of-the-art performance.\n","authors":["Aditya Murali","Deepak Alapatt","Pietro Mascagni","Armine Vardazaryan","Alain Garcia","Nariaki Okamoto","Didier Mutter","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2212.04155v3.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.07235v1","updated":"2023-11-13T10:55:05Z","published":"2023-11-13T10:55:05Z","title":"DeepMetricEye: Metric Depth Estimation in Periocular VR Imagery","summary":"  Despite the enhanced realism and immersion provided by VR headsets, users\nfrequently encounter adverse effects such as digital eye strain (DES), dry eye,\nand potential long-term visual impairment due to excessive eye stimulation from\nVR displays and pressure from the mask. Recent VR headsets are increasingly\nequipped with eye-oriented monocular cameras to segment ocular feature maps.\nYet, to compute the incident light stimulus and observe periocular condition\nalterations, it is imperative to transform these relative measurements into\nmetric dimensions. To bridge this gap, we propose a lightweight framework\nderived from the U-Net 3+ deep learning backbone that we re-optimised, to\nestimate measurable periocular depth maps. Compatible with any VR headset\nequipped with an eye-oriented monocular camera, our method reconstructs\nthree-dimensional periocular regions, providing a metric basis for related\nlight stimulus calculation protocols and medical guidelines. Navigating the\ncomplexities of data collection, we introduce a Dynamic Periocular Data\nGeneration (DPDG) environment based on UE MetaHuman, which synthesises\nthousands of training images from a small quantity of human facial scan data.\nEvaluated on a sample of 36 participants, our method exhibited notable efficacy\nin the periocular global precision evaluation experiment, and the pupil\ndiameter measurement.\n","authors":["Yitong Sun","Zijian Zhou","Cyriel Diels","Ali Asadipour"],"pdf_url":"https://arxiv.org/pdf/2311.07235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07234v1","updated":"2023-11-13T10:54:53Z","published":"2023-11-13T10:54:53Z","title":"Multi-task learning for joint weakly-supervised segmentation and aortic\n  arch anomaly classification in fetal cardiac MRI","summary":"  Congenital Heart Disease (CHD) is a group of cardiac malformations present\nalready during fetal life, representing the prevailing category of birth\ndefects globally. Our aim in this study is to aid 3D fetal vessel topology\nvisualisation in aortic arch anomalies, a group which encompasses a range of\nconditions with significant anatomical heterogeneity. We present a multi-task\nframework for automated multi-class fetal vessel segmentation from 3D black\nblood T2w MRI and anomaly classification. Our training data consists of binary\nmanual segmentation masks of the cardiac vessels' region in individual subjects\nand fully-labelled anomaly-specific population atlases. Our framework combines\ndeep learning label propagation using VoxelMorph with 3D Attention U-Net\nsegmentation and DenseNet121 anomaly classification. We target 11 cardiac\nvessels and three distinct aortic arch anomalies, including double aortic arch,\nright aortic arch, and suspected coarctation of the aorta. We incorporate an\nanomaly classifier into our segmentation pipeline, delivering a multi-task\nframework with the primary motivation of correcting topological inaccuracies of\nthe segmentation. The hypothesis is that the multi-task approach will encourage\nthe segmenter network to learn anomaly-specific features. As a secondary\nmotivation, an automated diagnosis tool may have the potential to enhance\ndiagnostic confidence in a decision support setting. Our results showcase that\nour proposed training strategy significantly outperforms label propagation and\na network trained exclusively on propagated labels. Our classifier outperforms\na classifier trained exclusively on T2w volume images, with an average balanced\naccuracy of 0.99 (0.01) after joint training. Adding a classifier improves the\nanatomical and topological accuracy of all correctly classified double aortic\narch subjects.\n","authors":["Paula Ramirez","Alena Uus","Milou P. M. van Poppel","Irina Grigorescu","Johannes K. Steinweg","David F. A. Lloyd","Kuberan Pushparajah","Andrew P. King","Maria Deprez"],"pdf_url":"https://arxiv.org/pdf/2311.07234v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2023:015"},{"id":"http://arxiv.org/abs/2302.01602v2","updated":"2023-11-13T10:33:32Z","published":"2023-02-03T08:54:55Z","title":"A Feature Selection Method for Driver Stress Detection Using Heart Rate\n  Variability and Breathing Rate","summary":"  Driver stress is a major cause of car accidents and death worldwide.\nFurthermore, persistent stress is a health problem, contributing to\nhypertension and other diseases of the cardiovascular system. Stress has a\nmeasurable impact on heart and breathing rates and stress levels can be\ninferred from such measurements. Galvanic skin response is a common test to\nmeasure the perspiration caused by both physiological and psychological stress,\nas well as extreme emotions. In this paper, galvanic skin response is used to\nestimate the ground truth stress levels. A feature selection technique based on\nthe minimal redundancy-maximal relevance method is then applied to multiple\nheart rate variability and breathing rate metrics to identify a novel and\noptimal combination for use in detecting stress. The support vector machine\nalgorithm with a radial basis function kernel was used along with these\nfeatures to reliably predict stress. The proposed method has achieved a high\nlevel of accuracy on the target dataset.\n","authors":["Ashkan Parsi","David O'Callaghan","Joseph Lemley"],"pdf_url":"https://arxiv.org/pdf/2302.01602v2.pdf","comment":"In Proceedings of the 15th International Conference on Machine Vision\n  (ICMV), Rome, Italy, 18-20 November 2022. arXiv admin note: text overlap with\n  arXiv:2206.03222"},{"id":"http://arxiv.org/abs/2311.07216v1","updated":"2023-11-13T10:17:00Z","published":"2023-11-13T10:17:00Z","title":"Few Shot Learning for the Classification of Confocal Laser\n  Endomicroscopy Images of Head and Neck Tumors","summary":"  The surgical removal of head and neck tumors requires safe margins, which are\nusually confirmed intraoperatively by means of frozen sections. This method is,\nin itself, an oversampling procedure, which has a relatively low sensitivity\ncompared to the definitive tissue analysis on paraffin-embedded sections.\nConfocal laser endomicroscopy (CLE) is an in-vivo imaging technique that has\nshown its potential in the live optical biopsy of tissue. An automated analysis\nof this notoriously difficult to interpret modality would help surgeons.\nHowever, the images of CLE show a wide variability of patterns, caused both by\nindividual factors but also, and most strongly, by the anatomical structures of\nthe imaged tissue, making it a challenging pattern recognition task. In this\nwork, we evaluate four popular few shot learning (FSL) methods towards their\ncapability of generalizing to unseen anatomical domains in CLE images. We\nevaluate this on images of sinunasal tumors (SNT) from five patients and on\nimages of the vocal folds (VF) from 11 patients using a cross-validation\nscheme. The best respective approach reached a median accuracy of 79.6% on the\nrather homogeneous VF dataset, but only of 61.6% for the highly diverse SNT\ndataset. Our results indicate that FSL on CLE images is viable, but strongly\naffected by the number of patients, as well as the diversity of anatomical\npatterns.\n","authors":["Marc Aubreville","Zhaoya Pan","Matti Sievert","Jonas Ammeling","Jonathan Ganz","Nicolai Oetter","Florian Stelzle","Ann-Kathrin Frenken","Katharina Breininger","Miguel Goncalves"],"pdf_url":"https://arxiv.org/pdf/2311.07216v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2311.07213v1","updated":"2023-11-13T10:13:59Z","published":"2023-11-13T10:13:59Z","title":"A method for quantifying sectoral optic disc pallor in fundus\n  photographs and its association with peripapillary RNFL thickness","summary":"  Purpose: To develop an automatic method of quantifying optic disc pallor in\nfundus photographs and determine associations with peripapillary retinal nerve\nfibre layer (pRNFL) thickness.\n  Methods: We used deep learning to segment the optic disc, fovea, and vessels\nin fundus photographs, and measured pallor. We assessed the relationship\nbetween pallor and pRNFL thickness derived from optical coherence tomography\nscans in 118 participants. Separately, we used images diagnosed by clinical\ninspection as pale (N=45) and assessed how measurements compared to healthy\ncontrols (N=46). We also developed automatic rejection thresholds, and tested\nthe software for robustness to camera type, image format, and resolution.\n  Results: We developed software that automatically quantified disc pallor\nacross several zones in fundus photographs. Pallor was associated with pRNFL\nthickness globally (\\b{eta} = -9.81 (SE = 3.16), p < 0.05), in the temporal\ninferior zone (\\b{eta} = -29.78 (SE = 8.32), p < 0.01), with the nasal/temporal\nratio (\\b{eta} = 0.88 (SE = 0.34), p < 0.05), and in the whole disc (\\b{eta} =\n-8.22 (SE = 2.92), p < 0.05). Furthermore, pallor was significantly higher in\nthe patient group. Lastly, we demonstrate the analysis to be robust to camera\ntype, image format, and resolution.\n  Conclusions: We developed software that automatically locates and quantifies\ndisc pallor in fundus photographs and found associations between pallor\nmeasurements and pRNFL thickness.\n  Translational relevance: We think our method will be useful for the\nidentification, monitoring and progression of diseases characterized by disc\npallor/optic atrophy, including glaucoma, compression, and potentially in\nneurodegenerative disorders.\n","authors":["Samuel Gibbon","Graciela Muniz-Terrera","Fabian SL Yii","Charlene Hamid","Simon Cox","Ian JC Maccormick","Andrew J Tatham","Craig Ritchie","Emanuele Trucco","Baljean Dhillon","Thomas J MacGillivray"],"pdf_url":"https://arxiv.org/pdf/2311.07213v1.pdf","comment":"44 pages, 20 figures, 7 tables, submitted"},{"id":"http://arxiv.org/abs/2306.11977v2","updated":"2023-11-13T10:01:33Z","published":"2023-06-21T02:08:27Z","title":"Encoding Enhanced Complex CNN for Accurate and Highly Accelerated MRI","summary":"  Magnetic resonance imaging (MRI) using hyperpolarized noble gases provides a\nway to visualize the structure and function of human lung, but the long imaging\ntime limits its broad research and clinical applications. Deep learning has\ndemonstrated great potential for accelerating MRI by reconstructing images from\nundersampled data. However, most existing deep conventional neural networks\n(CNN) directly apply square convolution to k-space data without considering the\ninherent properties of k-space sampling, limiting k-space learning efficiency\nand image reconstruction quality. In this work, we propose an encoding enhanced\n(EN2) complex CNN for highly undersampled pulmonary MRI reconstruction. EN2\nemploys convolution along either the frequency or phase-encoding direction,\nresembling the mechanisms of k-space sampling, to maximize the utilization of\nthe encoding correlation and integrity within a row or column of k-space. We\nalso employ complex convolution to learn rich representations from the complex\nk-space data. In addition, we develop a feature-strengthened modularized unit\nto further boost the reconstruction performance. Experiments demonstrate that\nour approach can accurately reconstruct hyperpolarized 129Xe and 1H lung MRI\nfrom 6-fold undersampled k-space data and provide lung function measurements\nwith minimal biases compared with fully-sampled image. These results\ndemonstrate the effectiveness of the proposed algorithmic components and\nindicate that the proposed approach could be used for accelerated pulmonary MRI\nin research and clinical lung disease patient care.\n","authors":["Zimeng Li","Sa Xiao","Cheng Wang","Haidong Li","Xiuchao Zhao","Caohui Duan","Qian Zhou","Qiuchen Rao","Yuan Fang","Junshuai Xie","Lei Shi","Fumin Guo","Chaohui Ye","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2306.11977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16532v2","updated":"2023-11-13T09:48:03Z","published":"2023-07-31T09:53:50Z","title":"Echoes Beyond Points: Unleashing the Power of Raw Radar Data in\n  Multi-modality Fusion","summary":"  Radar is ubiquitous in autonomous driving systems due to its low cost and\ngood adaptability to bad weather. Nevertheless, the radar detection performance\nis usually inferior because its point cloud is sparse and not accurate due to\nthe poor azimuth and elevation resolution. Moreover, point cloud generation\nalgorithms already drop weak signals to reduce the false targets which may be\nsuboptimal for the use of deep fusion. In this paper, we propose a novel method\nnamed EchoFusion to skip the existing radar signal processing pipeline and then\nincorporate the radar raw data with other sensors. Specifically, we first\ngenerate the Bird's Eye View (BEV) queries and then take corresponding spectrum\nfeatures from radar to fuse with other sensors. By this approach, our method\ncould utilize both rich and lossless distance and speed clues from radar echoes\nand rich semantic clues from images, making our method surpass all existing\nmethods on the RADIal dataset, and approach the performance of LiDAR. The code\nwill be released on https://github.com/tusen-ai/EchoFusion.\n","authors":["Yang Liu","Feng Wang","Naiyan Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.16532v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.05919v2","updated":"2023-11-13T09:39:49Z","published":"2023-11-10T08:07:16Z","title":"Inter-object Discriminative Graph Modeling for Indoor Scene Recognition","summary":"  Variable scene layouts and coexisting objects across scenes make indoor scene\nrecognition still a challenging task. Leveraging object information within\nscenes to enhance the distinguishability of feature representations has emerged\nas a key approach in this domain. Currently, most object-assisted methods use a\nseparate branch to process object information, combining object and scene\nfeatures heuristically. However, few of them pay attention to interpretably\nhandle the hidden discriminative knowledge within object information. In this\npaper, we propose to leverage discriminative object knowledge to enhance scene\nfeature representations. Initially, we capture the object-scene discriminative\nrelationships from a probabilistic perspective, which are transformed into an\nInter-Object Discriminative Prototype (IODP). Given the abundant prior\nknowledge from IODP, we subsequently construct a Discriminative Graph Network\n(DGN), in which pixel-level scene features are defined as nodes and the\ndiscriminative relationships between node features are encoded as edges. DGN\naims to incorporate inter-object discriminative knowledge into the image\nrepresentation through graph convolution. With the proposed IODP and DGN, we\nobtain state-of-the-art results on several widely used scene datasets,\ndemonstrating the effectiveness of the proposed approach.\n","authors":["Chuanxin Song","Hanbo Wu","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2311.05919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07198v1","updated":"2023-11-13T09:38:30Z","published":"2023-11-13T09:38:30Z","title":"MonoDiffusion: Self-Supervised Monocular Depth Estimation Using\n  Diffusion Model","summary":"  Over the past few years, self-supervised monocular depth estimation that does\nnot depend on ground-truth during the training phase has received widespread\nattention. Most efforts focus on designing different types of network\narchitectures and loss functions or handling edge cases, e.g., occlusion and\ndynamic objects. In this work, we introduce a novel self-supervised depth\nestimation framework, dubbed MonoDiffusion, by formulating it as an iterative\ndenoising process. Because the depth ground-truth is unavailable in the\ntraining phase, we develop a pseudo ground-truth diffusion process to assist\nthe diffusion in MonoDiffusion. The pseudo ground-truth diffusion gradually\nadds noise to the depth map generated by a pre-trained teacher model.\nMoreover,the teacher model allows applying a distillation loss to guide the\ndenoised depth. Further, we develop a masked visual condition mechanism to\nenhance the denoising ability of model. Extensive experiments are conducted on\nthe KITTI and Make3D datasets and the proposed MonoDiffusion outperforms prior\nstate-of-the-art competitors. The source code will be available at\nhttps://github.com/ShuweiShao/MonoDiffusion.\n","authors":["Shuwei Shao","Zhongcai Pei","Weihai Chen","Dingchi Sun","Peter C. Y. Chen","Zhengguo Li"],"pdf_url":"https://arxiv.org/pdf/2311.07198v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2309.10987v3","updated":"2023-11-13T09:35:24Z","published":"2023-09-20T01:04:57Z","title":"SpikingNeRF: Making Bio-inspired Neural Networks See through the Real\n  World","summary":"  Spiking neural networks (SNNs) have been thriving on numerous tasks to\nleverage their promising energy efficiency and exploit their potentialities as\nbiologically plausible intelligence. Meanwhile, the Neural Radiance Fields\n(NeRF) render high-quality 3D scenes with massive energy consumption, but few\nworks delve into the energy-saving solution with a bio-inspired approach. In\nthis paper, we propose SpikingNeRF, which aligns the radiance ray with the\ntemporal dimension of SNN, to naturally accommodate the SNN to the\nreconstruction of Radiance Fields. Thus, the computation turns into a\nspike-based, multiplication-free manner, reducing the energy consumption. In\nSpikingNeRF, each sampled point on the ray is matched onto a particular time\nstep, and represented in a hybrid manner where the voxel grids are maintained\nas well. Based on the voxel grids, sampled points are determined whether to be\nmasked for better training and inference. However, this operation also incurs\nirregular temporal length. We propose the temporal padding strategy to tackle\nthe masked samples to maintain regular temporal length, i.e., regular tensors,\nand the temporal condensing strategy to form a denser data structure for\nhardware-friendly computation. Extensive experiments on various datasets\ndemonstrate that our method reduces the 70.79% energy consumption on average\nand obtains comparable synthesis quality with the ANN baseline.\n","authors":["Xingting Yao","Qinghao Hu","Tielong Liu","Zitao Mo","Zeyu Zhu","Zhengyang Zhuge","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2309.10987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07188v1","updated":"2023-11-13T09:25:03Z","published":"2023-11-13T09:25:03Z","title":"Fitting tree model with CNN and geodesics to track vesselsand\n  application to Ultrasound Localization Microscopy data","summary":"  Segmentation of tubular structures in vascular imaging is a well studied\ntask, although it is rare that we try to infuse knowledge of the tree-like\nstructure of the regions to be detected. Our work focuses on detecting the\nimportant landmarks in the vascular network (via CNN performing both\nlocalization and classification of the points of interest) and representing\nvessels as the edges in some minimal distance tree graph. We leverage geodesic\nmethods relevant to the detection of vessels and their geometry, making use of\nthe space of positions and orientations so that 2D vessels can be accurately\nrepresented as trees. We build our model to carry tracking on Ultrasound\nLocalization Microscopy (ULM) data, proposing to build a good cost function for\ntracking on this type of data. We also test our framework on synthetic and eye\nfundus data. Results show that scarcity of well annotated ULM data is an\nobstacle to localization of vascular landmarks but the Orientation Score built\nfrom ULM data yields good geodesics for tracking blood vessels.\n","authors":["Théo Bertrand","Laurent D. Cohen"],"pdf_url":"https://arxiv.org/pdf/2311.07188v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2307.10499v2","updated":"2023-11-13T09:23:53Z","published":"2023-07-19T23:55:15Z","title":"Mining Conditional Part Semantics with Occluded Extrapolation for\n  Human-Object Interaction Detection","summary":"  Human-Object Interaction Detection is a crucial aspect of human-centric scene\nunderstanding, with important applications in various domains. Despite recent\nprogress in this field, recognizing subtle and detailed interactions remains\nchallenging. Existing methods try to use human-related clues to alleviate the\ndifficulty, but rely heavily on external annotations or knowledge, limiting\ntheir practical applicability in real-world scenarios. In this work, we propose\na novel Part Semantic Network (PSN) to solve this problem. The core of PSN is a\nConditional Part Attention (CPA) mechanism, where human features are taken as\nkeys and values, and the object feature is used as query for the computation in\na cross-attention mechanism. In this way, our model learns to automatically\nfocus on the most informative human parts conditioned on the involved object,\ngenerating more semantically meaningful features for interaction recognition.\nAdditionally, we propose an Occluded Part Extrapolation (OPE) strategy to\nfacilitate interaction recognition under occluded scenarios, which teaches the\nmodel to extrapolate detailed features from partially occluded ones. Our method\nconsistently outperforms prior approaches on the V-COCO and HICO-DET datasets,\nwithout external data or extra annotations. Additional ablation studies\nvalidate the effectiveness of each component of our proposed method.\n","authors":["Guangzhi Wang","Yangyang Guo","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2307.10499v2.pdf","comment":"Under huge modification"},{"id":"http://arxiv.org/abs/2311.07184v1","updated":"2023-11-13T09:19:14Z","published":"2023-11-13T09:19:14Z","title":"Cross-Axis Transformer with 2D Rotary Embeddings","summary":"  Despite lagging behind their modal cousins in many respects, Vision\nTransformers have provided an interesting opportunity to bridge the gap between\nsequence modeling and image modeling. Up until now however, vision transformers\nhave largely been held back, due to both computational inefficiency, and lack\nof proper handling of spatial dimensions. In this paper, we introduce the\nCross-Axis Transformer. CAT is a model inspired by both Axial Transformers, and\nMicrosoft's recent Retentive Network, that drastically reduces the required\nnumber of floating point operations required to process an image, while\nsimultaneously converging faster and more accurately than the Vision\nTransformers it replaces.\n","authors":["Lily Erickson"],"pdf_url":"https://arxiv.org/pdf/2311.07184v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.07170v1","updated":"2023-11-13T09:05:30Z","published":"2023-11-13T09:05:30Z","title":"Regenerating Arbitrary Video Sequences with Distillation Path-Finding","summary":"  If the video has long been mentioned as a widespread visualization form, the\nanimation sequence in the video is mentioned as storytelling for people.\nProducing an animation requires intensive human labor from skilled professional\nartists to obtain plausible animation in both content and motion direction,\nincredibly for animations with complex content, multiple moving objects, and\ndense movement. This paper presents an interactive framework to generate new\nsequences according to the users' preference on the starting frame. The\ncritical contrast of our approach versus prior work and existing commercial\napplications is that novel sequences with arbitrary starting frame are produced\nby our system with a consistent degree in both content and motion direction. To\nachieve this effectively, we first learn the feature correlation on the\nframeset of the given video through a proposed network called RSFNet. Then, we\ndevelop a novel path-finding algorithm, SDPF, which formulates the knowledge of\nmotion directions of the source video to estimate the smooth and plausible\nsequences. The extensive experiments show that our framework can produce new\nanimations on the cartoon and natural scenes and advance prior works and\ncommercial applications to enable users to obtain more predictable results.\n","authors":["Thi-Ngoc-Hanh Le","Sheng-Yi Yao","Chun-Te Wu","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2311.07170v1.pdf","comment":"This paper has been accepted for publication on IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), January 2023. Project website:\n  http://graphics.csie.ncku.edu.tw/SDPF"},{"id":"http://arxiv.org/abs/2310.04816v2","updated":"2023-11-13T09:03:09Z","published":"2023-10-07T14:13:14Z","title":"Hacking Generative Models with Differentiable Network Bending","summary":"  In this work, we propose a method to 'hack' generative models, pushing their\noutputs away from the original training distribution towards a new objective.\nWe inject a small-scale trainable module between the intermediate layers of the\nmodel and train it for a low number of iterations, keeping the rest of the\nnetwork frozen. The resulting output images display an uncanny quality, given\nby the tension between the original and new objectives that can be exploited\nfor artistic purposes.\n","authors":["Giacomo Aldegheri","Alina Rogalska","Ahmed Youssef","Eugenia Iofinova"],"pdf_url":"https://arxiv.org/pdf/2310.04816v2.pdf","comment":"12 pages, 10 figures, Machine Learning for Creativity and Design\n  Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07166v1","updated":"2023-11-13T09:01:50Z","published":"2023-11-13T09:01:50Z","title":"NDDepth: Normal-Distance Assisted Monocular Depth Estimation and\n  Completion","summary":"  Over the past few years, monocular depth estimation and completion have been\npaid more and more attention from the computer vision community because of\ntheir widespread applications. In this paper, we introduce novel physics\n(geometry)-driven deep learning frameworks for these two tasks by assuming that\n3D scenes are constituted with piece-wise planes. Instead of directly\nestimating the depth map or completing the sparse depth map, we propose to\nestimate the surface normal and plane-to-origin distance maps or complete the\nsparse surface normal and distance maps as intermediate outputs. To this end,\nwe develop a normal-distance head that outputs pixel-level surface normal and\ndistance. Meanwhile, the surface normal and distance maps are regularized by a\ndeveloped plane-aware consistency constraint, which are then transformed into\ndepth maps. Furthermore, we integrate an additional depth head to strengthen\nthe robustness of the proposed frameworks. Extensive experiments on the\nNYU-Depth-v2, KITTI and SUN RGB-D datasets demonstrate that our method exceeds\nin performance prior state-of-the-art monocular depth estimation and completion\ncompetitors. The source code will be available at\nhttps://github.com/ShuweiShao/NDDepth.\n","authors":["Shuwei Shao","Zhongcai Pei","Weihai Chen","Peter C. Y. Chen","Zhengguo Li"],"pdf_url":"https://arxiv.org/pdf/2311.07166v1.pdf","comment":"Extension of previous work arXiv:2309.10592"},{"id":"http://arxiv.org/abs/2311.07163v1","updated":"2023-11-13T08:58:34Z","published":"2023-11-13T08:58:34Z","title":"Enhancing Lightweight Neural Networks for Small Object Detection in IoT\n  Applications","summary":"  Advances in lightweight neural networks have revolutionized computer vision\nin a broad range of IoT applications, encompassing remote monitoring and\nprocess automation. However, the detection of small objects, which is crucial\nfor many of these applications, remains an underexplored area in current\ncomputer vision research, particularly for embedded devices. To address this\ngap, the paper proposes a novel adaptive tiling method that can be used on top\nof any existing object detector including the popular FOMO network for object\ndetection on microcontrollers. Our experimental results show that the proposed\ntiling method can boost the F1-score by up to 225% while reducing the average\nobject count error by up to 76%. Furthermore, the findings of this work suggest\nthat using a soft F1 loss over the popular binary cross-entropy loss can\nsignificantly reduce the negative impact of imbalanced data. Finally, we\nvalidate our approach by conducting experiments on the Sony Spresense\nmicrocontroller, showcasing the proposed method's ability to strike a balance\nbetween detection performance, low latency, and minimal memory consumption.\n","authors":["Liam Boyle","Nicolas Baumann","Seonyeong Heo","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2311.07163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07162v1","updated":"2023-11-13T08:56:56Z","published":"2023-11-13T08:56:56Z","title":"CycleGANAS: Differentiable Neural Architecture Search for CycleGAN","summary":"  We develop a Neural Architecture Search (NAS) framework for CycleGAN that\ncarries out unpaired image-to-image translation task. Extending previous NAS\ntechniques for Generative Adversarial Networks (GANs) to CycleGAN is not\nstraightforward due to the task difference and greater search space. We design\narchitectures that consist of a stack of simple ResNet-based cells and develop\na search method that effectively explore the large search space. We show that\nour framework, called CycleGANAS, not only effectively discovers\nhigh-performance architectures that either match or surpass the performance of\nthe original CycleGAN, but also successfully address the data imbalance by\nindividual architecture search for each translation direction. To our best\nknowledge, it is the first NAS result for CycleGAN and shed light on NAS for\nmore complex structures.\n","authors":["Taegun An","Changhee Joo"],"pdf_url":"https://arxiv.org/pdf/2311.07162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06403v3","updated":"2023-11-13T08:48:08Z","published":"2023-10-10T08:14:24Z","title":"Boundary Discretization and Reliable Classification Network for Temporal\n  Action Detection","summary":"  Temporal action detection aims to recognize the action category and determine\nthe starting and ending time of each action instance in untrimmed videos. The\nmixed methods have achieved remarkable performance by simply merging\nanchor-based and anchor-free approaches. However, there are still two crucial\nissues in the mixed framework: (1) Brute-force merging and handcrafted anchors\ndesign affect the performance and practical application of the mixed methods.\n(2) A large number of false positives in action category predictions further\nimpact the detection performance. In this paper, we propose a novel Boundary\nDiscretization and Reliable Classification Network (BDRC-Net) that addresses\nthe above issues by introducing boundary discretization and reliable\nclassification modules. Specifically, the boundary discretization module (BDM)\nelegantly merges anchor-based and anchor-free approaches in the form of\nboundary discretization, avoiding the handcrafted anchors design required by\ntraditional mixed methods. Furthermore, the reliable classification module\n(RCM) predicts reliable action categories to reduce false positives in action\ncategory predictions. Extensive experiments conducted on different benchmarks\ndemonstrate that our proposed method achieves favorable performance compared\nwith the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%\non THUMOS'14, outperforming the previous best by 1.5%. The code will be\nreleased at https://github.com/zhenyingfang/BDRC-Net.\n","authors":["Zhenying Fang"],"pdf_url":"https://arxiv.org/pdf/2310.06403v3.pdf","comment":"12 pages, Source code: https://github.com/zhenyingfang/BDRC-Net"},{"id":"http://arxiv.org/abs/2311.07152v1","updated":"2023-11-13T08:47:09Z","published":"2023-11-13T08:47:09Z","title":"Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object\n  Detection","summary":"  3D object Detection with LiDAR-camera encounters overfitting in algorithm\ndevelopment which is derived from the violation of some fundamental rules. We\nrefer to the data annotation in dataset construction for theory complementing\nand argue that the regression task prediction should not involve the feature\nfrom the camera branch. By following the cutting-edge perspective of 'Detecting\nAs Labeling', we propose a novel paradigm dubbed DAL. With the most classical\nelementary algorithms, a simple predicting pipeline is constructed by imitating\nthe data annotation process. Then we train it in the simplest way to minimize\nits dependency and strengthen its portability. Though simple in construction\nand training, the proposed DAL paradigm not only substantially pushes the\nperformance boundary but also provides a superior trade-off between speed and\naccuracy among all existing methods. With comprehensive superiority, DAL is an\nideal baseline for both future work development and practical deployment. The\ncode has been released to facilitate future work on\nhttps://github.com/HuangJunJie2017/BEVDet.\n","authors":["Junjie Huang","Yun Ye","Zhujin Liang","Yi Shan","Dalong Du"],"pdf_url":"https://arxiv.org/pdf/2311.07152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07150v1","updated":"2023-11-13T08:39:06Z","published":"2023-11-13T08:39:06Z","title":"Interaction is all You Need? A Study of Robots Ability to Understand and\n  Execute","summary":"  This paper aims to address a critical challenge in robotics, which is\nenabling them to operate seamlessly in human environments through natural\nlanguage interactions. Our primary focus is to equip robots with the ability to\nunderstand and execute complex instructions in coherent dialogs to facilitate\nintricate task-solving scenarios. To explore this, we build upon the Execution\nfrom Dialog History (EDH) task from the Teach benchmark. We employ a\nmulti-transformer model with BART LM. We observe that our best configuration\noutperforms the baseline with a success rate score of 8.85 and a\ngoal-conditioned success rate score of 14.02. In addition, we suggest an\nalternative methodology for completing this task. Moreover, we introduce a new\ntask by expanding the EDH task and making predictions about game plans instead\nof individual actions. We have evaluated multiple BART models and an LLaMA2\nLLM, which has achieved a ROGUE-L score of 46.77 for this task.\n","authors":["Kushal Koshti","Nidhir Bhavsar"],"pdf_url":"https://arxiv.org/pdf/2311.07150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04584v2","updated":"2023-11-13T08:32:51Z","published":"2023-11-08T10:27:36Z","title":"Weakly-supervised deepfake localization in diffusion-generated images","summary":"  The remarkable generative capabilities of denoising diffusion models have\nraised new concerns regarding the authenticity of the images we see every day\non the Internet. However, the vast majority of existing deepfake detection\nmodels are tested against previous generative approaches (e.g. GAN) and usually\nprovide only a \"fake\" or \"real\" label per image. We believe a more informative\noutput would be to augment the per-image label with a localization map\nindicating which regions of the input have been manipulated. To this end, we\nframe this task as a weakly-supervised localization problem and identify three\nmain categories of methods (based on either explanations, local scores or\nattention), which we compare on an equal footing by using the Xception network\nas the common backbone architecture. We provide a careful analysis of all the\nmain factors that parameterize the design space: choice of method, type of\nsupervision, dataset and generator used in the creation of manipulated images;\nour study is enabled by constructing datasets in which only one of the\ncomponents is varied. Our results show that weakly-supervised localization is\nattainable, with the best performing detection method (based on local scores)\nbeing less sensitive to the looser supervision than to the mismatch in terms of\ndataset or generator.\n","authors":["Dragos Tantaru","Elisabeta Oneata","Dan Oneata"],"pdf_url":"https://arxiv.org/pdf/2311.04584v2.pdf","comment":"Accepted at WACV'24"},{"id":"http://arxiv.org/abs/2307.08286v2","updated":"2023-11-13T08:25:48Z","published":"2023-07-17T07:16:28Z","title":"Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature\n  Connectivity","summary":"  Recent work has revealed many intriguing empirical phenomena in neural\nnetwork training, despite the poorly understood and highly complex loss\nlandscapes and training dynamics. One of these phenomena, Linear Mode\nConnectivity (LMC), has gained considerable attention due to the intriguing\nobservation that different solutions can be connected by a linear path in the\nparameter space while maintaining near-constant training and test losses. In\nthis work, we introduce a stronger notion of linear connectivity, Layerwise\nLinear Feature Connectivity (LLFC), which says that the feature maps of every\nlayer in different trained networks are also linearly connected. We provide\ncomprehensive empirical evidence for LLFC across a wide range of settings,\ndemonstrating that whenever two trained networks satisfy LMC (via either\nspawning or permutation methods), they also satisfy LLFC in nearly all the\nlayers. Furthermore, we delve deeper into the underlying factors contributing\nto LLFC, which reveal new insights into the spawning and permutation\napproaches. The study of LLFC transcends and advances our understanding of LMC\nby adopting a feature-learning perspective.\n","authors":["Zhanpeng Zhou","Yongyi Yang","Xiaojiang Yang","Junchi Yan","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2307.08286v2.pdf","comment":"25 pages, 23 figures"},{"id":"http://arxiv.org/abs/2310.03059v2","updated":"2023-11-13T07:46:39Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code will be released at\nhttps://github.com/Even-JK/PEFT-3D.\n","authors":["Ivan Tang","Ray Zhang","Zoey Guo"],"pdf_url":"https://arxiv.org/pdf/2310.03059v2.pdf","comment":"10 pages. The specialized PEFT framework for 3D pre-trained models,\n  which achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Even-JK/PEFT-3D"},{"id":"http://arxiv.org/abs/2311.07125v1","updated":"2023-11-13T07:34:53Z","published":"2023-11-13T07:34:53Z","title":"Attention-Challenging Multiple Instance Learning for Whole Slide Image\n  Classification","summary":"  Overfitting remains a significant challenge in the application of Multiple\nInstance Learning (MIL) methods for Whole Slide Image (WSI) analysis.\nVisualizing heatmaps reveals that current MIL methods focus on a subset of\npredictive instances, hindering effective model generalization. To tackle this,\nwe propose Attention-Challenging MIL (ACMIL), aimed at forcing the attention\nmechanism to capture more challenging predictive instances. ACMIL incorporates\ntwo techniques, Multiple Branch Attention (MBA) to capture richer predictive\ninstances and Stochastic Top-K Instance Masking (STKIM) to suppress simple\npredictive instances. Evaluation on three WSI datasets outperforms\nstate-of-the-art methods. Additionally, through heatmap visualization, UMAP\nvisualization, and attention value statistics, this paper comprehensively\nillustrates ACMIL's effectiveness in overcoming the overfitting challenge. The\nsource code is available at \\url{https://github.com/dazhangyu123/ACMIL}.\n","authors":["Yunlong Zhang","Honglin Li","Yuxuan Sun","Sunyi Zheng","Chenglu Zhu","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.07125v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2311.07113v1","updated":"2023-11-13T07:09:30Z","published":"2023-11-13T07:09:30Z","title":"SpectralGPT: Spectral Foundation Model","summary":"  The foundation model has recently garnered significant attention due to its\npotential to revolutionize the field of visual representation learning in a\nself-supervised manner. While most foundation models are tailored to\neffectively process RGB images for various visual tasks, there is a noticeable\ngap in research focused on spectral data, which offers valuable information for\nscene understanding, especially in remote sensing (RS) applications. To fill\nthis gap, we created for the first time a universal RS foundation model, named\nSpectralGPT, which is purpose-built to handle spectral RS images using a novel\n3D generative pretrained transformer (GPT). Compared to existing foundation\nmodels, SpectralGPT 1) accommodates input images with varying sizes,\nresolutions, time series, and regions in a progressive training fashion,\nenabling full utilization of extensive RS big data; 2) leverages 3D token\ngeneration for spatial-spectral coupling; 3) captures spectrally sequential\npatterns via multi-target reconstruction; 4) trains on one million spectral RS\nimages, yielding models with over 600 million parameters. Our evaluation\nhighlights significant performance improvements with pretrained SpectralGPT\nmodels, signifying substantial potential in advancing spectral RS big data\napplications within the field of geoscience across four downstream tasks:\nsingle/multi-label scene classification, semantic segmentation, and change\ndetection.\n","authors":["Danfeng Hong","Bing Zhang","Xuyang Li","Yuxuan Li","Chenyu Li","Jing Yao","Naoto Yokoya","Hao Li","Xiuping Jia","Antonio Plaza","Gamba Paolo","Jon Atli Benediktsson","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2311.07113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13086v3","updated":"2023-11-13T06:16:09Z","published":"2022-06-27T07:12:31Z","title":"RankSEG: A Consistent Ranking-based Framework for Segmentation","summary":"  Segmentation has emerged as a fundamental field of computer vision and\nnatural language processing, which assigns a label to every pixel/feature to\nextract regions of interest from an image/text. To evaluate the performance of\nsegmentation, the Dice and IoU metrics are used to measure the degree of\noverlap between the ground truth and the predicted segmentation. In this paper,\nwe establish a theoretical foundation of segmentation with respect to the\nDice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous\nto classification-calibration or Fisher consistency in classification. We prove\nthat the existing thresholding-based framework with most operating losses are\nnot consistent with respect to the Dice/IoU metrics, and thus may lead to a\nsuboptimal solution. To address this pitfall, we propose a novel consistent\nranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of\nthe Bayes segmentation rule. Three numerical algorithms with GPU parallel\nexecution are developed to implement the proposed framework in large-scale and\nhigh-dimensional segmentation. We study statistical properties of the proposed\nframework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and\nthe rate of convergence are also provided. The numerical effectiveness of\nRankDice/mRankDice is demonstrated in various simulated examples and\nFine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with\nstate-of-the-art deep learning architectures.\n","authors":["Ben Dai","Chunlin Li"],"pdf_url":"https://arxiv.org/pdf/2206.13086v3.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2311.07090v1","updated":"2023-11-13T05:38:44Z","published":"2023-11-13T05:38:44Z","title":"CLiF-VQA: Enhancing Video Quality Assessment by Incorporating High-Level\n  Semantic Information related to Human Feelings","summary":"  Video Quality Assessment (VQA) aims to simulate the process of perceiving\nvideo quality by the human visual system (HVS). The judgments made by HVS are\nalways influenced by human subjective feelings. However, most of the current\nVQA research focuses on capturing various distortions in the spatial and\ntemporal domains of videos, while ignoring the impact of human feelings. In\nthis paper, we propose CLiF-VQA, which considers both features related to human\nfeelings and spatial features of videos. In order to effectively extract\nfeatures related to human feelings from videos, we explore the consistency\nbetween CLIP and human feelings in video perception for the first time.\nSpecifically, we design multiple objective and subjective descriptions closely\nrelated to human feelings as prompts. Further we propose a novel CLIP-based\nsemantic feature extractor (SFE) which extracts features related to human\nfeelings by sliding over multiple regions of the video frame. In addition, we\nfurther capture the low-level-aware features of the video through a spatial\nfeature extraction module. The two different features are then aggregated\nthereby obtaining the quality score of the video. Extensive experiments show\nthat the proposed CLiF-VQA exhibits excellent performance on several VQA\ndatasets.\n","authors":["Yachun Mi","Yu Li","Yan Shu","Chen Hui","Puchao Zhou","Shaohui Liu"],"pdf_url":"https://arxiv.org/pdf/2311.07090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14303v4","updated":"2023-11-13T05:11:52Z","published":"2023-09-25T17:19:26Z","title":"Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for\n  Pixel-Level Semantic Segmentation","summary":"  Preparing training data for deep vision models is a labor-intensive task. To\naddress this, generative models have emerged as an effective solution for\ngenerating synthetic data. While current generative models produce image-level\ncategory labels, we propose a novel method for generating pixel-level semantic\nsegmentation labels using the text-to-image generative model Stable Diffusion\n(SD). By utilizing the text prompts, cross-attention, and self-attention of SD,\nwe introduce three new techniques: class-prompt appending, class-prompt\ncross-attention, and self-attention exponentiation. These techniques enable us\nto generate segmentation maps corresponding to synthetic images. These maps\nserve as pseudo-labels for training semantic segmenters, eliminating the need\nfor labor-intensive pixel-wise annotation. To account for the imperfections in\nour pseudo-labels, we incorporate uncertainty regions into the segmentation,\nallowing us to disregard loss from those regions. We conduct evaluations on two\ndatasets, PASCAL VOC and MSCOCO, and our approach significantly outperforms\nconcurrent work. Our benchmarks and code will be released at\nhttps://github.com/VinAIResearch/Dataset-Diffusion\n","authors":["Quang Nguyen","Truong Vu","Anh Tran","Khoi Nguyen"],"pdf_url":"https://arxiv.org/pdf/2309.14303v4.pdf","comment":"Accepted to NeurIPS 2023. Our project page:\n  https://dataset-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2311.02358v2","updated":"2023-11-13T04:50:26Z","published":"2023-11-04T09:57:50Z","title":"Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution\n  $-$ a Non-Denoising Model","summary":"  Large scale image super-resolution is a challenging computer vision task,\nsince vast information is missing in a highly degraded image, say for example\nforscale x16 super-resolution. Diffusion models are used successfully in recent\nyears in extreme super-resolution applications, in which Gaussian noise is used\nas a means to form a latent photo-realistic space, and acts as a link between\nthe space of latent vectors and the latent photo-realistic space. There are\nquite a few sophisticated mathematical derivations on mapping the statistics of\nGaussian noises making Diffusion Models successful. In this paper we propose a\nsimple approach which gets away from using Gaussian noise but adopts some basic\nstructures of diffusion models for efficient image super-resolution.\nEssentially, we propose a DNN to perform domain transfer between neighbor\ndomains, which can learn the differences in statistical properties to\nfacilitate gradual interpolation with results of reasonable quality. Further\nquality improvement is achieved by conditioning the domain transfer with\nreference to the input LR image. Experimental results show that our method\noutperforms not only state-of-the-art large scale super resolution models, but\nalso the current diffusion models for image super-resolution. The approach can\nreadily be extended to other image-to-image tasks, such as image enlightening,\ninpainting, denoising, etc.\n","authors":["Chun-Chuen Hui","Wan-Chi Siu","Ngai-Fong Law"],"pdf_url":"https://arxiv.org/pdf/2311.02358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07075v1","updated":"2023-11-13T04:48:33Z","published":"2023-11-13T04:48:33Z","title":"GazeForensics: DeepFake Detection via Gaze-guided Spatial Inconsistency\n  Learning","summary":"  DeepFake detection is pivotal in personal privacy and public safety. With the\niterative advancement of DeepFake techniques, high-quality forged videos and\nimages are becoming increasingly deceptive. Prior research has seen numerous\nattempts by scholars to incorporate biometric features into the field of\nDeepFake detection. However, traditional biometric-based approaches tend to\nsegregate biometric features from general ones and freeze the biometric feature\nextractor. These approaches resulted in the exclusion of valuable general\nfeatures, potentially leading to a performance decline and, consequently, a\nfailure to fully exploit the potential of biometric information in assisting\nDeepFake detection. Moreover, insufficient attention has been dedicated to\nscrutinizing gaze authenticity within the realm of DeepFake detection in recent\nyears. In this paper, we introduce GazeForensics, an innovative DeepFake\ndetection method that utilizes gaze representation obtained from a 3D gaze\nestimation model to regularize the corresponding representation within our\nDeepFake detection model, while concurrently integrating general features to\nfurther enhance the performance of our model. Experiment results reveal that\nour proposed GazeForensics outperforms the current state-of-the-art methods.\n","authors":["Qinlin He","Chunlei Peng","Dechuang Liu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03179v3","updated":"2023-11-13T03:59:36Z","published":"2023-09-06T17:39:05Z","title":"SLiMe: Segment Like Me","summary":"  Significant strides have been made using large vision-language models, like\nStable Diffusion (SD), for a variety of downstream tasks, including image\nediting, image correspondence, and 3D shape generation. Inspired by these\nadvancements, we explore leveraging these extensive vision-language models for\nsegmenting images at any desired granularity using as few as one annotated\nsample by proposing SLiMe. SLiMe frames this problem as an optimization task.\nSpecifically, given a single training image and its segmentation mask, we first\nextract attention maps, including our novel \"weighted accumulated\nself-attention map\" from the SD prior. Then, using the extracted attention\nmaps, the text embeddings of Stable Diffusion are optimized such that, each of\nthem, learn about a single segmented region from the training image. These\nlearned embeddings then highlight the segmented region in the attention maps,\nwhich in turn can then be used to derive the segmentation map. This enables\nSLiMe to segment any real-world image during inference with the granularity of\nthe segmented region in the training image, using just one example. Moreover,\nleveraging additional training data when available, i.e. few-shot, improves the\nperformance of SLiMe. We carried out a knowledge-rich set of experiments\nexamining various design factors and showed that SLiMe outperforms other\nexisting one-shot and few-shot segmentation methods.\n","authors":["Aliasghar Khani","Saeid Asgari Taghanaki","Aditya Sanghi","Ali Mahdavi Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2309.03179v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04498v3","updated":"2023-11-13T03:35:23Z","published":"2023-11-08T07:15:05Z","title":"NExT-Chat: An LMM for Chat, Detection and Segmentation","summary":"  The development of large language models (LLMs) has greatly advanced the\nfield of multimodal understanding, leading to the emergence of large multimodal\nmodels (LMMs). In order to enhance the level of visual comprehension, recent\nstudies have equipped LMMs with region-level understanding capabilities by\nrepresenting object bounding box coordinates as a series of text sequences\n(pixel2seq). In this paper, we introduce a novel paradigm for object location\nmodeling called pixel2emb method, where we ask the LMM to output the location\nembeddings and then decoded by different decoders. This paradigm allows for\ndifferent location formats (such as bounding boxes and masks) to be used in\nmultimodal conversations Furthermore, this kind of embedding based location\nmodeling enables the utilization of existing practices in localization tasks,\nsuch as detection and segmentation. In scenarios with limited resources, our\npixel2emb demonstrates superior performance compared to existing\nstate-of-the-art (SOTA) approaches in both the location input and output tasks\nunder fair comparison. Leveraging the proposed pixel2emb method, we train an\nLMM named NExT-Chat and demonstrate its capability of handling multiple tasks\nlike visual grounding, region caption, and grounded reasoning.\n","authors":["Ao Zhang","Wei Ji","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.04498v3.pdf","comment":"Technical Report (project page: https://next-chatv.github.io/)"},{"id":"http://arxiv.org/abs/2311.07044v1","updated":"2023-11-13T03:05:16Z","published":"2023-11-13T03:05:16Z","title":"$L_0$-Sampler: An $L_{0}$ Model Guided Volume Sampling for NeRF","summary":"  Since being proposed, Neural Radiance Fields (NeRF) have achieved great\nsuccess in related tasks, mainly adopting the hierarchical volume sampling\n(HVS) strategy for volume rendering. However, the HVS of NeRF approximates\ndistributions using piecewise constant functions, which provides a relatively\nrough estimation. Based on the observation that a well-trained weight function\n$w(t)$ and the $L_0$ distance between points and the surface have very high\nsimilarity, we propose $L_0$-Sampler by incorporating the $L_0$ model into\n$w(t)$ to guide the sampling process. Specifically, we propose to use piecewise\nexponential functions rather than piecewise constant functions for\ninterpolation, which can not only approximate quasi-$L_0$ weight distributions\nalong rays quite well but also can be easily implemented with few lines of code\nwithout additional computational burden. Stable performance improvements can be\nachieved by applying $L_0$-Sampler to NeRF and its related tasks like 3D\nreconstruction. Code is available at https://ustc3dv.github.io/L0-Sampler/ .\n","authors":["Liangchen Li","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07044v1.pdf","comment":"Project page: https://ustc3dv.github.io/L0-Sampler/"},{"id":"http://arxiv.org/abs/2311.07042v1","updated":"2023-11-13T02:54:17Z","published":"2023-11-13T02:54:17Z","title":"Open-Vocabulary Video Anomaly Detection","summary":"  Video anomaly detection (VAD) with weak supervision has achieved remarkable\nperformance in utilizing video-level labels to discriminate whether a video\nframe is normal or abnormal. However, current approaches are inherently limited\nto a closed-set setting and may struggle in open-world applications where there\ncan be anomaly categories in the test data unseen during training. A few recent\nstudies attempt to tackle a more realistic setting, open-set VAD, which aims to\ndetect unseen anomalies given seen anomalies and normal videos. However, such a\nsetting focuses on predicting frame anomaly scores, having no ability to\nrecognize the specific categories of anomalies, despite the fact that this\nability is essential for building more informed video surveillance systems.\nThis paper takes a step further and explores open-vocabulary video anomaly\ndetection (OVVAD), in which we aim to leverage pre-trained large models to\ndetect and categorize seen and unseen anomalies. To this end, we propose a\nmodel that decouples OVVAD into two mutually complementary tasks --\nclass-agnostic detection and class-specific classification -- and jointly\noptimizes both tasks. Particularly, we devise a semantic knowledge injection\nmodule to introduce semantic knowledge from large language models for the\ndetection task, and design a novel anomaly synthesis module to generate pseudo\nunseen anomaly videos with the help of large vision generation models for the\nclassification task. These semantic knowledge and synthesis anomalies\nsubstantially extend our model's capability in detecting and categorizing a\nvariety of seen and unseen anomalies. Extensive experiments on three\nwidely-used benchmarks demonstrate our model achieves state-of-the-art\nperformance on OVVAD task.\n","authors":["Peng Wu","Xuerong Zhou","Guansong Pang","Yujia Sun","Jing Liu","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07042v1.pdf","comment":"Submitted"},{"id":"http://arxiv.org/abs/2311.04766v2","updated":"2023-11-13T02:39:59Z","published":"2023-11-08T15:39:56Z","title":"DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D\n  Facial Animation","summary":"  In recent years, audio-driven 3D facial animation has gained significant\nattention, particularly in applications such as virtual reality, gaming, and\nvideo conferencing. However, accurately modeling the intricate and subtle\ndynamics of facial expressions remains a challenge. Most existing studies\napproach the facial animation task as a single regression problem, which often\nfail to capture the intrinsic inter-modal relationship between speech signals\nand 3D facial animation and overlook their inherent consistency. Moreover, due\nto the limited availability of 3D-audio-visual datasets, approaches learning\nwith small-size samples have poor generalizability that decreases the\nperformance. To address these issues, in this study, we propose a cross-modal\ndual-learning framework, termed DualTalker, aiming at improving data usage\nefficiency as well as relating cross-modal dependencies. The framework is\ntrained jointly with the primary task (audio-driven facial animation) and its\ndual task (lip reading) and shares common audio/motion encoder components. Our\njoint training framework facilitates more efficient data usage by leveraging\ninformation from both tasks and explicitly capitalizing on the complementary\nrelationship between facial motion and audio to improve performance.\nFurthermore, we introduce an auxiliary cross-modal consistency loss to mitigate\nthe potential over-smoothing underlying the cross-modal complementary\nrepresentations, enhancing the mapping of subtle facial expression dynamics.\nThrough extensive experiments and a perceptual user study conducted on the VOCA\nand BIWI datasets, we demonstrate that our approach outperforms current\nstate-of-the-art methods both qualitatively and quantitatively. We have made\nour code and video demonstrations available at\nhttps://github.com/sabrina-su/iadf.git.\n","authors":["Guinan Su","Yanwu Yang","Zhifeng Li"],"pdf_url":"https://arxiv.org/pdf/2311.04766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07033v1","updated":"2023-11-13T02:31:20Z","published":"2023-11-13T02:31:20Z","title":"TTMFN: Two-stream Transformer-based Multimodal Fusion Network for\n  Survival Prediction","summary":"  Survival prediction plays a crucial role in assisting clinicians with the\ndevelopment of cancer treatment protocols. Recent evidence shows that\nmultimodal data can help in the diagnosis of cancer disease and improve\nsurvival prediction. Currently, deep learning-based approaches have experienced\nincreasing success in survival prediction by integrating pathological images\nand gene expression data. However, most existing approaches overlook the\nintra-modality latent information and the complex inter-modality correlations.\nFurthermore, existing modalities do not fully exploit the immense\nrepresentational capabilities of neural networks for feature aggregation and\ndisregard the importance of relationships between features. Therefore, it is\nhighly recommended to address these issues in order to enhance the prediction\nperformance by proposing a novel deep learning-based method. We propose a novel\nframework named Two-stream Transformer-based Multimodal Fusion Network for\nsurvival prediction (TTMFN), which integrates pathological images and gene\nexpression data. In TTMFN, we present a two-stream multimodal co-attention\ntransformer module to take full advantage of the complex relationships between\ndifferent modalities and the potential connections within the modalities.\nAdditionally, we develop a multi-head attention pooling approach to effectively\naggregate the feature representations of the two modalities. The experiment\nresults on four datasets from The Cancer Genome Atlas demonstrate that TTMFN\ncan achieve the best performance or competitive results compared to the\nstate-of-the-art methods in predicting the overall survival of patients.\n","authors":["Ruiquan Ge","Xiangyang Hu","Rungen Huang","Gangyong Jia","Yaqi Wang","Renshu Gu","Changmiao Wang","Elazab Ahmed","Linyan Wang","Juan Ye","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2311.07033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03493v2","updated":"2023-11-13T02:13:40Z","published":"2023-09-07T06:05:28Z","title":"SAM3D: Segment Anything Model in Volumetric Medical Images","summary":"  Image segmentation remains a pivotal component in medical image analysis,\naiding in the extraction of critical information for precise diagnostic\npractices. With the advent of deep learning, automated image segmentation\nmethods have risen to prominence, showcasing exceptional proficiency in\nprocessing medical imagery. Motivated by the Segment Anything Model (SAM)-a\nfoundational model renowned for its remarkable precision and robust\ngeneralization capabilities in segmenting 2D natural images-we introduce SAM3D,\nan innovative adaptation tailored for 3D volumetric medical image analysis.\nUnlike current SAM-based methods that segment volumetric data by converting the\nvolume into separate 2D slices for individual analysis, our SAM3D model\nprocesses the entire 3D volume image in a unified approach. Extensive\nexperiments are conducted on multiple medical image datasets to demonstrate\nthat our network attains competitive results compared with other\nstate-of-the-art methods in 3D medical segmentation tasks while being\nsignificantly efficient in terms of parameters. Code and checkpoints are\navailable at https://github.com/UARK-AICV/SAM3D.\n","authors":["Nhat-Tan Bui","Dinh-Hieu Hoang","Minh-Triet Tran","Gianfranco Doretto","Donald Adjeroh","Brijesh Patel","Arabinda Choudhary","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2309.03493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07022v1","updated":"2023-11-13T02:13:13Z","published":"2023-11-13T02:13:13Z","title":"ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in\n  Video-Language Models","summary":"  With the ever-increasing popularity of pretrained Video-Language Models\n(VidLMs), there is a pressing need to develop robust evaluation methodologies\nthat delve deeper into their visio-linguistic capabilities. To address this\nchallenge, we present ViLMA (Video Language Model Assessment), a task-agnostic\nbenchmark that places the assessment of fine-grained capabilities of these\nmodels on a firm footing. Task-based evaluations, while valuable, fail to\ncapture the complexities and specific temporal aspects of moving images that\nVidLMs need to process. Through carefully curated counterfactuals, ViLMA offers\na controlled evaluation suite that sheds light on the true potential of these\nmodels, as well as their performance gaps compared to human-level\nunderstanding. ViLMA also includes proficiency tests, which assess basic\ncapabilities deemed essential to solving the main counterfactual tests. We show\nthat current VidLMs' grounding abilities are no better than those of\nvision-language models which use static images. This is especially striking\nonce the performance on proficiency tests is factored in. Our benchmark serves\nas a catalyst for future research on VidLMs, helping to highlight areas that\nstill need to be explored.\n","authors":["Ilker Kesen","Andrea Pedrotti","Mustafa Dogan","Michele Cafagna","Emre Can Acikgoz","Letitia Parcalabescu","Iacer Calixto","Anette Frank","Albert Gatt","Aykut Erdem","Erkut Erdem"],"pdf_url":"https://arxiv.org/pdf/2311.07022v1.pdf","comment":"Preprint. 48 pages, 22 figures, 10 tables"},{"id":"http://arxiv.org/abs/2310.00723v4","updated":"2023-11-13T02:03:10Z","published":"2023-10-01T16:48:48Z","title":"HOH: Markerless Multimodal Human-Object-Human Handover Dataset with\n  Large Object Count","summary":"  We present the HOH (Human-Object-Human) Handover Dataset, a large object\ncount dataset with 136 objects, to accelerate data-driven research on handover\nstudies, human-robot handover implementation, and artificial intelligence (AI)\non handover parameter estimation from 2D and 3D data of person interactions.\nHOH contains multi-view RGB and depth data, skeletons, fused point clouds,\ngrasp type and handedness labels, object, giver hand, and receiver hand 2D and\n3D segmentations, giver and receiver comfort ratings, and paired object\nmetadata and aligned 3D models for 2,720 handover interactions spanning 136\nobjects and 20 giver-receiver pairs-40 with role-reversal-organized from 40\nparticipants. We also show experimental results of neural networks trained\nusing HOH to perform grasp, orientation, and trajectory prediction. As the only\nfully markerless handover capture dataset, HOH represents natural human-human\nhandover interactions, overcoming challenges with markered datasets that\nrequire specific suiting for body tracking, and lack high-resolution hand\ntracking. To date, HOH is the largest handover dataset in number of objects,\nparticipants, pairs with role reversal accounted for, and total interactions\ncaptured.\n","authors":["Noah Wiederhold","Ava Megyeri","DiMaggio Paris","Sean Banerjee","Natasha Kholgade Banerjee"],"pdf_url":"https://arxiv.org/pdf/2310.00723v4.pdf","comment":"NeurIPS 2023 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2311.05927v2","updated":"2023-11-13T01:56:27Z","published":"2023-11-10T08:23:24Z","title":"Automated Sperm Assessment Framework and Neural Network Specialized for\n  Sperm Video Recognition","summary":"  Infertility is a global health problem, and an increasing number of couples\nare seeking medical assistance to achieve reproduction, at least half of which\nare caused by men. The success rate of assisted reproductive technologies\ndepends on sperm assessment, in which experts determine whether sperm can be\nused for reproduction based on morphology and motility of sperm. Previous sperm\nassessment studies with deep learning have used datasets comprising images that\ninclude only sperm heads, which cannot consider motility and other morphologies\nof sperm. Furthermore, the labels of the dataset are one-hot, which provides\ninsufficient support for experts, because assessment results are inconsistent\nbetween experts, and they have no absolute answer. Therefore, we constructed\nthe video dataset for sperm assessment whose videos include sperm head as well\nas neck and tail, and its labels were annotated with soft-label. Furthermore,\nwe proposed the sperm assessment framework and the neural network, RoSTFine,\nfor sperm video recognition. Experimental results showed that RoSTFine could\nimprove the sperm assessment performances compared to existing video\nrecognition models and focus strongly on important sperm parts (i.e., head and\nneck).\n","authors":["Takuro Fujii","Hayato Nakagawa","Teppei Takeshima","Yasushi Yumura","Tomoki Hamagami"],"pdf_url":"https://arxiv.org/pdf/2311.05927v2.pdf","comment":"Accepted at Winter Conference on Applications of Computer Vision\n  (WACV) 2024"},{"id":"http://arxiv.org/abs/2311.07002v1","updated":"2023-11-13T01:03:19Z","published":"2023-11-13T01:03:19Z","title":"PICS in Pics: Physics Informed Contour Selection for Rapid Image\n  Segmentation","summary":"  Effective training of deep image segmentation models is challenging due to\nthe need for abundant, high-quality annotations. Generating annotations is\nlaborious and time-consuming for human experts, especially in medical image\nsegmentation. To facilitate image annotation, we introduce Physics Informed\nContour Selection (PICS) - an interpretable, physics-informed algorithm for\nrapid image segmentation without relying on labeled data. PICS draws\ninspiration from physics-informed neural networks (PINNs) and an active contour\nmodel called snake. It is fast and computationally lightweight because it\nemploys cubic splines instead of a deep neural network as a basis function. Its\ntraining parameters are physically interpretable because they directly\nrepresent control knots of the segmentation curve. Traditional snakes involve\nminimization of the edge-based loss functionals by deriving the Euler-Lagrange\nequation followed by its numerical solution. However, PICS directly minimizes\nthe loss functional, bypassing the Euler Lagrange equations. It is the first\nsnake variant to minimize a region-based loss function instead of traditional\nedge-based loss functions. PICS uniquely models the three-dimensional (3D)\nsegmentation process with an unsteady partial differential equation (PDE),\nwhich allows accelerated segmentation via transfer learning. To demonstrate its\neffectiveness, we apply PICS for 3D segmentation of the left ventricle on a\npublicly available cardiac dataset. While doing so, we also introduce a new\nconvexity-preserving loss term that encodes the shape information of the left\nventricle to enhance PICS's segmentation quality. Overall, PICS presents\nseveral novelties in network architecture, transfer learning, and\nphysics-inspired losses for image segmentation, thereby showing promising\noutcomes and potential for further refinement.\n","authors":["Vikas Dwivedi","Balaji Srinivasan","Ganapathy Krishnamurthi"],"pdf_url":"https://arxiv.org/pdf/2311.07002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07977v2","updated":"2023-11-13T23:57:21Z","published":"2023-08-15T18:27:03Z","title":"YODA: You Only Diffuse Areas. An Area-Masked Diffusion Approach For\n  Image Super-Resolution","summary":"  This work introduces \"You Only Diffuse Areas\" (YODA), a novel method for\npartial diffusion in Single-Image Super-Resolution (SISR). The core idea is to\nutilize diffusion selectively on spatial regions based on attention maps\nderived from the low-resolution image and the current time step in the\ndiffusion process. This time-dependent targeting enables a more effective\nconversion to high-resolution outputs by focusing on areas that benefit the\nmost from the iterative refinement process, i.e., detail-rich objects. We\nempirically validate YODA by extending leading diffusion-based SISR methods SR3\nand SRDiff. Our experiments demonstrate new state-of-the-art performance gains\nin face and general SR across PSNR, SSIM, and LPIPS metrics. A notable finding\nis YODA's stabilization effect on training by reducing color shifts, especially\nwhen induced by small batch sizes, potentially contributing to\nresource-constrained scenarios. The proposed spatial and temporal adaptive\ndiffusion mechanism opens promising research directions, including developing\nenhanced attention map extraction techniques and optimizing inference latency\nbased on sparser diffusion.\n","authors":["Brian B. Moser","Stanislav Frolov","Federico Raue","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2308.07977v2.pdf","comment":"Brian B. Moser and Stanislav Frolov contributed equally"},{"id":"http://arxiv.org/abs/2311.06176v2","updated":"2023-11-13T23:49:35Z","published":"2023-11-10T16:48:24Z","title":"Automatic Report Generation for Histopathology images using pre-trained\n  Vision Transformers","summary":"  Deep learning for histopathology has been successfully used for disease\nclassification, image segmentation and more. However, combining image and text\nmodalities using current state-of-the-art methods has been a challenge due to\nthe high resolution of histopathology images. Automatic report generation for\nhistopathology images is one such challenge. In this work, we show that using\nan existing pre-trained Vision Transformer in a two-step process of first using\nit to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and then\nusing it as the encoder and an LSTM decoder for report generation, we can build\na fairly performant and portable report generation mechanism that takes into\naccount the whole of the high resolution image, instead of just the patches. We\nare also able to use representations from an existing powerful pre-trained\nhierarchical vision transformer and show its usefulness in not just zero shot\nclassification but also for report generation.\n","authors":["Saurav Sengupta","Donald E. Brown"],"pdf_url":"https://arxiv.org/pdf/2311.06176v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 09 pages"},{"id":"http://arxiv.org/abs/2311.07806v1","updated":"2023-11-13T23:40:24Z","published":"2023-11-13T23:40:24Z","title":"Assessing Test-time Variability for Interactive 3D Medical Image\n  Segmentation with Diverse Point Prompts","summary":"  Interactive segmentation model leverages prompts from users to produce robust\nsegmentation. This advancement is facilitated by prompt engineering, where\ninteractive prompts serve as strong priors during test-time. However, this is\nan inherently subjective and hard-to-reproduce process. The variability in user\nexpertise and inherently ambiguous boundaries in medical images can lead to\ninconsistent prompt selections, potentially affecting segmentation accuracy.\nThis issue has not yet been extensively explored for medical imaging. In this\npaper, we assess the test-time variability for interactive medical image\nsegmentation with diverse point prompts. For a given target region, the point\nis classified into three sub-regions: boundary, margin, and center. Our goal is\nto identify a straightforward and efficient approach for optimal prompt\nselection during test-time based on three considerations: (1) benefits of\nadditional prompts, (2) effects of prompt placement, and (3) strategies for\noptimal prompt selection. We conduct extensive experiments on the public\nMedical Segmentation Decathlon dataset for challenging colon tumor segmentation\ntask. We suggest an optimal strategy for prompt selection during test-time,\nsupported by comprehensive results. The code is publicly available at\nhttps://github.com/MedICL-VU/variability\n","authors":["Hao Li","Han Liu","Dewei Hu","Jiacheng Wang","Ipek Oguz"],"pdf_url":"https://arxiv.org/pdf/2311.07806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02914v2","updated":"2023-11-13T23:16:13Z","published":"2022-11-05T14:09:10Z","title":"Robust Reflection Removal with Flash-only Cues in the Wild","summary":"  We propose a simple yet effective reflection-free cue for robust reflection\nremoval from a pair of flash and ambient (no-flash) images. The reflection-free\ncue exploits a flash-only image obtained by subtracting the ambient image from\nthe corresponding flash image in raw data space. The flash-only image is\nequivalent to an image taken in a dark environment with only a flash on. This\nflash-only image is visually reflection-free and thus can provide robust cues\nto infer the reflection in the ambient image. Since the flash-only image\nusually has artifacts, we further propose a dedicated model that not only\nutilizes the reflection-free cue but also avoids introducing artifacts, which\nhelps accurately estimate reflection and transmission. Our experiments on\nreal-world images with various types of reflection demonstrate the\neffectiveness of our model with reflection-free flash-only cues: our model\noutperforms state-of-the-art reflection removal approaches by more than 5.23dB\nin PSNR. We extend our approach to handheld photography to address the\nmisalignment between the flash and no-flash pair. With misaligned training data\nand the alignment module, our aligned model outperforms our previous version by\nmore than 3.19dB in PSNR on a misaligned dataset. We also study using linear\nRGB images as training data. Our source code and dataset are publicly available\nat https://github.com/ChenyangLEI/flash-reflection-removal.\n","authors":["Chenyang Lei","Xudong Jiang","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2211.02914v2.pdf","comment":"Extension of CVPR 2021 paper [arXiv:2103.04273], submitted to TPAMI.\n  Our source code and dataset are publicly available at\n  http://github.com/ChenyangLEI/flash-reflection-removal"},{"id":"http://arxiv.org/abs/2211.13854v3","updated":"2023-11-13T23:10:24Z","published":"2022-11-25T01:37:48Z","title":"ComCLIP: Training-Free Compositional Image and Text Matching","summary":"  Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.\n","authors":["Kenan Jiang","Xuehai He","Ruize Xu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.13854v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07788v1","updated":"2023-11-13T22:46:43Z","published":"2023-11-13T22:46:43Z","title":"CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework\n  for Zero-Shot Electroencephalography Signal Conversion","summary":"  Electroencephalography (EEG) is a prominent non-invasive neuroimaging\ntechnique providing insights into brain function. Unfortunately, EEG data\nexhibit a high degree of noise and variability across subjects hampering\ngeneralizable signal extraction. Therefore, a key aim in EEG analysis is to\nextract the underlying neural activation (content) as well as to account for\nthe individual subject variability (style). We hypothesize that the ability to\nconvert EEG signals between tasks and subjects requires the extraction of\nlatent representations accounting for content and style. Inspired by recent\nadvancements in voice conversion technologies, we propose a novel contrastive\nsplit-latent permutation autoencoder (CSLP-AE) framework that directly\noptimizes for EEG conversion. Importantly, the latent representations are\nguided using contrastive learning to promote the latent splits to explicitly\nrepresent subject (style) and task (content). We contrast CSLP-AE to\nconventional supervised, unsupervised (AE), and self-supervised (contrastive\nlearning) training and find that the proposed approach provides favorable\ngeneralizable characterizations of subject and task. Importantly, the procedure\nalso enables zero-shot conversion between unseen subjects. While the present\nwork only considers conversion of EEG, the proposed CSLP-AE provides a general\nframework for signal conversion and extraction of content (task activation) and\nstyle (subject variability) components of general interest for the modeling and\nanalysis of biological signals.\n","authors":["Anders Vestergaard Nørskov","Alexander Neergaard Zahid","Morten Mørup"],"pdf_url":"https://arxiv.org/pdf/2311.07788v1.pdf","comment":"Accepted for publication at the 37th Conference on Neural Information\n  Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2311.07784v1","updated":"2023-11-13T22:21:27Z","published":"2023-11-13T22:21:27Z","title":"A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated\n  Class Incremental Learning for Vision Tasks","summary":"  Deep learning models often suffer from forgetting previously learned\ninformation when trained on new data. This problem is exacerbated in federated\nlearning (FL), where the data is distributed and can change independently for\neach user. Many solutions are proposed to resolve this catastrophic forgetting\nin a centralized setting. However, they do not apply directly to FL because of\nits unique complexities, such as privacy concerns and resource limitations. To\novercome these challenges, this paper presents a framework for\n\\textbf{federated class incremental learning} that utilizes a generative model\nto synthesize samples from past distributions. This data can be later exploited\nalongside the training data to mitigate catastrophic forgetting. To preserve\nprivacy, the generative model is trained on the server using data-free methods\nat the end of each task without requesting data from clients. Moreover, our\nsolution does not demand the users to store old data or models, which gives\nthem the freedom to join/leave the training at any time. Additionally, we\nintroduce SuperImageNet, a new regrouping of the ImageNet dataset specifically\ntailored for federated continual learning. We demonstrate significant\nimprovements compared to existing baselines through extensive experiments on\nmultiple datasets.\n","authors":["Sara Babakniya","Zalan Fabian","Chaoyang He","Mahdi Soltanolkotabi","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2311.07784v1.pdf","comment":"Accepted in NeurIPS 2023. arXiv admin note: text overlap with\n  arXiv:2307.00497"},{"id":"http://arxiv.org/abs/2311.07766v1","updated":"2023-11-13T21:32:37Z","published":"2023-11-13T21:32:37Z","title":"Vision-Language Integration in Multimodal Video Transformers (Partially)\n  Aligns with the Brain","summary":"  Integrating information from multiple modalities is arguably one of the\nessential prerequisites for grounding artificial intelligence systems with an\nunderstanding of the real world. Recent advances in video transformers that\njointly learn from vision, text, and sound over time have made some progress\ntoward this goal, but the degree to which these models integrate information\nfrom modalities still remains unclear. In this work, we present a promising\napproach for probing a pre-trained multimodal video transformer model by\nleveraging neuroscientific evidence of multimodal information processing in the\nbrain. Using brain recordings of participants watching a popular TV show, we\nanalyze the effects of multi-modal connections and interactions in a\npre-trained multi-modal video transformer on the alignment with uni- and\nmulti-modal brain regions. We find evidence that vision enhances masked\nprediction performance during language processing, providing support that\ncross-modal representations in models can benefit individual modalities.\nHowever, we don't find evidence of brain-relevant information captured by the\njoint multi-modal transformer representations beyond that captured by all of\nthe individual modalities. We finally show that the brain alignment of the\npre-trained joint representation can be improved by fine-tuning using a task\nthat requires vision-language inferences. Overall, our results paint an\noptimistic picture of the ability of multi-modal transformers to integrate\nvision and language in partially brain-relevant ways but also show that\nimproving the brain alignment of these models may require new approaches.\n","authors":["Dota Tianai Dong","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2311.07766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07765v1","updated":"2023-11-13T21:31:07Z","published":"2023-11-13T21:31:07Z","title":"FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based\n  Human Activity Recognition","summary":"  Motion sensors integrated into wearable and mobile devices provide valuable\ninformation about the device users. Machine learning and, recently, deep\nlearning techniques have been used to characterize sensor data. Mostly, a\nsingle task, such as recognition of activities, is targeted, and the data is\nprocessed centrally at a server or in a cloud environment. However, the same\nsensor data can be utilized for multiple tasks and distributed machine-learning\ntechniques can be used without the requirement of the transmission of data to a\ncentre. This paper explores Federated Transfer Learning in a Multi-Task manner\nfor both sensor-based human activity recognition and device position\nidentification tasks. The OpenHAR framework is used to train the models, which\ncontains ten smaller datasets. The aim is to obtain model(s) applicable for\nboth tasks in different datasets, which may include only some label types.\nMultiple experiments are carried in the Flower federated learning environment\nusing the DeepConvLSTM architecture. Results are presented for federated and\ncentralized versions under different parameters and restrictions. By utilizing\ntransfer learning and training a task-specific and personalized federated\nmodel, we obtained a similar accuracy with training each client individually\nand higher accuracy than a fully centralized approach.\n","authors":["Egemen İşgüder","Özlem Durmaz İncel"],"pdf_url":"https://arxiv.org/pdf/2311.07765v1.pdf","comment":"Subimtted to Asian Conference in Machine Learning (ACML) 2023,\n  Pattern Recognition in Health Analysis Workshop, 7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.19721v3","updated":"2023-11-13T21:28:24Z","published":"2023-10-30T16:49:03Z","title":"Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained\n  Image Foundation Models","summary":"  To address prevalent issues in medical imaging, such as data acquisition\nchallenges and label availability, transfer learning from natural to medical\nimage domains serves as a viable strategy to produce reliable segmentation\nresults. However, several existing barriers between domains need to be broken\ndown, including addressing contrast discrepancies, managing anatomical\nvariability, and adapting 2D pretrained models for 3D segmentation tasks. In\nthis paper, we propose ProMISe,a prompt-driven 3D medical image segmentation\nmodel using only a single point prompt to leverage knowledge from a pretrained\n2D image foundation model. In particular, we use the pretrained vision\ntransformer from the Segment Anything Model (SAM) and integrate lightweight\nadapters to extract depth-related (3D) spatial context without updating the\npretrained weights. For robust results, a hybrid network with complementary\nencoders is designed, and a boundary-aware loss is proposed to achieve precise\nboundaries. We evaluate our model on two public datasets for colon and pancreas\ntumor segmentations, respectively. Compared to the state-of-the-art\nsegmentation methods with and without prompt engineering, our proposed method\nachieves superior performance. The code is publicly available at\nhttps://github.com/MedICL-VU/ProMISe.\n","authors":["Hao Li","Han Liu","Dewei Hu","Jiacheng Wang","Ipek Oguz"],"pdf_url":"https://arxiv.org/pdf/2310.19721v3.pdf","comment":"updated acknowledgments and fixed typos"},{"id":"http://arxiv.org/abs/2311.07761v1","updated":"2023-11-13T21:21:43Z","published":"2023-11-13T21:21:43Z","title":"Amodal Optical Flow","summary":"  Optical flow estimation is very challenging in situations with transparent or\noccluded objects. In this work, we address these challenges at the task level\nby introducing Amodal Optical Flow, which integrates optical flow with amodal\nperception. Instead of only representing the visible regions, we define amodal\noptical flow as a multi-layered pixel-level motion field that encompasses both\nvisible and occluded regions of the scene. To facilitate research on this new\ntask, we extend the AmodalSynthDrive dataset to include pixel-level labels for\namodal optical flow estimation. We present several strong baselines, along with\nthe Amodal Flow Quality metric to quantify the performance in an interpretable\nmanner. Furthermore, we propose the novel AmodalFlowNet as an initial step\ntoward addressing this task. AmodalFlowNet consists of a transformer-based\ncost-volume encoder paired with a recurrent transformer decoder which\nfacilitates recurrent hierarchical feature propagation and amodal semantic\ngrounding. We demonstrate the tractability of amodal optical flow in extensive\nexperiments and show its utility for downstream tasks such as panoptic\ntracking. We make the dataset, code, and trained models publicly available at\nhttp://amodal-flow.cs.uni-freiburg.de.\n","authors":["Maximilian Luz","Rohit Mohan","Ahmed Rida Sekkat","Oliver Sawade","Elmar Matthes","Thomas Brox","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2311.07761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07750v1","updated":"2023-11-13T21:07:07Z","published":"2023-11-13T21:07:07Z","title":"SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models\n  for Multi-Label Chest X-Ray Classification","summary":"  Chest X-rays are widely used to diagnose thoracic diseases, but the lack of\ndetailed information about these abnormalities makes it challenging to develop\naccurate automated diagnosis systems, which is crucial for early detection and\neffective treatment. To address this challenge, we employed deep learning\ntechniques to identify patterns in chest X-rays that correspond to different\ndiseases. We conducted experiments on the \"ChestX-ray14\" dataset using various\npre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical\nmodels. The best individual model was the CoAtNet, which achieved an area under\nthe receiver operating characteristic curve (AUROC) of 84.2%. By combining the\npredictions of all trained models using a weighted average ensemble where the\nweight of each model was determined using differential evolution, we further\nimproved the AUROC to 85.4%, outperforming other state-of-the-art methods in\nthis field. Our findings demonstrate the potential of deep learning techniques,\nparticularly ensemble deep learning, for improving the accuracy of automatic\ndiagnosis of thoracic diseases from chest X-rays.\n","authors":["S. M. Nabil Ashraf","Md. Adyelullahil Mamun","Hasnat Md. Abdullah","Md. Golam Rabiul Alam"],"pdf_url":"https://arxiv.org/pdf/2311.07750v1.pdf","comment":"Accepted in International Conference on Computer and Information\n  Technology (ICCIT) 2023"},{"id":"http://arxiv.org/abs/2311.07734v1","updated":"2023-11-13T20:36:54Z","published":"2023-11-13T20:36:54Z","title":"Quality-Aware Prototype Memory for Face Representation Learning","summary":"  Prototype Memory is a powerful model for face representation learning. It\nenables the training of face recognition models using datasets of any size,\nwith on-the-fly generation of prototypes (classifier weights) and efficient\nways of their utilization. Prototype Memory demonstrated strong results in many\nface recognition benchmarks. However, the algorithm of prototype generation,\nused in it, is prone to the problems of imperfectly calculated prototypes in\ncase of low-quality or poorly recognizable faces in the images, selected for\nthe prototype creation. All images of the same person, presented in the\nmini-batch, used with equal weights, and the resulting averaged prototype could\nbe contaminated with imperfect embeddings of such face images. It can lead to\nmisdirected training signals and impair the performance of the trained face\nrecognition models. In this paper, we propose a simple and effective way to\nimprove Prototype Memory with quality-aware prototype generation. Quality-Aware\nPrototype Memory uses different weights for images of different quality in the\nprocess of prototype generation. With this improvement, prototypes get more\nvaluable information from high-quality images and less hurt by low-quality\nones. We propose and compare several methods of quality estimation and usage,\nperform extensive experiments on the different face recognition benchmarks and\ndemonstrate the advantages of the proposed model compared to the basic version\nof Prototype Memory.\n","authors":["Evgeny Smirnov","Vasiliy Galyuk","Evgeny Lukyanets"],"pdf_url":"https://arxiv.org/pdf/2311.07734v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.01523v2","updated":"2023-11-13T20:24:07Z","published":"2023-10-02T18:14:23Z","title":"Fetal-BET: Brain Extraction Tool for Fetal MRI","summary":"  Fetal brain extraction is a necessary first step in most computational fetal\nbrain MRI pipelines. However, it has been a very challenging task due to\nnon-standard fetal head pose, fetal movements during examination, and vastly\nheterogeneous appearance of the developing fetal brain and the neighboring\nfetal and maternal anatomy across various sequences and scanning conditions.\nDevelopment of a machine learning method to effectively address this task\nrequires a large and rich labeled dataset that has not been previously\navailable. As a result, there is currently no method for accurate fetal brain\nextraction on various fetal MRI sequences. In this work, we first built a large\nannotated dataset of approximately 72,000 2D fetal brain MRI images. Our\ndataset covers the three common MRI sequences including T2-weighted,\ndiffusion-weighted, and functional MRI acquired with different scanners.\nMoreover, it includes normal and pathological brains. Using this dataset, we\ndeveloped and validated deep learning methods, by exploiting the power of the\nU-Net style architectures, the attention mechanism, multi-contrast feature\nlearning, and data augmentation for fast, accurate, and generalizable automatic\nfetal brain extraction. Our approach leverages the rich information from\nmulti-contrast (multi-sequence) fetal MRI data, enabling precise delineation of\nthe fetal brain structures. Evaluations on independent test data show that our\nmethod achieves accurate brain extraction on heterogeneous test data acquired\nwith different scanners, on pathological brains, and at various gestational\nstages. This robustness underscores the potential utility of our deep learning\nmodel for fetal brain imaging and image analysis.\n","authors":["Razieh Faghihpirayesh","Davood Karimi","Deniz Erdoğmuş","Ali Gholipour"],"pdf_url":"https://arxiv.org/pdf/2310.01523v2.pdf","comment":"10 pages, 6 figures, 2 TABLES, This work has been submitted to the\n  IEEE Transactions on Medical Imaging for possible publication. Copyright may\n  be transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2311.07711v1","updated":"2023-11-13T19:51:46Z","published":"2023-11-13T19:51:46Z","title":"Histopathologic Cancer Detection","summary":"  Early diagnosis of the cancer cells is necessary for making an effective\ntreatment plan and for the health and safety of a patient. Nowadays, doctors\nusually use a histological grade that pathologists determine by performing a\nsemi-quantitative analysis of the histopathological and cytological features of\nhematoxylin-eosin (HE) stained histopathological images. This research\ncontributes a potential classification model for cancer prognosis to\nefficiently utilize the valuable information underlying the HE-stained\nhistopathological images. This work uses the PatchCamelyon benchmark datasets\nand trains them in a multi-layer perceptron and convolution model to observe\nthe model's performance in terms of precision, Recall, F1 Score, Accuracy, and\nAUC Score. The evaluation result shows that the baseline convolution model\noutperforms the baseline MLP model. Also, this paper introduced ResNet50 and\nInceptionNet models with data augmentation, where ResNet50 is able to beat the\nstate-of-the-art model. Furthermore, the majority vote and concatenation\nensemble were evaluated and provided the future direction of using transfer\nlearning and segmentation to understand the specific features.\n","authors":["Varan Singh Rohila","Neeraj Lalwani","Lochan Basyal"],"pdf_url":"https://arxiv.org/pdf/2311.07711v1.pdf","comment":"5 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.07634v1","updated":"2023-11-13T14:35:18Z","published":"2023-11-13T14:35:18Z","title":"ActiveDC: Distribution Calibration for Active Finetuning","summary":"  The pretraining-finetuning paradigm has gained popularity in various computer\nvision tasks. In this paradigm, the emergence of active finetuning arises due\nto the abundance of large-scale data and costly annotation requirements. Active\nfinetuning involves selecting a subset of data from an unlabeled pool for\nannotation, facilitating subsequent finetuning. However, the use of a limited\nnumber of training samples can lead to a biased distribution, potentially\nresulting in model overfitting. In this paper, we propose a new method called\nActiveDC for the active finetuning tasks. Firstly, we select samples for\nannotation by optimizing the distribution similarity between the subset to be\nselected and the entire unlabeled pool in continuous space. Secondly, we\ncalibrate the distribution of the selected samples by exploiting implicit\ncategory information in the unlabeled pool. The feature visualization provides\nan intuitive sense of the effectiveness of our approach to distribution\ncalibration. We conducted extensive experiments on three image classification\ndatasets with different sampling ratios. The results indicate that ActiveDC\nconsistently outperforms the baseline performance in all image classification\ntasks. The improvement is particularly significant when the sampling ratio is\nlow, with performance gains of up to 10%. Our code will be released.\n","authors":["Wenshuai Xu","Zhenhui Hu","Yu Lu","Jinzhou Meng","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07634v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.03402v2","updated":"2023-11-13T13:09:49Z","published":"2023-11-05T17:40:10Z","title":"CycleCL: Self-supervised Learning for Periodic Videos","summary":"  Analyzing periodic video sequences is a key topic in applications such as\nautomatic production systems, remote sensing, medical applications, or physical\ntraining. An example is counting repetitions of a physical exercise. Due to the\ndistinct characteristics of periodic data, self-supervised methods designed for\nstandard image datasets do not capture changes relevant to the progression of\nthe cycle and fail to ignore unrelated noise. They thus do not work well on\nperiodic data. In this paper, we propose CycleCL, a self-supervised learning\nmethod specifically designed to work with periodic data. We start from the\ninsight that a good visual representation for periodic data should be sensitive\nto the phase of a cycle, but be invariant to the exact repetition, i.e. it\nshould generate identical representations for a specific phase throughout all\nrepetitions. We exploit the repetitions in videos to design a novel contrastive\nlearning method based on a triplet loss that optimizes for these desired\nproperties. Our method uses pre-trained features to sample pairs of frames from\napproximately the same phase and negative pairs of frames from different\nphases. Then, we iterate between optimizing a feature encoder and resampling\ntriplets, until convergence. By optimizing a model this way, we are able to\nlearn features that have the mentioned desired properties. We evaluate CycleCL\non an industrial and multiple human actions datasets, where it significantly\noutperforms previous video-based self-supervised learning methods on all tasks.\n","authors":["Matteo Destro","Michael Gygli"],"pdf_url":"https://arxiv.org/pdf/2311.03402v2.pdf","comment":"Accepted at WACV 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2211.13308v4","updated":"2023-11-13T18:25:27Z","published":"2022-11-23T21:25:39Z","title":"SciRepEval: A Multi-Format Benchmark for Scientific Document\n  Representations","summary":"  Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks without further fine-tuning. However, existing\nbenchmarks for evaluating these representations fail to capture the diversity\nof relevant tasks. In response, we introduce SciRepEval, the first\ncomprehensive benchmark for training and evaluating scientific document\nrepresentations. It includes 24 challenging and realistic tasks, 8 of which are\nnew, across four formats: classification, regression, ranking and search. We\nthen use this benchmark to study and improve the generalization ability of\nscientific document representation models. We show how state-of-the-art models\nlike SPECTER and SciNCL struggle to generalize across the task formats, and\nthat simple multi-task training fails to improve them. However, a new approach\nthat learns multiple embeddings per document, each tailored to a different\nformat, can improve performance. We experiment with task-format-specific\ncontrol codes and adapters and find they outperform the existing\nsingle-embedding state-of-the-art by over 2 points absolute. We release the\nresulting family of multi-format models, called SPECTER2, for the community to\nuse and build on.\n","authors":["Amanpreet Singh","Mike D'Arcy","Arman Cohan","Doug Downey","Sergey Feldman"],"pdf_url":"https://arxiv.org/pdf/2211.13308v4.pdf","comment":"19 pages, 2 figures, 11 tables. Accepted in EMNLP 2023 Main\n  Conference"},{"id":"http://arxiv.org/abs/2310.19056v2","updated":"2023-11-13T15:09:01Z","published":"2023-10-29T16:04:10Z","title":"MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion","summary":"  Query expansion is a commonly-used technique in many search systems to better\nrepresent users' information needs with additional query terms. Existing\nstudies for this task usually propose to expand a query with retrieved or\ngenerated contextual documents. However, both types of methods have clear\nlimitations. For retrieval-based methods, the documents retrieved with the\noriginal query might not be accurate enough to reveal the search intent,\nespecially when the query is brief or ambiguous. For generation-based methods,\nexisting models can hardly be trained or aligned on a particular corpus, due to\nthe lack of corpus-specific labeled data. In this paper, we propose a novel\nLarge Language Model (LLM) based mutual verification framework for query\nexpansion, which alleviates the aforementioned limitations. Specifically, we\nfirst design a query-query-document generation pipeline, which can effectively\nleverage the contextual knowledge encoded in LLMs to generate sub-queries and\ncorresponding documents from multiple perspectives. Next, we employ a mutual\nverification method for both generated and retrieved contextual documents,\nwhere 1) retrieved documents are filtered with the external contextual\nknowledge in generated documents, and 2) generated documents are filtered with\nthe corpus-specific knowledge in retrieved documents. Overall, the proposed\nmethod allows retrieved and generated documents to complement each other to\nfinalize a better query expansion. We conduct extensive experiments on three\ninformation retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.\nThe results demonstrate that our method outperforms other baselines\nsignificantly.\n","authors":["Pengyue Jia","Yiding Liu","Xiangyu Zhao","Xiaopeng Li","Changying Hao","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.19056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07296v1","updated":"2023-11-13T12:36:53Z","published":"2023-11-13T12:36:53Z","title":"BIDRN: A Method of Bidirectional Recurrent Neural Network for Sentiment\n  Analysis","summary":"  Text mining research has grown in importance in recent years due to the\ntremendous increase in the volume of unstructured textual data. This has\nresulted in immense potential as well as obstacles in the sector, which may be\nefficiently addressed with adequate analytical and study methods. Deep\nBidirectional Recurrent Neural Networks are used in this study to analyze\nsentiment. The method is categorized as sentiment polarity analysis because it\nmay generate a dataset with sentiment labels. This dataset can be used to train\nand evaluate sentiment analysis models capable of extracting impartial\nopinions. This paper describes the Sentiment Analysis-Deep Bidirectional\nRecurrent Neural Networks (SA-BDRNN) Scheme, which seeks to overcome the\nchallenges and maximize the potential of text mining in the context of Big\nData. The current study proposes a SA-DBRNN Scheme that attempts to give a\nsystematic framework for sentiment analysis in the context of student input on\ninstitution choice. The purpose of this study is to compare the effectiveness\nof the proposed SA- DBRNN Scheme to existing frameworks to establish a robust\ndeep neural network that might serve as an adequate classification model in the\nfield of sentiment analysis.\n","authors":["Dr. D Muthusankar","Dr. P Kaladevi","Dr. V R Sadasivam","R Praveen"],"pdf_url":"https://arxiv.org/pdf/2311.07296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11772v3","updated":"2023-11-13T10:56:22Z","published":"2023-07-18T04:43:24Z","title":"AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment\n  enabled by Large Language Models","summary":"  The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named AutoAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nAutoAlign constructs a predicate-proximity-graph with the help of large\nlanguage models to automatically capture the similarity between predicates\nacross two KGs. For entity embeddings, AutoAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nAutoAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that AutoAlign improves the performance of entity\nalignment significantly compared to state-of-the-art methods.\n","authors":["Rui Zhang","Yixin Su","Bayu Distiawan Trisedya","Xiaoyan Zhao","Min Yang","Hong Cheng","Jianzhong Qi"],"pdf_url":"https://arxiv.org/pdf/2307.11772v3.pdf","comment":"14 pages, 5 figures, 4 tables, IEEE Transactions on Knowledge and\n  Data Engineering"},{"id":"http://arxiv.org/abs/2311.07229v1","updated":"2023-11-13T10:51:29Z","published":"2023-11-13T10:51:29Z","title":"Understanding the Influence of Data Characteristics on the Performance\n  of Point-of-Interest Recommendation Algorithms","summary":"  The performance of recommendation algorithms is closely tied to key\ncharacteristics of the data sets they use, such as sparsity, popularity bias,\nand preference distributions. In this paper, we conduct a comprehensive\nexplanatory analysis to shed light on the impact of a broad range of data\ncharacteristics within the point-of-interest (POI) recommendation domain. To\naccomplish this, we extend prior methodologies used to characterize traditional\nrecommendation problems by introducing new explanatory variables specifically\nrelevant to POI recommendation. We subdivide a POI recommendation data set on\nNew York City into domain-driven subsamples to measure the effect of varying\nthese characteristics on different state-of-the-art POI recommendation\nalgorithms in terms of accuracy, novelty, and item exposure. Our findings,\nobtained through the application of an explanatory framework employing\nmultiple-regression models, reveal that the relevant independent variables\nencompass all categories of data characteristics and account for as much as\n$R^2 = $ 85-90\\% of the accuracy and item exposure achieved by the algorithms.\nOur study reaffirms the pivotal role of prominent data characteristics, such as\ndensity, popularity bias, and the distribution of check-ins in POI\nrecommendation. Additionally, we unveil novel factors, such as the proximity of\nuser activity to the city center and the duration of user activity. In summary,\nour work reveals why certain POI recommendation algorithms excel in specific\nrecommendation problems and, conversely, offers practical insights into which\ndata characteristics should be modified (or explicitly recognized) to achieve\nbetter performance.\n","authors":["Linus W. Dietz","Pablo Sánchez","Alejandro Bellogín"],"pdf_url":"https://arxiv.org/pdf/2311.07229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11755v2","updated":"2023-11-13T10:36:04Z","published":"2023-05-19T15:42:00Z","title":"Visualization for Recommendation Explainability: A Survey and New\n  Perspectives","summary":"  Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.\n","authors":["Mohamed Amine Chatti","Mouadh Guesmi","Arham Muslim"],"pdf_url":"https://arxiv.org/pdf/2305.11755v2.pdf","comment":"Updated version Nov. 2023, 36 pages"},{"id":"http://arxiv.org/abs/2311.07204v1","updated":"2023-11-13T09:55:52Z","published":"2023-11-13T09:55:52Z","title":"On Elastic Language Models","summary":"  Large-scale pretrained language models have achieved compelling performance\nin a wide range of language understanding and information retrieval tasks.\nKnowledge distillation offers an opportunity to compress a large language model\nto a small one, in order to reach a reasonable latency-performance tradeoff.\nHowever, for scenarios where the number of requests (e.g., queries submitted to\na search engine) is highly variant, the static tradeoff attained by the\ncompressed language model might not always fit. Once a model is assigned with a\nstatic tradeoff, it could be inadequate in that the latency is too high when\nthe number of requests is large or the performance is too low when the number\nof requests is small. To this end, we propose an elastic language model\n(ElasticLM) that elastically adjusts the tradeoff according to the request\nstream. The basic idea is to introduce a compute elasticity to the compressed\nlanguage model, so that the tradeoff could vary on-the-fly along scalable and\ncontrollable compute. Specifically, we impose an elastic structure to enable\nElasticLM with compute elasticity and design an elastic optimization to learn\nElasticLM under compute elasticity. To serve ElasticLM, we apply an elastic\nschedule. Considering the specificity of information retrieval, we adapt\nElasticLM to dense retrieval and reranking and present ElasticDenser and\nElasticRanker respectively. Offline evaluation is conducted on a language\nunderstanding benchmark GLUE; and several information retrieval tasks including\nNatural Question, Trivia QA, and MS MARCO. The results show that ElasticLM\nalong with ElasticDenser and ElasticRanker can perform correctly and\ncompetitively compared with an array of static baselines. Furthermore, online\nsimulation with concurrency is also carried out. The results demonstrate that\nElasticLM can provide elastic tradeoffs with respect to varying request stream.\n","authors":["Chen Zhang","Benyou Wang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2311.07204v1.pdf","comment":"27 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2311.07054v1","updated":"2023-11-13T03:42:17Z","published":"2023-11-13T03:42:17Z","title":"Do LLMs Implicitly Exhibit User Discrimination in Recommendation? An\n  Empirical Study","summary":"  Recently, Large Language Models (LLMs) have enhanced user interaction,\nenabling seamless information retrieval and recommendations. However, concerns\nemerge as these LLMs have shown tendencies to display discrimination related to\nusers' sensitive characteristics (such as gender), leading to explicit user\nunfairness. Furthermore, our analysis uncovers a more discreet variant of bias\nin LLMs, defined as implicit user unfairness, wherein these models demonstrate\ndiscriminatory recommendation behaviors based solely on non-sensitive user\ndetails, like usernames or email addresses. This subtle form of unfairness,\nwhile more pervasive, poses a significant threat to the ethical integrity and\nrights of minority user groups. To comprehensively explore implicit user\nunfairness, our analysis unfolds in three key steps: (1) We uncover the reasons\nfor this implicit user unfairness: LLMs can infer users' sensitive attributes\nfrom non-sensitive attributes (e.g. user names) due to their extensive world\nknowledge. (2) Our findings expose that the magnitude of implicit user\nunfairness within LLMs surpasses the level of explicit user unfairness observed\nin traditional recommender models, signifying a more alarming issue of\nunfairness, i.e. some non-sensitive features of users like names may result in\nmore serious discrimination phenomena. (3) We analyze the long-term effect of\nimplicit user unfairness, identifying that it will reinforce information\nbubbles at an accelerated rate compared to traditional RS. We emphasize the\nneed to identify and mitigate implicit user unfairness, aiming to avert the\npotential human-LLMs recommendation systems deterioration.\n","authors":["Chen Xu","Wenjie Wang","Yuxin Li","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.07054v1.pdf","comment":"No"},{"id":"http://arxiv.org/abs/2310.09400v2","updated":"2023-11-13T22:33:37Z","published":"2023-10-13T20:52:18Z","title":"Contextual Collaboration: Uniting Collaborative Filtering with\n  Pre-trained Language Models","summary":"  Traditional recommender systems have predominantly relied on identity\nrepresentations (IDs) to characterize users and items. In contrast, the\nemergence of pre-trained language model (PLM) en-coders has significantly\nenriched the modeling of contextual item descriptions. While PLMs excel in\naddressing few-shot, zero-shot, and unified modeling scenarios, they often\noverlook the critical collaborative filtering signal. This omission gives rise\nto two pivotal challenges: (1) Collaborative Contextualization, aiming for the\nseamless integration of collaborative signals with contextual representations.\n(2) The necessity to bridge the representation gap between ID-based and\ncontextual representations while preserving their contextual semantics. In this\npaper, we introduce CollabContext, a novel model that skillfully merges\ncollaborative filtering signals with contextual representations, aligning these\nrepresentations within the contextual space while retaining essential\ncontextual semantics. Experimental results across three real-world datasets\nshowcase substantial improvements. Through its capability in collaborative\ncontextualization, CollabContext demonstrates remarkable enhancements in\nrecommendation performance, particularly in cold-start scenarios. The code is\navailable after the conference accepts the paper.\n","authors":["Chen Wang","Liangwei Yang","Zhiwei Liu","Xiaolong Liu","Mingdai Yang","Yueqing Liang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2310.09400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07742v1","updated":"2023-11-13T20:49:01Z","published":"2023-11-13T20:49:01Z","title":"Modeling Sequences as Star Graphs to Address Over-smoothing in\n  Self-attentive Sequential Recommendation","summary":"  Self-attention (SA) mechanisms have been widely used in developing sequential\nrecommendation (SR) methods, and demonstrated state-of-the-art performance.\nHowever, in this paper, we show that self-attentive SR methods substantially\nsuffer from the over-smoothing issue that item embeddings within a sequence\nbecome increasingly similar across attention blocks. As widely demonstrated in\nthe literature, this issue could lead to a loss of information in individual\nitems, and significantly degrade models' scalability and performance. To\naddress the over-smoothing issue, in this paper, we view items within a\nsequence constituting a star graph and develop a method, denoted as MSSG, for\nSR. Different from existing self-attentive methods, MSSG introduces an\nadditional internal node to specifically capture the global information within\nthe sequence, and does not require information propagation among items. This\ndesign fundamentally addresses the over-smoothing issue and enables MSSG a\nlinear time complexity with respect to the sequence length. We compare MSSG\nwith ten state-of-the-art baseline methods on six public benchmark datasets.\nOur experimental results demonstrate that MSSG significantly outperforms the\nbaseline methods, with an improvement of as much as 10.10%. Our analysis shows\nthe superior scalability of MSSG over the state-of-the-art self-attentive\nmethods. Our complexity analysis and run-time performance comparison together\nshow that MSSG is both theoretically and practically more efficient than\nself-attentive methods. Our analysis of the attention weights learned in\nSA-based methods indicates that on sparse recommendation data, modeling\ndependencies in all item pairs using the SA mechanism yields limited\ninformation gain, and thus, might not benefit the recommendation performance\n","authors":["Bo Peng","Ziqi Chen","Srinivasan Parthasarathy","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2311.07742v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.07997"},{"id":"http://arxiv.org/abs/2311.07715v1","updated":"2023-11-13T19:56:18Z","published":"2023-11-13T19:56:18Z","title":"PolyIE: A Dataset of Information Extraction from Polymer Material\n  Scientific Literature","summary":"  Scientific information extraction (SciIE), which aims to automatically\nextract information from scientific literature, is becoming more important than\never. However, there are no existing SciIE datasets for polymer materials,\nwhich is an important class of materials used ubiquitously in our daily lives.\nTo bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer\nmaterials. POLYIE is curated from 146 full-length polymer scholarly articles,\nwhich are annotated with different named entities (i.e., materials, properties,\nvalues, conditions) as well as their N-ary relations by domain experts. POLYIE\npresents several unique challenges due to diverse lexical formats of entities,\nambiguity between entities, and variable-length relations. We evaluate\nstate-of-the-art named entity extraction and relation extraction models on\nPOLYIE, analyze their strengths and weaknesses, and highlight some difficult\ncases for these models. To the best of our knowledge, POLYIE is the first SciIE\nbenchmark for polymer materials, and we hope it will lead to more research\nefforts from the community on this challenging task. Our code and data are\navailable on: https://github.com/jerry3027/PolyIE.\n","authors":["Jerry Junyang Cheung","Yuchen Zhuang","Yinghao Li","Pranav Shetty","Wantian Zhao","Sanjeev Grampurohit","Rampi Ramprasad","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07715v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.02082v2","updated":"2023-11-13T19:54:57Z","published":"2023-10-20T19:36:03Z","title":"Semantic Modelling of Organizational Knowledge as a Basis for Enterprise\n  Data Governance 4.0 -- Application to a Unified Clinical Data Model","summary":"  Individuals and organizations cope with an always-growing data amount,\nheterogeneous in contents and formats. A prerequisite to get value out this\ndata and minimise inherent risks related to multiple usages is an adequate data\nmanagement process yielding data quality and control over its lifecycle. Common\ndata governance frameworks relying on people, policies and processes falls\nshort of the overwhelming data complexity. Yet, harnessing this complexity is\nnecessary to achieve high quality standards. The later will condition the\noutcome of any downstream data usage, including generative artificial\nintelligence trained on this data. In this paper, we report our concrete\nexperience establishing a simple, cost-efficient framework, that enables\nmetadata-driven, agile and (semi-)automated data governance (i.e. Data\nGovernance 4.0). We explain how we implement and use this framework to\nintegrate 25 years of clinical study data at enterprise scale, in a fully\nproductive environment. The framework encompasses both methodologies and\ntechnologies leveraging semantic web principles. We built a knowledge graph\ndescribing avatars of data assets in their business context including\ngovernance principles. Multiple ontologies articulated by an enterprise upper\nontology enable key governance actions such as FAIRification, lifecycle\nmanagement, definition of roles and responsibilities, lineage across\ntransformations and provenance from source systems. This metadata model is the\nkeystone to data governance 4.0: a semi-automatized data management process,\ntaking in account the business context in an agile manner to adapt governance\nconstraints to each use case and dynamically tune it based on business changes.\n","authors":["Miguel AP Oliveira","Stephane Manara","Bruno Molé","Thomas Muller","Aurélien Guillouche","Lysann Hesske","Bruce Jordan","Gilles Hubert","Chinmay Kulkarni","Pralipta Jagdev","Cedric R. Berger"],"pdf_url":"https://arxiv.org/pdf/2311.02082v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.07575v1","updated":"2023-11-13T18:59:47Z","published":"2023-11-13T18:59:47Z","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for\n  Multi-modal Large Language Models","summary":"  We present SPHINX, a versatile multi-modal large language model (MLLM) with a\njoint mixing of model weights, tuning tasks, and visual embeddings. First, for\nstronger vision-language alignment, we unfreeze the large language model (LLM)\nduring pre-training, and introduce a weight mix strategy between LLMs trained\nby real-world and synthetic data. By directly integrating the weights from two\ndomains, the mixed LLM can efficiently incorporate diverse semantics with\nfavorable robustness. Then, to enable multi-purpose capabilities, we mix a\nvariety of tasks for joint visual instruction tuning, and design task-specific\ninstructions to avoid inter-task conflict. In addition to the basic visual\nquestion answering, we include more challenging tasks such as region-level\nunderstanding, caption grounding, document layout detection, and human pose\nestimation, contributing to mutual enhancement over different scenarios.\nAdditionally, we propose to extract comprehensive visual embeddings from\nvarious network architectures, pre-training paradigms, and information\ngranularity, providing language models with more robust image representations.\nBased on our proposed joint mixing, SPHINX exhibits superior multi-modal\nunderstanding capabilities on a wide range of applications. On top of this, we\nfurther propose an efficient strategy aiming to better capture fine-grained\nappearances of high-resolution images. With a mixing of different scales and\nhigh-resolution sub-images, SPHINX attains exceptional visual parsing and\nreasoning performance on existing evaluation benchmarks. We hope our work may\ncast a light on the exploration of joint mixing in future MLLM research. Code\nis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.\n","authors":["Ziyi Lin","Chris Liu","Renrui Zhang","Peng Gao","Longtian Qiu","Han Xiao","Han Qiu","Chen Lin","Wenqi Shao","Keqin Chen","Jiaming Han","Siyuan Huang","Yichi Zhang","Xuming He","Hongsheng Li","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2311.07575v1.pdf","comment":"Work in progress. Code and demos are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"id":"http://arxiv.org/abs/2311.07568v1","updated":"2023-11-13T18:56:33Z","published":"2023-11-13T18:56:33Z","title":"Feature emergence via margin maximization: case studies in algebraic\n  tasks","summary":"  Understanding the internal representations learned by neural networks is a\ncornerstone challenge in the science of machine learning. While there have been\nsignificant recent strides in some cases towards understanding how neural\nnetworks implement specific target functions, this paper explores a\ncomplementary question -- why do networks arrive at particular computational\nstrategies? Our inquiry focuses on the algebraic learning tasks of modular\naddition, sparse parities, and finite group operations. Our primary theoretical\nfindings analytically characterize the features learned by stylized neural\nnetworks for these algebraic tasks. Notably, our main technique demonstrates\nhow the principle of margin maximization alone can be used to fully specify the\nfeatures learned by the network. Specifically, we prove that the trained\nnetworks utilize Fourier features to perform modular addition and employ\nfeatures corresponding to irreducible group-theoretic representations to\nperform compositions in general groups, aligning closely with the empirical\nobservations of Nanda et al. and Chughtai et al. More generally, we hope our\ntechniques can help to foster a deeper understanding of why neural networks\nadopt specific computational strategies.\n","authors":["Depen Morwani","Benjamin L. Edelman","Costin-Andrei Oncescu","Rosie Zhao","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2311.07568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07565v1","updated":"2023-11-13T18:54:43Z","published":"2023-11-13T18:54:43Z","title":"Exploration via linearly perturbed loss minimisation","summary":"  We introduce exploration via linear loss perturbations (EVILL), a randomised\nexploration method for structured stochastic bandit problems that works by\nsolving for the minimiser of a linearly perturbed regularised negative\nlog-likelihood function. We show that, for the case of generalised linear\nbandits, EVILL reduces to perturbed history exploration (PHE), a method where\nexploration is done by training on randomly perturbed rewards. In doing so, we\nprovide a simple and clean explanation of when and why random reward\nperturbations give rise to good bandit algorithms. With the data-dependent\nperturbations we propose, not present in previous PHE-type methods, EVILL is\nshown to match the performance of Thompson-sampling-style\nparameter-perturbation methods, both in theory and in practice. Moreover, we\nshow an example outside of generalised linear bandits where PHE leads to\ninconsistent estimates, and thus linear regret, while EVILL remains performant.\nLike PHE, EVILL can be implemented in just a few lines of code.\n","authors":["David Janz","Shuai Liu","Alex Ayoub","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2311.07565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07564v1","updated":"2023-11-13T18:54:17Z","published":"2023-11-13T18:54:17Z","title":"Can Authorship Attribution Models Distinguish Speakers in Speech\n  Transcripts?","summary":"  Authorship verification is the problem of determining if two distinct writing\nsamples share the same author and is typically concerned with the attribution\nof written text. In this paper, we explore the attribution of transcribed\nspeech, which poses novel challenges. The main challenge is that many stylistic\nfeatures, such as punctuation and capitalization, are not available or\nreliable. Therefore, we expect a priori that transcribed speech is a more\nchallenging domain for attribution. On the other hand, other stylistic\nfeatures, such as speech disfluencies, may enable more successful attribution\nbut, being specific to speech, require special purpose models. To better\nunderstand the challenges of this setting, we contribute the first systematic\nstudy of speaker attribution based solely on transcribed speech. Specifically,\nwe propose a new benchmark for speaker attribution focused on conversational\nspeech transcripts. To control for spurious associations of speakers with\ntopic, we employ both conversation prompts and speakers' participating in the\nsame conversation to construct challenging verification trials of varying\ndifficulties. We establish the state of the art on this new benchmark by\ncomparing a suite of neural and non-neural baselines, finding that although\nwritten text attribution models achieve surprisingly good performance in\ncertain settings, they struggle in the hardest settings we consider.\n","authors":["Cristina Aggazzotti","Nicholas Andrews","Elizabeth Allyn Smith"],"pdf_url":"https://arxiv.org/pdf/2311.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07563v1","updated":"2023-11-13T18:53:50Z","published":"2023-11-13T18:53:50Z","title":"Learning Control Policies of Hodgkin-Huxley Neuronal Dynamics","summary":"  We present a neural network approach for closed-loop deep brain stimulation\n(DBS). We cast the problem of finding an optimal neurostimulation strategy as a\ncontrol problem. In this setting, control policies aim to optimize therapeutic\noutcomes by tailoring the parameters of a DBS system, typically via electrical\nstimulation, in real time based on the patient's ongoing neuronal activity. We\napproximate the value function offline using a neural network to enable\ngenerating controls (stimuli) in real time via the feedback form. The neuronal\nactivity is characterized by a nonlinear, stiff system of differential\nequations as dictated by the Hodgkin-Huxley model. Our training process\nleverages the relationship between Pontryagin's maximum principle and\nHamilton-Jacobi-Bellman equations to update the value function estimates\nsimultaneously. Our numerical experiments illustrate the accuracy of our\napproach for out-of-distribution samples and the robustness to moderate shocks\nand disturbances in the system.\n","authors":["Malvern Madondo","Deepanshu Verma","Lars Ruthotto","Nicholas Au Yong"],"pdf_url":"https://arxiv.org/pdf/2311.07563v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 12 pages"},{"id":"http://arxiv.org/abs/2311.07558v1","updated":"2023-11-13T18:51:57Z","published":"2023-11-13T18:51:57Z","title":"Data-Efficient Task Generalization via Probabilistic Model-based Meta\n  Reinforcement Learning","summary":"  We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning\n(Meta-RL) algorithm designed to efficiently adapt control policies to changing\ndynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swift\nadaptation to new dynamics with minimal interaction data. Existing Meta-RL\nmethods require abundant meta-learning data, limiting their applicability in\nsettings such as robotics, where data is costly to obtain. To address this,\nPACOH-RL incorporates regularization and epistemic uncertainty quantification\nin both the meta-learning and task adaptation stages. When facing new dynamics,\nwe use these uncertainty estimates to effectively guide exploration and data\ncollection. Overall, this enables positive transfer, even when access to data\nfrom prior tasks or dynamic settings is severely limited. Our experiment\nresults demonstrate that PACOH-RL outperforms model-based RL and model-based\nMeta-RL baselines in adapting to new dynamic conditions. Finally, on a real\nrobotic car, we showcase the potential for efficient RL policy adaptation in\ndiverse, data-scarce conditions.\n","authors":["Arjun Bhardwaj","Jonas Rothfuss","Bhavya Sukhija","Yarden As","Marco Hutter","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2311.07558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11816v2","updated":"2023-11-13T18:51:42Z","published":"2023-06-20T18:19:17Z","title":"Learning to Generate Better Than Your LLM","summary":"  Reinforcement learning (RL) has emerged as a powerful paradigm for\nfine-tuning Large Language Models (LLMs) for text generation. In particular,\nrecent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with\nusers after finetuning with RL. Capitalizing on key properties of text\ngeneration, we seek to investigate RL algorithms beyond general purpose\nalgorithms like Proximal Policy Optimization (PPO). In particular, we extend RL\nalgorithms to allow them to interact with a dynamic black-box guide LLM and\npropose RL with guided feedback (RLGF), a suite of RL algorithms for LLM\nfine-tuning. We provide two ways for the guide LLM to interact with the LLM to\nbe optimized for maximizing rewards. The guide LLM can generate text which\nserves as additional starting states for the RL optimization procedure. The\nguide LLM can also be used to complete the partial sentences generated by the\nLLM that is being optimized, treating the guide LLM as an expert to imitate and\nsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,\nand TL;DR summarization tasks. We show that our RL algorithms achieve higher\nperformance than supervised learning (SL) and the RL baseline PPO,\ndemonstrating the benefit of interaction with the guide LLM. On both CommonGen\nand TL;DR, we not only outperform our SL baselines but also improve upon PPO\nacross a variety of metrics beyond the one we optimized for. Our code can be\nfound at https://github.com/Cornell-RL/tril.\n","authors":["Jonathan D. Chang","Kiante Brantley","Rajkumar Ramamurthy","Dipendra Misra","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2306.11816v2.pdf","comment":"23 pages, 5 figures, 7 tables, 4 algorithms"},{"id":"http://arxiv.org/abs/2311.01455v2","updated":"2023-11-13T18:40:10Z","published":"2023-11-02T17:59:21Z","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning\n  via Generative Simulation","summary":"  We present RoboGen, a generative robotic agent that automatically learns\ndiverse robotic skills at scale via generative simulation. RoboGen leverages\nthe latest advancements in foundation and generative models. Instead of\ndirectly using or adapting these models to produce policies or low-level\nactions, we advocate for a generative scheme, which uses these models to\nautomatically generate diversified tasks, scenes, and training supervisions,\nthereby scaling up robotic skill learning with minimal human supervision. Our\napproach equips a robotic agent with a self-guided propose-generate-learn\ncycle: the agent first proposes interesting tasks and skills to develop, and\nthen generates corresponding simulation environments by populating pertinent\nobjects and assets with proper spatial configurations. Afterwards, the agent\ndecomposes the proposed high-level task into sub-tasks, selects the optimal\nlearning approach (reinforcement learning, motion planning, or trajectory\noptimization), generates required training supervision, and then learns\npolicies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried\nrepeatedly, producing an endless stream of skill demonstrations associated with\ndiverse tasks and environments.\n","authors":["Yufei Wang","Zhou Xian","Feng Chen","Tsun-Hsuan Wang","Yian Wang","Zackory Erickson","David Held","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2311.01455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07550v1","updated":"2023-11-13T18:39:44Z","published":"2023-11-13T18:39:44Z","title":"Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks\n  for Tabular Data","summary":"  Deep neural networks (DNNs) have shown great promise in various domains.\nAlongside these developments, vulnerabilities associated with DNN training,\nsuch as backdoor attacks, are a significant concern. These attacks involve the\nsubtle insertion of triggers during model training, allowing for manipulated\npredictions. More recently, DNNs for tabular data have gained increasing\nattention due to the rise of transformer models.\n  Our research presents a comprehensive analysis of backdoor attacks on tabular\ndata using DNNs, particularly focusing on transformer-based networks. Given the\ninherent complexities of tabular data, we explore the challenges of embedding\nbackdoors. Through systematic experimentation across benchmark datasets, we\nuncover that transformer-based DNNs for tabular data are highly susceptible to\nbackdoor attacks, even with minimal feature value alterations. Our results\nindicate nearly perfect attack success rates (approx100%) by introducing novel\nbackdoor attack strategies to tabular data. Furthermore, we evaluate several\ndefenses against these attacks, identifying Spectral Signatures as the most\neffective one. Our findings highlight the urgency to address such\nvulnerabilities and provide insights into potential countermeasures for\nsecuring DNN models against backdoors on tabular data.\n","authors":["Bart Pleiter","Behrad Tajalli","Stefanos Koffas","Gorka Abad","Jing Xu","Martha Larson","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2311.07550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07548v1","updated":"2023-11-13T18:37:07Z","published":"2023-11-13T18:37:07Z","title":"Interpretable Fine-Tuning for Graph Neural Network Surrogate Models","summary":"  Data-based surrogate modeling has surged in capability in recent years with\nthe emergence of graph neural networks (GNNs), which can operate directly on\nmesh-based representations of data. The goal of this work is to introduce an\ninterpretable fine-tuning strategy for GNNs, with application to unstructured\nmesh-based fluid dynamics modeling. The end result is a fine-tuned GNN that\nadds interpretability to a pre-trained baseline GNN through an adaptive\nsub-graph sampling strategy that isolates regions in physical space\nintrinsically linked to the forecasting task, while retaining the predictive\ncapability of the baseline. The structures identified by the fine-tuned GNNs,\nwhich are adaptively produced in the forward pass as explicit functions of the\ninput, serve as an accessible link between the baseline model architecture, the\noptimization goal, and known problem-specific physics. Additionally, through a\nregularization procedure, the fine-tuned GNNs can also be used to identify,\nduring inference, graph nodes that correspond to a majority of the anticipated\nforecasting error, adding a novel interpretable error-tagging capability to\nbaseline models. Demonstrations are performed using unstructured flow data\nsourced from flow over a backward-facing step at high Reynolds numbers.\n","authors":["Shivam Barwey","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2311.07548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07541v1","updated":"2023-11-13T18:31:48Z","published":"2023-11-13T18:31:48Z","title":"mlscorecheck: Testing the consistency of reported performance scores and\n  experiments in machine learning","summary":"  Addressing the reproducibility crisis in artificial intelligence through the\nvalidation of reported experimental results is a challenging task. It\nnecessitates either the reimplementation of techniques or a meticulous\nassessment of papers for deviations from the scientific method and best\nstatistical practices. To facilitate the validation of reported results, we\nhave developed numerical techniques capable of identifying inconsistencies\nbetween reported performance scores and various experimental setups in machine\nlearning problems, including binary/multiclass classification and regression.\nThese consistency tests are integrated into the open-source package\nmlscorecheck, which also provides specific test bundles designed to detect\nsystematically recurring flaws in various fields, such as retina image\nprocessing and synthetic minority oversampling.\n","authors":["György Kovács","Attila Fazekas"],"pdf_url":"https://arxiv.org/pdf/2311.07541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07538v1","updated":"2023-11-13T18:28:25Z","published":"2023-11-13T18:28:25Z","title":"Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided\n  Classifiers","summary":"  Recent approaches have explored language-guided classifiers capable of\nclassifying examples from novel tasks when provided with task-specific natural\nlanguage explanations, instructions or prompts (Sanh et al., 2022; R. Menon et\nal., 2022). While these classifiers can generalize in zero-shot settings, their\ntask performance often varies substantially between different language\nexplanations in unpredictable ways (Lu et al., 2022; Gonen et al., 2022). Also,\ncurrent approaches fail to leverage unlabeled examples that may be available in\nmany scenarios. Here, we introduce TALC, a framework that uses data programming\nto adapt a language-guided classifier for a new task during inference when\nprovided with explanations from multiple teachers and unlabeled test examples.\nOur results show that TALC consistently outperforms a competitive baseline from\nprior work by an impressive 9.3% (relative improvement). Further, we\ndemonstrate the robustness of TALC to variations in the quality and quantity of\nprovided explanations, highlighting its potential in scenarios where learning\nfrom multiple teachers or a crowd is involved. Our code is available at:\nhttps://github.com/WeiKangda/TALC.git.\n","authors":["Kangda Wei","Sayan Ghosh","Rakesh R. Menon","Shashank Srivastava"],"pdf_url":"https://arxiv.org/pdf/2311.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1902.11122v4","updated":"2023-11-13T18:25:42Z","published":"2019-02-22T10:09:11Z","title":"Deep Learning in Cardiology","summary":"  The medical field is creating large amount of data that physicians are unable\nto decipher and use efficiently. Moreover, rule-based expert systems are\ninefficient in solving complicated medical tasks or for creating insights using\nbig data. Deep learning has emerged as a more accurate and effective technology\nin a wide range of medical problems such as diagnosis, prediction and\nintervention. Deep learning is a representation learning method that consists\nof layers that transform the data non-linearly, thus, revealing hierarchical\nrelationships and structures. In this review we survey deep learning\napplication papers that use structured data, signal and imaging modalities from\ncardiology. We discuss the advantages and limitations of applying deep learning\nin cardiology that also apply in medicine in general, while proposing certain\ndirections as the most viable for clinical use.\n","authors":["Paschalis Bizopoulos","Dimitrios Koutsouris"],"pdf_url":"https://arxiv.org/pdf/1902.11122v4.pdf","comment":"27 pages, 2 figures, 10 tables"},{"id":"http://arxiv.org/abs/2211.13308v4","updated":"2023-11-13T18:25:27Z","published":"2022-11-23T21:25:39Z","title":"SciRepEval: A Multi-Format Benchmark for Scientific Document\n  Representations","summary":"  Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks without further fine-tuning. However, existing\nbenchmarks for evaluating these representations fail to capture the diversity\nof relevant tasks. In response, we introduce SciRepEval, the first\ncomprehensive benchmark for training and evaluating scientific document\nrepresentations. It includes 24 challenging and realistic tasks, 8 of which are\nnew, across four formats: classification, regression, ranking and search. We\nthen use this benchmark to study and improve the generalization ability of\nscientific document representation models. We show how state-of-the-art models\nlike SPECTER and SciNCL struggle to generalize across the task formats, and\nthat simple multi-task training fails to improve them. However, a new approach\nthat learns multiple embeddings per document, each tailored to a different\nformat, can improve performance. We experiment with task-format-specific\ncontrol codes and adapters and find they outperform the existing\nsingle-embedding state-of-the-art by over 2 points absolute. We release the\nresulting family of multi-format models, called SPECTER2, for the community to\nuse and build on.\n","authors":["Amanpreet Singh","Mike D'Arcy","Arman Cohan","Doug Downey","Sergey Feldman"],"pdf_url":"https://arxiv.org/pdf/2211.13308v4.pdf","comment":"19 pages, 2 figures, 11 tables. Accepted in EMNLP 2023 Main\n  Conference"},{"id":"http://arxiv.org/abs/2301.11975v3","updated":"2023-11-13T18:24:41Z","published":"2023-01-27T20:22:18Z","title":"Byte Pair Encoding for Symbolic Music","summary":"  When used with deep learning, the symbolic music modality is often coupled\nwith language model architectures. To do so, the music needs to be tokenized,\ni.e. converted into a sequence of discrete tokens. This can be achieved by\ndifferent approaches, as music can be composed of simultaneous tracks, of\nsimultaneous notes with several attributes. Until now, the proposed\ntokenizations rely on small vocabularies of tokens describing the note\nattributes and time events, resulting in fairly long token sequences, and a\nsub-optimal use of the embedding space of language models. Recent research has\nput efforts on reducing the overall sequence length by merging embeddings or\ncombining tokens. In this paper, we show that Byte Pair Encoding, a compression\ntechnique widely used for natural language, significantly decreases the\nsequence length while increasing the vocabulary size. By doing so, we leverage\nthe embedding capabilities of such models with more expressive tokens,\nresulting in both better results and faster inference in generation and\nclassification tasks. The source code is shared on Github, along with a\ncompanion website. Finally, BPE is directly implemented in MidiTok, allowing\nthe reader to easily benefit from this method.\n","authors":["Nathan Fradet","Nicolas Gutowski","Fabien Chhel","Jean-Pierre Briot"],"pdf_url":"https://arxiv.org/pdf/2301.11975v3.pdf","comment":"EMNLP 2023, source code: https://github.com/Natooz/BPE-Symbolic-Music"},{"id":"http://arxiv.org/abs/2309.08534v2","updated":"2023-11-13T18:24:16Z","published":"2023-09-15T16:52:29Z","title":"Towards Last-layer Retraining for Group Robustness with Fewer\n  Annotations","summary":"  Empirical risk minimization (ERM) of neural networks is prone to\nover-reliance on spurious correlations and poor generalization on minority\ngroups. The recent deep feature reweighting (DFR) technique achieves\nstate-of-the-art group robustness via simple last-layer retraining, but it\nrequires held-out group and class annotations to construct a group-balanced\nreweighting dataset. In this work, we examine this impractical requirement and\nfind that last-layer retraining can be surprisingly effective with no group\nannotations (other than for model selection) and only a handful of class\nannotations. We first show that last-layer retraining can greatly improve\nworst-group accuracy even when the reweighting dataset has only a small\nproportion of worst-group data. This implies a \"free lunch\" where holding out a\nsubset of training data to retrain the last layer can substantially outperform\nERM on the entire dataset with no additional data or annotations. To further\nimprove group robustness, we introduce a lightweight method called selective\nlast-layer finetuning (SELF), which constructs the reweighting dataset using\nmisclassifications or disagreements. Our empirical and theoretical results\npresent the first evidence that model disagreement upsamples worst-group data,\nenabling SELF to nearly match DFR on four well-established benchmarks across\nvision and language tasks with no group annotations and less than 3% of the\nheld-out class annotations. Our code is available at\nhttps://github.com/tmlabonte/last-layer-retraining.\n","authors":["Tyler LaBonte","Vidya Muthukumar","Abhishek Kumar"],"pdf_url":"https://arxiv.org/pdf/2309.08534v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07537v1","updated":"2023-11-13T18:23:46Z","published":"2023-11-13T18:23:46Z","title":"Estimating optical vegetation indices with Sentinel-1 SAR data and\n  AutoML","summary":"  Current optical vegetation indices (VIs) for monitoring forest ecosystems are\nwidely used in various applications. However, continuous monitoring based on\noptical satellite data can be hampered by atmospheric effects such as clouds.\nOn the contrary, synthetic aperture radar (SAR) data can offer insightful and\nsystematic forest monitoring with complete time series due to signal\npenetration through clouds and day and night acquisitions. The goal of this\nwork is to overcome the issues affecting optical data with SAR data and serve\nas a substitute for estimating optical VIs for forests using machine learning.\nTime series of four VIs (LAI, FAPAR, EVI and NDVI) were estimated using\nmultitemporal Sentinel-1 SAR and ancillary data. This was enabled by creating a\npaired multi-temporal and multi-modal dataset in Google Earth Engine (GEE),\nincluding temporally and spatially aligned Sentinel-1, Sentinel-2, digital\nelevation model (DEM), weather and land cover datasets (MMT-GEE). The use of\nancillary features generated from DEM and weather data improved the results.\nThe open-source Automatic Machine Learning (AutoML) approach, auto-sklearn,\noutperformed Random Forest Regression for three out of four VIs, while a 1-hour\noptimization length was enough to achieve sufficient results with an R2 of\n69-84% low errors (0.05-0.32 of MAE depending on VI). Great agreement was also\nfound for selected case studies in the time series analysis and in the spatial\ncomparison between the original and estimated SAR-based VIs. In general,\ncompared to VIs from currently freely available optical satellite data and\navailable global VI products, a better temporal resolution (up to 240\nmeasurements/year) and a better spatial resolution (20 m) were achieved using\nestimated SAR-based VIs. A great advantage of the SAR-based VI is the ability\nto detect abrupt forest changes with a sub-weekly temporal accuracy.\n","authors":["Daniel Paluba","Bertrand Le Saux","Francesco Sarti","Přemysl Stych"],"pdf_url":"https://arxiv.org/pdf/2311.07537v1.pdf","comment":"Full research article. 30 pages, 13 figures, 8 tables"},{"id":"http://arxiv.org/abs/2009.06412v7","updated":"2023-11-13T18:22:19Z","published":"2020-09-10T08:05:06Z","title":"Comprehensive Comparison of Deep Learning Models for Lung and COVID-19\n  Lesion Segmentation in CT scans","summary":"  Recently there has been an explosion in the use of Deep Learning (DL) methods\nfor medical image segmentation. However the field's reliability is hindered by\nthe lack of a common base of reference for accuracy/performance evaluation and\nthe fact that previous research uses different datasets for evaluation. In this\npaper, an extensive comparison of DL models for lung and COVID-19 lesion\nsegmentation in Computerized Tomography (CT) scans is presented, which can also\nbe used as a benchmark for testing medical image segmentation models. Four DL\narchitectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomly\ninitialized and pretrained encoders (variations of VGG, DenseNet, ResNet,\nResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct\n200 tested models. Three experimental setups are conducted for lung\nsegmentation, lesion segmentation and lesion segmentation using the original\nlung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20\nfor validation) is used for training/validation and a different public dataset\nconsisting of 829 images from 9 CT scan volumes for testing. Multiple findings\nare provided including the best architecture-encoder models for each experiment\nas well as mean Dice results for each experiment, architecture and encoder\nindependently. Finally, the upper bounds improvements when using lung masks as\na preprocessing step or when using pretrained models are quantified. The source\ncode and 600 pretrained models for the three experiments are provided, suitable\nfor fine-tuning in experimental setups without GPU capabilities.\n","authors":["Paschalis Bizopoulos","Nicholas Vretos","Petros Daras"],"pdf_url":"https://arxiv.org/pdf/2009.06412v7.pdf","comment":"20 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.07534v1","updated":"2023-11-13T18:21:33Z","published":"2023-11-13T18:21:33Z","title":"Unsupervised Musical Object Discovery from Audio","summary":"  Current object-centric learning models such as the popular SlotAttention\narchitecture allow for unsupervised visual scene decomposition. Our novel\nMusicSlots method adapts SlotAttention to the audio domain, to achieve\nunsupervised music decomposition. Since concepts of opacity and occlusion in\nvision have no auditory analogues, the softmax normalization of alpha masks in\nthe decoders of visual object-centric models is not well-suited for decomposing\naudio objects. MusicSlots overcomes this problem. We introduce a\nspectrogram-based multi-object music dataset tailored to evaluate\nobject-centric learning on western tonal music. MusicSlots achieves good\nperformance on unsupervised note discovery and outperforms several established\nbaselines on supervised note property prediction tasks.\n","authors":["Joonsu Gha","Vincent Herrmann","Benjamin Grewe","Jürgen Schmidhuber","Anand Gopalakrishnan"],"pdf_url":"https://arxiv.org/pdf/2311.07534v1.pdf","comment":"Accepted to Machine Learning for Audio, NeurIPS 2023"},{"id":"http://arxiv.org/abs/2306.08698v3","updated":"2023-11-13T18:19:17Z","published":"2023-06-14T18:38:32Z","title":"Phase Transitions of Civil Unrest across Countries and Time","summary":"  Phase transitions, characterized by abrupt shifts between macroscopic\npatterns of organization, are ubiquitous in complex systems. Despite\nconsiderable research in the physical and natural sciences, the empirical study\nof this phenomenon in societal systems is relatively underdeveloped. The goal\nof this study is to explore whether the dynamics of collective civil unrest can\nbe plausibly characterized as a sequence of recurrent phase shifts, with each\nphase having measurable and identifiable latent characteristics. Building on\nprevious efforts to characterize civil unrest as a self-organized critical\nsystem, we introduce a macro-level statistical model of civil unrest and\nevaluate its plausibility using a comprehensive dataset of civil unrest events\nin 170 countries from 1946 to 2017. Our findings demonstrate that the\nmacro-level phase model effectively captures the characteristics of civil\nunrest data from diverse countries globally and that universal mechanisms may\nunderlie certain aspects of the dynamics of civil unrest. We also introduce a\nscale to quantify a country's long-term unrest per unit of time and show that\ncivil unrest events tend to cluster geographically, with the magnitude of civil\nunrest concentrated in specific regions. Our approach has the potential to\nidentify and measure phase transitions in various collective human phenomena\nbeyond civil unrest, contributing to a better understanding of complex social\nsystems.\n","authors":["Dan Braha"],"pdf_url":"https://arxiv.org/pdf/2306.08698v3.pdf","comment":"Main paper (57 pages); Supporting Information (144 pages) will be\n  available upon request. To appear in npj Complexity"},{"id":"http://arxiv.org/abs/2311.07527v1","updated":"2023-11-13T18:13:55Z","published":"2023-11-13T18:13:55Z","title":"Automatic Identification of Driving Maneuver Patterns using a Robust\n  Hidden Semi-Markov Models","summary":"  There is an increase in interest to model driving maneuver patterns via the\nautomatic unsupervised clustering of naturalistic sequential kinematic driving\ndata. The patterns learned are often used in transportation research areas such\nas eco-driving, road safety, and intelligent vehicles. One such model capable\nof modeling these patterns is the Hierarchical Dirichlet Process Hidden\nSemi-Markov Model (HDP-HSMM), as it is often used to estimate data\nsegmentation, state duration, and transition probabilities. While this model is\na powerful tool for automatically clustering observed sequential data, the\nexisting HDP-HSMM estimation suffers from an inherent tendency to overestimate\nthe number of states. This can result in poor estimation, which can potentially\nimpact impact transportation research through incorrect inference of driving\npatterns. In this paper, a new robust HDP-HSMM (rHDP-HSMM) method is proposed\nto reduce the number of redundant states and improve the consistency of the\nmodel's estimation. Both a simulation study and a case study using naturalistic\ndriving data are presented to demonstrate the effectiveness of the proposed\nrHDP-HSMM in identifying and inference of driving maneuver patterns.\n","authors":["Matthew Aguirre","Wenbo Sun"," Jionghua"," Jin","Yang Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10455v3","updated":"2023-11-13T18:10:23Z","published":"2023-07-19T20:54:08Z","title":"A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect\n  Dataset","summary":"  In an effort to catalog insect biodiversity, we propose a new large dataset\nof hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is\ntaxonomically classified by an expert, and also has associated genetic\ninformation including raw nucleotide barcode sequences and assigned barcode\nindex numbers, which are genetically-based proxies for species classification.\nThis paper presents a curated million-image dataset, primarily to train\ncomputer-vision models capable of providing image-based taxonomic assessment,\nhowever, the dataset also presents compelling characteristics, the study of\nwhich would be of interest to the broader machine learning community. Driven by\nthe biological nature inherent to the dataset, a characteristic long-tailed\nclass-imbalance distribution is exhibited. Furthermore, taxonomic labelling is\na hierarchical classification scheme, presenting a highly fine-grained\nclassification problem at lower levels. Beyond spurring interest in\nbiodiversity research within the machine learning community, progress on\ncreating an image-based taxonomic classifier will also further the ultimate\ngoal of all BIOSCAN research: to lay the foundation for a comprehensive survey\nof global biodiversity. This paper introduces the dataset and explores the\nclassification task through the implementation and analysis of a baseline\nclassifier.\n","authors":["Zahra Gharaee","ZeMing Gong","Nicholas Pellegrino","Iuliia Zarubiieva","Joakim Bruslund Haurum","Scott C. Lowe","Jaclyn T. A. McKeown","Chris C. Y. Ho","Joschka McLeod","Yi-Yun C Wei","Jireh Agda","Sujeevan Ratnasingham","Dirk Steinke","Angel X. Chang","Graham W. Taylor","Paul Fieguth"],"pdf_url":"https://arxiv.org/pdf/2307.10455v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07519v1","updated":"2023-11-13T18:00:06Z","published":"2023-11-13T18:00:06Z","title":"Machine Learning For Beamline Steering","summary":"  Beam steering is the process involving the calibration of the angle and\nposition at which a particle accelerator's electron beam is incident upon the\nx-ray target with respect to the rotation axis of the collimator. Beam Steering\nis an essential task for light sources. In the case under study, the LINAC To\nUndulator (LTU) section of the beamline is difficult to aim. Each use of the\naccelerator requires re-calibration of the magnets in this section. This\ninvolves a substantial amount of time and effort from human operators, while\nreducing scientific throughput of the light source. We investigate the use of\ndeep neural networks to assist in this task. The deep learning models are\ntrained on archival data and then validated on simulation data. The performance\nof the deep learning model is contrasted against that of trained human\noperators.\n","authors":["Isaac Kante"],"pdf_url":"https://arxiv.org/pdf/2311.07519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07518v1","updated":"2023-11-13T17:59:37Z","published":"2023-11-13T17:59:37Z","title":"FEMDA: a unified framework for discriminant analysis","summary":"  Although linear and quadratic discriminant analysis are widely recognized\nclassical methods, they can encounter significant challenges when dealing with\nnon-Gaussian distributions or contaminated datasets. This is primarily due to\ntheir reliance on the Gaussian assumption, which lacks robustness. We first\nexplain and review the classical methods to address this limitation and then\npresent a novel approach that overcomes these issues. In this new approach, the\nmodel considered is an arbitrary Elliptically Symmetrical (ES) distribution per\ncluster with its own arbitrary scale parameter. This flexible model allows for\npotentially diverse and independent samples that may not follow identical\ndistributions. By deriving a new decision rule, we demonstrate that\nmaximum-likelihood parameter estimation and classification are simple,\nefficient, and robust compared to state-of-the-art methods.\n","authors":["Pierre Houdouin","Matthieu Jonckheere","Frederic Pascal"],"pdf_url":"https://arxiv.org/pdf/2311.07518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19452v3","updated":"2023-11-13T17:57:19Z","published":"2023-05-30T23:23:25Z","title":"Bigger, Better, Faster: Human-level Atari with human-level efficiency","summary":"  We introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.\n","authors":["Max Schwarzer","Johan Obando-Ceron","Aaron Courville","Marc Bellemare","Rishabh Agarwal","Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2305.19452v3.pdf","comment":"ICML 2023, revised version"},{"id":"http://arxiv.org/abs/2311.07513v1","updated":"2023-11-13T17:56:45Z","published":"2023-11-13T17:56:45Z","title":"A Hypothesis on Good Practices for AI-based Systems for Financial Time\n  Series Forecasting: Towards Domain-Driven XAI Methods","summary":"  Machine learning and deep learning have become increasingly prevalent in\nfinancial prediction and forecasting tasks, offering advantages such as\nenhanced customer experience, democratising financial services, improving\nconsumer protection, and enhancing risk management. However, these complex\nmodels often lack transparency and interpretability, making them challenging to\nuse in sensitive domains like finance. This has led to the rise of eXplainable\nArtificial Intelligence (XAI) methods aimed at creating models that are easily\nunderstood by humans. Classical XAI methods, such as LIME and SHAP, have been\ndeveloped to provide explanations for complex models. While these methods have\nmade significant contributions, they also have limitations, including\ncomputational complexity, inherent model bias, sensitivity to data sampling,\nand challenges in dealing with feature dependence. In this context, this paper\nexplores good practices for deploying explainability in AI-based systems for\nfinance, emphasising the importance of data quality, audience-specific methods,\nconsideration of data properties, and the stability of explanations. These\npractices aim to address the unique challenges and requirements of the\nfinancial industry and guide the development of effective XAI tools.\n","authors":["Branka Hadji Misheva","Joerg Osterrieder"],"pdf_url":"https://arxiv.org/pdf/2311.07513v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2302.08624v6","updated":"2023-11-13T17:56:19Z","published":"2023-02-16T23:29:22Z","title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis","summary":"  We introduce InstructABSA, an instruction learning paradigm for Aspect-Based\nSentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,\nand neutral examples to each training sample, and instruction tune the model\n(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.\nExperimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that\nInstructABSA outperforms the previous state-of-the-art (SOTA) approaches on\nTerm Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair\nExtraction (ASPE) subtasks. In particular, InstructABSA outperforms the\nprevious state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the\nRest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%\npoints, surpassing 7x larger models. We also get competitive results on AOOE,\nAOPE, and AOSTE subtasks indicating strong generalization ability to all\nsubtasks. Exploring sample efficiency reveals that just 50% train data is\nrequired to get competitive results with other instruction tuning approaches.\nLastly, we assess the quality of instructions and observe that InstructABSA's\nperformance experiences a decline of ~10% when adding misleading examples.\n","authors":["Kevin Scaria","Himanshu Gupta","Siddharth Goyal","Saurabh Arjun Sawant","Swaroop Mishra","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2302.08624v6.pdf","comment":"4 pages, 3 figures, 9 tables, 9 appendix pages"},{"id":"http://arxiv.org/abs/2311.07511v1","updated":"2023-11-13T17:55:28Z","published":"2023-11-13T17:55:28Z","title":"Machine learning for uncertainty estimation in fusing precipitation\n  observations from satellites and ground-based gauges","summary":"  To form precipitation datasets that are accurate and, at the same time, have\nhigh spatial densities, data from satellites and gauges are often merged in the\nliterature. However, uncertainty estimates for the data acquired in this manner\nare scarcely provided, although the importance of uncertainty quantification in\npredictive modelling is widely recognized. Furthermore, the benefits that\nmachine learning can bring to the task of providing such estimates have not\nbeen broadly realized and properly explored through benchmark experiments. The\npresent study aims at filling in this specific gap by conducting the first\nbenchmark tests on the topic. On a large dataset that comprises 15-year-long\nmonthly data spanning across the contiguous United States, we extensively\ncompared six learners that are, by their construction, appropriate for\npredictive uncertainty quantification. These are the quantile regression (QR),\nquantile regression forests (QRF), generalized random forests (GRF), gradient\nboosting machines (GBM), light gradient boosting machines (LightGBM) and\nquantile regression neural networks (QRNN). The comparison referred to the\ncompetence of the learners in issuing predictive quantiles at nine levels that\nfacilitate a good approximation of the entire predictive probability\ndistribution, and was primarily based on the quantile and continuous ranked\nprobability skill scores. Three types of predictor variables (i.e., satellite\nprecipitation variables, distances between a point of interest and satellite\ngrid points, and elevation at a point of interest) were used in the comparison\nand were additionally compared with each other. This additional comparison was\nbased on the explainable machine learning concept of feature importance. The\nresults suggest that the order from the best to the worst of the learners for\nthe task investigated is the following: LightGBM, QRF, GRF, GBM, QRNN and QR...\n","authors":["Georgia Papacharalampous","Hristos Tyralis","Nikolaos Doulamis","Anastasios Doulamis"],"pdf_url":"https://arxiv.org/pdf/2311.07511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07510v1","updated":"2023-11-13T17:55:07Z","published":"2023-11-13T17:55:07Z","title":"Explicit Foundation Model Optimization with Self-Attentive Feed-Forward\n  Neural Units","summary":"  Iterative approximation methods using backpropagation enable the optimization\nof neural networks, but they remain computationally expensive, especially when\nused at scale. This paper presents an efficient alternative for optimizing\nneural networks that reduces the costs of scaling neural networks and provides\nhigh-efficiency optimizations for low-resource applications. We will discuss a\ngeneral result about feed-forward neural networks and then extend this solution\nto compositional (mult-layer) networks, which are applied to a simplified\ntransformer block containing feed-forward and self-attention layers. These\nmodels are used to train highly-specified and complex multi-layer neural\narchitectures that we refer to as self-attentive feed-forward unit (SAFFU)\nlayers, which we use to develop a transformer that appears to generalize well\nover small, cognitively-feasible, volumes of data. Testing demonstrates\nexplicit solutions outperform models optimized by backpropagation alone.\nMoreover, further application of backpropagation after explicit solutions leads\nto better optima from smaller scales of data, training effective models from\nmuch less data is enabled by explicit solution warm starts. We then carry out\nablation experiments training a roadmap of about 250 transformer models over\n1-million tokens to determine ideal settings. We find that multiple different\narchitectural variants produce highly-performant models, and discover from this\nablation that some of the best are not the most parameterized. This appears to\nindicate well-generalized models could be reached using less data by using\nexplicit solutions, and that architectural exploration using explicit solutions\npays dividends in guiding the search for efficient variants with fewer\nparameters, and which could be incorporated into low-resource hardware where AI\nmight be embodied.\n","authors":["Jake Ryland Williams","Haoran Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.07510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07504v1","updated":"2023-11-13T17:45:28Z","published":"2023-11-13T17:45:28Z","title":"STEM Rebalance: A Novel Approach for Tackling Imbalanced Datasets using\n  SMOTE, Edited Nearest Neighbour, and Mixup","summary":"  Imbalanced datasets in medical imaging are characterized by skewed class\nproportions and scarcity of abnormal cases. When trained using such data,\nmodels tend to assign higher probabilities to normal cases, leading to biased\nperformance. Common oversampling techniques such as SMOTE rely on local\ninformation and can introduce marginalization issues. This paper investigates\nthe potential of using Mixup augmentation that combines two training examples\nalong with their corresponding labels to generate new data points as a generic\nvicinal distribution. To this end, we propose STEM, which combines SMOTE-ENN\nand Mixup at the instance level. This integration enables us to effectively\nleverage the entire distribution of minority classes, thereby mitigating both\nbetween-class and within-class imbalances. We focus on the breast cancer\nproblem, where imbalanced datasets are prevalent. The results demonstrate the\neffectiveness of STEM, which achieves AUC values of 0.96 and 0.99 in the\nDigital Database for Screening Mammography and Wisconsin Breast Cancer\n(Diagnostics) datasets, respectively. Moreover, this method shows promising\npotential when applied with an ensemble of machine learning (ML) classifiers.\n","authors":["Yumnah Hasan","Fatemeh Amerehi","Patrick Healy","Conor Ryan"],"pdf_url":"https://arxiv.org/pdf/2311.07504v1.pdf","comment":"7 pages, 4 figures, International Conference on Intelligent Computer\n  Communication and Processing"},{"id":"http://arxiv.org/abs/2311.07498v1","updated":"2023-11-13T17:38:07Z","published":"2023-11-13T17:38:07Z","title":"Reducing the Need for Backpropagation and Discovering Better Optima With\n  Explicit Optimizations of Neural Networks","summary":"  Iterative differential approximation methods that rely upon backpropagation\nhave enabled the optimization of neural networks; however, at present, they\nremain computationally expensive, especially when training models at scale. In\nthis paper, we propose a computationally efficient alternative for optimizing\nneural networks that can both reduce the costs of scaling neural networks and\nprovide high-efficiency optimizations for low-resource applications. We derive\nan explicit solution to a simple feed-forward language model (LM) by\nmathematically analyzing its gradients. This solution generalizes from\nsingle-layer LMs to the class of all single-layer feed-forward\nsoftmax-activated neural models trained on positive-valued features, as is\ndemonstrated by our extension of this solution application to MNIST digit\nclassification. For both LM and digit classifiers, we find computationally that\nexplicit solutions perform near-optimality in experiments showing that 1)\niterative optimization only marginally improves the explicit solution\nparameters and 2) randomly initialized parameters iteratively optimize towards\nthe explicit solution. We also preliminarily apply the explicit solution\nlocally by layer in multi-layer networks and discuss how the solution's\ncomputational savings increase with model complexity -- for both single- and\nmult-layer applications of the explicit solution, we emphasize that the optima\nachieved cannot be reached by backpropagation alone, i.e., better optima appear\ndiscoverable only after explicit solutions are applied. Finally, we discuss the\nsolution's computational savings alongside its impact on model interpretability\nand suggest future directions for the derivation of explicit solutions to\ncomplex- and multi-layer architectures.\n","authors":["Jake Ryland Williams","Haoran Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.07498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13761v2","updated":"2023-11-13T17:33:43Z","published":"2023-06-23T19:55:41Z","title":"CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation","summary":"  Deep learning has been extensively used in wireless communication problems,\nincluding channel estimation. Although several data-driven approaches exist, a\nfair and realistic comparison between them is difficult due to inconsistencies\nin the experimental conditions and the lack of a standardized experimental\ndesign. In addition, the performance of data-driven approaches is often\ncompared based on empirical analysis. The lack of reproducibility and\navailability of standardized evaluation tools (e.g., datasets, codebases)\nhinder the development and progress of data-driven methods for channel\nestimation and wireless communication in general. In this work, we introduce an\ninitiative to build benchmarks that unify several data-driven OFDM channel\nestimation approaches. Specifically, we present CeBed (a testbed for channel\nestimation) including different datasets covering various systems models and\npropagation conditions along with the implementation of ten deep and\ntraditional baselines. This benchmark considers different practical aspects\nsuch as the robustness of the data-driven models, the number and the\narrangement of pilots, and the number of receive antennas. This work offers a\ncomprehensive and unified framework to help researchers evaluate and design\ndata-driven channel estimation algorithms.\n","authors":["Amal Feriani","Di Wu","Steve Liu","Greg Dudek"],"pdf_url":"https://arxiv.org/pdf/2306.13761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07485v1","updated":"2023-11-13T17:25:06Z","published":"2023-11-13T17:25:06Z","title":"EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient\n  Federated Learning","summary":"  Federated Learning (FL) is a decentralized machine learning paradigm that\nenables collaborative model training across dispersed nodes without having to\nforce individual nodes to share data. However, its broad adoption is hindered\nby the high communication costs of transmitting a large number of model\nparameters. This paper presents EvoFed, a novel approach that integrates\nEvolutionary Strategies (ES) with FL to address these challenges. EvoFed\nemploys a concept of 'fitness-based information sharing', deviating\nsignificantly from the conventional model-based FL. Rather than exchanging the\nactual updated model parameters, each node transmits a distance-based\nsimilarity measure between the locally updated model and each member of the\nnoise-perturbed model population. Each node, as well as the server, generates\nan identical population set of perturbed models in a completely synchronized\nfashion using the same random seeds. With properly chosen noise variance and\npopulation size, perturbed models can be combined to closely reflect the actual\nmodel updated using the local dataset, allowing the transmitted similarity\nmeasures (or fitness values) to carry nearly the complete information about the\nmodel parameters. As the population size is typically much smaller than the\nnumber of model parameters, the savings in communication load is large. The\nserver aggregates these fitness values and is able to update the global model.\nThis global fitness vector is then disseminated back to the nodes, each of\nwhich applies the same update to be synchronized to the global model. Our\nanalysis shows that EvoFed converges, and our experimental results validate\nthat at the cost of increased local processing loads, EvoFed achieves\nperformance comparable to FedAvg while reducing overall communication\nrequirements drastically in various practical settings.\n","authors":["Mohammad Mahdi Rahimi","Hasnain Irshad Bhatti","Younghyun Park","Humaira Kousar","Jaekyun Moon"],"pdf_url":"https://arxiv.org/pdf/2311.07485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16963v2","updated":"2023-11-13T17:21:27Z","published":"2023-05-26T14:19:17Z","title":"Semantic segmentation of sparse irregular point clouds for leaf/wood\n  discrimination","summary":"  LiDAR (Light Detection and Ranging) has become an essential part of the\nremote sensing toolbox used for biosphere monitoring. In particular, LiDAR\nprovides the opportunity to map forest leaf area with unprecedented accuracy,\nwhile leaf area has remained an important source of uncertainty affecting\nmodels of gas exchanges between the vegetation and the atmosphere. Unmanned\nAerial Vehicles (UAV) are easy to mobilize and therefore allow frequent\nrevisits to track the response of vegetation to climate change. However,\nminiature sensors embarked on UAVs usually provide point clouds of limited\ndensity, which are further affected by a strong decrease in density from top to\nbottom of the canopy due to progressively stronger occlusion. In such a\ncontext, discriminating leaf points from wood points presents a significant\nchallenge due in particular to strong class imbalance and spatially irregular\nsampling intensity. Here we introduce a neural network model based on the\nPointnet ++ architecture which makes use of point geometry only (excluding any\nspectral information). To cope with local data sparsity, we propose an\ninnovative sampling scheme which strives to preserve local important geometric\ninformation. We also propose a loss function adapted to the severe class\nimbalance. We show that our model outperforms state-of-the-art alternatives on\nUAV point clouds. We discuss future possible improvements, particularly\nregarding much denser point clouds acquired from below the canopy.\n","authors":["Yuchen Bai","Jean-Baptiste Durand","Florence Forbes","Grégoire Vincent"],"pdf_url":"https://arxiv.org/pdf/2305.16963v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07474v1","updated":"2023-11-13T17:08:34Z","published":"2023-11-13T17:08:34Z","title":"A Federated Data Fusion-Based Prognostic Model for Applications with\n  Multi-Stream Incomplete Signals","summary":"  Most prognostic methods require a decent amount of data for model training.\nIn reality, however, the amount of historical data owned by a single\norganization might be small or not large enough to train a reliable prognostic\nmodel. To address this challenge, this article proposes a federated prognostic\nmodel that allows multiple users to jointly construct a failure time prediction\nmodel using their multi-stream, high-dimensional, and incomplete data while\nkeeping each user's data local and confidential. The prognostic model first\nemploys multivariate functional principal component analysis to fuse the\nmulti-stream degradation signals. Then, the fused features coupled with the\ntimes-to-failure are utilized to build a (log)-location-scale regression model\nfor failure prediction. To estimate parameters using distributed datasets and\nkeep the data privacy of all participants, we propose a new federated algorithm\nfor feature extraction. Numerical studies indicate that the performance of the\nproposed model is the same as that of classic non-federated prognostic models\nand is better than that of the models constructed by each user itself.\n","authors":["Madi Arabi","Xiaolei Fang"],"pdf_url":"https://arxiv.org/pdf/2311.07474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07468v1","updated":"2023-11-13T17:01:12Z","published":"2023-11-13T17:01:12Z","title":"Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation\n  of the Reversal Curse","summary":"  Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B. However, it may encounter\nconfusion when presented with questions concerning B. We contend that the\nreversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.\n","authors":["Ang Lv","Kaiyi Zhang","Shufang Xie","Quan Tu","Yuhan Chen","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2311.07468v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.10317v2","updated":"2023-11-13T16:57:10Z","published":"2023-07-19T05:44:35Z","title":"FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning","summary":"  Federated Learning (FL) offers a collaborative training framework, allowing\nmultiple clients to contribute to a shared model without compromising data\nprivacy. Due to the heterogeneous nature of local datasets, updated client\nmodels may overfit and diverge from one another, commonly known as the problem\nof client drift. In this paper, we propose FedBug (Federated Learning with\nBottom-Up Gradual Unfreezing), a novel FL framework designed to effectively\nmitigate client drift. FedBug adaptively leverages the client model parameters,\ndistributed by the server at each global round, as the reference points for\ncross-client alignment. Specifically, on the client side, FedBug begins by\nfreezing the entire model, then gradually unfreezes the layers, from the input\nlayer to the output layer. This bottom-up approach allows models to train the\nnewly thawed layers to project data into a latent space, wherein the separating\nhyperplanes remain consistent across all clients. We theoretically analyze\nFedBug in a novel over-parameterization FL setup, revealing its superior\nconvergence rate compared to FedAvg. Through comprehensive experiments,\nspanning various datasets, training conditions, and network architectures, we\nvalidate the efficacy of FedBug. Our contributions encompass a novel FL\nframework, theoretical analysis, and empirical validation, demonstrating the\nwide potential and applicability of FedBug.\n","authors":["Chia-Hsiang Kao","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2307.10317v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.07466v1","updated":"2023-11-13T16:53:51Z","published":"2023-11-13T16:53:51Z","title":"On Measuring Faithfulness of Natural Language Explanations","summary":"  Large language models (LLMs) can explain their own predictions, through\npost-hoc or Chain-of-Thought (CoT) explanations. However the LLM could make up\nreasonably sounding explanations that are unfaithful to its underlying\nreasoning. Recent work has designed tests that aim to judge the faithfulness of\neither post-hoc or CoT explanations. In this paper we argue that existing\nfaithfulness tests are not actually measuring faithfulness in terms of the\nmodels' inner workings, but only evaluate their self-consistency on the output\nlevel. The aims of our work are two-fold. i) We aim to clarify the status of\nexisting faithfulness tests in terms of model explainability, characterising\nthem as self-consistency tests instead. This assessment we underline by\nconstructing a Comparative Consistency Bank for self-consistency tests that for\nthe first time compares existing tests on a common suite of 11 open-source LLMs\nand 5 datasets -- including ii) our own proposed self-consistency measure\nCC-SHAP. CC-SHAP is a new fine-grained measure (not test) of LLM\nself-consistency that compares a model's input contributions to answer\nprediction and generated explanation. With CC-SHAP, we aim to take a step\nfurther towards measuring faithfulness with a more interpretable and\nfine-grained method. Code available at\n\\url{https://github.com/Heidelberg-NLP/CC-SHAP}\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2311.07466v1.pdf","comment":"10 main paper pages, 17 appendix pages"},{"id":"http://arxiv.org/abs/2310.18424v2","updated":"2023-11-13T16:48:01Z","published":"2023-10-27T18:48:54Z","title":"Fast Machine Learning Method with Vector Embedding on Orthonormal Basis\n  and Spectral Transform","summary":"  This paper presents a novel fast machine learning method that leverages two\ntechniques: Vector Embedding on Orthonormal Basis (VEOB) and Spectral Transform\n(ST). The VEOB converts the original data encoding into a vector embedding with\ncoordinates projected onto orthonormal bases. The Singular Value Decomposition\n(SVD) technique is used to calculate the vector basis and projection\ncoordinates, leading to an enhanced distance measurement in the embedding space\nand facilitating data compression by preserving the projection vectors\nassociated with the largest singular values. On the other hand, ST transforms\nsequence of vector data into spectral space. By applying the Discrete Cosine\nTransform (DCT) and selecting the most significant components, it streamlines\nthe handling of lengthy vector sequences. The paper provides examples of word\nembedding, text chunk embedding, and image embedding, implemented in Julia\nlanguage with a vector database. It also investigates unsupervised learning and\nsupervised learning using this method, along with strategies for handling large\ndata volumes.\n","authors":["Louis Yu Lu"],"pdf_url":"https://arxiv.org/pdf/2310.18424v2.pdf","comment":"update 9. Strategies for managing large data volumes with 9.1. Using\n  incremental SVD"},{"id":"http://arxiv.org/abs/2205.15680v4","updated":"2023-11-13T16:46:24Z","published":"2022-05-31T10:43:18Z","title":"Simulator-Based Inference with Waldo: Confidence Regions by Leveraging\n  Prediction Algorithms and Posterior Estimators for Inverse Problems","summary":"  Prediction algorithms, such as deep neural networks (DNNs), are used in many\ndomain sciences to directly estimate internal parameters of interest in\nsimulator-based models, especially in settings where the observations include\nimages or complex high-dimensional data. In parallel, modern neural density\nestimators, such as normalizing flows, are becoming increasingly popular for\nuncertainty quantification, especially when both parameters and observations\nare high-dimensional. However, parameter inference is an inverse problem and\nnot a prediction task; thus, an open challenge is to construct conditionally\nvalid and precise confidence regions, with a guaranteed probability of covering\nthe true parameters of the data-generating process, no matter what the\n(unknown) parameter values are, and without relying on large-sample theory.\nMany simulator-based inference (SBI) methods are indeed known to produce biased\nor overly confident parameter regions, yielding misleading uncertainty\nestimates. This paper presents WALDO, a novel method to construct confidence\nregions with finite-sample conditional validity by leveraging prediction\nalgorithms or posterior estimators that are currently widely adopted in SBI.\nWALDO reframes the well-known Wald test statistic, and uses a computationally\nefficient regression-based machinery for classical Neyman inversion of\nhypothesis tests. We apply our method to a recent high-energy physics problem,\nwhere prediction with DNNs has previously led to estimates with prediction\nbias. We also illustrate how our approach can correct overly confident\nposterior regions computed with normalizing flows.\n","authors":["Luca Masserano","Tommaso Dorigo","Rafael Izbicki","Mikael Kuusela","Ann B. Lee"],"pdf_url":"https://arxiv.org/pdf/2205.15680v4.pdf","comment":"15 pages, 10 figures, code available at\n  https://github.com/lee-group-cmu/lf2i"},{"id":"http://arxiv.org/abs/2311.07461v1","updated":"2023-11-13T16:44:29Z","published":"2023-11-13T16:44:29Z","title":"On Self-Supervised Dynamic Incremental Regularised Adaptation","summary":"  In this paper, we overview a recent method for dynamic domain adaptation\nnamed DIRA, which relies on a few samples in addition to a regularisation\napproach named elastic weight consolidation to achieve state-of-the-art (SOTA)\ndomain adaptation results. DIRA has been previously shown to perform\ncompetitively with SOTA unsupervised adaption techniques. However, a limitation\nof DIRA is that it relies on labels to be provided for the few samples used in\nadaption. This makes it a supervised technique. In this paper, we discuss a\nproposed alteration to the DIRA method to make it self-supervised i.e. remove\nthe need for providing labels. Experiments on our proposed alteration will be\nprovided in future work.\n","authors":["Abanoub Ghobrial","Kerstin Eder"],"pdf_url":"https://arxiv.org/pdf/2311.07461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03691v3","updated":"2023-11-13T16:39:55Z","published":"2022-03-07T20:23:46Z","title":"HyperMixer: An MLP-based Low Cost Alternative to Transformers","summary":"  Transformer-based architectures are the model of choice for natural language\nunderstanding, but they come at a significant cost, as they have quadratic\ncomplexity in the input length, require a lot of training data, and can be\ndifficult to tune. In the pursuit of lower costs, we investigate simple\nMLP-based architectures. We find that existing architectures such as MLPMixer,\nwhich achieves token mixing through a static MLP applied to each feature\nindependently, are too detached from the inductive biases required for natural\nlanguage understanding. In this paper, we propose a simple variant, HyperMixer,\nwhich forms the token mixing MLP dynamically using hypernetworks. Empirically,\nwe demonstrate that our model performs better than alternative MLP-based\nmodels, and on par with Transformers. In contrast to Transformers, HyperMixer\nachieves these results at substantially lower costs in terms of processing\ntime, training data, and hyperparameter tuning.\n","authors":["Florian Mai","Arnaud Pannatier","Fabio Fehr","Haolin Chen","Francois Marelli","Francois Fleuret","James Henderson"],"pdf_url":"https://arxiv.org/pdf/2203.03691v3.pdf","comment":"Published at ACL 2023"},{"id":"http://arxiv.org/abs/2311.07454v1","updated":"2023-11-13T16:35:34Z","published":"2023-11-13T16:35:34Z","title":"Causal Discovery under Latent Class Confounding","summary":"  Directed acyclic graphs are used to model the causal structure of a system.\n``Causal discovery'' describes the problem of learning this structure from\ndata. When data is an aggregate from multiple sources (populations or\nenvironments), global confounding obscures conditional independence properties\nthat drive many causal discovery algorithms. For this reason, existing causal\ndiscovery algorithms are not suitable for the multiple-source setting. We\ndemonstrate that, if the confounding is of bounded cardinality (i.e. the data\ncomes from a limited number of sources), causal discovery can still be\nachieved. The feasibility of this problem is governed by a trade-off between\nthe cardinality of the global confounder, the cardinalities of the observed\nvariables, and the sparsity of the causal structure.\n","authors":["Bijan Mazaheri","Spencer Gordon","Yuval Rabani","Leonard Schulman"],"pdf_url":"https://arxiv.org/pdf/2311.07454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07452v1","updated":"2023-11-13T16:34:59Z","published":"2023-11-13T16:34:59Z","title":"Explainable Boosting Machines with Sparsity -- Maintaining\n  Explainability in High-Dimensional Settings","summary":"  Compared to \"black-box\" models, like random forests and deep neural networks,\nexplainable boosting machines (EBMs) are considered \"glass-box\" models that can\nbe competitively accurate while also maintaining a higher degree of\ntransparency and explainability. However, EBMs become readily less transparent\nand harder to interpret in high-dimensional settings with many predictor\nvariables; they also become more difficult to use in production due to\nincreases in scoring time. We propose a simple solution based on the least\nabsolute shrinkage and selection operator (LASSO) that can help introduce\nsparsity by reweighting the individual model terms and removing the less\nrelevant ones, thereby allowing these models to maintain their transparency and\nrelatively fast scoring times in higher-dimensional settings. In short,\npost-processing a fitted EBM with many (i.e., possibly hundreds or thousands)\nof terms using the LASSO can help reduce the model's complexity and drastically\nimprove scoring time. We illustrate the basic idea using two real-world\nexamples with code.\n","authors":["Brandon M. Greenwell","Annika Dahlmann","Saurabh Dhoble"],"pdf_url":"https://arxiv.org/pdf/2311.07452v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.01436v2","updated":"2023-11-13T16:24:24Z","published":"2023-08-02T21:16:05Z","title":"Price-Aware Deep Learning for Electricity Markets","summary":"  While deep learning gradually penetrates operational planning, its inherent\nprediction errors may significantly affect electricity prices. This letter\nexamines how prediction errors propagate into electricity prices, revealing\nnotable pricing errors and their spatial disparity in congested power systems.\nTo improve fairness, we propose to embed electricity market-clearing\noptimization as a deep learning layer. Differentiating through this layer\nallows for balancing between prediction and pricing errors, as oppose to\nminimizing prediction errors alone. This layer implicitly optimizes fairness\nand controls the spatial distribution of price errors across the system. We\nshowcase the price-aware deep learning in the nexus of wind power forecasting\nand short-term electricity market clearing.\n","authors":["Vladimir Dvorkin","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2308.01436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07444v1","updated":"2023-11-13T16:18:58Z","published":"2023-11-13T16:18:58Z","title":"On the Robustness of Neural Collapse and the Neural Collapse of\n  Robustness","summary":"  Neural Collapse refers to the curious phenomenon in the end of training of a\nneural network, where feature vectors and classification weights converge to a\nvery simple geometrical arrangement (a simplex). While it has been observed\nempirically in various cases and has been theoretically motivated, its\nconnection with crucial properties of neural networks, like their\ngeneralization and robustness, remains unclear. In this work, we study the\nstability properties of these simplices. We find that the simplex structure\ndisappears under small adversarial attacks, and that perturbed examples \"leap\"\nbetween simplex vertices. We further analyze the geometry of networks that are\noptimized to be robust against adversarial perturbations of the input, and find\nthat Neural Collapse is a pervasive phenomenon in these cases as well, with\nclean and perturbed representations forming aligned simplices, and giving rise\nto a robust simple nearest-neighbor classifier. By studying the propagation of\nthe amount of collapse inside the network, we identify novel properties of both\nrobust and non-robust machine learning models, and show that earlier, unlike\nlater layers maintain reliable simplices on perturbed data.\n","authors":["Jingtong Su","Ya Shi Zhang","Nikolaos Tsilivis","Julia Kempe"],"pdf_url":"https://arxiv.org/pdf/2311.07444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07439v1","updated":"2023-11-13T16:15:20Z","published":"2023-11-13T16:15:20Z","title":"Investigating Multi-Pivot Ensembling with Massively Multilingual Machine\n  Translation Models","summary":"  Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. As an alternative, we propose MaxEns, a\ncombination strategy that is biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n","authors":["Alireza Mohammadshahi","Jannis Vamvas","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2311.07439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19680v2","updated":"2023-11-13T16:12:02Z","published":"2023-10-30T16:00:13Z","title":"Integrating Pre-trained Language Model into Neural Machine Translation","summary":"  Neural Machine Translation (NMT) has become a significant technology in\nnatural language processing through extensive research and development.\nHowever, the deficiency of high-quality bilingual language pair data still\nposes a major challenge to improving NMT performance. Recent studies are\nexploring the use of contextual information from pre-trained language model\n(PLM) to address this problem. Yet, the issue of incompatibility between PLM\nand NMT model remains unresolved. This study proposes a PLM-integrated NMT\n(PiNMT) model to overcome the identified problems. The PiNMT model consists of\nthree critical components, PLM Multi Layer Converter, Embedding Fusion, and\nCosine Alignment, each playing a vital role in providing effective PLM\ninformation to NMT. Furthermore, two training strategies, Separate Learning\nRates and Dual Step Training, are also introduced in this paper. By\nimplementing the proposed PiNMT model and training strategy, we achieved\nstate-of-the-art performance on the IWSLT'14 En$\\leftrightarrow$De dataset.\nThis study's outcomes are noteworthy as they demonstrate a novel approach for\nefficiently integrating PLM with NMT to overcome incompatibility and enhance\nperformance.\n","authors":["Soon-Jae Hwang","Chang-Sung Jeong"],"pdf_url":"https://arxiv.org/pdf/2310.19680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17010v3","updated":"2023-11-13T16:06:33Z","published":"2023-06-29T15:06:21Z","title":"milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human\n  Motion Sensing","summary":"  Approaching the era of ubiquitous computing, human motion sensing plays a\ncrucial role in smart systems for decision making, user interaction, and\npersonalized services. Extensive research has been conducted on human tracking,\npose estimation, gesture recognition, and activity recognition, which are\npredominantly based on cameras in traditional methods. However, the intrusive\nnature of cameras limits their use in smart home applications. To address this,\nmmWave radars have gained popularity due to their privacy-friendly features. In\nthis work, we propose milliFlow, a novel deep learning method for scene flow\nestimation as a complementary motion information for mmWave point cloud,\nserving as an intermediate level of features and directly benefiting downstream\nhuman motion sensing tasks. Experimental results demonstrate the superior\nperformance of our method with an average 3D endpoint error of 4.6cm,\nsignificantly surpassing the competing approaches. Furthermore, by\nincorporating scene flow information, we achieve remarkable improvements in\nhuman activity recognition, human parsing, and human body part tracking. To\nfoster further research in this area, we will provide our codebase and dataset\nfor open access upon acceptance.\n","authors":["Fangqiang Ding","Zhen Luo","Peijun Zhao","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2306.17010v3.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.02775v2","updated":"2023-11-13T16:03:15Z","published":"2023-11-05T21:43:02Z","title":"ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using\n  Open-Source LLMs","summary":"  Responding to the thousands of student questions on online QA platforms each\nsemester has a considerable human cost, particularly in computing courses with\nrapidly growing enrollments. To address the challenges of scalable and\nintelligent question-answering (QA), we introduce an innovative solution that\nleverages open-source Large Language Models (LLMs) from the LLaMA-2 family to\nensure data privacy. Our approach combines augmentation techniques such as\nretrieval augmented generation (RAG), supervised fine-tuning (SFT), and\nlearning from human preferences data using Direct Preference Optimization\n(DPO). Through extensive experimentation on a Piazza dataset from an\nintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs of\npreference data, we demonstrate a significant 30% improvement in the quality of\nanswers, with RAG being a particularly impactful addition. Our contributions\ninclude the development of a novel architecture for educational QA, extensive\nevaluations of LLM performance utilizing both human assessments and LLM-based\nmetrics, and insights into the challenges and future directions of educational\ndata processing. This work paves the way for the development of CHATA, an\nintelligent QA assistant customizable for courses with an online QA platform\n","authors":["Yann Hicke","Anmol Agarwal","Qianou Ma","Paul Denny"],"pdf_url":"https://arxiv.org/pdf/2311.02775v2.pdf","comment":"Updates for camera-ready submission"},{"id":"http://arxiv.org/abs/2311.07427v1","updated":"2023-11-13T16:01:43Z","published":"2023-11-13T16:01:43Z","title":"Boolean Variation and Boolean Logic BackPropagation","summary":"  The notion of variation is introduced for the Boolean set and based on which\nBoolean logic backpropagation principle is developed. Using this concept, deep\nmodels can be built with weights and activations being Boolean numbers and\noperated with Boolean logic instead of real arithmetic. In particular, Boolean\ndeep models can be trained directly in the Boolean domain without latent\nweights. No gradient but logic is synthesized and backpropagated through\nlayers.\n","authors":["Van Minh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.07427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07426v1","updated":"2023-11-13T16:00:16Z","published":"2023-11-13T16:00:16Z","title":"Optimising Human-AI Collaboration by Learning Convincing Explanations","summary":"  Machine learning models are being increasingly deployed to take, or assist in\ntaking, complicated and high-impact decisions, from quasi-autonomous vehicles\nto clinical decision support systems. This poses challenges, particularly when\nmodels have hard-to-detect failure modes and are able to take actions without\noversight. In order to handle this challenge, we propose a method for a\ncollaborative system that remains safe by having a human ultimately making\ndecisions, while giving the model the best opportunity to convince and debate\nthem with interpretable explanations. However, the most helpful explanation\nvaries among individuals and may be inconsistent across stated preferences. To\nthis end we develop an algorithm, Ardent, to efficiently learn a ranking\nthrough interaction and best assist humans complete a task. By utilising a\ncollaborative approach, we can ensure safety and improve performance while\naddressing transparency and accountability concerns. Ardent enables efficient\nand effective decision-making by adapting to individual preferences for\nexplanations, which we validate through extensive simulations alongside a user\nstudy involving a challenging image classification task, demonstrating\nconsistent improvement over competing systems.\n","authors":["Alex J. Chan","Alihan Huyuk","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2311.07426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11630v2","updated":"2023-11-13T15:58:10Z","published":"2023-08-10T07:33:00Z","title":"Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using\n  Transfer Learning","summary":"  We present and experimentally evaluate using transfer learning to address\nexperimental data scarcity when training neural network (NN) models for\nMach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach\ninvolves pre-training the model using synthetic data generated from a less\naccurate analytical model and fine-tuning with experimental data. Our\ninvestigation demonstrates that this method yields significant reductions in\nmodeling errors compared to using an analytical model, or a standalone NN model\nwhen training data is limited. Utilizing regularization techniques and ensemble\naveraging, we achieve < 1 dB root-mean-square error on the matrix weights\nimplemented by a 3x3 photonic chip while using only 25% of the available data.\n","authors":["Ali Cem","Ognjen Jovanovic","Siqi Yan","Yunhong Ding","Darko Zibar","Francesco Da Ros"],"pdf_url":"https://arxiv.org/pdf/2308.11630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07417v1","updated":"2023-11-13T15:54:27Z","published":"2023-11-13T15:54:27Z","title":"Mitigating Backdoors within Deep Neural Networks in Data-limited\n  Configuration","summary":"  As the capacity of deep neural networks (DNNs) increases, their need for huge\namounts of data significantly grows. A common practice is to outsource the\ntraining process or collect more data over the Internet, which introduces the\nrisks of a backdoored DNN. A backdoored DNN shows normal behavior on clean data\nwhile behaving maliciously once a trigger is injected into a sample at the test\ntime. In such cases, the defender faces multiple difficulties. First, the\navailable clean dataset may not be sufficient for fine-tuning and recovering\nthe backdoored DNN. Second, it is impossible to recover the trigger in many\nreal-world applications without information about it. In this paper, we\nformulate some characteristics of poisoned neurons. This backdoor\nsuspiciousness score can rank network neurons according to their activation\nvalues, weights, and their relationship with other neurons in the same layer.\nOur experiments indicate the proposed method decreases the chance of attacks\nbeing successful by more than 50% with a tiny clean dataset, i.e., ten clean\nsamples for the CIFAR-10 dataset, without significantly deteriorating the\nmodel's performance. Moreover, the proposed method runs three times as fast as\nbaselines.\n","authors":["Soroush Hashemifar","Saeed Parsa","Morteza Zakeri-Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2311.07417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07416v1","updated":"2023-11-13T15:54:09Z","published":"2023-11-13T15:54:09Z","title":"Three-dimensional granular flow simulation using graph neural\n  network-based learned simulator","summary":"  Reliable evaluations of geotechnical hazards like landslides and debris flow\nrequire accurate simulation of granular flow dynamics. Traditional numerical\nmethods can simulate the complex behaviors of such flows that involve\nsolid-like to fluid-like transitions, but they are computationally intractable\nwhen simulating large-scale systems. Surrogate models based on statistical or\nmachine learning methods are a viable alternative, but they are typically\nempirical and rely on a confined set of parameters in evaluating associated\nrisks. Due to their permutation-dependent learning, conventional machine\nlearning models require an unreasonably large amount of training data for\nbuilding generalizable surrogate models. We employ a graph neural network\n(GNN), a novel deep learning technique, to develop a GNN-based simulator (GNS)\nfor granular flows to address these issues. Graphs represent the state of\ngranular flows and interactions, like the exchange of energy and momentum\nbetween grains, and GNN learns the local interaction law. GNS takes the current\nstate of the granular flow and estimates the next state using Euler explicit\nintegration. We train GNS on a limited set of granular flow trajectories and\nevaluate its performance in a three-dimensional granular column collapse\ndomain. GNS successfully reproduces the overall behaviors of column collapses\nwith various aspect ratios that were not encountered during training. The\ncomputation speed of GNS outperforms high-fidelity numerical simulators by 300\ntimes.\n","authors":["Yongjin Choi","Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2311.07416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01544v2","updated":"2023-11-13T15:33:35Z","published":"2023-11-02T18:55:53Z","title":"Divergent Token Metrics: Measuring degradation to prune away LLM\n  components -- and optimize quantization","summary":"  Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. Their ever-increasing size, however, raised\nconcerns about their effective deployment and the need for LLM compressions.\nThis study introduces the Divergent Token metrics (DTMs), a novel approach for\nassessing compressed LLMs, addressing the limitations of traditional perplexity\nor accuracy measures that fail to accurately reflect text generation quality.\nDTMs focus on token divergence, that allow deeper insights into the subtleties\nof model compression, i.p. when evaluating component's impacts individually.\nUtilizing the First Divergent Token metric (FDTM) in model sparsification\nreveals that a quarter of all attention components can be pruned beyond 90% on\nthe Llama-2 model family, still keeping SOTA performance. For quantization FDTM\nsuggests that over 80% of parameters can naively be transformed to int8 without\nspecial outlier management. These evaluations indicate the necessity of\nchoosing appropriate compressions for parameters individually-and that FDTM can\nidentify those-while standard metrics result in deteriorated outcomes.\n","authors":["Björn Deiseroth","Max Meuer","Nikolas Gritsch","Constantin Eichenberg","Patrick Schramowski","Matthias Aßenmacher","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2311.01544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10743v3","updated":"2023-11-13T15:19:02Z","published":"2023-01-25T18:05:55Z","title":"Tighter Bounds on the Expressivity of Transformer Encoders","summary":"  Characterizing neural networks in terms of better-understood formal systems\nhas the potential to yield new insights into the power and limitations of these\nnetworks. Doing so for transformers remains an active area of research.\nBhattamishra and others have shown that transformer encoders are at least as\nexpressive as a certain kind of counter machine, while Merrill and Sabharwal\nhave shown that fixed-precision transformer encoders recognize only languages\nin uniform $TC^0$. We connect and strengthen these results by identifying a\nvariant of first-order logic with counting quantifiers that is simultaneously\nan upper bound for fixed-precision transformer encoders and a lower bound for\ntransformer encoders. This brings us much closer than before to an exact\ncharacterization of the languages that transformer encoders recognize.\n","authors":["David Chiang","Peter Cholak","Anand Pillay"],"pdf_url":"https://arxiv.org/pdf/2301.10743v3.pdf","comment":"Presented at ICML 2023. Typo corrections in Appendix B and Section\n  8.1"},{"id":"http://arxiv.org/abs/2311.07389v1","updated":"2023-11-13T15:14:50Z","published":"2023-11-13T15:14:50Z","title":"Transpose Attack: Stealing Datasets with Bidirectional Training","summary":"  Deep neural networks are normally executed in the forward direction. However,\nin this work, we identify a vulnerability that enables models to be trained in\nboth directions and on different tasks. Adversaries can exploit this capability\nto hide rogue models within seemingly legitimate models. In addition, in this\nwork we show that neural networks can be taught to systematically memorize and\nretrieve specific samples from datasets. Together, these findings expose a\nnovel method in which adversaries can exfiltrate datasets from protected\nlearning environments under the guise of legitimate models. We focus on the\ndata exfiltration attack and show that modern architectures can be used to\nsecretly exfiltrate tens of thousands of samples with high fidelity, high\nenough to compromise data privacy and even train new models. Moreover, to\nmitigate this threat we propose a novel approach for detecting infected models.\n","authors":["Guy Amit","Mosh Levy","Yisroel Mirsky"],"pdf_url":"https://arxiv.org/pdf/2311.07389v1.pdf","comment":"NDSS24 paper"},{"id":"http://arxiv.org/abs/2306.04220v6","updated":"2023-11-13T15:12:45Z","published":"2023-06-07T07:51:05Z","title":"Look Beneath the Surface: Exploiting Fundamental Symmetry for\n  Sample-Efficient Offline RL","summary":"  Offline reinforcement learning (RL) offers an appealing approach to\nreal-world tasks by learning policies from pre-collected datasets without\ninteracting with the environment. However, the performance of existing offline\nRL algorithms heavily depends on the scale and state-action space coverage of\ndatasets. Real-world data collection is often expensive and uncontrollable,\nleading to small and narrowly covered datasets and posing significant\nchallenges for practical deployments of offline RL. In this paper, we provide a\nnew insight that leveraging the fundamental symmetry of system dynamics can\nsubstantially enhance offline RL performance under small datasets.\nSpecifically, we propose a Time-reversal symmetry (T-symmetry) enforced\nDynamics Model (TDM), which establishes consistency between a pair of forward\nand reverse latent dynamics. TDM provides both well-behaved representations for\nsmall datasets and a new reliability measure for OOD samples based on\ncompliance with the T-symmetry. These can be readily used to construct a new\noffline RL algorithm (TSRL) with less conservative policy constraints and a\nreliable latent space data augmentation procedure. Based on extensive\nexperiments, we find TSRL achieves great performance on small benchmark\ndatasets with as few as 1% of the original samples, which significantly\noutperforms the recent offline RL algorithms in terms of data efficiency and\ngeneralizability.Code is available at: https://github.com/pcheng2/TSRL\n","authors":["Peng Cheng","Xianyuan Zhan","Zhihao Wu","Wenjia Zhang","Shoucheng Song","Han Wang","Youfang Lin","Li Jiang"],"pdf_url":"https://arxiv.org/pdf/2306.04220v6.pdf","comment":"Accepted in NeurIPS 2023; The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2305.18450v2","updated":"2023-11-13T15:09:49Z","published":"2023-05-29T04:00:19Z","title":"GBG++: A Fast and Stable Granular Ball Generation Method for\n  Classification","summary":"  Granular ball computing (GBC), as an efficient, robust, and scalable learning\nmethod, has become a popular research topic of granular computing. GBC includes\ntwo stages: granular ball generation (GBG) and multi-granularity learning based\non the granular ball (GB). However, the stability and efficiency of existing\nGBG methods need to be further improved due to their strong dependence on\n$k$-means or $k$-division. In addition, GB-based classifiers only unilaterally\nconsider the GB's geometric characteristics to construct classification rules,\nbut the GB's quality is ignored. Therefore, in this paper, based on the\nattention mechanism, a fast and stable GBG (GBG++) method is proposed first.\nSpecifically, the proposed GBG++ method only needs to calculate the distances\nfrom the data-driven center to the undivided samples when splitting each GB\ninstead of randomly selecting the center and calculating the distances between\nit and all samples. Moreover, an outlier detection method is introduced to\nidentify local outliers. Consequently, the GBG++ method can significantly\nimprove effectiveness, robustness, and efficiency while being absolutely\nstable. Second, considering the influence of the sample size within the GB on\nthe GB's quality, based on the GBG++ method, an improved GB-based $k$-nearest\nneighbors algorithm (GB$k$NN++) is presented, which can reduce\nmisclassification at the class boundary. Finally, the experimental results\nindicate that the proposed method outperforms several existing GB-based\nclassifiers and classical machine learning classifiers on $24$ public benchmark\ndatasets.\n","authors":["Qin Xie","Qinghua Zhang","Shuyin Xia","Fan Zhao","Chengying Wu","Guoyin Wang","Weiping Ding"],"pdf_url":"https://arxiv.org/pdf/2305.18450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07383v1","updated":"2023-11-13T15:08:59Z","published":"2023-11-13T15:08:59Z","title":"LM-Polygraph: Uncertainty Estimation for Language Models","summary":"  Recent advancements in the capabilities of large language models (LLMs) have\npaved the way for a myriad of groundbreaking applications in various fields.\nHowever, a significant challenge arises as these models often \"hallucinate\",\ni.e., fabricate facts without providing users an apparent means to discern the\nveracity of their statements. Uncertainty estimation (UE) methods are one path\nto safer, more responsible, and more effective use of LLMs. However, to date,\nresearch on UE methods for LLMs has been focused primarily on theoretical\nrather than engineering contributions. In this work, we tackle this issue by\nintroducing LM-Polygraph, a framework with implementations of a battery of\nstate-of-the-art UE methods for LLMs in text generation tasks, with unified\nprogram interfaces in Python. Additionally, it introduces an extendable\nbenchmark for consistent evaluation of UE techniques by researchers, and a demo\nweb application that enriches the standard chat dialog with confidence scores,\nempowering end-users to discern unreliable responses. LM-Polygraph is\ncompatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and\nGPT-4, and is designed to support future releases of similarly-styled LMs.\n","authors":["Ekaterina Fadeeva","Roman Vashurin","Akim Tsvigun","Artem Vazhentsev","Sergey Petrakov","Kirill Fedyanin","Daniil Vasilev","Elizaveta Goncharova","Alexander Panchenko","Maxim Panov","Timothy Baldwin","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2311.07383v1.pdf","comment":"Accepted at EMNLP-2023"},{"id":"http://arxiv.org/abs/2305.18831v3","updated":"2023-11-13T14:53:01Z","published":"2023-05-30T08:24:01Z","title":"Convolutional Monge Mapping Normalization for learning on sleep data","summary":"  In many machine learning applications on signals and biomedical data,\nespecially electroencephalogram (EEG), one major challenge is the variability\nof the data across subjects, sessions, and hardware devices. In this work, we\npropose a new method called Convolutional Monge Mapping Normalization (CMMN),\nwhich consists in filtering the signals in order to adapt their power spectrum\ndensity (PSD) to a Wasserstein barycenter estimated on training data. CMMN\nrelies on novel closed-form solutions for optimal transport mappings and\nbarycenters and provides individual test time adaptation to new data without\nneeding to retrain a prediction model. Numerical experiments on sleep EEG data\nshow that CMMN leads to significant and consistent performance gains\nindependent from the neural network architecture when adapting between\nsubjects, sessions, and even datasets collected with different hardware.\nNotably our performance gain is on par with much more numerically intensive\nDomain Adaptation (DA) methods and can be used in conjunction with those for\neven better performances.\n","authors":["Théo Gnassounou","Rémi Flamary","Alexandre Gramfort"],"pdf_url":"https://arxiv.org/pdf/2305.18831v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13169v2","updated":"2023-11-13T14:50:06Z","published":"2023-05-22T15:57:53Z","title":"A Pretrainer's Guide to Training Data: Measuring the Effects of Data\n  Age, Domain Coverage, Quality, & Toxicity","summary":"  Pretraining is the preliminary and fundamental step in developing capable\nlanguage models (LM). Despite this, pretraining data design is critically\nunder-documented and often guided by empirically unsupported intuitions. To\naddress this, we pretrain 28 1.5B parameter decoder-only models, training on\ndata curated (1) at different times, (2) with varying toxicity and quality\nfilters, and (3) with different domain compositions. First, we quantify the\neffect of pretraining data age. A temporal shift between evaluation data and\npretraining data leads to performance degradation, which is not overcome by\nfinetuning. Second, we explore the effect of quality and toxicity filters,\nshowing a trade-off between performance on standard benchmarks and risk of\ntoxic generations. Our findings indicate there does not exist a\none-size-fits-all solution to filtering training data. We also find that the\neffects of different types of filtering are not predictable from text domain\ncharacteristics. Lastly, we empirically validate that the inclusion of\nheterogeneous data sources, like books and web, is broadly beneficial and\nwarrants greater prioritization. These findings constitute the largest set of\nexperiments to validate, quantify, and expose many undocumented intuitions\nabout text pretraining, which we hope will help support more informed\ndata-centric decisions in LM development.\n","authors":["Shayne Longpre","Gregory Yauney","Emily Reif","Katherine Lee","Adam Roberts","Barret Zoph","Denny Zhou","Jason Wei","Kevin Robinson","David Mimno","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2305.13169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11373v2","updated":"2023-11-13T14:48:53Z","published":"2023-09-20T14:54:48Z","title":"Learning and DiSentangling Patient Static Information from Time-series\n  Electronic HEalth Record (STEER)","summary":"  Recent work in machine learning for healthcare has raised concerns about\npatient privacy and algorithmic fairness. For example, previous work has shown\nthat patient self-reported race can be predicted from medical data that does\nnot explicitly contain racial information. However, the extent of data\nidentification is unknown, and we lack ways to develop models whose outcomes\nare minimally affected by such information. Here we systematically investigated\nthe ability of time-series electronic health record data to predict patient\nstatic information. We found that not only the raw time-series data, but also\nlearned representations from machine learning models, can be trained to predict\na variety of static information with area under the receiver operating\ncharacteristic curve as high as 0.851 for biological sex, 0.869 for binarized\nage and 0.810 for self-reported race. Such high predictive performance can be\nextended to a wide range of comorbidity factors and exists even when the model\nwas trained for different tasks, using different cohorts, using different model\narchitectures and databases. Given the privacy and fairness concerns these\nfindings pose, we develop a variational autoencoder-based approach that learns\na structured latent space to disentangle patient-sensitive attributes from\ntime-series data. Our work thoroughly investigates the ability of machine\nlearning models to encode patient static information from time-series\nelectronic health records and introduces a general approach to protect\npatient-sensitive attribute information for downstream tasks.\n","authors":["Wei Liao","Joel Voldman"],"pdf_url":"https://arxiv.org/pdf/2309.11373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02392v2","updated":"2023-11-13T14:46:44Z","published":"2023-02-05T14:22:41Z","title":"Offline Minimax Soft-Q-learning Under Realizability and Partial Coverage","summary":"  In offline reinforcement learning (RL) we have no opportunity to explore so\nwe must make assumptions that the data is sufficient to guide picking a good\npolicy, taking the form of assuming some coverage, realizability, Bellman\ncompleteness, and/or hard margin (gap). In this work we propose value-based\nalgorithms for offline RL with PAC guarantees under just partial coverage,\nspecifically, coverage of just a single comparator policy, and realizability of\nsoft (entropy-regularized) Q-function of the single policy and a related\nfunction defined as a saddle point of certain minimax optimization problem.\nThis offers refined and generally more lax conditions for offline RL. We\nfurther show an analogous result for vanilla Q-functions under a soft margin\ncondition. To attain these guarantees, we leverage novel minimax learning\nalgorithms to accurately estimate soft or vanilla Q-functions with\n$L^2$-convergence guarantees. Our algorithms' loss functions arise from casting\nthe estimation problems as nonlinear convex optimization problems and\nLagrangifying.\n","authors":["Masatoshi Uehara","Nathan Kallus","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2302.02392v2.pdf","comment":"The original title of this paper was \"Refined Value-Based Offline RL\n  under Realizability and Partial Coverage,\" but it was later changed. This\n  paper has been accepted for NeurIPS 2023"},{"id":"http://arxiv.org/abs/2306.00074v3","updated":"2023-11-13T14:44:17Z","published":"2023-05-31T18:00:14Z","title":"Human-Aligned Calibration for AI-Assisted Decision Making","summary":"  Whenever a binary classifier is used to provide decision support, it\ntypically provides both a label prediction and a confidence value. Then, the\ndecision maker is supposed to use the confidence value to calibrate how much to\ntrust the prediction. In this context, it has been often argued that the\nconfidence value should correspond to a well calibrated estimate of the\nprobability that the predicted label matches the ground truth label. However,\nmultiple lines of empirical evidence suggest that decision makers have\ndifficulties at developing a good sense on when to trust a prediction using\nthese confidence values. In this paper, our goal is first to understand why and\nthen investigate how to construct more useful confidence values. We first argue\nthat, for a broad class of utility functions, there exist data distributions\nfor which a rational decision maker is, in general, unlikely to discover the\noptimal decision policy using the above confidence values -- an optimal\ndecision maker would need to sometimes place more (less) trust on predictions\nwith lower (higher) confidence values. However, we then show that, if the\nconfidence values satisfy a natural alignment property with respect to the\ndecision maker's confidence on her own predictions, there always exists an\noptimal decision policy under which the level of trust the decision maker would\nneed to place on predictions is monotone on the confidence values, facilitating\nits discoverability. Further, we show that multicalibration with respect to the\ndecision maker's confidence on her own predictions is a sufficient condition\nfor alignment. Experiments on four different AI-assisted decision making tasks\nwhere a classifier provides decision support to real human experts validate our\ntheoretical results and suggest that alignment may lead to better decisions.\n","authors":["Nina L. Corvelo Benz","Manuel Gomez Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2306.00074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06440v3","updated":"2023-11-13T14:33:01Z","published":"2023-07-12T20:10:14Z","title":"No Train No Gain: Revisiting Efficient Training Algorithms For\n  Transformer-based Language Models","summary":"  The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n","authors":["Jean Kaddour","Oscar Key","Piotr Nawrot","Pasquale Minervini","Matt J. Kusner"],"pdf_url":"https://arxiv.org/pdf/2307.06440v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2212.01368v2","updated":"2023-11-13T14:31:18Z","published":"2022-12-02T18:51:10Z","title":"Fast Non-Rigid Radiance Fields from Monocularized Data","summary":"  The reconstruction and novel view synthesis of dynamic scenes recently gained\nincreased attention. As reconstruction from large-scale multi-view data\ninvolves immense memory and computational requirements, recent benchmark\ndatasets provide collections of single monocular views per timestamp sampled\nfrom multiple (virtual) cameras. We refer to this form of inputs as\n\"monocularized\" data. Existing work shows impressive results for synthetic\nsetups and forward-facing real-world data, but is often limited in the training\nspeed and angular range for generating novel views. This paper addresses these\nlimitations and proposes a new method for full 360{\\deg} inward-facing novel\nview synthesis of non-rigidly deforming scenes. At the core of our method are:\n1) An efficient deformation module that decouples the processing of spatial and\ntemporal information for accelerated training and inference; and 2) A static\nmodule representing the canonical scene as a fast hash-encoded neural radiance\nfield. In addition to existing synthetic monocularized data, we systematically\nanalyze the performance on real-world inward-facing scenes using a newly\nrecorded challenging dataset sampled from a synchronized large-scale multi-view\nrig. In both cases, our method is significantly faster than previous methods,\nconverging in less than 7 minutes and achieving real-time framerates at 1K\nresolution, while obtaining a higher visual accuracy for generated novel views.\nOur source code and data is available at our project page\nhttps://graphics.tu-bs.de/publications/kappel2022fast.\n","authors":["Moritz Kappel","Vladislav Golyanik","Susana Castillo","Christian Theobalt","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2212.01368v2.pdf","comment":"18 pages, 14 figures; project page:\n  https://graphics.tu-bs.de/publications/kappel2022fast"},{"id":"http://arxiv.org/abs/2311.07366v1","updated":"2023-11-13T14:28:21Z","published":"2023-11-13T14:28:21Z","title":"arfpy: A python package for density estimation and generative modeling\n  with adversarial random forests","summary":"  This paper introduces $\\textit{arfpy}$, a python implementation of\nAdversarial Random Forests (ARF) (Watson et al., 2023), which is a lightweight\nprocedure for synthesizing new data that resembles some given data. The\nsoftware $\\textit{arfpy}$ equips practitioners with straightforward\nfunctionalities for both density estimation and generative modeling. The method\nis particularly useful for tabular data and its competitive performance is\ndemonstrated in previous literature. As a major advantage over the mostly deep\nlearning based alternatives, $\\textit{arfpy}$ combines the method's reduced\nrequirements in tuning efforts and computational resources with a user-friendly\npython interface. This supplies audiences across scientific fields with\nsoftware to generate data effortlessly.\n","authors":["Kristin Blesch","Marvin N. Wright"],"pdf_url":"https://arxiv.org/pdf/2311.07366v1.pdf","comment":"The software is available at https://github.com/bips-hb/arfpy"},{"id":"http://arxiv.org/abs/2311.07355v1","updated":"2023-11-13T14:19:36Z","published":"2023-11-13T14:19:36Z","title":"ADAMM: Anomaly Detection of Attributed Multi-graphs with Metadata: A\n  Unified Neural Network Approach","summary":"  Given a complex graph database of node- and edge-attributed multi-graphs as\nwell as associated metadata for each graph, how can we spot the anomalous\ninstances? Many real-world problems can be cast as graph inference tasks where\nthe graph representation could capture complex relational phenomena (e.g.,\ntransactions among financial accounts in a journal entry), along with metadata\nreflecting tabular features (e.g. approver, effective date, etc.). While\nnumerous anomaly detectors based on Graph Neural Networks (GNNs) have been\nproposed, none are capable of directly handling directed graphs with\nmulti-edges and self-loops. Furthermore, the simultaneous handling of\nrelational and tabular features remains an unexplored area. In this work we\npropose ADAMM, a novel graph neural network model that handles directed\nmulti-graphs, providing a unified end-to-end architecture that fuses metadata\nand graph-level representation learning through an unsupervised anomaly\ndetection objective. Experiments on datasets from two different domains,\nnamely, general-ledger journal entries from different firms (accounting) as\nwell as human GPS trajectories from thousands of individuals (urban mobility)\nvalidate ADAMM's generality and detection effectiveness of expert-guided and\nground-truth anomalies. Notably, ADAMM outperforms existing baselines that\nhandle the two data modalities (graph and metadata) separately with post hoc\nsynthesis efforts.\n","authors":["Konstantinos Sotiropoulos","Lingxiao Zhao","Pierre Jinghong Liang","Leman Akoglu"],"pdf_url":"https://arxiv.org/pdf/2311.07355v1.pdf","comment":"Accepted at IEEE BigData 2023"},{"id":"http://arxiv.org/abs/2308.10248v3","updated":"2023-11-13T14:05:13Z","published":"2023-08-20T12:21:05Z","title":"Activation Addition: Steering Language Models Without Optimization","summary":"  Reliably controlling the behavior of large language models is a pressing open\nproblem. Existing methods include supervised finetuning, reinforcement learning\nfrom human feedback, prompt engineering and guided decoding. We instead\ninvestigate activation engineering: modifying activations at inference-time to\npredictably alter model behavior. We bias the forward pass with a 'steering\nvector' implicitly specified through natural language. Past work learned these\nsteering vectors; our Activation Addition (ActAdd) method instead computes them\nby taking the activation differences which result from pairs of prompts.\n  We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate\nthe effect on Llama-13B and GPT-J-6B. Our approach yields inference-time\ncontrol over high-level properties of output & preserves performance on\noff-target topics. The method requires far less compute and implementation\neffort than finetuning and RLHF, allows for natural language specification by\nusers, and its overhead scales naturally with model size.\n","authors":["Alexander Matt Turner","Lisa Thiergart","David Udell","Gavin Leech","Ulisse Mini","Monte MacDiarmid"],"pdf_url":"https://arxiv.org/pdf/2308.10248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07344v1","updated":"2023-11-13T14:01:04Z","published":"2023-11-13T14:01:04Z","title":"Missing Value Imputation for Multi-attribute Sensor Data Streams via\n  Message Propagation (Extended Version)","summary":"  Sensor data streams occur widely in various real-time applications in the\ncontext of the Internet of Things (IoT). However, sensor data streams feature\nmissing values due to factors such as sensor failures, communication errors, or\ndepleted batteries. Missing values can compromise the quality of real-time\nanalytics tasks and downstream applications. Existing imputation methods either\nmake strong assumptions about streams or have low efficiency. In this study, we\naim to accurately and efficiently impute missing values in data streams that\nsatisfy only general characteristics in order to benefit real-time applications\nmore widely. First, we propose a message propagation imputation network (MPIN)\nthat is able to recover the missing values of data instances in a time window.\nWe give a theoretical analysis of why MPIN is effective. Second, we present a\ncontinuous imputation framework that consists of data update and model update\nmechanisms to enable MPIN to perform continuous imputation both effectively and\nefficiently. Extensive experiments on multiple real datasets show that MPIN can\noutperform the existing data imputers by wide margins and that the continuous\nimputation framework is efficient and accurate.\n","authors":["Xiao Li","Huan Li","Hua Lu","Christian S. Jensen","Varun Pandey","Volker Markl"],"pdf_url":"https://arxiv.org/pdf/2311.07344v1.pdf","comment":"Accepted at VLDB 2024"},{"id":"http://arxiv.org/abs/2311.07343v1","updated":"2023-11-13T13:55:52Z","published":"2023-11-13T13:55:52Z","title":"Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning","summary":"  While interests in tabular deep learning has significantly grown,\nconventional tree-based models still outperform deep learning methods. To\nnarrow this performance gap, we explore the innovative retrieval mechanism, a\nmethodology that allows neural networks to refer to other data points while\nmaking predictions. Our experiments reveal that retrieval-based training,\nespecially when fine-tuning the pretrained TabPFN model, notably surpasses\nexisting methods. Moreover, the extensive pretraining plays a crucial role to\nenhance the performance of the model. These insights imply that blending the\nretrieval mechanism with pretraining and transfer learning schemes offers\nconsiderable potential for advancing the field of tabular deep learning.\n","authors":["Felix den Breejen","Sangmin Bae","Stephen Cha","Tae-Young Kim","Seoung Hyun Koh","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2311.07343v1.pdf","comment":"Table Representation Learning Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2112.13254v3","updated":"2023-11-13T13:55:32Z","published":"2021-12-25T16:30:13Z","title":"On Dynamic Pricing with Covariates","summary":"  We consider dynamic pricing with covariates under a generalized linear demand\nmodel: a seller can dynamically adjust the price of a product over a horizon of\n$T$ time periods, and at each time period $t$, the demand of the product is\njointly determined by the price and an observable covariate vector\n$x_t\\in\\mathbb{R}^d$ through a generalized linear model with unknown\nco-efficients. Most of the existing literature assumes the covariate vectors\n$x_t$'s are independently and identically distributed (i.i.d.); the few papers\nthat relax this assumption either sacrifice model generality or yield\nsub-optimal regret bounds. In this paper, we show that UCB and Thompson\nsampling-based pricing algorithms can achieve an $O(d\\sqrt{T}\\log T)$ regret\nupper bound without assuming any statistical structure on the covariates $x_t$.\nOur upper bound on the regret matches the lower bound up to logarithmic\nfactors. We thus show that (i) the i.i.d. assumption is not necessary for\nobtaining low regret, and (ii) the regret bound can be independent of the\n(inverse) minimum eigenvalue of the covariance matrix of the $x_t$'s, a\nquantity present in previous bounds. Moreover, we consider a constrained\nsetting of the dynamic pricing problem where there is a limited and\nunreplenishable inventory and we develop theoretical results that relate the\nbest achievable algorithm performance to a variation measure with respect to\nthe temporal distribution shift of the covariates. We also discuss conditions\nunder which a better regret is achievable and demonstrate the proposed\nalgorithms' performance with numerical experiments.\n","authors":["Hanzhao Wang","Kalyan Talluri","Xiaocheng Li"],"pdf_url":"https://arxiv.org/pdf/2112.13254v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01258v4","updated":"2023-11-13T13:37:52Z","published":"2022-11-02T16:39:42Z","title":"Instance-Dependent Generalization Bounds via Optimal Transport","summary":"  Existing generalization bounds fail to explain crucial factors that drive the\ngeneralization of modern neural networks. Since such bounds often hold\nuniformly over all parameters, they suffer from over-parametrization and fail\nto account for the strong inductive bias of initialization and stochastic\ngradient descent. As an alternative, we propose a novel optimal transport\ninterpretation of the generalization problem. This allows us to derive\ninstance-dependent generalization bounds that depend on the local Lipschitz\nregularity of the learned prediction function in the data space. Therefore, our\nbounds are agnostic to the parametrization of the model and work well when the\nnumber of training samples is much smaller than the number of parameters. With\nsmall modifications, our approach yields accelerated rates for data on\nlow-dimensional manifolds and guarantees under distribution shifts. We\nempirically analyze our generalization bounds for neural networks, showing that\nthe bound values are meaningful and capture the effect of popular\nregularization methods during training.\n","authors":["Songyan Hou","Parnian Kassraie","Anastasis Kratsios","Andreas Krause","Jonas Rothfuss"],"pdf_url":"https://arxiv.org/pdf/2211.01258v4.pdf","comment":"Journal of Machine Learning Research (JMLR), 51 pages"},{"id":"http://arxiv.org/abs/2311.07326v1","updated":"2023-11-13T13:27:59Z","published":"2023-11-13T13:27:59Z","title":"MetaSymNet: A Dynamic Symbolic Regression Network Capable of Evolving\n  into Arbitrary Formulations","summary":"  Mathematical formulas serve as the means of communication between humans and\nnature, encapsulating the operational laws governing natural phenomena. The\nconcise formulation of these laws is a crucial objective in scientific research\nand an important challenge for artificial intelligence (AI). While traditional\nartificial neural networks (MLP) excel at data fitting, they often yield\nuninterpretable black box results that hinder our understanding of the\nrelationship between variables x and predicted values y. Moreover, the fixed\nnetwork architecture in MLP often gives rise to redundancy in both network\nstructure and parameters. To address these issues, we propose MetaSymNet, a\nnovel neural network that dynamically adjusts its structure in real-time,\nallowing for both expansion and contraction. This adaptive network employs the\nPANGU meta function as its activation function, which is a unique type capable\nof evolving into various basic functions during training to compose\nmathematical formulas tailored to specific needs. We then evolve the neural\nnetwork into a concise, interpretable mathematical expression. To evaluate\nMetaSymNet's performance, we compare it with four state-of-the-art symbolic\nregression algorithms across more than 10 public datasets comprising 222\nformulas. Our experimental results demonstrate that our algorithm outperforms\nothers consistently regardless of noise presence or absence. Furthermore, we\nassess MetaSymNet against MLP and SVM regarding their fitting ability and\nextrapolation capability, these are two essential aspects of machine learning\nalgorithms. The findings reveal that our algorithm excels in both areas.\nFinally, we compared MetaSymNet with MLP using iterative pruning in network\nstructure complexity. The results show that MetaSymNet's network structure\ncomplexity is obviously less than MLP under the same goodness of fit.\n","authors":["Yanjie Li","Weijun Li","Lina Yu","Min Wu","Jinyi Liu","Wenqiang Li","Meilan Hao","Shu Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2311.07326v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2311.05304v2","updated":"2023-11-13T13:27:39Z","published":"2023-11-09T12:01:32Z","title":"Data Valuation and Detections in Federated Learning","summary":"  Federated Learning (FL) enables collaborative model training while preserving\nthe privacy of raw data. A challenge in this framework is the fair and\nefficient valuation of data, which is crucial for incentivizing clients to\ncontribute high-quality data in the FL task. In scenarios involving numerous\ndata clients within FL, it is often the case that only a subset of clients and\ndatasets are pertinent to a specific learning task, while others might have\neither a negative or negligible impact on the model training process. This\npaper introduces a novel privacy-preserving method for evaluating client\ncontributions and selecting relevant datasets without a pre-specified training\nalgorithm in an FL task. Our proposed approach FedBary, utilizes Wasserstein\ndistance within the federated context, offering a new solution for data\nvaluation in the FL framework. This method ensures transparent data valuation\nand efficient computation of the Wasserstein barycenter and reduces the\ndependence on validation datasets. Through extensive empirical experiments and\ntheoretical analyses, we demonstrate the potential of this data valuation\nmethod as a promising avenue for FL research.\n","authors":["Wenqian Li","Shuran Fu","Fengrui Zhang","Yan Pang"],"pdf_url":"https://arxiv.org/pdf/2311.05304v2.pdf","comment":"Fixed some experimental errors and typos"},{"id":"http://arxiv.org/abs/2311.07324v1","updated":"2023-11-13T13:24:09Z","published":"2023-11-13T13:24:09Z","title":"DAGC: Data-Volume-Aware Adaptive Sparsification Gradient Compression for\n  Distributed Machine Learning in Mobile Computing","summary":"  Distributed machine learning (DML) in mobile environments faces significant\ncommunication bottlenecks. Gradient compression has emerged as an effective\nsolution to this issue, offering substantial benefits in environments with\nlimited bandwidth and metered data. Yet, they encounter severe performance drop\nin non-IID environments due to a one-size-fits-all compression approach, which\ndoes not account for the varying data volumes across workers. Assigning varying\ncompression ratios to workers with distinct data distributions and volumes is\nthus a promising solution. This study introduces an analysis of distributed SGD\nwith non-uniform compression, which reveals that the convergence rate\n(indicative of the iterations needed to achieve a certain accuracy) is\ninfluenced by compression ratios applied to workers with differing volumes.\nAccordingly, we frame relative compression ratio assignment as an $n$-variables\nchi-square nonlinear optimization problem, constrained by a fixed and limited\ncommunication budget. We propose DAGC-R, which assigns the worker handling\nlarger data volumes the conservative compression. Recognizing the computational\nlimitations of mobile devices, we DAGC-A, which are computationally less\ndemanding and enhances the robustness of the absolute gradient compressor in\nnon-IID scenarios. Our experiments confirm that both the DAGC-A and DAGC-R can\nachieve better performance when dealing with highly imbalanced data volume\ndistribution and restricted communication.\n","authors":["Rongwei Lu","Yutong Jiang","Yinan Mao","Chen Tang","Bin Chen","Laizhong Cui","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07323v1","updated":"2023-11-13T13:22:21Z","published":"2023-11-13T13:22:21Z","title":"A Voting Approach for Explainable Classification with Rule Learning","summary":"  State-of-the-art results in typical classification tasks are mostly achieved\nby unexplainable machine learning methods, like deep neural networks, for\ninstance. Contrarily, in this paper, we investigate the application of rule\nlearning methods in such a context. Thus, classifications become based on\ncomprehensible (first-order) rules, explaining the predictions made. In\ngeneral, however, rule-based classifications are less accurate than\nstate-of-the-art results (often significantly). As main contribution, we\nintroduce a voting approach combining both worlds, aiming to achieve comparable\nresults as (unexplainable) state-of-the-art methods, while still providing\nexplanations in the form of deterministic rules. Considering a variety of\nbenchmark data sets including a use case of significant interest to insurance\nindustries, we prove that our approach not only clearly outperforms ordinary\nrule learning methods, but also yields results on a par with state-of-the-art\noutcomes.\n","authors":["Albert Nössig","Tobias Hell","Georg Moser"],"pdf_url":"https://arxiv.org/pdf/2311.07323v1.pdf","comment":"34 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.07315v1","updated":"2023-11-13T13:10:52Z","published":"2023-11-13T13:10:52Z","title":"An introduction to reinforcement learning for neuroscience","summary":"  Reinforcement learning has a rich history in neuroscience, from early work on\ndopamine as a reward prediction error signal for temporal difference learning\n(Schultz et al., 1997) to recent work suggesting that dopamine could implement\na form of 'distributional reinforcement learning' popularized in deep learning\n(Dabney et al., 2020). Throughout this literature, there has been a tight link\nbetween theoretical advances in reinforcement learning and neuroscientific\nexperiments and findings. As a result, the theories describing our experimental\ndata have become increasingly complex and difficult to navigate. In this\nreview, we cover the basic theory underlying classical work in reinforcement\nlearning and build up to an introductory overview of methods used in modern\ndeep reinforcement learning that have found applications in systems\nneuroscience. We start with an overview of the reinforcement learning problem\nand classical temporal difference algorithms, followed by a discussion of\n'model-free' and 'model-based' reinforcement learning together with methods\nsuch as DYNA and successor representations that fall in between these two\ncategories. Throughout these sections, we highlight the close parallels between\nthe machine learning methods and related work in both experimental and\ntheoretical neuroscience. We then provide an introduction to deep reinforcement\nlearning with examples of how these methods have been used to model different\nlearning phenomena in the systems neuroscience literature, such as\nmeta-reinforcement learning (Wang et al., 2018) and distributional\nreinforcement learning (Dabney et al., 2020). Code that implements the methods\ndiscussed in this work and generates the figures is also provided.\n","authors":["Kristopher T. Jensen"],"pdf_url":"https://arxiv.org/pdf/2311.07315v1.pdf","comment":"Code available at:\n  https://colab.research.google.com/drive/1kWOz2Uxn0cf2c4YizqIXQKWyxeYd6wvL?usp=sharing"},{"id":"http://arxiv.org/abs/2311.07312v1","updated":"2023-11-13T13:07:48Z","published":"2023-11-13T13:07:48Z","title":"C-Procgen: Empowering Procgen with Controllable Contexts","summary":"  We present C-Procgen, an enhanced suite of environments on top of the Procgen\nbenchmark. C-Procgen provides access to over 200 unique game contexts across 16\ngames. It allows for detailed configuration of environments, ranging from game\nmechanics to agent attributes. This makes the procedural generation process,\npreviously a black-box in Procgen, more transparent and adaptable for various\nresearch needs.The upgrade enhances dynamic context management and\nindividualized assignments, while maintaining computational efficiency.\nC-Procgen's controllable contexts make it applicable in diverse reinforcement\nlearning research areas, such as learning dynamics analysis, curriculum\nlearning, and transfer learning. We believe that C-Procgen will fill a gap in\nthe current literature and offer a valuable toolkit for future works.\n","authors":["Zhenxiong Tan","Kaixin Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07289v1","updated":"2023-11-13T12:33:33Z","published":"2023-11-13T12:33:33Z","title":"A probabilistic forecast methodology for volatile electricity prices in\n  the Australian National Electricity Market","summary":"  The South Australia region of the Australian National Electricity Market\n(NEM) displays some of the highest levels of price volatility observed in\nmodern electricity markets. This paper outlines an approach to probabilistic\nforecasting under these extreme conditions, including spike filtration and\nseveral post-processing steps. We propose using quantile regression as an\nensemble tool for probabilistic forecasting, with our combined forecasts\nachieving superior results compared to all constituent models. Within our\nensemble framework, we demonstrate that averaging models with varying training\nlength periods leads to a more adaptive model and increased prediction\naccuracy. The applicability of the final model is evaluated by comparing our\nmedian forecasts with the point forecasts available from the Australian NEM\noperator, with our model outperforming these NEM forecasts by a significant\nmargin.\n","authors":["Cameron Cornell","Nam Trong Dinh","S. Ali Pourmousavi"],"pdf_url":"https://arxiv.org/pdf/2311.07289v1.pdf","comment":"This manuscript has been submitted to International Journal of\n  Forecasting for possible publication"},{"id":"http://arxiv.org/abs/2311.07286v1","updated":"2023-11-13T12:28:00Z","published":"2023-11-13T12:28:00Z","title":"Explaining black boxes with a SMILE: Statistical Model-agnostic\n  Interpretability with Local Explanations","summary":"  Machine learning is currently undergoing an explosion in capability,\npopularity, and sophistication. However, one of the major barriers to\nwidespread acceptance of machine learning (ML) is trustworthiness: most ML\nmodels operate as black boxes, their inner workings opaque and mysterious, and\nit can be difficult to trust their conclusions without understanding how those\nconclusions are reached. Explainability is therefore a key aspect of improving\ntrustworthiness: the ability to better understand, interpret, and anticipate\nthe behaviour of ML models. To this end, we propose SMILE, a new method that\nbuilds on previous approaches by making use of statistical distance measures to\nimprove explainability while remaining applicable to a wide range of input data\ndomains.\n","authors":["Koorosh Aslansefat","Mojgan Hashemian","Martin Walker","Mohammed Naveed Akram","Ioannis Sorokos","Yiannis Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2311.07286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07284v1","updated":"2023-11-13T12:26:25Z","published":"2023-11-13T12:26:25Z","title":"Learning Arithmetic Formulas in the Presence of Noise: A General\n  Framework and Applications to Unsupervised Learning","summary":"  We present a general framework for designing efficient algorithms for\nunsupervised learning problems, such as mixtures of Gaussians and subspace\nclustering. Our framework is based on a meta algorithm that learns arithmetic\ncircuits in the presence of noise, using lower bounds. This builds upon the\nrecent work of Garg, Kayal and Saha (FOCS 20), who designed such a framework\nfor learning arithmetic circuits without any noise. A key ingredient of our\nmeta algorithm is an efficient algorithm for a novel problem called Robust\nVector Space Decomposition. We show that our meta algorithm works well when\ncertain matrices have sufficiently large smallest non-zero singular values. We\nconjecture that this condition holds for smoothed instances of our problems,\nand thus our framework would yield efficient algorithms for these problems in\nthe smoothed setting.\n","authors":["Pritam Chandra","Ankit Garg","Neeraj Kayal","Kunal Mittal","Tanmay Sinha"],"pdf_url":"https://arxiv.org/pdf/2311.07284v1.pdf","comment":"85 pages, comments welcome"},{"id":"http://arxiv.org/abs/2311.07283v1","updated":"2023-11-13T12:25:45Z","published":"2023-11-13T12:25:45Z","title":"Predictive and Prescriptive Analytics for Multi-Site Modeling of Frail\n  and Elderly Patient Services","summary":"  Recent research has highlighted the potential of linking predictive and\nprescriptive analytics. However, it remains widely unexplored how both\nparadigms could benefit from one another to address today's major challenges in\nhealthcare. One of these is smarter planning of resource capacities for frail\nand elderly inpatient wards, addressing the societal challenge of an aging\npopulation. Frail and elderly patients typically suffer from multimorbidity and\nrequire more care while receiving medical treatment. The aim of this research\nis to assess how various predictive and prescriptive analytical methods, both\nindividually and in tandem, contribute to addressing the operational challenges\nwithin an area of healthcare that is growing in demand. Clinical and\ndemographic patient attributes are gathered from more than 165,000 patient\nrecords and used to explain and predict length of stay. To that extent, we\nemploy Classification and Regression Trees (CART) analysis to establish this\nrelationship. On the prescriptive side, deterministic and two-stage stochastic\nprograms are developed to determine how to optimally plan for beds and ward\nstaff with the objective to minimize cost. Furthermore, the two analytical\nmethodologies are linked by generating demand for the prescriptive models using\nthe CART groupings. The results show the linked methodologies provided\ndifferent but similar results compared to using averages and in doing so,\ncaptured a more realistic real-world variation in the patient length of stay.\nOur research reveals that healthcare managers should consider using predictive\nand prescriptive models to make more informed decisions. By combining\npredictive and prescriptive analytics, healthcare managers can move away from\nrelying on averages and incorporate the unique characteristics of their\npatients to create more robust planning decisions, mitigating risks caused by\nvariations in demand.\n","authors":["Elizabeth Williams","Daniel Gartner","Paul Harper"],"pdf_url":"https://arxiv.org/pdf/2311.07283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03529v2","updated":"2023-11-13T12:14:05Z","published":"2023-10-05T13:29:46Z","title":"Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality\n  of Formal Deep Networks","summary":"  We identify hidden layers inside a deep neural network (DNN) with group\nactions on the data domain, and formulate a formal deep network as a dual voice\ntransform with respect to the Koopman operator, a linear representation of the\ngroup action. Based on the group theoretic arguments, particularly by using\nSchur's lemma, we show a simple proof of the universality of DNNs.\n","authors":["Sho Sonoda","Yuka Hashimoto","Isao Ishikawa","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2310.03529v2.pdf","comment":"NeurReps 2023"},{"id":"http://arxiv.org/abs/2306.13948v2","updated":"2023-11-13T12:08:31Z","published":"2023-06-24T12:10:16Z","title":"Unleashing Realistic Air Quality Forecasting: Introducing the\n  Ready-to-Use PurpleAirSF Dataset","summary":"  Air quality forecasting has garnered significant attention recently, with\ndata-driven models taking center stage due to advancements in machine learning\nand deep learning models. However, researchers face challenges with complex\ndata acquisition and the lack of open-sourced datasets, hindering efficient\nmodel validation. This paper introduces PurpleAirSF, a comprehensive and easily\naccessible dataset collected from the PurpleAir network. With its high temporal\nresolution, various air quality measures, and diverse geographical coverage,\nthis dataset serves as a useful tool for researchers aiming to develop novel\nforecasting models, study air pollution patterns, and investigate their impacts\non health and the environment. We present a detailed account of the data\ncollection and processing methods employed to build PurpleAirSF. Furthermore,\nwe conduct preliminary experiments using both classic and modern\nspatio-temporal forecasting models, thereby establishing a benchmark for future\nair quality forecasting tasks.\n","authors":["Jingwei Zuo","Wenbin Li","Michele Baldo","Hakim Hacid"],"pdf_url":"https://arxiv.org/pdf/2306.13948v2.pdf","comment":"Accepted by ACM SIGSPATIAL 2023"},{"id":"http://arxiv.org/abs/2311.07263v1","updated":"2023-11-13T12:02:46Z","published":"2023-11-13T12:02:46Z","title":"LT-ViT: A Vision Transformer for multi-label Chest X-ray classification","summary":"  Vision Transformers (ViTs) are widely adopted in medical imaging tasks, and\nsome existing efforts have been directed towards vision-language training for\nChest X-rays (CXRs). However, we envision that there still exists a potential\nfor improvement in vision-only training for CXRs using ViTs, by aggregating\ninformation from multiple scales, which has been proven beneficial for\nnon-transformer networks. Hence, we have developed LT-ViT, a transformer that\nutilizes combined attention between image tokens and randomly initialized\nauxiliary tokens that represent labels. Our experiments demonstrate that LT-ViT\n(1) surpasses the state-of-the-art performance using pure ViTs on two publicly\navailable CXR datasets, (2) is generalizable to other pre-training methods and\ntherefore is agnostic to model initialization, and (3) enables model\ninterpretability without grad-cam and its variants.\n","authors":["Umar Marikkar","Sara Atito","Muhammad Awais","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2311.07263v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07271v4","updated":"2023-11-13T11:58:00Z","published":"2023-03-09T20:09:15Z","title":"Provably Convergent Plug-and-Play Quasi-Newton Methods","summary":"  Plug-and-Play (PnP) methods are a class of efficient iterative methods that\naim to combine data fidelity terms and deep denoisers using classical\noptimization algorithms, such as ISTA or ADMM, with applications in inverse\nproblems and imaging. Provable PnP methods are a subclass of PnP methods with\nconvergence guarantees, such as fixed point convergence or convergence to\ncritical points of some energy function. Many existing provable PnP methods\nimpose heavy restrictions on the denoiser or fidelity function, such as\nnon-expansiveness or strict convexity, respectively. In this work, we propose a\nnovel algorithmic approach incorporating quasi-Newton steps into a provable PnP\nframework based on proximal denoisers, resulting in greatly accelerated\nconvergence while retaining light assumptions on the denoiser. By\ncharacterizing the denoiser as the proximal operator of a weakly convex\nfunction, we show that the fixed points of the proposed quasi-Newton PnP\nalgorithm are critical points of a weakly convex function. Numerical\nexperiments on image deblurring and super-resolution demonstrate 2--8x faster\nconvergence as compared to other provable PnP methods with similar\nreconstruction quality.\n","authors":["Hong Ye Tan","Subhadip Mukherjee","Junqi Tang","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2303.07271v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03530v2","updated":"2023-11-13T11:56:32Z","published":"2023-10-05T13:30:37Z","title":"Joint Group Invariant Functions on Data-Parameter Domain Induce\n  Universal Neural Networks","summary":"  The symmetry and geometry of input data are considered to be encoded in the\ninternal data representation inside the neural network, but the specific\nencoding rule has been less investigated. In this study, we present a\nsystematic method to induce a generalized neural network and its right inverse\noperator, called the ridgelet transform, from a joint group invariant function\non the data-parameter domain. Since the ridgelet transform is an inverse, (1)\nit can describe the arrangement of parameters for the network to represent a\ntarget function, which is understood as the encoding rule, and (2) it implies\nthe universality of the network. Based on the group representation theory, we\npresent a new simple proof of the universality by using Schur's lemma in a\nunified manner covering a wide class of networks, for example, the original\nridgelet transform, formal deep networks, and the dual voice transform. Since\ntraditional universality theorems were demonstrated based on functional\nanalysis, this study sheds light on the group theoretic aspect of the\napproximation theory, connecting geometric deep learning to abstract harmonic\nanalysis.\n","authors":["Sho Sonoda","Hideyuki Ishi","Isao Ishikawa","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2310.03530v2.pdf","comment":"NeurReps 2023"},{"id":"http://arxiv.org/abs/2311.07259v1","updated":"2023-11-13T11:49:55Z","published":"2023-11-13T11:49:55Z","title":"Towards Bounding Causal Effects under Markov Equivalence","summary":"  Predicting the effect of unseen interventions is a fundamental research\nquestion across the data sciences. It is well established that, in general,\nsuch questions cannot be answered definitively from observational data, e.g.,\nas a consequence of unobserved confounding. A generalization of this task is to\ndetermine non-trivial bounds on causal effects induced by the data, also known\nas the task of partial causal identification. In the literature, several\nalgorithms have been developed for solving this problem. Most, however, require\na known parametric form or a fully specified causal diagram as input, which is\nusually not available in practical applications. In this paper, we assume as\ninput a less informative structure known as a Partial Ancestral Graph, which\nrepresents a Markov equivalence class of causal diagrams and is learnable from\nobservational data. In this more \"data-driven\" setting, we provide a systematic\nalgorithm to derive bounds on causal effects that can be computed analytically.\n","authors":["Alexis Bellot"],"pdf_url":"https://arxiv.org/pdf/2311.07259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07247v1","updated":"2023-11-13T11:29:38Z","published":"2023-11-13T11:29:38Z","title":"Simultaneous Clutter Detection and Semantic Segmentation of Moving\n  Objects for Automotive Radar Data","summary":"  The unique properties of radar sensors, such as their robustness to adverse\nweather conditions, make them an important part of the environment perception\nsystem of autonomous vehicles. One of the first steps during the processing of\nradar point clouds is often the detection of clutter, i.e. erroneous points\nthat do not correspond to real objects. Another common objective is the\nsemantic segmentation of moving road users. These two problems are handled\nstrictly separate from each other in literature. The employed neural networks\nare always focused entirely on only one of the tasks. In contrast to this, we\nexamine ways to solve both tasks at the same time with a single jointly used\nmodel. In addition to a new augmented multi-head architecture, we also devise a\nmethod to represent a network's predictions for the two tasks with only one\noutput value. This novel approach allows us to solve the tasks simultaneously\nwith the same inference time as a conventional task-specific model. In an\nextensive evaluation, we show that our setup is highly effective and\noutperforms every existing network for semantic segmentation on the RadarScenes\ndataset.\n","authors":["Johannes Kopp","Dominik Kellner","Aldi Piroli","Vinzenz Dallabetta","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2311.07247v1.pdf","comment":"Published at IEEE International Conference of Intelligent\n  Transportation Systems (ITSC), Bilbao, ESP, 2023"},{"id":"http://arxiv.org/abs/2307.11772v3","updated":"2023-11-13T10:56:22Z","published":"2023-07-18T04:43:24Z","title":"AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment\n  enabled by Large Language Models","summary":"  The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named AutoAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nAutoAlign constructs a predicate-proximity-graph with the help of large\nlanguage models to automatically capture the similarity between predicates\nacross two KGs. For entity embeddings, AutoAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nAutoAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that AutoAlign improves the performance of entity\nalignment significantly compared to state-of-the-art methods.\n","authors":["Rui Zhang","Yixin Su","Bayu Distiawan Trisedya","Xiaoyan Zhao","Min Yang","Hong Cheng","Jianzhong Qi"],"pdf_url":"https://arxiv.org/pdf/2307.11772v3.pdf","comment":"14 pages, 5 figures, 4 tables, IEEE Transactions on Knowledge and\n  Data Engineering"},{"id":"http://arxiv.org/abs/2311.07234v1","updated":"2023-11-13T10:54:53Z","published":"2023-11-13T10:54:53Z","title":"Multi-task learning for joint weakly-supervised segmentation and aortic\n  arch anomaly classification in fetal cardiac MRI","summary":"  Congenital Heart Disease (CHD) is a group of cardiac malformations present\nalready during fetal life, representing the prevailing category of birth\ndefects globally. Our aim in this study is to aid 3D fetal vessel topology\nvisualisation in aortic arch anomalies, a group which encompasses a range of\nconditions with significant anatomical heterogeneity. We present a multi-task\nframework for automated multi-class fetal vessel segmentation from 3D black\nblood T2w MRI and anomaly classification. Our training data consists of binary\nmanual segmentation masks of the cardiac vessels' region in individual subjects\nand fully-labelled anomaly-specific population atlases. Our framework combines\ndeep learning label propagation using VoxelMorph with 3D Attention U-Net\nsegmentation and DenseNet121 anomaly classification. We target 11 cardiac\nvessels and three distinct aortic arch anomalies, including double aortic arch,\nright aortic arch, and suspected coarctation of the aorta. We incorporate an\nanomaly classifier into our segmentation pipeline, delivering a multi-task\nframework with the primary motivation of correcting topological inaccuracies of\nthe segmentation. The hypothesis is that the multi-task approach will encourage\nthe segmenter network to learn anomaly-specific features. As a secondary\nmotivation, an automated diagnosis tool may have the potential to enhance\ndiagnostic confidence in a decision support setting. Our results showcase that\nour proposed training strategy significantly outperforms label propagation and\na network trained exclusively on propagated labels. Our classifier outperforms\na classifier trained exclusively on T2w volume images, with an average balanced\naccuracy of 0.99 (0.01) after joint training. Adding a classifier improves the\nanatomical and topological accuracy of all correctly classified double aortic\narch subjects.\n","authors":["Paula Ramirez","Alena Uus","Milou P. M. van Poppel","Irina Grigorescu","Johannes K. Steinweg","David F. A. Lloyd","Kuberan Pushparajah","Andrew P. King","Maria Deprez"],"pdf_url":"https://arxiv.org/pdf/2311.07234v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2023:015"},{"id":"http://arxiv.org/abs/2311.07231v1","updated":"2023-11-13T10:52:44Z","published":"2023-11-13T10:52:44Z","title":"Error Analysis of Option Pricing via Deep PDE Solvers: Empirical Study","summary":"  Option pricing, a fundamental problem in finance, often requires solving\nnon-linear partial differential equations (PDEs). When dealing with multi-asset\noptions, such as rainbow options, these PDEs become high-dimensional, leading\nto challenges posed by the curse of dimensionality. While deep learning-based\nPDE solvers have recently emerged as scalable solutions to this\nhigh-dimensional problem, their empirical and quantitative accuracy remains not\nwell-understood, hindering their real-world applicability. In this study, we\naimed to offer actionable insights into the utility of Deep PDE solvers for\npractical option pricing implementation. Through comparative experiments, we\nassessed the empirical performance of these solvers in high-dimensional\ncontexts. Our investigation identified three primary sources of errors in Deep\nPDE solvers: (i) errors inherent in the specifications of the target option and\nunderlying assets, (ii) errors originating from the asset model simulation\nmethods, and (iii) errors stemming from the neural network training. Through\nablation studies, we evaluated the individual impact of each error source. Our\nresults indicate that the Deep BSDE method (DBSDE) is superior in performance\nand exhibits robustness against variations in option specifications. In\ncontrast, some other methods are overly sensitive to option specifications,\nsuch as time to expiration. We also find that the performance of these methods\nimproves inversely proportional to the square root of batch size and the number\nof time steps. This observation can aid in estimating computational resources\nfor achieving desired accuracies with Deep PDE solvers.\n","authors":["Rawin Assabumrungrat","Kentaro Minami","Masanori Hirano"],"pdf_url":"https://arxiv.org/pdf/2311.07231v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.07222v1","updated":"2023-11-13T10:40:17Z","published":"2023-11-13T10:40:17Z","title":"Neural General Circulation Models","summary":"  General circulation models (GCMs) are the foundation of weather and climate\nprediction. GCMs are physics-based simulators which combine a numerical solver\nfor large-scale dynamics with tuned representations for small-scale processes\nsuch as cloud formation. Recently, machine learning (ML) models trained on\nreanalysis data achieved comparable or better skill than GCMs for deterministic\nweather forecasting. However, these models have not demonstrated improved\nensemble forecasts, or shown sufficient stability for long-term weather and\nclimate simulations. Here we present the first GCM that combines a\ndifferentiable solver for atmospheric dynamics with ML components, and show\nthat it can generate forecasts of deterministic weather, ensemble weather and\nclimate on par with the best ML and physics-based methods. NeuralGCM is\ncompetitive with ML models for 1-10 day forecasts, and with the European Centre\nfor Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.\nWith prescribed sea surface temperature, NeuralGCM can accurately track climate\nmetrics such as global mean temperature for multiple decades, and climate\nforecasts with 140 km resolution exhibit emergent phenomena such as realistic\nfrequency and trajectories of tropical cyclones. For both weather and climate,\nour approach offers orders of magnitude computational savings over conventional\nGCMs. Our results show that end-to-end deep learning is compatible with tasks\nperformed by conventional GCMs, and can enhance the large-scale physical\nsimulations that are essential for understanding and predicting the Earth\nsystem.\n","authors":["Dmitrii Kochkov","Janni Yuval","Ian Langmore","Peter Norgaard","Jamie Smith","Griffin Mooers","James Lottes","Stephan Rasp","Peter Düben","Milan Klöwer","Sam Hatfield","Peter Battaglia","Alvaro Sanchez-Gonzalez","Matthew Willson","Michael P. Brenner","Stephan Hoyer"],"pdf_url":"https://arxiv.org/pdf/2311.07222v1.pdf","comment":"67 pages, 34 figures"},{"id":"http://arxiv.org/abs/2302.01602v2","updated":"2023-11-13T10:33:32Z","published":"2023-02-03T08:54:55Z","title":"A Feature Selection Method for Driver Stress Detection Using Heart Rate\n  Variability and Breathing Rate","summary":"  Driver stress is a major cause of car accidents and death worldwide.\nFurthermore, persistent stress is a health problem, contributing to\nhypertension and other diseases of the cardiovascular system. Stress has a\nmeasurable impact on heart and breathing rates and stress levels can be\ninferred from such measurements. Galvanic skin response is a common test to\nmeasure the perspiration caused by both physiological and psychological stress,\nas well as extreme emotions. In this paper, galvanic skin response is used to\nestimate the ground truth stress levels. A feature selection technique based on\nthe minimal redundancy-maximal relevance method is then applied to multiple\nheart rate variability and breathing rate metrics to identify a novel and\noptimal combination for use in detecting stress. The support vector machine\nalgorithm with a radial basis function kernel was used along with these\nfeatures to reliably predict stress. The proposed method has achieved a high\nlevel of accuracy on the target dataset.\n","authors":["Ashkan Parsi","David O'Callaghan","Joseph Lemley"],"pdf_url":"https://arxiv.org/pdf/2302.01602v2.pdf","comment":"In Proceedings of the 15th International Conference on Machine Vision\n  (ICMV), Rome, Italy, 18-20 November 2022. arXiv admin note: text overlap with\n  arXiv:2206.03222"},{"id":"http://arxiv.org/abs/2303.10180v3","updated":"2023-11-13T10:25:29Z","published":"2023-03-17T10:05:20Z","title":"Towards Real-World Applications of Personalized Anesthesia Using Policy\n  Constraint Q Learning for Propofol Infusion Control","summary":"  Automated anesthesia promises to enable more precise and personalized\nanesthetic administration and free anesthesiologists from repetitive tasks,\nallowing them to focus on the most critical aspects of a patient's surgical\ncare. Current research has typically focused on creating simulated environments\nfrom which agents can learn. These approaches have demonstrated good\nexperimental results, but are still far from clinical application. In this\npaper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement\nlearning algorithm for solving the problem of learning anesthesia strategies on\nreal clinical datasets, is proposed. Conservative Q-Learning was first\nintroduced to alleviate the problem of Q function overestimation in an offline\ncontext. A policy constraint term is added to agent training to keep the policy\ndistribution of the agent and the anesthesiologist consistent to ensure safer\ndecisions made by the agent in anesthesia scenarios. The effectiveness of PCQL\nwas validated by extensive experiments on a real clinical anesthesia dataset.\nExperimental results show that PCQL is predicted to achieve higher gains than\nthe baseline approach while maintaining good agreement with the reference dose\ngiven by the anesthesiologist, using less total dose, and being more responsive\nto the patient's vital signs. In addition, the confidence intervals of the\nagent were investigated, which were able to cover most of the clinical\ndecisions of the anesthesiologist. Finally, an interpretable method, SHAP, was\nused to analyze the contributing components of the model predictions to\nincrease the transparency of the model.\n","authors":["Xiuding Cai","Jiao Chen","Yaoyao Zhu","Beimin Wang","Yu Yao"],"pdf_url":"https://arxiv.org/pdf/2303.10180v3.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.02921v2","updated":"2023-11-13T10:17:43Z","published":"2023-11-06T07:28:16Z","title":"Edge2Node: Reducing Edge Prediction to Node Classification","summary":"  Despite the success of graph neural network models in node classification,\nedge prediction (the task of predicting missing or potential links between\nnodes in a graph) remains a challenging problem for these models. A common\napproach for edge prediction is to first obtain the embeddings of two nodes,\nand then a predefined scoring function is used to predict the existence of an\nedge between the two nodes. In this paper, we introduce a new approach called\nEdge2Node (E2N) which directly obtains an embedding for each edge, without the\nneed for a scoring function. To do this, we create a new graph H based on the\ngraph G given for the edge prediction task, and then reduce the edge prediction\ntask on G to a node classification task on H. Our E2N method can be easily\napplied to any edge prediction task with superior performance and lower\ncomputational costs.\n  Our E2N method beats the best-known methods on the leaderboards for ogbl-ppa,\nogbl-collab, and ogbl-ddi datasets by 25.89%, 24.19%, and 0.34% improvements,\nrespectively.\n","authors":["Zahed Rahmati","Ali Rahmati","Dariush Kazemi"],"pdf_url":"https://arxiv.org/pdf/2311.02921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16521v2","updated":"2023-11-13T10:04:22Z","published":"2023-09-28T15:27:28Z","title":"Generating Personalized Insulin Treatments Strategies with Deep\n  Conditional Generative Time Series Models","summary":"  We propose a novel framework that combines deep generative time series models\nwith decision theory for generating personalized treatment strategies. It\nleverages historical patient trajectory data to jointly learn the generation of\nrealistic personalized treatment and future outcome trajectories through deep\ngenerative time series models. In particular, our framework enables the\ngeneration of novel multivariate treatment strategies tailored to the\npersonalized patient history and trained for optimal expected future outcomes\nbased on conditional expected utility maximization. We demonstrate our\nframework by generating personalized insulin treatment strategies and blood\nglucose predictions for hospitalized diabetes patients, showcasing the\npotential of our approach for generating improved personalized treatment\nstrategies. Keywords: deep generative model, probabilistic decision support,\npersonalized treatment generation, insulin and blood glucose prediction\n","authors":["Manuel Schürch","Xiang Li","Ahmed Allam","Giulia Rathmes","Amina Mollaysa","Claudia Cavelti-Weder","Michael Krauthammer"],"pdf_url":"https://arxiv.org/pdf/2309.16521v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 17 pages"},{"id":"http://arxiv.org/abs/2311.07204v1","updated":"2023-11-13T09:55:52Z","published":"2023-11-13T09:55:52Z","title":"On Elastic Language Models","summary":"  Large-scale pretrained language models have achieved compelling performance\nin a wide range of language understanding and information retrieval tasks.\nKnowledge distillation offers an opportunity to compress a large language model\nto a small one, in order to reach a reasonable latency-performance tradeoff.\nHowever, for scenarios where the number of requests (e.g., queries submitted to\na search engine) is highly variant, the static tradeoff attained by the\ncompressed language model might not always fit. Once a model is assigned with a\nstatic tradeoff, it could be inadequate in that the latency is too high when\nthe number of requests is large or the performance is too low when the number\nof requests is small. To this end, we propose an elastic language model\n(ElasticLM) that elastically adjusts the tradeoff according to the request\nstream. The basic idea is to introduce a compute elasticity to the compressed\nlanguage model, so that the tradeoff could vary on-the-fly along scalable and\ncontrollable compute. Specifically, we impose an elastic structure to enable\nElasticLM with compute elasticity and design an elastic optimization to learn\nElasticLM under compute elasticity. To serve ElasticLM, we apply an elastic\nschedule. Considering the specificity of information retrieval, we adapt\nElasticLM to dense retrieval and reranking and present ElasticDenser and\nElasticRanker respectively. Offline evaluation is conducted on a language\nunderstanding benchmark GLUE; and several information retrieval tasks including\nNatural Question, Trivia QA, and MS MARCO. The results show that ElasticLM\nalong with ElasticDenser and ElasticRanker can perform correctly and\ncompetitively compared with an array of static baselines. Furthermore, online\nsimulation with concurrency is also carried out. The results demonstrate that\nElasticLM can provide elastic tradeoffs with respect to varying request stream.\n","authors":["Chen Zhang","Benyou Wang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2311.07204v1.pdf","comment":"27 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2311.07202v1","updated":"2023-11-13T09:41:32Z","published":"2023-11-13T09:41:32Z","title":"Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model\n  Predictive Control","summary":"  Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive\nControl (MPC) successfully attains globally optimal solutions by upholding\nconvexity within the MPC framework. However, current ICNN architectures\nencounter the issue of vanishing gradients, which limits their ability to serve\nas deep neural networks for complex tasks. Additionally, the current neural\nnetwork-based MPC, including conventional neural network-based MPC and\nICNN-based MPC, faces slower convergence speed when compared to MPC based on\nfirst-principles models. In this study, we leverage the principles of ICNNs to\npropose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific\ngoal of reducing convergence time and mitigating the vanishing gradient problem\nwhile ensuring closed-loop stability. From a simulation study of a nonlinear\nchemical reactor, we observed a mitigation of vanishing gradient problem and a\nreduction in convergence time, with a percentage decrease of 46.7%, 31.3%, and\n20.2% compared to baseline plain RNN, plain LSTM, and Input Convex Recurrent\nNeural Network, respectively.\n","authors":["Zihao Wang","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2311.07202v1.pdf","comment":"Submitted to 6th Annual Learning for Dynamics & Control Conference\n  (L4DC 2024)"},{"id":"http://arxiv.org/abs/2303.06532v2","updated":"2023-11-13T09:39:30Z","published":"2023-03-12T01:20:49Z","title":"Automated Design of Metaheuristic Algorithms: A Survey","summary":"  Metaheuristics have gained great success in academia and practice because\ntheir search logic can be applied to any problem with available solution\nrepresentation, solution quality evaluation, and certain notions of locality.\nManually designing metaheuristic algorithms for solving a target problem is\ncriticized for being laborious, error-prone, and requiring intensive\nspecialized knowledge. This gives rise to increasing interest in automated\ndesign of metaheuristic algorithms. With computing power to fully explore\npotential design choices, the automated design could reach and even surpass\nhuman-level design and could make high-performance algorithms accessible to a\nmuch wider range of researchers and practitioners. This paper presents a broad\npicture of automated design of metaheuristic algorithms, by conducting a survey\non the common grounds and representative techniques in terms of design space,\ndesign strategies, performance evaluation strategies, and target problems in\nthis field.\n","authors":["Qi Zhao","Qiqi Duan","Bai Yan","Shi Cheng","Yuhui Shi"],"pdf_url":"https://arxiv.org/pdf/2303.06532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07191v1","updated":"2023-11-13T09:31:14Z","published":"2023-11-13T09:31:14Z","title":"Applying Large Language Models for Causal Structure Learning in Non\n  Small Cell Lung Cancer","summary":"  Causal discovery is becoming a key part in medical AI research. These methods\ncan enhance healthcare by identifying causal links between biomarkers,\ndemographics, treatments and outcomes. They can aid medical professionals in\nchoosing more impactful treatments and strategies. In parallel, Large Language\nModels (LLMs) have shown great potential in identifying patterns and generating\ninsights from text data. In this paper we investigate applying LLMs to the\nproblem of determining the directionality of edges in causal discovery.\nSpecifically, we test our approach on a deidentified set of Non Small Cell Lung\nCancer(NSCLC) patients that have both electronic health record and genomic\npanel data. Graphs are validated using Bayesian Dirichlet estimators using\ntabular data. Our result shows that LLMs can accurately predict the\ndirectionality of edges in causal graphs, outperforming existing\nstate-of-the-art methods. These findings suggests that LLMs can play a\nsignificant role in advancing causal discovery and help us better understand\ncomplex systems.\n","authors":["Narmada Naik","Ayush Khandelwal","Mohit Joshi","Madhusudan Atre","Hollis Wright","Kavya Kannan","Scott Hill","Giridhar Mamidipudi","Ganapati Srinivasa","Carlo Bifulco","Brian Piening","Kevin Matlock"],"pdf_url":"https://arxiv.org/pdf/2311.07191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07180v1","updated":"2023-11-13T09:11:55Z","published":"2023-11-13T09:11:55Z","title":"Knowledge Graph Representations to enhance Intensive Care Time-Series\n  Predictions","summary":"  Intensive Care Units (ICU) require comprehensive patient data integration for\nenhanced clinical outcome predictions, crucial for assessing patient\nconditions. Recent deep learning advances have utilized patient time series\ndata, and fusion models have incorporated unstructured clinical reports,\nimproving predictive performance. However, integrating established medical\nknowledge into these models has not yet been explored. The medical domain's\ndata, rich in structural relationships, can be harnessed through knowledge\ngraphs derived from clinical ontologies like the Unified Medical Language\nSystem (UMLS) for better predictions. Our proposed methodology integrates this\nknowledge with ICU data, improving clinical decision modeling. It combines\ngraph representations with vital signs and clinical reports, enhancing\nperformance, especially when data is missing. Additionally, our model includes\nan interpretability component to understand how knowledge graph nodes affect\npredictions.\n","authors":["Samyak Jain","Manuel Burger","Gunnar Rätsch","Rita Kuznetsova"],"pdf_url":"https://arxiv.org/pdf/2311.07180v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 11 pages"},{"id":"http://arxiv.org/abs/2311.07178v1","updated":"2023-11-13T09:09:52Z","published":"2023-11-13T09:09:52Z","title":"Game Solving with Online Fine-Tuning","summary":"  Game solving is a similar, yet more difficult task than mastering a game.\nSolving a game typically means to find the game-theoretic value (outcome given\noptimal play), and optionally a full strategy to follow in order to achieve\nthat outcome. The AlphaZero algorithm has demonstrated super-human level play,\nand its powerful policy and value predictions have also served as heuristics in\ngame solving. However, to solve a game and obtain a full strategy, a winning\nresponse must be found for all possible moves by the losing player. This\nincludes very poor lines of play from the losing side, for which the AlphaZero\nself-play process will not encounter. AlphaZero-based heuristics can be highly\ninaccurate when evaluating these out-of-distribution positions, which occur\nthroughout the entire search. To address this issue, this paper investigates\napplying online fine-tuning while searching and proposes two methods to learn\ntailor-designed heuristics for game solving. Our experiments show that using\nonline fine-tuning can solve a series of challenging 7x7 Killall-Go problems,\nusing only 23.54% of computation time compared to the baseline without online\nfine-tuning. Results suggest that the savings scale with problem size. Our\nmethod can further be extended to any tree search algorithm for problem\nsolving. Our code is available at\nhttps://rlg.iis.sinica.edu.tw/papers/neurips2023-online-fine-tuning-solver.\n","authors":["Ti-Rong Wu","Hung Guei","Ting Han Wei","Chung-Chin Shih","Jui-Te Chin","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2311.07178v1.pdf","comment":"Accepted by the 37th Conference on Neural Information Processing\n  Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2310.04816v2","updated":"2023-11-13T09:03:09Z","published":"2023-10-07T14:13:14Z","title":"Hacking Generative Models with Differentiable Network Bending","summary":"  In this work, we propose a method to 'hack' generative models, pushing their\noutputs away from the original training distribution towards a new objective.\nWe inject a small-scale trainable module between the intermediate layers of the\nmodel and train it for a low number of iterations, keeping the rest of the\nnetwork frozen. The resulting output images display an uncanny quality, given\nby the tension between the original and new objectives that can be exploited\nfor artistic purposes.\n","authors":["Giacomo Aldegheri","Alina Rogalska","Ahmed Youssef","Eugenia Iofinova"],"pdf_url":"https://arxiv.org/pdf/2310.04816v2.pdf","comment":"12 pages, 10 figures, Machine Learning for Creativity and Design\n  Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07162v1","updated":"2023-11-13T08:56:56Z","published":"2023-11-13T08:56:56Z","title":"CycleGANAS: Differentiable Neural Architecture Search for CycleGAN","summary":"  We develop a Neural Architecture Search (NAS) framework for CycleGAN that\ncarries out unpaired image-to-image translation task. Extending previous NAS\ntechniques for Generative Adversarial Networks (GANs) to CycleGAN is not\nstraightforward due to the task difference and greater search space. We design\narchitectures that consist of a stack of simple ResNet-based cells and develop\na search method that effectively explore the large search space. We show that\nour framework, called CycleGANAS, not only effectively discovers\nhigh-performance architectures that either match or surpass the performance of\nthe original CycleGAN, but also successfully address the data imbalance by\nindividual architecture search for each translation direction. To our best\nknowledge, it is the first NAS result for CycleGAN and shed light on NAS for\nmore complex structures.\n","authors":["Taegun An","Changhee Joo"],"pdf_url":"https://arxiv.org/pdf/2311.07162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07735v2","updated":"2023-11-13T08:50:53Z","published":"2023-07-15T07:19:29Z","title":"Faster Algorithms for Structured Linear and Kernel Support Vector\n  Machines","summary":"  Quadratic programming is a ubiquitous prototype in convex programming. Many\ncombinatorial optimizations on graphs and machine learning problems can be\nformulated as quadratic programming; for example, Support Vector Machines\n(SVMs). Linear and kernel SVMs have been among the most popular models in\nmachine learning over the past three decades, prior to the deep learning era.\n  Generally, a quadratic program has an input size of $\\Theta(n^2)$, where $n$\nis the number of variables. Assuming the Strong Exponential Time Hypothesis\n($\\textsf{SETH}$), it is known that no $O(n^{2-o(1)})$ algorithm exists\n(Backurs, Indyk, and Schmidt, NIPS'17). However, problems such as SVMs usually\nfeature much smaller input sizes: one is given $n$ data points, each of\ndimension $d$, with $d \\ll n$. Furthermore, SVMs are variants with only $O(1)$\nlinear constraints. This suggests that faster algorithms are feasible, provided\nthe program exhibits certain underlying structures.\n  In this work, we design the first nearly-linear time algorithm for solving\nquadratic programs whenever the quadratic objective has small treewidth or\nadmits a low-rank factorization, and the number of linear constraints is small.\nConsequently, we obtain a variety of results for SVMs:\n  * For linear SVM, where the quadratic constraint matrix has treewidth $\\tau$,\nwe can solve the corresponding program in time $\\widetilde\nO(n\\tau^{(\\omega+1)/2}\\log(1/\\epsilon))$;\n  * For linear SVM, where the quadratic constraint matrix admits a low-rank\nfactorization of rank-$k$, we can solve the corresponding program in time\n$\\widetilde O(nk^{(\\omega+1)/2}\\log(1/\\epsilon))$;\n  * For Gaussian kernel SVM, where the data dimension $d = \\Theta(\\log n)$ and\nthe squared dataset radius is small, we can solve it in time\n$O(n^{1+o(1)}\\log(1/\\epsilon))$. We also prove that when the squared dataset\nradius is large, then $\\Omega(n^{2-o(1)})$ time is required.\n","authors":["Yuzhou Gu","Zhao Song","Lichen Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.07735v2.pdf","comment":"New results: almost-linear time algorithm for Gaussian kernel SVM and\n  complementary lower bounds. Abstract shortened to meet arxiv requirement"},{"id":"http://arxiv.org/abs/2302.09526v2","updated":"2023-11-13T08:43:45Z","published":"2023-02-19T09:55:18Z","title":"Mixed Semi-Supervised Generalized-Linear-Regression with applications to\n  Deep-Learning and Interpolators","summary":"  We present a methodology for using unlabeled data to design semi supervised\nlearning (SSL) methods that improve the prediction performance of supervised\nlearning for regression tasks. The main idea is to design different mechanisms\nfor integrating the unlabeled data, and include in each of them a mixing\nparameter $\\alpha$, controlling the weight given to the unlabeled data.\nFocusing on Generalized Linear Models (GLM) and linear interpolators classes of\nmodels, we analyze the characteristics of different mixing mechanisms, and\nprove that in all cases, it is invariably beneficial to integrate the unlabeled\ndata with some nonzero mixing ratio $\\alpha>0$, in terms of predictive\nperformance. Moreover, we provide a rigorous framework to estimate the best\nmixing ratio $\\alpha^*$ where mixed SSL delivers the best predictive\nperformance, while using the labeled and unlabeled data on hand.\n  The effectiveness of our methodology in delivering substantial improvement\ncompared to the standard supervised models, in a variety of settings, is\ndemonstrated empirically through extensive simulation, in a manner that\nsupports the theoretical analysis. We also demonstrate the applicability of our\nmethodology (with some intuitive modifications) to improve more complex models,\nsuch as deep neural networks, in real-world regression tasks.\n","authors":["Oren Yuval","Saharon Rosset"],"pdf_url":"https://arxiv.org/pdf/2302.09526v2.pdf","comment":"48 pages 10 figures"},{"id":"http://arxiv.org/abs/2307.08286v2","updated":"2023-11-13T08:25:48Z","published":"2023-07-17T07:16:28Z","title":"Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature\n  Connectivity","summary":"  Recent work has revealed many intriguing empirical phenomena in neural\nnetwork training, despite the poorly understood and highly complex loss\nlandscapes and training dynamics. One of these phenomena, Linear Mode\nConnectivity (LMC), has gained considerable attention due to the intriguing\nobservation that different solutions can be connected by a linear path in the\nparameter space while maintaining near-constant training and test losses. In\nthis work, we introduce a stronger notion of linear connectivity, Layerwise\nLinear Feature Connectivity (LLFC), which says that the feature maps of every\nlayer in different trained networks are also linearly connected. We provide\ncomprehensive empirical evidence for LLFC across a wide range of settings,\ndemonstrating that whenever two trained networks satisfy LMC (via either\nspawning or permutation methods), they also satisfy LLFC in nearly all the\nlayers. Furthermore, we delve deeper into the underlying factors contributing\nto LLFC, which reveal new insights into the spawning and permutation\napproaches. The study of LLFC transcends and advances our understanding of LMC\nby adopting a feature-learning perspective.\n","authors":["Zhanpeng Zhou","Yongyi Yang","Xiaojiang Yang","Junchi Yan","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2307.08286v2.pdf","comment":"25 pages, 23 figures"},{"id":"http://arxiv.org/abs/2311.01568v2","updated":"2023-11-13T08:15:37Z","published":"2023-11-02T19:44:59Z","title":"Anytime-Competitive Reinforcement Learning with Policy Prior","summary":"  This paper studies the problem of Anytime-Competitive Markov Decision Process\n(A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim\nto optimize the expected reward while constraining the expected cost over\nrandom dynamics, but the cost in a specific episode can still be\nunsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the\nexpected reward while guaranteeing a bounded cost in each round of any episode\nagainst a policy prior. We propose a new algorithm, called Anytime-Competitive\nReinforcement Learning (ACRL), which provably guarantees the anytime cost\nconstraints. The regret analysis shows the policy asymptotically matches the\noptimal reward achievable under the anytime competitive constraints.\nExperiments on the application of carbon-intelligent computing verify the\nreward performance and cost constraint guarantee of ACRL.\n","authors":["Jianyi Yang","Pengfei Li","Tongxin Li","Adam Wierman","Shaolei Ren"],"pdf_url":"https://arxiv.org/pdf/2311.01568v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07143v1","updated":"2023-11-13T08:14:29Z","published":"2023-11-13T08:14:29Z","title":"Learning Symmetrization for Equivariance with Orbit Distance\n  Minimization","summary":"  We present a general framework for symmetrizing an arbitrary neural-network\narchitecture and making it equivariant with respect to a given group. We build\nupon the proposals of Kim et al. (2023); Kaba et al. (2023) for symmetrization,\nand improve them by replacing their conversion of neural features into group\nrepresentations, with an optimization whose loss intuitively measures the\ndistance between group orbits. This change makes our approach applicable to a\nbroader range of matrix groups, such as the Lorentz group O(1, 3), than these\ntwo proposals. We experimentally show our method's competitiveness on the SO(2)\nimage classification task, and also its increased generality on the task with\nO(1, 3). Our implementation will be made accessible at\nhttps://github.com/tiendatnguyen-vision/Orbit-symmetrize.\n","authors":["Tien Dat Nguyen","Jinwoo Kim","Hongseok Yang","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2311.07143v1.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2311.07141v1","updated":"2023-11-13T08:13:55Z","published":"2023-11-13T08:13:55Z","title":"SABAF: Removing Strong Attribute Bias from Neural Networks with\n  Adversarial Filtering","summary":"  Ensuring a neural network is not relying on protected attributes (e.g., race,\nsex, age) for prediction is crucial in advancing fair and trustworthy AI. While\nseveral promising methods for removing attribute bias in neural networks have\nbeen proposed, their limitations remain under-explored. To that end, in this\nwork, we mathematically and empirically reveal the limitation of existing\nattribute bias removal methods in presence of strong bias and propose a new\nmethod that can mitigate this limitation. Specifically, we first derive a\ngeneral non-vacuous information-theoretical upper bound on the performance of\nany attribute bias removal method in terms of the bias strength, revealing that\nthey are effective only when the inherent bias in the dataset is relatively\nweak. Next, we derive a necessary condition for the existence of any method\nthat can remove attribute bias regardless of the bias strength. Inspired by\nthis condition, we then propose a new method using an adversarial objective\nthat directly filters out protected attributes in the input space while\nmaximally preserving all other attributes, without requiring any specific\ntarget label. The proposed method achieves state-of-the-art performance in both\nstrong and moderate bias settings. We provide extensive experiments on\nsynthetic, image, and census datasets, to verify the derived theoretical bound\nand its consequences in practice, and evaluate the effectiveness of the\nproposed method in removing strong attribute bias.\n","authors":["Jiazhi Li","Mahyar Khayatkhoei","Jiageng Zhu","Hanchen Xie","Mohamed E. Hussein","Wael AbdAlmageed"],"pdf_url":"https://arxiv.org/pdf/2311.07141v1.pdf","comment":"35 pages, 18 figures, 32 tables. Code will be released at\n  https://github.com/jiazhi412/strong_attribute_bias. arXiv admin note: text\n  overlap with arXiv:2310.04955"},{"id":"http://arxiv.org/abs/2311.07139v1","updated":"2023-11-13T08:11:09Z","published":"2023-11-13T08:11:09Z","title":"Analyzing and Predicting Low-Listenership Trends in a Large-Scale Mobile\n  Health Program: A Preliminary Investigation","summary":"  Mobile health programs are becoming an increasingly popular medium for\ndissemination of health information among beneficiaries in less privileged\ncommunities. Kilkari is one of the world's largest mobile health programs which\ndelivers time sensitive audio-messages to pregnant women and new mothers. We\nhave been collaborating with ARMMAN, a non-profit in India which operates the\nKilkari program, to identify bottlenecks to improve the efficiency of the\nprogram. In particular, we provide an initial analysis of the trajectories of\nbeneficiaries' interaction with the mHealth program and examine elements of the\nprogram that can be potentially enhanced to boost its success. We cluster the\ncohort into different buckets based on listenership so as to analyze\nlistenership patterns for each group that could help boost program success. We\nalso demonstrate preliminary results on using historical data in a time-series\nprediction to identify beneficiary dropouts and enable NGOs in devising timely\ninterventions to strengthen beneficiary retention.\n","authors":["Arshika Lalan","Shresth Verma","Kumar Madhu Sudan","Amrita Mahale","Aparna Hegde","Milind Tambe","Aparna Taneja"],"pdf_url":"https://arxiv.org/pdf/2311.07139v1.pdf","comment":"Accepted to Data Science for Social Good Workshop, KDD 2023"},{"id":"http://arxiv.org/abs/2302.13262v3","updated":"2023-11-13T08:09:52Z","published":"2023-02-26T08:26:01Z","title":"Modulated Neural ODEs","summary":"  Neural ordinary differential equations (NODEs) have been proven useful for\nlearning non-linear dynamics of arbitrary trajectories. However, current NODE\nmethods capture variations across trajectories only via the initial state value\nor by auto-regressive encoder updates. In this work, we introduce Modulated\nNeural ODEs (MoNODEs), a novel framework that sets apart dynamics states from\nunderlying static factors of variation and improves the existing NODE methods.\nIn particular, we introduce $\\textit{time-invariant modulator variables}$ that\nare learned from the data. We incorporate our proposed framework into four\nexisting NODE variants. We test MoNODE on oscillating systems, videos and human\nwalking trajectories, where each trajectory has trajectory-specific modulation.\nOur framework consistently improves the existing model ability to generalize to\nnew dynamic parameterizations and to perform far-horizon forecasting. In\naddition, we verify that the proposed modulator variables are informative of\nthe true unknown factors of variation as measured by $R^2$ scores.\n","authors":["Ilze Amanda Auzina","Çağatay Yıldız","Sara Magliacane","Matthias Bethge","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2302.13262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15935v3","updated":"2023-11-13T08:00:10Z","published":"2022-05-31T16:27:57Z","title":"Bias-inducing geometries: an exactly solvable data model with fairness\n  implications","summary":"  Machine learning (ML) may be oblivious to human bias but it is not immune to\nits perpetuation. Marginalisation and iniquitous group representation are often\ntraceable in the very data used for training, and may be reflected or even\nenhanced by the learning models. In the present work, we aim at clarifying the\nrole played by data geometry in the emergence of ML bias. We introduce an\nexactly solvable high-dimensional model of data imbalance, where parametric\ncontrol over the many bias-inducing factors allows for an extensive exploration\nof the bias inheritance mechanism. Through the tools of statistical physics, we\nanalytically characterise the typical properties of learning models trained in\nthis synthetic framework and obtain exact predictions for the observables that\nare commonly employed for fairness assessment. Despite the simplicity of the\ndata model, we retrace and unpack typical unfairness behaviour observed on\nreal-world datasets. We also obtain a detailed analytical characterisation of a\nclass of bias mitigation strategies. We first consider a basic loss-reweighing\nscheme, which allows for an implicit minimisation of different unfairness\nmetrics, and quantify the incompatibilities between some existing fairness\ncriteria. Then, we consider a novel mitigation strategy based on a matched\ninference approach, consisting in the introduction of coupled learning models.\nOur theoretical analysis of this approach shows that the coupled strategy can\nstrike superior fairness-accuracy trade-offs.\n","authors":["Stefano Sarao Mannelli","Federica Gerace","Negar Rostamzadeh","Luca Saglietti"],"pdf_url":"https://arxiv.org/pdf/2205.15935v3.pdf","comment":"9 pages + methods + SI"},{"id":"http://arxiv.org/abs/2208.14673v2","updated":"2023-11-13T07:55:07Z","published":"2022-08-31T08:00:04Z","title":"Incremental Learning in Diagonal Linear Networks","summary":"  Diagonal linear networks (DLNs) are a toy simplification of artificial neural\nnetworks; they consist in a quadratic reparametrization of linear regression\ninducing a sparse implicit regularization. In this paper, we describe the\ntrajectory of the gradient flow of DLNs in the limit of small initialization.\nWe show that incremental learning is effectively performed in the limit:\ncoordinates are successively activated, while the iterate is the minimizer of\nthe loss constrained to have support on the active coordinates only. This shows\nthat the sparse implicit regularization of DLNs decreases with time. This work\nis restricted to the underparametrized regime with anti-correlated features for\ntechnical reasons.\n","authors":["Raphaël Berthier"],"pdf_url":"https://arxiv.org/pdf/2208.14673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03059v2","updated":"2023-11-13T07:46:39Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code will be released at\nhttps://github.com/Even-JK/PEFT-3D.\n","authors":["Ivan Tang","Ray Zhang","Zoey Guo"],"pdf_url":"https://arxiv.org/pdf/2310.03059v2.pdf","comment":"10 pages. The specialized PEFT framework for 3D pre-trained models,\n  which achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Even-JK/PEFT-3D"},{"id":"http://arxiv.org/abs/2311.07126v1","updated":"2023-11-13T07:39:13Z","published":"2023-11-13T07:39:13Z","title":"How to Do Machine Learning with Small Data? -- A Review from an\n  Industrial Perspective","summary":"  Artificial intelligence experienced a technological breakthrough in science,\nindustry, and everyday life in the recent few decades. The advancements can be\ncredited to the ever-increasing availability and miniaturization of\ncomputational resources that resulted in exponential data growth. However,\nbecause of the insufficient amount of data in some cases, employing machine\nlearning in solving complex tasks is not straightforward or even possible. As a\nresult, machine learning with small data experiences rising importance in data\nscience and application in several fields. The authors focus on interpreting\nthe general term of \"small data\" and their engineering and industrial\napplication role. They give a brief overview of the most important industrial\napplications of machine learning and small data. Small data is defined in terms\nof various characteristics compared to big data, and a machine learning\nformalism was introduced. Five critical challenges of machine learning with\nsmall data in industrial applications are presented: unlabeled data, imbalanced\ndata, missing data, insufficient data, and rare events. Based on those\ndefinitions, an overview of the considerations in domain representation and\ndata acquisition is given along with a taxonomy of machine learning approaches\nin the context of small data.\n","authors":["Ivan Kraljevski","Yong Chul Ju","Dmitrij Ivanov","Constanze Tschöpe","Matthias Wolff"],"pdf_url":"https://arxiv.org/pdf/2311.07126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02577v2","updated":"2023-11-13T07:16:00Z","published":"2023-11-05T06:46:26Z","title":"Steady-State Analysis and Online Learning for Queues with Hawkes\n  Arrivals","summary":"  We investigate the long-run behavior of single-server queues with Hawkes\narrivals and general service distributions and related optimization problems.\nIn detail, utilizing novel coupling techniques, we establish finite moment\nbounds for the stationary distribution of the workload and busy period\nprocesses. In addition, we are able to show that, those queueing processes\nconverge exponentially fast to their stationary distribution. Based on these\ntheoretic results, we develop an efficient numerical algorithm to solve the\noptimal staffing problem for the Hawkes queues in a data-driven manner.\nNumerical results indicate a sharp difference in staffing for Hawkes queues,\ncompared to the classic GI/GI/1 model, especially in the heavy-traffic regime.\n","authors":["Xinyun Chen","Guiyu Hong"],"pdf_url":"https://arxiv.org/pdf/2311.02577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00334v2","updated":"2023-11-13T07:12:55Z","published":"2023-11-01T07:01:19Z","title":"MetisFL: An Embarrassingly Parallelized Controller for Scalable &\n  Efficient Federated Learning Workflows","summary":"  A Federated Learning (FL) system typically consists of two core processing\nentities: the federation controller and the learners. The controller is\nresponsible for managing the execution of FL workflows across learners and the\nlearners for training and evaluating federated models over their private\ndatasets. While executing an FL workflow, the FL system has no control over the\ncomputational resources or data of the participating learners. Still, it is\nresponsible for other operations, such as model aggregation, task dispatching,\nand scheduling. These computationally heavy operations generally need to be\nhandled by the federation controller. Even though many FL systems have been\nrecently proposed to facilitate the development of FL workflows, most of these\nsystems overlook the scalability of the controller. To meet this need, we\ndesigned and developed a novel FL system called MetisFL, where the federation\ncontroller is the first-class citizen. MetisFL re-engineers all the operations\nconducted by the federation controller to accelerate the training of\nlarge-scale FL workflows. By quantitatively comparing MetisFL against other\nstate-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a\n10-fold wall-clock time execution boost across a wide range of challenging FL\nworkflows with increasing model sizes and federation sites.\n","authors":["Dimitris Stripelis","Chrysovalantis Anastasiou","Patrick Toral","Armaghan Asghar","Jose Luis Ambite"],"pdf_url":"https://arxiv.org/pdf/2311.00334v2.pdf","comment":"15 pages, 11 figures, Accepted at DistributedML '23"},{"id":"http://arxiv.org/abs/2311.07114v1","updated":"2023-11-13T07:09:45Z","published":"2023-11-13T07:09:45Z","title":"Novel models for fatigue life prediction under wideband random loads\n  based on machine learning","summary":"  Machine learning as a data-driven solution has been widely applied in the\nfield of fatigue lifetime prediction. In this paper, three models for wideband\nfatigue life prediction are built based on three machine learning models, i.e.\nsupport vector machine (SVM), Gaussian process regression (GPR) and artificial\nneural network (ANN). The generalization ability of the models is enhanced by\nemploying numerous power spectra samples with different bandwidth parameters\nand a variety of material properties related to fatigue life. Sufficient Monte\nCarlo numerical simulations demonstrate that the newly developed machine\nlearning models are superior to the traditional frequency-domain models in\nterms of life prediction accuracy and the ANN model has the best overall\nperformance among the three developed machine learning models.\n","authors":["Hong Sun","Yuanying Qiu","Jing Li","Jin Bai","Ming Peng"],"pdf_url":"https://arxiv.org/pdf/2311.07114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07110v1","updated":"2023-11-13T06:52:56Z","published":"2023-11-13T06:52:56Z","title":"Adversarial Purification for Data-Driven Power System Event Classifiers\n  with Diffusion Models","summary":"  The global deployment of the phasor measurement units (PMUs) enables\nreal-time monitoring of the power system, which has stimulated considerable\nresearch into machine learning-based models for event detection and\nclassification. However, recent studies reveal that machine learning-based\nmethods are vulnerable to adversarial attacks, which can fool the event\nclassifiers by adding small perturbations to the raw PMU data. To mitigate the\nthreats posed by adversarial attacks, research on defense strategies is\nurgently needed. This paper proposes an effective adversarial purification\nmethod based on the diffusion model to counter adversarial attacks on the\nmachine learning-based power system event classifier. The proposed method\nincludes two steps: injecting noise into the PMU data; and utilizing a\npre-trained neural network to eliminate the added noise while simultaneously\nremoving perturbations introduced by the adversarial attacks. The proposed\nadversarial purification method significantly increases the accuracy of the\nevent classifier under adversarial attacks while satisfying the requirements of\nreal-time operations. In addition, the theoretical analysis reveals that the\nproposed diffusion model-based adversarial purification method decreases the\ndistance between the original and compromised PMU data, which reduces the\nimpacts of adversarial attacks. The empirical results on a large-scale\nreal-world PMU dataset validate the effectiveness and computational efficiency\nof the proposed adversarial purification method.\n","authors":["Yuanbin Cheng","Koji Yamashita","Jim Follum","Nanpeng Yu"],"pdf_url":"https://arxiv.org/pdf/2311.07110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13086v3","updated":"2023-11-13T06:16:09Z","published":"2022-06-27T07:12:31Z","title":"RankSEG: A Consistent Ranking-based Framework for Segmentation","summary":"  Segmentation has emerged as a fundamental field of computer vision and\nnatural language processing, which assigns a label to every pixel/feature to\nextract regions of interest from an image/text. To evaluate the performance of\nsegmentation, the Dice and IoU metrics are used to measure the degree of\noverlap between the ground truth and the predicted segmentation. In this paper,\nwe establish a theoretical foundation of segmentation with respect to the\nDice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous\nto classification-calibration or Fisher consistency in classification. We prove\nthat the existing thresholding-based framework with most operating losses are\nnot consistent with respect to the Dice/IoU metrics, and thus may lead to a\nsuboptimal solution. To address this pitfall, we propose a novel consistent\nranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of\nthe Bayes segmentation rule. Three numerical algorithms with GPU parallel\nexecution are developed to implement the proposed framework in large-scale and\nhigh-dimensional segmentation. We study statistical properties of the proposed\nframework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and\nthe rate of convergence are also provided. The numerical effectiveness of\nRankDice/mRankDice is demonstrated in various simulated examples and\nFine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with\nstate-of-the-art deep learning architectures.\n","authors":["Ben Dai","Chunlin Li"],"pdf_url":"https://arxiv.org/pdf/2206.13086v3.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2311.07079v1","updated":"2023-11-13T05:08:26Z","published":"2023-11-13T05:08:26Z","title":"Sample Dominance Aware Framework via Non-Parametric Estimation for\n  Spontaneous Brain-Computer Interface","summary":"  Deep learning has shown promise in decoding brain signals, such as\nelectroencephalogram (EEG), in the field of brain-computer interfaces (BCIs).\nHowever, the non-stationary characteristics of EEG signals pose challenges for\ntraining neural networks to acquire appropriate knowledge. Inconsistent EEG\nsignals resulting from these non-stationary characteristics can lead to poor\nperformance. Therefore, it is crucial to investigate and address sample\ninconsistency to ensure robust performance in spontaneous BCIs. In this study,\nwe introduce the concept of sample dominance as a measure of EEG signal\ninconsistency and propose a method to modulate its effect on network training.\nWe present a two-stage dominance score estimation technique that compensates\nfor performance degradation caused by sample inconsistencies. Our proposed\nmethod utilizes non-parametric estimation to infer sample inconsistency and\nassigns each sample a dominance score. This score is then aggregated with the\nloss function during training to modulate the impact of sample inconsistency.\nFurthermore, we design a curriculum learning approach that gradually increases\nthe influence of inconsistent signals during training to improve overall\nperformance. We evaluate our proposed method using public spontaneous BCI\ndataset. The experimental results confirm that our findings highlight the\nimportance of addressing sample dominance for achieving robust performance in\nspontaneous BCIs.\n","authors":["Byeong-Hoo Lee","Byoung-Hee Kwon","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2311.07079v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.12816v3","updated":"2023-11-13T05:07:41Z","published":"2023-03-22T07:34:33Z","title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient\n  Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream applications. Conventional KGE\nmethods require high-dimensional representations to learn the complex structure\nof knowledge graph, but lead to oversized model parameters. Recent advances\nreduce parameters by low-dimensional entity representations, while developing\ntechniques (e.g., knowledge distillation or reinvented representation forms) to\ncompensate for reduced dimension. However, such operations introduce\ncomplicated computations and model designs that may not benefit large knowledge\ngraphs. To seek a simple strategy to improve the parameter efficiency of\nconventional KGE models, we take inspiration from that deeper neural networks\nrequire exponentially fewer parameters to achieve expressiveness comparable to\nwider networks for compositional structures. We view all entity representations\nas a single-layer embedding network, and conventional KGE methods that adopt\nhigh-dimensional entity representations equal widening the embedding network to\ngain expressiveness. To achieve parameter efficiency, we instead propose a\ndeeper embedding network for entity representations, i.e., a narrow entity\nembedding layer plus a multi-layer dimension lifting network (LiftNet).\nExperiments on three public datasets show that by integrating LiftNet, four\nconventional KGE methods with 16-dimensional representations achieve comparable\nlink prediction accuracy as original models that adopt 512-dimensional\nrepresentations, saving 68.4% to 96.9% parameters.\n","authors":["Borui Cai","Yong Xiang","Longxiang Gao","Di Wu","He Zhang","Jiong Jin","Tom Luan"],"pdf_url":"https://arxiv.org/pdf/2303.12816v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01231v2","updated":"2023-11-13T04:49:28Z","published":"2023-07-03T07:54:54Z","title":"A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based\n  Matching Algorithms","summary":"  Entity resolution (ER) is the process of identifying records that refer to\nthe same entities within one or across multiple databases. Numerous techniques\nhave been developed to tackle ER challenges over the years, with recent\nemphasis placed on machine and deep learning methods for the matching phase.\nHowever, the quality of the benchmark datasets typically used in the\nexperimental evaluations of learning-based matching algorithms has not been\nexamined in the literature. To cover this gap, we propose four different\napproaches to assessing the difficulty and appropriateness of 13 established\ndatasets: two theoretical approaches, which involve new measures of linearity\nand existing measures of complexity, and two practical approaches: the\ndifference between the best non-linear and linear matchers, as well as the\ndifference between the best learning-based matcher and the perfect oracle. Our\nanalysis demonstrates that most of the popular datasets pose rather easy\nclassification tasks. As a result, they are not suitable for properly\nevaluating learning-based matching algorithms. To address this issue, we\npropose a new methodology for yielding benchmark datasets. We put it into\npractice by creating four new matching tasks, and we verify that these new\nbenchmarks are more challenging and therefore more suitable for further\nadvancements in the field.\n","authors":["George Papadakis","Nishadi Kirielle","Peter Christen","Themis Palpanas"],"pdf_url":"https://arxiv.org/pdf/2307.01231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10981v2","updated":"2023-11-13T04:40:17Z","published":"2023-07-20T16:09:07Z","title":"PATROL: Privacy-Oriented Pruning for Collaborative Inference Against\n  Model Inversion Attacks","summary":"  Collaborative inference has been a promising solution to enable\nresource-constrained edge devices to perform inference using state-of-the-art\ndeep neural networks (DNNs). In collaborative inference, the edge device first\nfeeds the input to a partial DNN locally and then uploads the intermediate\nresult to the cloud to complete the inference. However, recent research\nindicates model inversion attacks (MIAs) can reconstruct input data from\nintermediate results, posing serious privacy concerns for collaborative\ninference. Existing perturbation and cryptography techniques are inefficient\nand unreliable in defending against MIAs while performing accurate inference.\nThis paper provides a viable solution, named PATROL, which develops\nprivacy-oriented pruning to balance privacy, efficiency, and utility of\ncollaborative inference. PATROL takes advantage of the fact that later layers\nin a DNN can extract more task-specific features. Given limited local resources\nfor collaborative inference, PATROL intends to deploy more layers at the edge\nbased on pruning techniques to enforce task-specific features for inference and\nreduce task-irrelevant but sensitive features for privacy preservation. To\nachieve privacy-oriented pruning, PATROL introduces two key components:\nLipschitz regularization and adversarial reconstruction training, which\nincrease the reconstruction errors by reducing the stability of MIAs and\nenhance the target inference model by adversarial training, respectively. On a\nreal-world collaborative inference task, vehicle re-identification, we\ndemonstrate the superior performance of PATROL in terms of against MIAs.\n","authors":["Shiwei Ding","Lan Zhang","Miao Pan","Xiaoyong Yuan"],"pdf_url":"https://arxiv.org/pdf/2307.10981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07073v1","updated":"2023-11-13T04:40:13Z","published":"2023-11-13T04:40:13Z","title":"Exposition on over-squashing problem on GNNs: Current Methods,\n  Benchmarks and Challenges","summary":"  Graph-based message-passing neural networks (MPNNs) have achieved remarkable\nsuccess in both node and graph-level learning tasks. However, several\nidentified problems, including over-smoothing (OSM), limited expressive power,\nand over-squashing (OSQ), still limit the performance of MPNNs. In particular,\nOSQ serves as the latest identified problem, where MPNNs gradually lose their\nlearning accuracy when long-range dependencies between graph nodes are\nrequired. In this work, we provide an exposition on the OSQ problem by\nsummarizing different formulations of OSQ from current literature, as well as\nthe three different categories of approaches for addressing the OSQ problem. In\naddition, we also discuss the alignment between OSQ and expressive power and\nthe trade-off between OSQ and OSM. Furthermore, we summarize the empirical\nmethods leveraged from existing works to verify the efficiency of OSQ\nmitigation approaches, with illustrations of their computational complexities.\nLastly, we list some open questions that are of interest for further\nexploration of the OSQ problem along with potential directions from the best of\nour knowledge.\n","authors":["Dai Shi","Andi Han","Lequan Lin","Yi Guo","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15616v2","updated":"2023-11-13T04:37:42Z","published":"2023-05-24T23:24:18Z","title":"Reversible and irreversible bracket-based dynamics for deep graph neural\n  networks","summary":"  Recent works have shown that physics-inspired architectures allow the\ntraining of deep graph neural networks (GNNs) without oversmoothing. The role\nof these physics is unclear, however, with successful examples of both\nreversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena\nproducing comparable results despite diametrically opposed mechanisms, and\nfurther complications arising due to empirical departures from mathematical\ntheory. This work presents a series of novel GNN architectures based upon\nstructure-preserving bracket-based dynamical systems, which are provably\nguaranteed to either conserve energy or generate positive dissipation with\nincreasing depth. It is shown that the theoretically principled framework\nemployed here allows for inherently explainable constructions, which\ncontextualize departures from theory in current architectures and better\nelucidate the roles of reversibility and irreversibility in network\nperformance.\n","authors":["Anthony Gruber","Kookjin Lee","Nathaniel Trask"],"pdf_url":"https://arxiv.org/pdf/2305.15616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07065v1","updated":"2023-11-13T04:11:25Z","published":"2023-11-13T04:11:25Z","title":"Non-approximability of constructive global $\\mathcal{L}^2$ minimizers by\n  gradient descent in Deep Learning","summary":"  We analyze geometric aspects of the gradient descent algorithm in Deep\nLearning (DL) networks. In particular, we prove that the globally minimizing\nweights and biases for the $\\mathcal{L}^2$ cost obtained constructively in\n[Chen-Munoz Ewald 2023] for underparametrized ReLU DL networks can generically\nnot be approximated via the gradient descent flow. We therefore conclude that\nthe method introduced in [Chen-Munoz Ewald 2023] is disjoint from the gradient\ndescent method.\n","authors":["Thomas Chen","Patricia Muñoz Ewald"],"pdf_url":"https://arxiv.org/pdf/2311.07065v1.pdf","comment":"AMS Latex, 7 pages"},{"id":"http://arxiv.org/abs/2207.01678v2","updated":"2023-11-13T04:08:57Z","published":"2022-07-04T19:05:08Z","title":"FACT: High-Dimensional Random Forests Inference","summary":"  Quantifying the usefulness of individual features in random forests learning\ncan greatly enhance its interpretability. Existing studies have shown that some\npopularly used feature importance measures for random forests suffer from the\nbias issue. In addition, there lack comprehensive size and power analyses for\nmost of these existing methods. In this paper, we approach the problem via\nhypothesis testing, and suggest a framework of the self-normalized\nfeature-residual correlation test (FACT) for evaluating the significance of a\ngiven feature in the random forests model with bias-resistance property, where\nour null hypothesis concerns whether the feature is conditionally independent\nof the response given all other features. Such an endeavor on random forests\ninference is empowered by some recent developments on high-dimensional random\nforests consistency. Under a fairly general high-dimensional nonparametric\nmodel setting with dependent features, we formally establish that FACT can\nprovide theoretically justified feature importance test with controlled type I\nerror and enjoy appealing power property. The theoretical results and\nfinite-sample advantages of the newly suggested method are illustrated with\nseveral simulation examples and an economic forecasting application.\n","authors":["Chien-Ming Chi","Yingying Fan","Jinchi Lv"],"pdf_url":"https://arxiv.org/pdf/2207.01678v2.pdf","comment":"42 pages, 3 figures"},{"id":"http://arxiv.org/abs/2309.03179v3","updated":"2023-11-13T03:59:36Z","published":"2023-09-06T17:39:05Z","title":"SLiMe: Segment Like Me","summary":"  Significant strides have been made using large vision-language models, like\nStable Diffusion (SD), for a variety of downstream tasks, including image\nediting, image correspondence, and 3D shape generation. Inspired by these\nadvancements, we explore leveraging these extensive vision-language models for\nsegmenting images at any desired granularity using as few as one annotated\nsample by proposing SLiMe. SLiMe frames this problem as an optimization task.\nSpecifically, given a single training image and its segmentation mask, we first\nextract attention maps, including our novel \"weighted accumulated\nself-attention map\" from the SD prior. Then, using the extracted attention\nmaps, the text embeddings of Stable Diffusion are optimized such that, each of\nthem, learn about a single segmented region from the training image. These\nlearned embeddings then highlight the segmented region in the attention maps,\nwhich in turn can then be used to derive the segmentation map. This enables\nSLiMe to segment any real-world image during inference with the granularity of\nthe segmented region in the training image, using just one example. Moreover,\nleveraging additional training data when available, i.e. few-shot, improves the\nperformance of SLiMe. We carried out a knowledge-rich set of experiments\nexamining various design factors and showed that SLiMe outperforms other\nexisting one-shot and few-shot segmentation methods.\n","authors":["Aliasghar Khani","Saeid Asgari Taghanaki","Aditya Sanghi","Ali Mahdavi Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2309.03179v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04349v2","updated":"2023-11-13T03:49:27Z","published":"2023-07-10T05:18:18Z","title":"RLTF: Reinforcement Learning from Unit Test Feedback","summary":"  The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, current representative works\neither rely solely on offline frameworks, limiting the exploration of new\nsample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these\nissues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,\na novel online RL framework with unit test feedback of multi-granularity for\nrefining code LLMs. Our approach generates data in real-time during training\nand simultaneously utilizes fine-grained feedback signals to guide the model\ntowards producing higher-quality code. Extensive experiments show that RLTF\nachieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.\n","authors":["Jiate Liu","Yiqin Zhu","Kaiwen Xiao","Qiang Fu","Xiao Han","Wei Yang","Deheng Ye"],"pdf_url":"https://arxiv.org/pdf/2307.04349v2.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2311.07052v1","updated":"2023-11-13T03:36:18Z","published":"2023-11-13T03:36:18Z","title":"Towards the Law of Capacity Gap in Distilling Language Models","summary":"  Language model (LM) distillation is a trending area that aims to distil the\nknowledge resided in a large teacher LM to a small student one. While various\nmethods have been proposed to push the distillation to its limits, it is still\na pain distilling LMs when a large capacity gap is exhibited between the\nteacher and the student LMs. The pain is mainly resulted by the curse of\ncapacity gap, which describes that a larger teacher LM cannot always lead to a\nbetter student LM than one distilled from a smaller teacher LM due to the\naffect of capacity gap increment. That is, there is likely an optimal point\nyielding the best student LM along the scaling course of the teacher LM. Even\nworse, the curse of capacity gap can be only partly yet not fully lifted as\nindicated in previous studies.\n  However, the tale is not ever one-sided. Although a larger teacher LM has\nbetter performance than a smaller teacher LM, it is much more\nresource-demanding especially in the context of recent large LMs (LLMs).\nConsequently, instead of sticking to lifting the curse, leaving the curse as is\nshould be arguably fine. Even better, in this paper, we reveal that the optimal\ncapacity gap is almost consistent across different student scales and\narchitectures, fortunately turning the curse into the law of capacity gap. The\nlaw later guides us to distil a 3B student LM (termed MiniMA) from a 7B teacher\nLM (adapted LLaMA2-7B). MiniMA is demonstrated to yield a new\ncompute-performance pareto frontier among existing 3B LMs on commonly used\nbenchmarks, and its instruction-tuned version (termed MiniChat) outperforms a\nwide range of 3B competitors in GPT4 evaluation and could even compete with\nseveral 7B chat models.\n","authors":["Chen Zhang","Dawei Song","Zheyu Ye","Yan Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07052v1.pdf","comment":"22 pages, 8 figures, 12 tables, work in progress. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA"},{"id":"http://arxiv.org/abs/2310.14450v2","updated":"2023-11-13T03:22:32Z","published":"2023-10-22T23:23:44Z","title":"TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings","summary":"  Stance detection is important for understanding different attitudes and\nbeliefs on the Internet. However, given that a passage's stance toward a given\ntopic is often highly dependent on that topic, building a stance detection\nmodel that generalizes to unseen topics is difficult. In this work, we propose\nusing contrastive learning as well as an unlabeled dataset of news articles\nthat cover a variety of different topics to train topic-agnostic/TAG and\ntopic-aware/TAW embeddings for use in downstream stance detection. Combining\nthese embeddings in our full TATA model, we achieve state-of-the-art\nperformance across several public stance detection datasets (0.771 $F_1$-score\non the Zero-shot VAST dataset). We release our code and data at\nhttps://github.com/hanshanley/tata.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2310.14450v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2212.01382v5","updated":"2023-11-13T02:35:18Z","published":"2022-11-30T01:40:59Z","title":"Welfare and Fairness in Multi-objective Reinforcement Learning","summary":"  We study fair multi-objective reinforcement learning in which an agent must\nlearn a policy that simultaneously achieves high reward on multiple dimensions\nof a vector-valued reward. Motivated by the fair resource allocation\nliterature, we model this as an expected welfare maximization problem, for some\nnonlinear fair welfare function of the vector of long-term cumulative rewards.\nOne canonical example of such a function is the Nash Social Welfare, or\ngeometric mean, the log transform of which is also known as the Proportional\nFairness objective. We show that even approximately optimal optimization of the\nexpected Nash Social Welfare is computationally intractable even in the tabular\ncase. Nevertheless, we provide a novel adaptation of Q-learning that combines\nnonlinear scalarized learning updates and non-stationary action selection to\nlearn effective policies for optimizing nonlinear welfare functions. We show\nthat our algorithm is provably convergent, and we demonstrate experimentally\nthat our approach outperforms techniques based on linear scalarization,\nmixtures of optimal linear scalarizations, or stationary action selection for\nthe Nash Social Welfare Objective.\n","authors":["Zimeng Fan","Nianli Peng","Muhang Tian","Brandon Fain"],"pdf_url":"https://arxiv.org/pdf/2212.01382v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07025v1","updated":"2023-11-13T02:14:54Z","published":"2023-11-13T02:14:54Z","title":"Embarassingly Simple Dataset Distillation","summary":"  Dataset distillation extracts a small set of synthetic training samples from\na large dataset with the goal of achieving competitive performance on test data\nwhen trained on this sample. In this work, we tackle dataset distillation at\nits core by treating it directly as a bilevel optimization problem.\nRe-examining the foundational back-propagation through time method, we study\nthe pronounced variance in the gradients, computational burden, and long-term\ndependencies. We introduce an improved method: Random Truncated Backpropagation\nThrough Time (RaT-BPTT) to address them. RaT-BPTT incorporates a truncation\ncoupled with a random window, effectively stabilizing the gradients and\nspeeding up the optimization while covering long dependencies. This allows us\nto establish new state-of-the-art for a variety of standard dataset benchmarks.\nA deeper dive into the nature of distilled data unveils pronounced\nintercorrelation. In particular, subsets of distilled datasets tend to exhibit\nmuch worse performance than directly distilled smaller datasets of the same\nsize. Leveraging RaT-BPTT, we devise a boosting mechanism that generates\ndistilled datasets that contain subsets with near optimal performance across\ndifferent data budgets.\n","authors":["Feng Yunzhen","Vedantam Ramakrishna","Kempe Julia"],"pdf_url":"https://arxiv.org/pdf/2311.07025v1.pdf","comment":"Short version appears at NeurIPS 2023 WANT workshop"},{"id":"http://arxiv.org/abs/2307.15691v2","updated":"2023-11-13T01:56:51Z","published":"2023-07-28T17:37:47Z","title":"ODTlearn: A Package for Learning Optimal Decision Trees for Prediction\n  and Prescription","summary":"  ODTLearn is an open-source Python package that provides methods for learning\noptimal decision trees for high-stakes predictive and prescriptive tasks based\non the mixed-integer optimization (MIO) framework proposed in Aghaei et al.\n(2019) and several of its extensions. The current version of the package\nprovides implementations for learning optimal classification trees, optimal\nfair classification trees, optimal classification trees robust to distribution\nshifts, and optimal prescriptive trees from observational data. We have\ndesigned the package to be easy to maintain and extend as new optimal decision\ntree problem classes, reformulation strategies, and solution algorithms are\nintroduced. To this end, the package follows object-oriented design principles\nand supports both commercial (Gurobi) and open source (COIN-OR branch and cut)\nsolvers. The package documentation and an extensive user guide can be found at\nhttps://d3m-research-group.github.io/odtlearn/. Additionally, users can view\nthe package source code and submit feature requests and bug reports by visiting\nhttps://github.com/D3M-Research-Group/odtlearn.\n","authors":["Patrick Vossler","Sina Aghaei","Nathan Justin","Nathanael Jo","Andrés Gómez","Phebe Vayanos"],"pdf_url":"https://arxiv.org/pdf/2307.15691v2.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2206.15215v3","updated":"2023-11-13T01:52:03Z","published":"2022-06-30T11:59:40Z","title":"Learning nonparametric ordinary differential equations from noisy data","summary":"  Learning nonparametric systems of Ordinary Differential Equations (ODEs) dot\nx = f(t,x) from noisy data is an emerging machine learning topic. We use the\nwell-developed theory of Reproducing Kernel Hilbert Spaces (RKHS) to define\ncandidates for f for which the solution of the ODE exists and is unique.\nLearning f consists of solving a constrained optimization problem in an RKHS.\nWe propose a penalty method that iteratively uses the Representer theorem and\nEuler approximations to provide a numerical solution. We prove a generalization\nbound for the L2 distance between x and its estimator and provide experimental\ncomparisons with the state-of-the-art.\n","authors":["Kamel Lahouel","Michael Wells","Victor Rielly","Ethan Lew","David Lovitz","Bruno M. Jedynak"],"pdf_url":"https://arxiv.org/pdf/2206.15215v3.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.07013v1","updated":"2023-11-13T01:48:08Z","published":"2023-11-13T01:48:08Z","title":"A PAC-Bayesian Perspective on the Interpolating Information Criterion","summary":"  Deep learning is renowned for its theory-practice gap, whereby principled\ntheory typically fails to provide much beneficial guidance for implementation\nin practice. This has been highlighted recently by the benign overfitting\nphenomenon: when neural networks become sufficiently large to interpolate the\ndataset perfectly, model performance appears to improve with increasing model\nsize, in apparent contradiction with the well-known bias-variance tradeoff.\nWhile such phenomena have proven challenging to theoretically study for general\nmodels, the recently proposed Interpolating Information Criterion (IIC)\nprovides a valuable theoretical framework to examine performance for\noverparameterized models. Using the IIC, a PAC-Bayes bound is obtained for a\ngeneral class of models, characterizing factors which influence generalization\nperformance in the interpolating regime. From the provided bound, we quantify\nhow the test error for overparameterized models achieving effectively zero\ntraining error depends on the quality of the implicit regularization imposed by\ne.g. the combination of model, optimizer, and parameter-initialization scheme;\nthe spectrum of the empirical neural tangent kernel; curvature of the loss\nlandscape; and noise present in the data.\n","authors":["Liam Hodgkinson","Chris van der Heide","Robert Salomone","Fred Roosta","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2311.07013v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2210.16458v3","updated":"2023-11-13T01:14:08Z","published":"2022-10-29T01:21:42Z","title":"Reformulating van Rijsbergen's $F_β$ metric for weighted binary\n  cross-entropy","summary":"  The separation of performance metrics from gradient based loss functions may\nnot always give optimal results and may miss vital aggregate information. This\npaper investigates incorporating a performance metric alongside differentiable\nloss functions to inform training outcomes. The goal is to guide model\nperformance and interpretation by assuming statistical distributions on this\nperformance metric for dynamic weighting. The focus is on van Rijsbergens\n$F_{\\beta}$ metric -- a popular choice for gauging classification performance.\nThrough distributional assumptions on the $F_{\\beta}$, an intermediary link can\nbe established to the standard binary cross-entropy via dynamic penalty\nweights. First, the $F_{\\beta}$ metric is reformulated to facilitate assuming\nstatistical distributions with accompanying proofs for the cumulative density\nfunction. These probabilities are used within a knee curve algorithm to find an\noptimal $\\beta$ or $\\beta_{opt}$. This $\\beta_{opt}$ is used as a weight or\npenalty in the proposed weighted binary cross-entropy. Experimentation on\npublicly available data along with benchmark analysis mostly yields better and\ninterpretable results as compared to the baseline for both imbalanced and\nbalanced classes. For example, for the IMDB text data with known labeling\nerrors, a 14% boost in $F_1$ score is shown. The results also reveal\ncommonalities between the penalty model families derived in this paper and the\nsuitability of recall-centric or precision-centric parameters used in the\noptimization. The flexibility of this methodology can enhance interpretation.\n","authors":["Satesh Ramdhani"],"pdf_url":"https://arxiv.org/pdf/2210.16458v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07002v1","updated":"2023-11-13T01:03:19Z","published":"2023-11-13T01:03:19Z","title":"PICS in Pics: Physics Informed Contour Selection for Rapid Image\n  Segmentation","summary":"  Effective training of deep image segmentation models is challenging due to\nthe need for abundant, high-quality annotations. Generating annotations is\nlaborious and time-consuming for human experts, especially in medical image\nsegmentation. To facilitate image annotation, we introduce Physics Informed\nContour Selection (PICS) - an interpretable, physics-informed algorithm for\nrapid image segmentation without relying on labeled data. PICS draws\ninspiration from physics-informed neural networks (PINNs) and an active contour\nmodel called snake. It is fast and computationally lightweight because it\nemploys cubic splines instead of a deep neural network as a basis function. Its\ntraining parameters are physically interpretable because they directly\nrepresent control knots of the segmentation curve. Traditional snakes involve\nminimization of the edge-based loss functionals by deriving the Euler-Lagrange\nequation followed by its numerical solution. However, PICS directly minimizes\nthe loss functional, bypassing the Euler Lagrange equations. It is the first\nsnake variant to minimize a region-based loss function instead of traditional\nedge-based loss functions. PICS uniquely models the three-dimensional (3D)\nsegmentation process with an unsteady partial differential equation (PDE),\nwhich allows accelerated segmentation via transfer learning. To demonstrate its\neffectiveness, we apply PICS for 3D segmentation of the left ventricle on a\npublicly available cardiac dataset. While doing so, we also introduce a new\nconvexity-preserving loss term that encodes the shape information of the left\nventricle to enhance PICS's segmentation quality. Overall, PICS presents\nseveral novelties in network architecture, transfer learning, and\nphysics-inspired losses for image segmentation, thereby showing promising\noutcomes and potential for further refinement.\n","authors":["Vikas Dwivedi","Balaji Srinivasan","Ganapathy Krishnamurthi"],"pdf_url":"https://arxiv.org/pdf/2311.07002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13724v2","updated":"2023-11-13T23:58:17Z","published":"2023-06-23T18:13:15Z","title":"Review of compressed embedding layers and their applications for\n  recommender systems","summary":"  We review the literature on trainable, compressed embedding layers and\ndiscuss their applicability for compressing gigantic neural recommender\nsystems. We also report the results we measured with our compressed embedding\nlayers.\n","authors":["Tamas Hajgato"],"pdf_url":"https://arxiv.org/pdf/2306.13724v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2304.06193v2","updated":"2023-11-13T23:44:19Z","published":"2023-04-12T23:55:56Z","title":"Learning Over Contracting and Lipschitz Closed-Loops for\n  Partially-Observed Nonlinear Systems (Extended Version)","summary":"  This paper presents a policy parameterization for learning-based control on\nnonlinear, partially-observed dynamical systems. The parameterization is based\non a nonlinear version of the Youla parameterization and the recently proposed\nRecurrent Equilibrium Network (REN) class of models. We prove that the\nresulting Youla-REN parameterization automatically satisfies stability\n(contraction) and user-tunable robustness (Lipschitz) conditions on the\nclosed-loop system. This means it can be used for safe learning-based control\nwith no additional constraints or projections required to enforce stability or\nrobustness. We test the new policy class in simulation on two reinforcement\nlearning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum.\nWe find that the Youla-REN performs similarly to existing learning-based and\noptimal control methods while also ensuring stability and exhibiting improved\nrobustness to adversarial disturbances.\n","authors":["Nicholas H. Barbara","Ruigang Wang","Ian R. Manchester"],"pdf_url":"https://arxiv.org/pdf/2304.06193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.03475v2","updated":"2023-11-13T23:32:29Z","published":"2022-10-07T11:58:08Z","title":"Winner Takes It All: Training Performant RL Populations for\n  Combinatorial Optimization","summary":"  Applying reinforcement learning (RL) to combinatorial optimization problems\nis attractive as it removes the need for expert knowledge or pre-solved\ninstances. However, it is unrealistic to expect an agent to solve these (often\nNP-)hard problems in a single shot at inference due to their inherent\ncomplexity. Thus, leading approaches often implement additional search\nstrategies, from stochastic sampling and beam search to explicit fine-tuning.\nIn this paper, we argue for the benefits of learning a population of\ncomplementary policies, which can be simultaneously rolled out at inference. To\nthis end, we introduce Poppy, a simple training procedure for populations.\nInstead of relying on a predefined or hand-crafted notion of diversity, Poppy\ninduces an unsupervised specialization targeted solely at maximizing the\nperformance of the population. We show that Poppy produces a set of\ncomplementary policies, and obtains state-of-the-art RL results on four popular\nNP-hard problems: traveling salesman, capacitated vehicle routing, 0-1\nknapsack, and job-shop scheduling.\n","authors":["Nathan Grinsztajn","Daniel Furelos-Blanco","Shikha Surana","Clément Bonnet","Thomas D. Barrett"],"pdf_url":"https://arxiv.org/pdf/2210.03475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07798v1","updated":"2023-11-13T23:25:18Z","published":"2023-11-13T23:25:18Z","title":"Probabilistic Physics-integrated Neural Differentiable Modeling for\n  Isothermal Chemical Vapor Infiltration Process","summary":"  Chemical vapor infiltration (CVI) is a widely adopted manufacturing technique\nused in producing carbon-carbon and carbon-silicon carbide composites. These\nmaterials are especially valued in the aerospace and automotive industries for\ntheir robust strength and lightweight characteristics. The densification\nprocess during CVI critically influences the final performance, quality, and\nconsistency of these composite materials. Experimentally optimizing the CVI\nprocesses is challenging due to long experimental time and large optimization\nspace. To address these challenges, this work takes a modeling-centric\napproach. Due to the complexities and limited experimental data of the\nisothermal CVI densification process, we have developed a data-driven\npredictive model using the physics-integrated neural differentiable (PiNDiff)\nmodeling framework. An uncertainty quantification feature has been embedded\nwithin the PiNDiff method, bolstering the model's reliability and robustness.\nThrough comprehensive numerical experiments involving both synthetic and\nreal-world manufacturing data, the proposed method showcases its capability in\nmodeling densification during the CVI process. This research highlights the\npotential of the PiNDiff framework as an instrumental tool for advancing our\nunderstanding, simulation, and optimization of the CVI manufacturing process,\nparticularly when faced with sparse data and an incomplete description of the\nunderlying physics.\n","authors":["Deepak Akhare","Zeping Chen","Richard Gulotty","Tengfei Luo","Jian-Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07797v1","updated":"2023-11-13T23:15:29Z","published":"2023-11-13T23:15:29Z","title":"Explainable History Distillation by Marked Temporal Point Process","summary":"  Explainability of machine learning models is mandatory when researchers\nintroduce these commonly believed black boxes to real-world tasks, especially\nhigh-stakes ones. In this paper, we build a machine learning system to\nautomatically generate explanations of happened events from history by \\gls{ca}\nbased on the \\acrfull{tpp}. Specifically, we propose a new task called\n\\acrfull{ehd}. This task requires a model to distill as few events as possible\nfrom observed history. The target is that the event distribution conditioned on\nleft events predicts the observed future noticeably worse. We then regard\ndistilled events as the explanation for the future. To efficiently solve\n\\acrshort{ehd}, we rewrite the task into a \\gls{01ip} and directly estimate the\nsolution to the program by a model called \\acrfull{model}. This work fills the\ngap between our task and existing works, which only spot the difference between\nfactual and counterfactual worlds after applying a predefined modification to\nthe environment. Experiment results on Retweet and StackOverflow datasets prove\nthat \\acrshort{model} significantly outperforms other \\acrshort{ehd} baselines\nand can reveal the rationale underpinning real-world processes.\n","authors":["Sishun Liu","Ke Deng","Yan Wang","Xiuzhen Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07790v1","updated":"2023-11-13T22:55:56Z","published":"2023-11-13T22:55:56Z","title":"Leveraging Hamilton-Jacobi PDEs with time-dependent Hamiltonians for\n  continual scientific machine learning","summary":"  We address two major challenges in scientific machine learning (SciML):\ninterpretability and computational efficiency. We increase the interpretability\nof certain learning processes by establishing a new theoretical connection\nbetween optimization problems arising from SciML and a generalized Hopf\nformula, which represents the viscosity solution to a Hamilton-Jacobi partial\ndifferential equation (HJ PDE) with time-dependent Hamiltonian. Namely, we show\nthat when we solve certain regularized learning problems with integral-type\nlosses, we actually solve an optimal control problem and its associated HJ PDE\nwith time-dependent Hamiltonian. This connection allows us to reinterpret\nincremental updates to learned models as the evolution of an associated HJ PDE\nand optimal control problem in time, where all of the previous information is\nintrinsically encoded in the solution to the HJ PDE. As a result, existing HJ\nPDE solvers and optimal control algorithms can be reused to design new\nefficient training approaches for SciML that naturally coincide with the\ncontinual learning framework, while avoiding catastrophic forgetting. As a\nfirst exploration of this connection, we consider the special case of linear\nregression and leverage our connection to develop a new Riccati-based\nmethodology for solving these learning problems that is amenable to continual\nlearning applications. We also provide some corresponding numerical examples\nthat demonstrate the potential computational and memory advantages our\nRiccati-based approach can provide.\n","authors":["Paula Chen","Tingwei Meng","Zongren Zou","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2311.07790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07788v1","updated":"2023-11-13T22:46:43Z","published":"2023-11-13T22:46:43Z","title":"CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework\n  for Zero-Shot Electroencephalography Signal Conversion","summary":"  Electroencephalography (EEG) is a prominent non-invasive neuroimaging\ntechnique providing insights into brain function. Unfortunately, EEG data\nexhibit a high degree of noise and variability across subjects hampering\ngeneralizable signal extraction. Therefore, a key aim in EEG analysis is to\nextract the underlying neural activation (content) as well as to account for\nthe individual subject variability (style). We hypothesize that the ability to\nconvert EEG signals between tasks and subjects requires the extraction of\nlatent representations accounting for content and style. Inspired by recent\nadvancements in voice conversion technologies, we propose a novel contrastive\nsplit-latent permutation autoencoder (CSLP-AE) framework that directly\noptimizes for EEG conversion. Importantly, the latent representations are\nguided using contrastive learning to promote the latent splits to explicitly\nrepresent subject (style) and task (content). We contrast CSLP-AE to\nconventional supervised, unsupervised (AE), and self-supervised (contrastive\nlearning) training and find that the proposed approach provides favorable\ngeneralizable characterizations of subject and task. Importantly, the procedure\nalso enables zero-shot conversion between unseen subjects. While the present\nwork only considers conversion of EEG, the proposed CSLP-AE provides a general\nframework for signal conversion and extraction of content (task activation) and\nstyle (subject variability) components of general interest for the modeling and\nanalysis of biological signals.\n","authors":["Anders Vestergaard Nørskov","Alexander Neergaard Zahid","Morten Mørup"],"pdf_url":"https://arxiv.org/pdf/2311.07788v1.pdf","comment":"Accepted for publication at the 37th Conference on Neural Information\n  Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2311.07784v1","updated":"2023-11-13T22:21:27Z","published":"2023-11-13T22:21:27Z","title":"A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated\n  Class Incremental Learning for Vision Tasks","summary":"  Deep learning models often suffer from forgetting previously learned\ninformation when trained on new data. This problem is exacerbated in federated\nlearning (FL), where the data is distributed and can change independently for\neach user. Many solutions are proposed to resolve this catastrophic forgetting\nin a centralized setting. However, they do not apply directly to FL because of\nits unique complexities, such as privacy concerns and resource limitations. To\novercome these challenges, this paper presents a framework for\n\\textbf{federated class incremental learning} that utilizes a generative model\nto synthesize samples from past distributions. This data can be later exploited\nalongside the training data to mitigate catastrophic forgetting. To preserve\nprivacy, the generative model is trained on the server using data-free methods\nat the end of each task without requesting data from clients. Moreover, our\nsolution does not demand the users to store old data or models, which gives\nthem the freedom to join/leave the training at any time. Additionally, we\nintroduce SuperImageNet, a new regrouping of the ImageNet dataset specifically\ntailored for federated continual learning. We demonstrate significant\nimprovements compared to existing baselines through extensive experiments on\nmultiple datasets.\n","authors":["Sara Babakniya","Zalan Fabian","Chaoyang He","Mahdi Soltanolkotabi","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2311.07784v1.pdf","comment":"Accepted in NeurIPS 2023. arXiv admin note: text overlap with\n  arXiv:2307.00497"},{"id":"http://arxiv.org/abs/2306.14809v2","updated":"2023-11-13T21:56:13Z","published":"2023-06-26T16:11:11Z","title":"Tanimoto Random Features for Scalable Molecular Machine Learning","summary":"  The Tanimoto coefficient is commonly used to measure the similarity between\nmolecules represented as discrete fingerprints, either as a distance metric or\na positive definite kernel. While many kernel methods can be accelerated using\nrandom feature approximations, at present there is a lack of such\napproximations for the Tanimoto kernel. In this paper we propose two kinds of\nnovel random features to allow this kernel to scale to large datasets, and in\nthe process discover a novel extension of the kernel to real-valued vectors. We\ntheoretically characterize these random features, and provide error bounds on\nthe spectral norm of the Gram matrix. Experimentally, we show that these random\nfeatures are effective at approximating the Tanimoto coefficient of real-world\ndatasets and are useful for molecular property prediction and optimization\ntasks.\n","authors":["Austin Tripp","Sergio Bacallado","Sukriti Singh","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2306.14809v2.pdf","comment":"Camera-ready version presented at NeurIPS 2023. Updates include:\n  notation changes, better description of features in section 4, updated\n  experiments, link to code"},{"id":"http://arxiv.org/abs/2311.07772v1","updated":"2023-11-13T21:42:38Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n","authors":["Tomer Bar Nathan","Gilad Deutch","Nadav Magar","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08411v2","updated":"2023-11-13T21:36:13Z","published":"2022-09-17T21:40:02Z","title":"DynaConF: Dynamic Forecasting of Non-Stationary Time-Series","summary":"  Deep learning has shown impressive results in a variety of time series\nforecasting tasks, where modeling the conditional distribution of the future\ngiven the past is the essence. However, when this conditional distribution is\nnon-stationary, it poses challenges for these models to learn consistently and\nto predict accurately. In this work, we propose a new method to model\nnon-stationary conditional distributions over time by clearly decoupling\nstationary conditional distribution modeling from non-stationary dynamics\nmodeling. Our method is based on a Bayesian dynamic model that can adapt to\nconditional distribution changes and a deep conditional distribution model that\nhandles multivariate time series using a factorized output space. Our\nexperimental results on synthetic and real-world datasets show that our model\ncan adapt to non-stationary time series better than state-of-the-art deep\nlearning solutions.\n","authors":["Siqi Liu","Andreas Lehrmann"],"pdf_url":"https://arxiv.org/pdf/2209.08411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07766v1","updated":"2023-11-13T21:32:37Z","published":"2023-11-13T21:32:37Z","title":"Vision-Language Integration in Multimodal Video Transformers (Partially)\n  Aligns with the Brain","summary":"  Integrating information from multiple modalities is arguably one of the\nessential prerequisites for grounding artificial intelligence systems with an\nunderstanding of the real world. Recent advances in video transformers that\njointly learn from vision, text, and sound over time have made some progress\ntoward this goal, but the degree to which these models integrate information\nfrom modalities still remains unclear. In this work, we present a promising\napproach for probing a pre-trained multimodal video transformer model by\nleveraging neuroscientific evidence of multimodal information processing in the\nbrain. Using brain recordings of participants watching a popular TV show, we\nanalyze the effects of multi-modal connections and interactions in a\npre-trained multi-modal video transformer on the alignment with uni- and\nmulti-modal brain regions. We find evidence that vision enhances masked\nprediction performance during language processing, providing support that\ncross-modal representations in models can benefit individual modalities.\nHowever, we don't find evidence of brain-relevant information captured by the\njoint multi-modal transformer representations beyond that captured by all of\nthe individual modalities. We finally show that the brain alignment of the\npre-trained joint representation can be improved by fine-tuning using a task\nthat requires vision-language inferences. Overall, our results paint an\noptimistic picture of the ability of multi-modal transformers to integrate\nvision and language in partially brain-relevant ways but also show that\nimproving the brain alignment of these models may require new approaches.\n","authors":["Dota Tianai Dong","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2311.07766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07765v1","updated":"2023-11-13T21:31:07Z","published":"2023-11-13T21:31:07Z","title":"FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based\n  Human Activity Recognition","summary":"  Motion sensors integrated into wearable and mobile devices provide valuable\ninformation about the device users. Machine learning and, recently, deep\nlearning techniques have been used to characterize sensor data. Mostly, a\nsingle task, such as recognition of activities, is targeted, and the data is\nprocessed centrally at a server or in a cloud environment. However, the same\nsensor data can be utilized for multiple tasks and distributed machine-learning\ntechniques can be used without the requirement of the transmission of data to a\ncentre. This paper explores Federated Transfer Learning in a Multi-Task manner\nfor both sensor-based human activity recognition and device position\nidentification tasks. The OpenHAR framework is used to train the models, which\ncontains ten smaller datasets. The aim is to obtain model(s) applicable for\nboth tasks in different datasets, which may include only some label types.\nMultiple experiments are carried in the Flower federated learning environment\nusing the DeepConvLSTM architecture. Results are presented for federated and\ncentralized versions under different parameters and restrictions. By utilizing\ntransfer learning and training a task-specific and personalized federated\nmodel, we obtained a similar accuracy with training each client individually\nand higher accuracy than a fully centralized approach.\n","authors":["Egemen İşgüder","Özlem Durmaz İncel"],"pdf_url":"https://arxiv.org/pdf/2311.07765v1.pdf","comment":"Subimtted to Asian Conference in Machine Learning (ACML) 2023,\n  Pattern Recognition in Health Analysis Workshop, 7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.07763v1","updated":"2023-11-13T21:26:24Z","published":"2023-11-13T21:26:24Z","title":"The Disagreement Problem in Faithfulness Metrics","summary":"  The field of explainable artificial intelligence (XAI) aims to explain how\nblack-box machine learning models work. Much of the work centers around the\nholy grail of providing post-hoc feature attributions to any model\narchitecture. While the pace of innovation around novel methods has slowed\ndown, the question remains of how to choose a method, and how to make it fit\nfor purpose. Recently, efforts around benchmarking XAI methods have suggested\nmetrics for that purpose -- but there are many choices. That bounty of choice\nstill leaves an end user unclear on how to proceed. This paper focuses on\ncomparing metrics with the aim of measuring faithfulness of local explanations\non tabular classification problems -- and shows that the current metrics don't\nagree; leaving users unsure how to choose the most faithful explanations.\n","authors":["Brian Barr","Noah Fatsi","Leif Hancox-Li","Peter Richter","Daniel Proano","Caleb Mok"],"pdf_url":"https://arxiv.org/pdf/2311.07763v1.pdf","comment":"6 pages (excluding refs and appendix)"},{"id":"http://arxiv.org/abs/2106.06854v2","updated":"2023-11-13T21:19:45Z","published":"2021-06-12T20:21:38Z","title":"A Deep Reinforcement Learning Approach to Marginalized Importance\n  Sampling with the Successor Representation","summary":"  Marginalized importance sampling (MIS), which measures the density ratio\nbetween the state-action occupancy of a target policy and that of a sampling\ndistribution, is a promising approach for off-policy evaluation. However,\ncurrent state-of-the-art MIS methods rely on complex optimization tricks and\nsucceed mostly on simple toy problems. We bridge the gap between MIS and deep\nreinforcement learning by observing that the density ratio can be computed from\nthe successor representation of the target policy. The successor representation\ncan be trained through deep reinforcement learning methodology and decouples\nthe reward optimization from the dynamics of the environment, making the\nresulting algorithm stable and applicable to high-dimensional domains. We\nevaluate the empirical performance of our approach on a variety of challenging\nAtari and MuJoCo environments.\n","authors":["Scott Fujimoto","David Meger","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2106.06854v2.pdf","comment":"ICML 2021"},{"id":"http://arxiv.org/abs/2306.11797v2","updated":"2023-11-13T21:17:16Z","published":"2023-06-20T18:00:05Z","title":"Towards a robust and reliable deep learning approach for detection of\n  compact binary mergers in gravitational wave data","summary":"  The ability of deep learning (DL) approaches to learn generalised signal and\nnoise models, coupled with their fast inference on GPUs, holds great promise\nfor enhancing gravitational-wave (GW) searches in terms of speed, parameter\nspace coverage, and search sensitivity. However, the opaque nature of DL models\nseverely harms their reliability. In this work, we meticulously develop a DL\nmodel stage-wise and work towards improving its robustness and reliability.\nFirst, we address the problems in maintaining the purity of training data by\nderiving a new metric that better reflects the visual strength of the 'chirp'\nsignal features in the data. Using a reduced, smooth representation obtained\nthrough a variational auto-encoder (VAE), we build a classifier to search for\ncompact binary coalescence (CBC) signals. Our tests on real LIGO data show an\nimpressive performance of the model. However, upon probing the robustness of\nthe model through adversarial attacks, its simple failure modes were\nidentified, underlining how such models can still be highly fragile. As a first\nstep towards bringing robustness, we retrain the model in a novel framework\ninvolving a generative adversarial network (GAN). Over the course of training,\nthe model learns to eliminate the primary modes of failure identified by the\nadversaries. Although absolute robustness is practically impossible to achieve,\nwe demonstrate some fundamental improvements earned through such training, like\nsparseness and reduced degeneracy in the extracted features at different layers\ninside the model. We show that these gains are achieved at practically zero\nloss in terms of model performance on real LIGO data before and after GAN\ntraining. Through a direct search on 8.8 days of LIGO data, we recover two\nsignificant CBC events from GWTC-2.1, GW190519_153544 and GW190521_074359. We\nalso report the search sensitivity obtained from an injection study.\n","authors":["Shreejit Jadhav","Mihir Shrivastava","Sanjit Mitra"],"pdf_url":"https://arxiv.org/pdf/2306.11797v2.pdf","comment":"22 pages, 22 figures"},{"id":"http://arxiv.org/abs/2311.07750v1","updated":"2023-11-13T21:07:07Z","published":"2023-11-13T21:07:07Z","title":"SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models\n  for Multi-Label Chest X-Ray Classification","summary":"  Chest X-rays are widely used to diagnose thoracic diseases, but the lack of\ndetailed information about these abnormalities makes it challenging to develop\naccurate automated diagnosis systems, which is crucial for early detection and\neffective treatment. To address this challenge, we employed deep learning\ntechniques to identify patterns in chest X-rays that correspond to different\ndiseases. We conducted experiments on the \"ChestX-ray14\" dataset using various\npre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical\nmodels. The best individual model was the CoAtNet, which achieved an area under\nthe receiver operating characteristic curve (AUROC) of 84.2%. By combining the\npredictions of all trained models using a weighted average ensemble where the\nweight of each model was determined using differential evolution, we further\nimproved the AUROC to 85.4%, outperforming other state-of-the-art methods in\nthis field. Our findings demonstrate the potential of deep learning techniques,\nparticularly ensemble deep learning, for improving the accuracy of automatic\ndiagnosis of thoracic diseases from chest X-rays.\n","authors":["S. M. Nabil Ashraf","Md. Adyelullahil Mamun","Hasnat Md. Abdullah","Md. Golam Rabiul Alam"],"pdf_url":"https://arxiv.org/pdf/2311.07750v1.pdf","comment":"Accepted in International Conference on Computer and Information\n  Technology (ICCIT) 2023"},{"id":"http://arxiv.org/abs/2311.07744v1","updated":"2023-11-13T20:54:52Z","published":"2023-11-13T20:54:52Z","title":"Dynamic Local Attention with Hierarchical Patching for Irregular\n  Clinical Time Series","summary":"  Irregular multivariate time series data is prevalent in the clinical and\nhealthcare domains. It is characterized by time-wise and feature-wise\nirregularities, making it challenging for machine learning methods to work\nwith. To solve this, we introduce a new model architecture composed of two\nmodules: (1) DLA, a Dynamic Local Attention mechanism that uses learnable\nqueries and feature-specific local windows when computing the self-attention\noperation. This results in aggregating irregular time steps raw input within\neach window to a harmonized regular latent space representation while taking\ninto account the different features' sampling rates. (2) A hierarchical MLP\nmixer that processes the output of DLA through multi-scale patching to leverage\ninformation at various scales for the downstream tasks. Our approach\noutperforms state-of-the-art methods on three real-world datasets, including\nthe latest clinical MIMIC IV dataset.\n","authors":["Xingyu Chen","Xiaochen Zheng","Amina Mollaysa","Manuel Schürch","Ahmed Allam","Michael Krauthammer"],"pdf_url":"https://arxiv.org/pdf/2311.07744v1.pdf","comment":"Findings of Machine Learning for Health (ML4H) 2023"},{"id":"http://arxiv.org/abs/2310.16600v2","updated":"2023-11-13T20:22:03Z","published":"2023-10-25T12:45:49Z","title":"Balancing central and marginal rejection when combining independent\n  significance tests","summary":"  A common approach to evaluating the significance of a collection of\n$p$-values combines them with a pooling function, in particular when the\noriginal data are not available. These pooled $p$-values convert a sample of\n$p$-values into a single number which behaves like a univariate $p$-value. To\nclarify discussion of these functions, a telescoping series of alternative\nhypotheses are introduced that communicate the strength and prevalence of\nnon-null evidence in the $p$-values before general pooling formulae are\ndiscussed. A pattern noticed in the UMP pooled $p$-value for a particular\nalternative motivates the definition and discussion of central and marginal\nrejection levels at $\\alpha$. It is proven that central rejection is always\ngreater than or equal to marginal rejection, motivating a quotient to measure\nthe balance between the two for pooled $p$-values. A combining function based\non the $\\chi^2_{\\kappa}$ quantile transformation is proposed to control this\nquotient and shown to be robust to mis-specified parameters relative to the\nUMP. Different powers for different parameter settings motivate a map of\nplausible alternatives based on where this pooled $p$-value is minimized.\n","authors":["Chris Salahub","Wayne Oldford"],"pdf_url":"https://arxiv.org/pdf/2310.16600v2.pdf","comment":"55 page, 18 figures, public technical report"},{"id":"http://arxiv.org/abs/2311.07726v1","updated":"2023-11-13T20:10:53Z","published":"2023-11-13T20:10:53Z","title":"A Simple Quantum Blockmodeling with Qubits and Permutations","summary":"  Blockmodeling of a given problem represented by an $N\\times N$ adjacency\nmatrix can be found by swapping rows and columns of the matrix (i.e.\nmultiplying matrix from left and right by a permutation matrix). In general,\nthrough performing this task, row and column permutations affect the fitness\nvalue in optimization: For an $N\\times N$ matrix, it requires $O(N)$\ncomputations to find (or update) the fitness value of a candidate solution.\n  On quantum computers, permutations can be applied in parallel and\nefficiently, and their implementations can be as simple as a single qubit\noperation (a NOT gate on a qubit) which takes an $O(1)$ time algorithmic step.\nIn this paper, using permutation matrices, we describe a quantum blockmodeling\nfor data analysis tasks. In the model, the measurement outcome of a small group\nof qubits are mapped to indicate the fitness value. Therefore, we show that it\nis possible to find or update the fitness value in $O(log(N))$ time. This lead\nus to show that when the number of iterations are less than $log(N)$ time, it\nmay be possible to reach the same solution exponentially faster on quantum\ncomputers in comparison to classical computers. In addition, since on quantum\ncircuits the different sequence of permutations can be applied in parallel\n(superpositon), the machine learning task in this model can be implemented more\nefficiently on quantum computers.\n","authors":["Ammar Daskin"],"pdf_url":"https://arxiv.org/pdf/2311.07726v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2310.18784v2","updated":"2023-11-13T20:07:45Z","published":"2023-10-28T18:53:41Z","title":"High-probability Convergence Bounds for Nonlinear Stochastic Gradient\n  Descent Under Heavy-tailed Noise","summary":"  Several recent works have studied the convergence \\textit{in high\nprobability} of stochastic gradient descent (SGD) and its clipped variant.\nCompared to vanilla SGD, clipped SGD is practically more stable and has the\nadditional theoretical benefit of logarithmic dependence on the failure\nprobability. However, the convergence of other practical nonlinear variants of\nSGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved\ncommunication efficiency or accelerated convergence is much less understood. In\nthis work, we study the convergence bounds \\textit{in high probability} of a\nbroad class of nonlinear SGD methods. For strongly convex loss functions with\nLipschitz continuous gradients, we prove a logarithmic dependence on the\nfailure probability, even when the noise is heavy-tailed. Strictly more general\nthan the results for clipped SGD, our results hold for any nonlinearity with\nbounded (component-wise or joint) outputs, such as clipping, normalization, and\nquantization. Further, existing results with heavy-tailed noise assume bounded\n$\\eta$-th central moments, with $\\eta \\in (1,2]$. In contrast, our refined\nanalysis works even for $\\eta=1$, strictly relaxing the noise moment\nassumptions in the literature.\n","authors":["Aleksandar Armacki","Pranay Sharma","Gauri Joshi","Dragana Bajovic","Dusan Jakovetic","Soummya Kar"],"pdf_url":"https://arxiv.org/pdf/2310.18784v2.pdf","comment":"27 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.07723v1","updated":"2023-11-13T20:07:36Z","published":"2023-11-13T20:07:36Z","title":"Generalization Analogies (GENIES): A Testbed for Generalizing AI\n  Oversight to Hard-To-Measure Domains","summary":"  As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENaralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n","authors":["Joshua Clymer","Garrett Baker","Rohan Subramani","Sam Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07723v1.pdf","comment":"Code: https://github.com/Joshuaclymer/GENIES Website:\n  https://joshuaclymer.github.io/generalization-analogies-website/"},{"id":"http://arxiv.org/abs/2311.07715v1","updated":"2023-11-13T19:56:18Z","published":"2023-11-13T19:56:18Z","title":"PolyIE: A Dataset of Information Extraction from Polymer Material\n  Scientific Literature","summary":"  Scientific information extraction (SciIE), which aims to automatically\nextract information from scientific literature, is becoming more important than\never. However, there are no existing SciIE datasets for polymer materials,\nwhich is an important class of materials used ubiquitously in our daily lives.\nTo bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer\nmaterials. POLYIE is curated from 146 full-length polymer scholarly articles,\nwhich are annotated with different named entities (i.e., materials, properties,\nvalues, conditions) as well as their N-ary relations by domain experts. POLYIE\npresents several unique challenges due to diverse lexical formats of entities,\nambiguity between entities, and variable-length relations. We evaluate\nstate-of-the-art named entity extraction and relation extraction models on\nPOLYIE, analyze their strengths and weaknesses, and highlight some difficult\ncases for these models. To the best of our knowledge, POLYIE is the first SciIE\nbenchmark for polymer materials, and we hope it will lead to more research\nefforts from the community on this challenging task. Our code and data are\navailable on: https://github.com/jerry3027/PolyIE.\n","authors":["Jerry Junyang Cheung","Yuchen Zhuang","Yinghao Li","Pranav Shetty","Wantian Zhao","Sanjeev Grampurohit","Rampi Ramprasad","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07715v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2302.03683v2","updated":"2023-11-13T19:52:04Z","published":"2023-02-07T18:58:25Z","title":"Linear Partial Monitoring for Sequential Decision-Making: Algorithms,\n  Regret Bounds and Applications","summary":"  Partial monitoring is an expressive framework for sequential decision-making\nwith an abundance of applications, including graph-structured and dueling\nbandits, dynamic pricing and transductive feedback models. We survey and extend\nrecent results on the linear formulation of partial monitoring that naturally\ngeneralizes the standard linear bandit setting. The main result is that a\nsingle algorithm, information-directed sampling (IDS), is (nearly) worst-case\nrate optimal in all finite-action games. We present a simple and unified\nanalysis of stochastic partial monitoring, and further extend the model to the\ncontextual and kernelized setting.\n","authors":["Johannes Kirschner","Tor Lattimore","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2302.03683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07708v1","updated":"2023-11-13T19:46:22Z","published":"2023-11-13T19:46:22Z","title":"Reinforcement Learning for Solving Stochastic Vehicle Routing Problem","summary":"  This study addresses a gap in the utilization of Reinforcement Learning (RL)\nand Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing\nProblem (SVRP) that involves the challenging task of optimizing vehicle routes\nunder uncertain conditions. We propose a novel end-to-end framework that\ncomprehensively addresses the key sources of stochasticity in SVRP and utilizes\nan RL agent with a simple yet effective architecture and a tailored training\nmethod. Through comparative analysis, our proposed model demonstrates superior\nperformance compared to a widely adopted state-of-the-art metaheuristic,\nachieving a significant 3.43% reduction in travel costs. Furthermore, the model\nexhibits robustness across diverse SVRP settings, highlighting its adaptability\nand ability to learn optimal routing strategies in varying environments. The\npublicly available implementation of our framework serves as a valuable\nresource for future research endeavors aimed at advancing RL-based solutions\nfor SVRP.\n","authors":["Zangir Iklassov","Ikboljon Sobirov","Ruben Solozabal","Martin Takac"],"pdf_url":"https://arxiv.org/pdf/2311.07708v1.pdf","comment":"14 pages, accepted to ACML24"}],"Multimedia":[{"id":"http://arxiv.org/abs/2311.07547v1","updated":"2023-11-13T18:36:50Z","published":"2023-11-13T18:36:50Z","title":"GPT-4V(ision) as A Social Media Analysis Engine","summary":"  Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.\n","authors":["Hanjia Lyu","Jinfa Huang","Daoan Zhang","Yongsheng Yu","Xinyi Mou","Jinsheng Pan","Zhengyuan Yang","Zhongyu Wei","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2311.07547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07399v1","updated":"2023-11-13T15:27:23Z","published":"2023-11-13T15:27:23Z","title":"Context-Aware Adaptive Prefetching for DASH Streaming over 5G Networks","summary":"  The increasing consumption of video streams and the demand for higher-quality\ncontent drive the evolution of telecommunication networks and the development\nof new network accelerators to boost media delivery while optimizing network\nusage. Multi-access Edge Computing (MEC) enables the possibility to enforce\nmedia delivery by deploying caching instances at the network edge, close to the\nRadio Access Network (RAN). Thus, the content can be prefetched and served from\nthe MEC host, reducing network traffic and increasing the Quality of Service\n(QoS) and the Quality of Experience (QoE). This paper proposes a novel\nmechanism to prefetch Dynamic Adaptive Streaming over HTTP (DASH) streams at\nthe MEC, employing a Machine Learning (ML) classification model to select the\nmedia segments to prefetch. The model is trained with media session metrics to\nimprove the forecasts with application layer information. The proposal is\ntested with Mobile Network Operators (MNOs)' 5G MEC and RAN and compared with\nother strategies by assessing cache and player's performance metrics.\n","authors":["Juncal Uriol","Inhar Yeregui","Alvaro Gabilondo","Roberto Viola","Pablo Angueira","Jon Montalban"],"pdf_url":"https://arxiv.org/pdf/2311.07399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07391v1","updated":"2023-11-13T15:15:05Z","published":"2023-11-13T15:15:05Z","title":"Multi-Layer Monitoring at the Edge for Vehicular Video Streaming: Field\n  Trials","summary":"  In an increasingly connected world, wireless networks' monitoring and\ncharacterization are of vital importance. Service and application providers\nneed to have a detailed understanding of network performance to offer new\nsolutions tailored to the needs of today's society. In the context of mobility,\nin-vehicle infotainment services are expected to stand out among other popular\nconnected vehicle services, so it is essential that communication networks are\nable to satisfy the Quality of Service (QoS) and Quality of Experience (QoE)\nrequirements needed for these type of services. This paper investigates a\nmulti-layer network performance monitoring architecture at the edge providing\nQoS, QoE, and localization information for vehicular video streaming\napplications in real-time over 5G networks. In order to conduct field trials\nand show test results, Mobile Network Operators (MNOs)' 5G Standalone (SA)\nnetwork and Multi-access Edge Computing (MEC) infrastructure are used to\nprovide connectivity and edge computing resources to a vehicle equipped with a\n5G modem.\n","authors":["Inhar Yeregui","Juncal Uriol","Roberto Viola","Pablo Angueira","Jasone Astorga","Jon Montalban"],"pdf_url":"https://arxiv.org/pdf/2311.07391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10065v2","updated":"2023-11-13T08:44:28Z","published":"2023-06-15T03:49:24Z","title":"Taming Diffusion Models for Music-driven Conducting Motion Generation","summary":"  Generating the motion of orchestral conductors from a given piece of symphony\nmusic is a challenging task since it requires a model to learn semantic music\nfeatures and capture the underlying distribution of real conducting motion.\nPrior works have applied Generative Adversarial Networks (GAN) to this task,\nbut the promising diffusion model, which recently showed its advantages in\nterms of both training stability and output quality, has not been exploited in\nthis context. This paper presents Diffusion-Conductor, a novel DDIM-based\napproach for music-driven conducting motion generation, which integrates the\ndiffusion model to a two-stage learning framework. We further propose a random\nmasking strategy to improve the feature robustness, and use a pair of geometric\nloss functions to impose additional regularizations and increase motion\ndiversity. We also design several novel metrics, including Frechet Gesture\nDistance (FGD) and Beat Consistency Score (BC) for a more comprehensive\nevaluation of the generated motion. Experimental results demonstrate the\nadvantages of our model.\n","authors":["Zhuoran Zhao","Jinbin Bai","Delong Chen","Debang Wang","Yubo Pan"],"pdf_url":"https://arxiv.org/pdf/2306.10065v2.pdf","comment":"Accepted by AAAI 2023 Summer Symposium with Best Paper Award"},{"id":"http://arxiv.org/abs/2305.13583v4","updated":"2023-11-13T00:09:47Z","published":"2023-05-23T01:24:15Z","title":"Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical\n  Fusion for Multimodal Affect Recognition","summary":"  Fusing multiple modalities has proven effective for multimodal information\nprocessing. However, the incongruity between modalities poses a challenge for\nmultimodal fusion, especially in affect recognition. In this study, we first\nanalyze how the salient affective information in one modality can be affected\nby the other, and demonstrate that inter-modal incongruity exists latently in\ncrossmodal attention. Based on this finding, we propose the Hierarchical\nCrossmodal Transformer with Dynamic Modality Gating (HCT-DMG), a lightweight\nincongruity-aware model, which dynamically chooses the primary modality in each\ntraining batch and reduces fusion times by leveraging the learned hierarchy in\nthe latent space to alleviate incongruity. The experimental evaluation on five\nbenchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP (sentiment and emotion),\nwhere incongruity implicitly lies in hard samples, as well as UR-FUNNY (humour)\nand MUStaRD (sarcasm), where incongruity is common, verifies the efficacy of\nour approach, showing that HCT-DMG: 1) outperforms previous multimodal models\nwith a reduced size of approximately 0.8M parameters; 2) recognizes hard\nsamples where incongruity makes affect recognition difficult; 3) mitigates the\nincongruity at the latent level in crossmodal attention.\n","authors":["Yaoting Wang","Yuanchao Li","Paul Pu Liang","Louis-Philippe Morency","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2305.13583v4.pdf","comment":"*First two authors contributed equally"},{"id":"http://arxiv.org/abs/2309.07376v2","updated":"2023-11-13T22:03:01Z","published":"2023-09-14T01:39:40Z","title":"VCD: A Video Conferencing Dataset for Video Compression","summary":"  Commonly used datasets for evaluating video codecs are all very high quality\nand not representative of video typically used in video conferencing scenarios.\nWe present the Video Conferencing Dataset (VCD) for evaluating video codecs for\nreal-time communication, the first such dataset focused on video conferencing.\nVCD includes a wide variety of camera qualities and spatial and temporal\ninformation. It includes both desktop and mobile scenarios and two types of\nvideo background processing. We report the compression efficiency of H.264,\nH.265, H.266, and AV1 in low-delay settings on VCD and compare it with the\nnon-video conferencing datasets UVC, MLC-JVC, and HEVC. The results show the\nsource quality and the scenarios have a significant effect on the compression\nefficiency of all the codecs. VCD enables the evaluation and tuning of codecs\nfor this important scenario. The VCD is publicly available as an open-source\ndataset at https://github.com/microsoft/VCD.\n","authors":["Babak Naderi","Ross Cutler","Nabakumar Singh Khongbantabam","Yasaman Hosseinkashi","Henrik Turbell","Albert Sadovnikov","Quan Zhou"],"pdf_url":"https://arxiv.org/pdf/2309.07376v2.pdf","comment":null}]},"2023-11-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.06985v1","updated":"2023-11-12T23:14:43Z","published":"2023-11-12T23:14:43Z","title":"SELF-EXPLAIN: Teaching Large Language Models to Reason Complex Questions\n  by Themselves","summary":"  Large language models (LLMs) can generate intermediate reasoning steps. To\nelicit the reliable reasoning, the common practice is to employ few-shot\nchain-of-thought prompting, where several in-context demonstrations for\nreasoning are prepended to the question. However, such chain-of-thought\nexamples are expensive to craft, especially for professional domains, and can\nhave high variance depending on human annotators. Therefore, this work\ninvestigates whether LLMs can teach themselves to reason without human-crafted\ndemonstrations. We propose SELF-EXPLAIN to generate CoT examples by LLMs\ninspired by \"encoding specificity\" in human memory retrieval. We find using\nself-explanations makes LLMs more confident, more calibrated and less biased\nwhen answering complex questions. Moreover, we find prompting with\nself-explanations can even significantly outperform using human-crafted CoTs on\nseveral complex question answering dataset.\n","authors":["Jiachen Zhao","Zonghai Yao","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.06985v1.pdf","comment":"Workshop on robustness of zero/few-shot learning in foundation models\n  @ NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.06899v1","updated":"2023-11-12T17:18:21Z","published":"2023-11-12T17:18:21Z","title":"Flames: Benchmarking Value Alignment of Chinese Large Language Models","summary":"  The widespread adoption of large language models (LLMs) across various\nregions underscores the urgent need to evaluate their alignment with human\nvalues. Current benchmarks, however, fall short of effectively uncovering\nsafety vulnerabilities in LLMs. Despite numerous models achieving high scores\nand 'topping the chart' in these evaluations, there is still a significant gap\nin LLMs' deeper alignment with human values and achieving genuine harmlessness.\nTo this end, this paper proposes the first highly adversarial benchmark named\nFlames, consisting of 2,251 manually crafted prompts, ~18.7K model responses\nwith fine-grained annotations, and a specified scorer. Our framework\nencompasses both common harmlessness principles, such as fairness, safety,\nlegality, and data protection, and a unique morality dimension that integrates\nspecific Chinese values such as harmony. Based on the framework, we carefully\ndesign adversarial prompts that incorporate complex scenarios and jailbreaking\nmethods, mostly with implicit malice. By prompting mainstream LLMs with such\nadversarially constructed prompts, we obtain model responses, which are then\nrigorously annotated for evaluation. Our findings indicate that all the\nevaluated LLMs demonstrate relatively poor performance on Flames, particularly\nin the safety and fairness dimensions. Claude emerges as the best-performing\nmodel overall, but with its harmless rate being only 63.08% while GPT-4 only\nscores 39.04%. The complexity of Flames has far exceeded existing benchmarks,\nsetting a new challenge for contemporary LLMs and highlighting the need for\nfurther alignment of LLMs. To efficiently evaluate new models on the benchmark,\nwe develop a specified scorer capable of scoring LLMs across multiple\ndimensions, achieving an accuracy of 77.4%. The Flames Benchmark is publicly\navailable on https://github.com/AIFlames/Flames.\n","authors":["Kexin Huang","Xiangyang Liu","Qianyu Guo","Tianxiang Sun","Jiawei Sun","Yaru Wang","Zeyang Zhou","Yixu Wang","Yan Teng","Xipeng Qiu","Yingchun Wang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2311.06899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06898v1","updated":"2023-11-12T17:16:46Z","published":"2023-11-12T17:16:46Z","title":"Retrieval and Generative Approaches for a Pregnancy Chatbot in Nepali\n  with Stemmed and Non-Stemmed Data : A Comparative Study","summary":"  The field of Natural Language Processing which involves the use of artificial\nintelligence to support human languages has seen tremendous growth due to its\nhigh-quality features. Its applications such as language translation, chatbots,\nvirtual assistants, search autocomplete, and autocorrect are widely used in\nvarious domains including healthcare, advertising, customer service, and target\nadvertising. To provide pregnancy-related information a health domain chatbot\nhas been proposed and this work explores two different NLP-based approaches for\ndeveloping the chatbot. The first approach is a multiclass classification-based\nretrieval approach using BERTbased multilingual BERT and multilingual\nDistilBERT while the other approach employs a transformer-based generative\nchatbot for pregnancy-related information. The performance of both stemmed and\nnon-stemmed datasets in Nepali language has been analyzed for each approach.\nThe experimented results indicate that BERT-based pre-trained models perform\nwell on non-stemmed data whereas scratch transformer models have better\nperformance on stemmed data. Among the models tested the DistilBERT model\nachieved the highest training and validation accuracy and testing accuracy of\n0.9165 on the retrieval-based model architecture implementation on the\nnon-stemmed dataset. Similarly, in the generative approach architecture\nimplementation with transformer 1 gram BLEU and 2 gram BLEU scores of 0.3570\nand 0.1413 respectively were achieved.\n","authors":["Sujan Poudel","Nabin Ghimire","Bipesh Subedi","Saugat Singh"],"pdf_url":"https://arxiv.org/pdf/2311.06898v1.pdf","comment":"7 pages, 5 figures, 4 tables. In proceedings of the International\n  Conference on Technologies for Computer, Electrical, Electronics &\n  Communication (ICT-CEEL 2023), Bhaktapur, Nepal"},{"id":"http://arxiv.org/abs/2310.04313v2","updated":"2023-11-12T17:10:32Z","published":"2023-10-06T15:19:39Z","title":"KoMultiText: Large-Scale Korean Text Dataset for Classifying Biased\n  Speech in Real-World Online Services","summary":"  With the growth of online services, the need for advanced text classification\nalgorithms, such as sentiment analysis and biased text detection, has become\nincreasingly evident. The anonymous nature of online services often leads to\nthe presence of biased and harmful language, posing challenges to maintaining\nthe health of online communities. This phenomenon is especially relevant in\nSouth Korea, where large-scale hate speech detection algorithms have not yet\nbeen broadly explored. In this paper, we introduce \"KoMultiText\", a new\ncomprehensive, large-scale dataset collected from a well-known South Korean SNS\nplatform. Our proposed dataset provides annotations including (1) Preferences,\n(2) Profanities, and (3) Nine types of Bias for the text samples, enabling\nmulti-task learning for simultaneous classification of user-generated texts.\nLeveraging state-of-the-art BERT-based language models, our approach surpasses\nhuman-level accuracy across diverse classification tasks, as measured by\nvarious metrics. Beyond academic contributions, our work can provide practical\nsolutions for real-world hate speech and bias mitigation, contributing directly\nto the improvement of online community health. Our work provides a robust\nfoundation for future research aiming to improve the quality of online\ndiscourse and foster societal well-being. All source codes and datasets are\npublicly accessible at https://github.com/Dasol-Choi/KoMultiText.\n","authors":["Dasol Choi","Jooyoung Song","Eunsun Lee","Jinwoo Seo","Heejune Park","Dongbin Na"],"pdf_url":"https://arxiv.org/pdf/2310.04313v2.pdf","comment":"Accepted to the NeurIPS 2023 Workshop on Socially Responsible\n  Language Modelling Research (SoLaR)"},{"id":"http://arxiv.org/abs/2311.04913v2","updated":"2023-11-12T16:32:16Z","published":"2023-11-01T18:41:50Z","title":"An Improved Transformer-based Model for Detecting Phishing, Spam, and\n  Ham: A Large Language Model Approach","summary":"  Phishing and spam detection is long standing challenge that has been the\nsubject of much academic research. Large Language Models (LLM) have vast\npotential to transform society and provide new and innovative approaches to\nsolve well-established challenges. Phishing and spam have caused financial\nhardships and lost time and resources to email users all over the world and\nfrequently serve as an entry point for ransomware threat actors. While\ndetection approaches exist, especially heuristic-based approaches, LLMs offer\nthe potential to venture into a new unexplored area for understanding and\nsolving this challenge. LLMs have rapidly altered the landscape from business,\nconsumers, and throughout academia and demonstrate transformational potential\nfor the potential of society. Based on this, applying these new and innovative\napproaches to email detection is a rational next step in academic research. In\nthis work, we present IPSDM, our model based on fine-tuning the BERT family of\nmodels to specifically detect phishing and spam email. We demonstrate our\nfine-tuned version, IPSDM, is able to better classify emails in both unbalanced\nand balanced datasets. This work serves as an important first step towards\nemploying LLMs to improve the security of our information systems.\n","authors":["Suhaima Jamal","Hayden Wimmer"],"pdf_url":"https://arxiv.org/pdf/2311.04913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17688v2","updated":"2023-11-12T14:33:20Z","published":"2023-10-26T17:59:06Z","title":"Managing AI Risks in an Era of Rapid Progress","summary":"  In this short consensus paper, we outline risks from upcoming, advanced AI\nsystems. We examine large-scale social harms and malicious uses, as well as an\nirreversible loss of human control over autonomous AI systems. In light of\nrapid and continuing AI progress, we propose urgent priorities for AI R&D and\ngovernance.\n","authors":["Yoshua Bengio","Geoffrey Hinton","Andrew Yao","Dawn Song","Pieter Abbeel","Yuval Noah Harari","Ya-Qin Zhang","Lan Xue","Shai Shalev-Shwartz","Gillian Hadfield","Jeff Clune","Tegan Maharaj","Frank Hutter","Atılım Güneş Baydin","Sheila McIlraith","Qiqi Gao","Ashwin Acharya","David Krueger","Anca Dragan","Philip Torr","Stuart Russell","Daniel Kahneman","Jan Brauner","Sören Mindermann"],"pdf_url":"https://arxiv.org/pdf/2310.17688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06858v1","updated":"2023-11-12T14:20:55Z","published":"2023-11-12T14:20:55Z","title":"Can Large Language Models Augment a Biomedical Ontology with missing\n  Concepts and Relations?","summary":"  Ontologies play a crucial role in organizing and representing knowledge.\nHowever, even current ontologies do not encompass all relevant concepts and\nrelationships. Here, we explore the potential of large language models (LLM) to\nexpand an existing ontology in a semi-automated fashion. We demonstrate our\napproach on the biomedical ontology SNOMED-CT utilizing semantic relation types\nfrom the widely used UMLS semantic network. We propose a method that uses\nconversational interactions with an LLM to analyze clinical practice guidelines\n(CPGs) and detect the relationships among the new medical concepts that are not\npresent in SNOMED-CT. Our initial experimentation with the conversational\nprompts yielded promising preliminary results given a manually generated gold\nstandard, directing our future potential improvements.\n","authors":["Antonio Zaitoun","Tomer Sagi","Szymon Wilk","Mor Peleg"],"pdf_url":"https://arxiv.org/pdf/2311.06858v1.pdf","comment":"Presented as a short paper at the Knowledge Representation for\n  Healthcare 2023 workshop"},{"id":"http://arxiv.org/abs/2311.06855v1","updated":"2023-11-12T14:12:19Z","published":"2023-11-12T14:12:19Z","title":"DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial\n  Training","summary":"  This paper focuses on the DialFRED task, which is the task of embodied\ninstruction following in a setting where an agent can actively ask questions\nabout the task. To address this task, we propose DialMAT. DialMAT introduces\nMoment-based Adversarial Training, which incorporates adversarial perturbations\ninto the latent space of language, image, and action. Additionally, it\nintroduces a crossmodal parallel feature extraction mechanism that applies\nfoundation models to both language and image. We evaluated our model using a\ndataset constructed from the DialFRED dataset and demonstrated superior\nperformance compared to the baseline method in terms of success rate and path\nweighted success rate. The model secured the top position in the DialFRED\nChallenge, which took place at the CVPR 2023 Embodied AI workshop.\n","authors":["Kanta Kaneda","Ryosuke Korekata","Yuiga Wada","Shunya Nagashima","Motonari Kambara","Yui Iioka","Haruka Matsuo","Yuto Imai","Takayuki Nishimura","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2311.06855v1.pdf","comment":"Accepted for presentation at Fourth Annual Embodied AI Workshop at\n  CVPR"},{"id":"http://arxiv.org/abs/2311.06851v1","updated":"2023-11-12T14:01:38Z","published":"2023-11-12T14:01:38Z","title":"Automatic Textual Normalization for Hate Speech Detection","summary":"  Social media data is a valuable resource for research, yet it contains a wide\nrange of non-standard words (NSW). These irregularities hinder the effective\noperation of NLP tools. Current state-of-the-art methods for the Vietnamese\nlanguage address this issue as a problem of lexical normalization, involving\nthe creation of manual rules or the implementation of multi-staged deep\nlearning frameworks, which necessitate extensive efforts to craft intricate\nrules. In contrast, our approach is straightforward, employing solely a\nsequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset\nfor textual normalization, comprising 2,181 human-annotated comments with an\ninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for\ntextual normalization, our results reveal that the accuracy achieved falls\nslightly short of 70%. Nevertheless, textual normalization enhances the\naccuracy of the Hate Speech Detection (HSD) task by approximately 2%,\ndemonstrating its potential to improve the performance of complex NLP tasks.\nOur dataset is accessible for research purposes.\n","authors":["Anh Thi-Hoang Nguyen","Dung Ha Nguyen","Nguyet Thi Nguyen","Khanh Thanh-Duy Ho","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.06851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06838v1","updated":"2023-11-12T13:30:38Z","published":"2023-11-12T13:30:38Z","title":"GIELLM: Japanese General Information Extraction Large Language Model\n  Utilizing Mutual Reinforcement Effect","summary":"  Information Extraction (IE) stands as a cornerstone in natural language\nprocessing, traditionally segmented into distinct sub-tasks. The advent of\nLarge Language Models (LLMs) heralds a paradigm shift, suggesting the\nfeasibility of a singular model addressing multiple IE subtasks. In this vein,\nwe introduce the General Information Extraction Large Language Model (GIELLM),\nwhich integrates text Classification, Sentiment Analysis, Named Entity\nRecognition, Relation Extraction, and Event Extraction using a uniform\ninput-output schema. This innovation marks the first instance of a model\nsimultaneously handling such a diverse array of IE subtasks. Notably, the\nGIELLM leverages the Mutual Reinforcement Effect (MRE), enhancing performance\nin integrated tasks compared to their isolated counterparts. Our experiments\ndemonstrate State-of-the-Art (SOTA) results in five out of six Japanese mixed\ndatasets, significantly surpassing GPT-3.5-Turbo. Further, an independent\nevaluation using the novel Text Classification Relation and Event\nExtraction(TCREE) dataset corroborates the synergistic advantages of MRE in\ntext and word classification. This breakthrough paves the way for most IE\nsubtasks to be subsumed under a singular LLM framework. Specialized fine-tune\ntask-specific models are no longer needed.\n","authors":["Chengguang Gan","Qinghao Zhang","Tatsunori Mori"],"pdf_url":"https://arxiv.org/pdf/2311.06838v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.09017v2","updated":"2023-11-12T13:20:05Z","published":"2023-10-13T11:28:02Z","title":"Dont Add, dont Miss: Effective Content Preserving Generation from\n  Pre-Selected Text Spans","summary":"  The recently introduced Controlled Text Reduction (CTR) task isolates the\ntext generation step within typical summarization-style tasks. It does so by\nchallenging models to generate coherent text conforming to pre-selected content\nwithin the input text (``highlights''). This framing enables increased\nmodularity in summarization-like tasks, allowing to couple a single CTR model\nwith various content-selection setups and modules. However, there are currently\nno reliable CTR models, while the performance of the existing baseline for the\ntask is mediocre, falling short of practical utility. Here, we address this gap\nby introducing a high-quality, open-source CTR model that tackles two prior key\nlimitations: inadequate enforcement of the content-preservation constraint, and\nsuboptimal silver training data. Addressing these, we amplify the\ncontent-preservation constraint in both training, via RL, and inference, via a\ncontrolled decoding strategy. Further, we substantially improve the silver\ntraining data quality via GPT-4 distillation. Overall, pairing the distilled\ndataset with the highlight-adherence strategies yields marked gains over the\ncurrent baseline, of up to 30 ROUGE-L points, providing a reliable CTR model\nfor downstream use.\n","authors":["Aviv Slobodkin","Avi Caciularu","Eran Hirsch","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2310.09017v2.pdf","comment":"EMNLP 2023, findings"},{"id":"http://arxiv.org/abs/2310.11877v2","updated":"2023-11-12T13:14:36Z","published":"2023-10-18T11:01:09Z","title":"The Curious Case of Hallucinatory (Un)answerability: Finding Truths in\n  the Hidden States of Over-Confident Large Language Models","summary":"  Large language models (LLMs) have been shown to possess impressive\ncapabilities, while also raising crucial concerns about the faithfulness of\ntheir responses. A primary issue arising in this context is the management of\n(un)answerable queries by LLMs, which often results in hallucinatory behavior\ndue to overconfidence. In this paper, we explore the behavior of LLMs when\npresented with (un)answerable queries. We ask: do models represent the fact\nthat the question is (un)answerable when generating a hallucinatory answer? Our\nresults show strong indications that such models encode the answerability of an\ninput query, with the representation of the first decoded token often being a\nstrong indicator. These findings shed new light on the spatial organization\nwithin the latent representations of LLMs, unveiling previously unexplored\nfacets of these models. Moreover, they pave the way for the development of\nimproved decoding techniques with better adherence to factual generation,\nparticularly in scenarios where query (un)answerability is a concern.\n","authors":["Aviv Slobodkin","Omer Goldman","Avi Caciularu","Ido Dagan","Shauli Ravfogel"],"pdf_url":"https://arxiv.org/pdf/2310.11877v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2303.08553v2","updated":"2023-11-12T12:52:31Z","published":"2023-03-15T12:13:30Z","title":"The Image of the Process Interpretation of Regular Expressions is Not\n  Closed under Bisimulation Collapse","summary":"  Axiomatization and expressibility problems for Milner's process semantics\n(1984) of regular expressions modulo bisimilarity have turned out to be\ndifficult for the full class of expressions with deadlock 0 and empty step~1.\nWe report on a phenomenon that arises from the added presence of 1 when 0 is\navailable, and that brings a crucial reason for this difficulty into focus. To\nwit, while interpretations of 1-free regular expressions are closed under\nbisimulation collapse, this is not the case for the interpretations of\narbitrary regular expressions.\n  Process graph interpretations of 1-free regular expressions satisfy the loop\nexistence and elimination property LEE, which is preserved under bisimulation\ncollapse. These features of LEE were applied for showing that an equational\nproof system for 1-free regular expressions modulo bisimilarity is complete,\nand that it is decidable in polynomial time whether a process graph is\nbisimilar to the interpretation of a 1-free regular expression.\n  While interpretations of regular expressions do not satisfy the property LEE\nin general, we show that LEE can be recovered by refined interpretations as\ngraphs with 1-transitions refined interpretations with 1-transitions (which are\nsimilar to silent steps for automata). This suggests that LEE can be expedient\nalso for the general axiomatization and expressibility problems. But a new\nphenomenon emerges that needs to be addressed: the property of a process graph\n`to can be refined into a process graph with 1-transitions and with LEE' is not\npreserved under bisimulation collapse. We provide a 10-vertex graph with two\n1-transitions that satisfies LEE, and in which a pair of bisimilar vertices\ncannot be collapsed on to each other while preserving the refinement property.\nThis implies that the image of the process interpretation of regular\nexpressions is not closed under bisimulation collapse.\n","authors":["Clemens Grabmayer"],"pdf_url":"https://arxiv.org/pdf/2303.08553v2.pdf","comment":"Report (14 p. + 10 p. app) written for a submission in Jan 2021 (now\n  with added explanation of relation with subsequent work that was published\n  earlier) concerning the crucial observation underlying the crystallization\n  process in arXiv:2209.12188 version 2: extension of Prop. 2.12 to \"under star\n  1-free\" expressions, and correction in its proof (added termination subterm\n  to extraction function)"},{"id":"http://arxiv.org/abs/2307.11760v7","updated":"2023-11-12T12:35:41Z","published":"2023-07-14T00:57:12Z","title":"Large Language Models Understand and Can be Enhanced by Emotional\n  Stimuli","summary":"  Emotional intelligence significantly impacts our daily behaviors and\ninteractions. Although Large Language Models (LLMs) are increasingly viewed as\na stride toward artificial general intelligence, exhibiting impressive\nperformance in numerous tasks, it is still uncertain if LLMs can genuinely\ngrasp psychological emotional stimuli. Understanding and responding to\nemotional cues gives humans a distinct advantage in problem-solving. In this\npaper, we take the first step towards exploring the ability of LLMs to\nunderstand emotional stimuli. To this end, we first conduct automatic\nexperiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,\nLlama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative\napplications that represent comprehensive evaluation scenarios. Our automatic\nexperiments show that LLMs have a grasp of emotional intelligence, and their\nperformance can be improved with emotional prompts (which we call\n\"EmotionPrompt\" that combines the original prompt with emotional stimuli),\ne.g., 8.00% relative performance improvement in Instruction Induction and 115%\nin BIG-Bench. In addition to those deterministic tasks that can be\nautomatically evaluated using existing metrics, we conducted a human study with\n106 participants to assess the quality of generative tasks using both vanilla\nand emotional prompts. Our human study results demonstrate that EmotionPrompt\nsignificantly boosts the performance of generative tasks (10.9% average\nimprovement in terms of performance, truthfulness, and responsibility metrics).\nWe provide an in-depth discussion regarding why EmotionPrompt works for LLMs\nand the factors that may influence its performance. We posit that EmotionPrompt\nheralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs\ninteraction.\n","authors":["Cheng Li","Jindong Wang","Yixuan Zhang","Kaijie Zhu","Wenxin Hou","Jianxun Lian","Fang Luo","Qiang Yang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2307.11760v7.pdf","comment":"Technical report; updated the std error for human study; short\n  version (v1) was accepted by LLM@IJCAI'23; 32 pages; more work:\n  https://llm-enhance.github.io/"},{"id":"http://arxiv.org/abs/2311.06818v1","updated":"2023-11-12T11:51:05Z","published":"2023-11-12T11:51:05Z","title":"Cricket Player Profiling: Unraveling Strengths and Weaknesses Using Text\n  Commentary Data","summary":"  Devising player-specific strategies in cricket necessitates a meticulous\nunderstanding of each player's unique strengths and weaknesses. Nevertheless,\nthe absence of a definitive computational approach to extract such insights\nfrom cricket players poses a significant challenge. This paper seeks to address\nthis gap by establishing computational models designed to extract the rules\ngoverning player strengths and weaknesses, thereby facilitating the development\nof tailored strategies for individual players. The complexity of this endeavor\nlies in several key areas: the selection of a suitable dataset, the precise\ndefinition of strength and weakness rules, the identification of an appropriate\nlearning algorithm, and the validation of the derived rules. To tackle these\nchallenges, we propose the utilization of unstructured data, specifically\ncricket text commentary, as a valuable resource for constructing comprehensive\nstrength and weakness rules for cricket players. We also introduce\ncomputationally feasible definitions for the construction of these rules, and\npresent a dimensionality reduction technique for the rule-building process. In\norder to showcase the practicality of this approach, we conduct an in-depth\nanalysis of cricket player strengths and weaknesses using a vast corpus of more\nthan one million text commentaries. Furthermore, we validate the constructed\nrules through two distinct methodologies: intrinsic and extrinsic. The outcomes\nof this research are made openly accessible, including the collected data,\nsource code, and results for over 250 cricket players, which can be accessed at\nhttps://bit.ly/2PKuzx8.\n","authors":["Swarup Ranjan Behera","Vijaya V. Saradhi"],"pdf_url":"https://arxiv.org/pdf/2311.06818v1.pdf","comment":"The initial work was published in the ICMLA 2019 conference"},{"id":"http://arxiv.org/abs/2311.06815v1","updated":"2023-11-12T11:40:57Z","published":"2023-11-12T11:40:57Z","title":"Evaluation of GPT-4 for chest X-ray impression generation: A reader\n  study on performance and perception","summary":"  The remarkable generative capabilities of multimodal foundation models are\ncurrently being explored for a variety of applications. Generating radiological\nimpressions is a challenging task that could significantly reduce the workload\nof radiologists. In our study we explored and analyzed the generative abilities\nof GPT-4 for Chest X-ray impression generation. To generate and evaluate\nimpressions of chest X-rays based on different input modalities (image, text,\ntext and image), a blinded radiological report was written for 25-cases of the\npublicly available NIH-dataset. GPT-4 was given image, finding section or both\nsequentially to generate an input dependent impression. In a blind randomized\nreading, 4-radiologists rated the impressions and were asked to classify the\nimpression origin (Human, AI), providing justification for their decision.\nLastly text model evaluation metrics and their correlation with the\nradiological score (summation of the 4 dimensions) was assessed. According to\nthe radiological score, the human-written impression was rated highest,\nalthough not significantly different to text-based impressions. The automated\nevaluation metrics showed moderate to substantial correlations to the\nradiological score for the image impressions, however individual scores were\nhighly divergent among inputs, indicating insufficient representation of\nradiological quality. Detection of AI-generated impressions varied by input and\nwas 61% for text-based impressions. Impressions classified as AI-generated had\nsignificantly worse radiological scores even when written by a radiologist,\nindicating potential bias. Our study revealed significant discrepancies between\na radiological assessment and common automatic evaluation metrics depending on\nthe model input. The detection of AI-generated findings is subject to bias that\nhighly rated impressions are perceived as human-written.\n","authors":["Sebastian Ziegelmayer","Alexander W. Marka","Nicolas Lenhart","Nadja Nehls","Stefan Reischl","Felix Harder","Andreas Sauter","Marcus Makowski","Markus Graf","Joshua Gawlitza"],"pdf_url":"https://arxiv.org/pdf/2311.06815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06807v1","updated":"2023-11-12T11:09:30Z","published":"2023-11-12T11:09:30Z","title":"On the Robustness of Question Rewriting Systems to Questions of Varying\n  Hardness","summary":"  In conversational question answering (CQA), the task of question\nrewriting~(QR) in context aims to rewrite a context-dependent question into an\nequivalent self-contained question that gives the same answer. In this paper,\nwe are interested in the robustness of a QR system to questions varying in\nrewriting hardness or difficulty. Since there is a lack of questions classified\nbased on their rewriting hardness, we first propose a heuristic method to\nautomatically classify questions into subsets of varying hardness, by measuring\nthe discrepancy between a question and its rewrite. To find out what makes\nquestions hard or easy for rewriting, we then conduct a human evaluation to\nannotate the rewriting hardness of questions. Finally, to enhance the\nrobustness of QR systems to questions of varying hardness, we propose a novel\nlearning framework for QR that first trains a QR model independently on each\nsubset of questions of a certain level of hardness, then combines these QR\nmodels as one joint model for inference. Experimental results on two datasets\nshow that our framework improves the overall performance compared to the\nbaselines.\n","authors":["Hai Ye","Hwee Tou Ng","Wenjuan Han"],"pdf_url":"https://arxiv.org/pdf/2311.06807v1.pdf","comment":"ACL'22, main, long paper"},{"id":"http://arxiv.org/abs/2311.06805v1","updated":"2023-11-12T11:01:10Z","published":"2023-11-12T11:01:10Z","title":"Tunable Soft Prompts are Messengers in Federated Learning","summary":"  Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models using decentralized data sources, alleviating\nprivacy concerns that arise from directly sharing local data. However, the lack\nof model privacy protection in FL becomes an unneglectable challenge,\nespecially when people want to federally finetune models based on a proprietary\nlarge language model. In this study, we propose a novel FL training approach\nthat accomplishes information exchange among participants via tunable soft\nprompts. These soft prompts, updated and transmitted between the server and\nclients, assume the role of the global model parameters and serve as messengers\nto deliver useful knowledge from the local data and global model. As the global\nmodel itself is not required to be shared and the local training is conducted\nbased on an auxiliary model with fewer parameters than the global model, the\nproposed approach provides protection for the global model while reducing\ncommunication and computation costs in FL. Extensive experiments show the\neffectiveness of the proposed approach compared to several baselines. We have\nreleased the source code at\n\\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.\n","authors":["Chenhe Dong","Yuexiang Xie","Bolin Ding","Ying Shen","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2311.06805v1.pdf","comment":"Accepted by EMNLP-23"},{"id":"http://arxiv.org/abs/2305.13528v2","updated":"2023-11-12T10:17:53Z","published":"2023-05-22T22:47:32Z","title":"Transfer-Free Data-Efficient Multilingual Slot Labeling","summary":"  Slot labeling (SL) is a core component of task-oriented dialogue (ToD)\nsystems, where slots and corresponding values are usually language-, task- and\ndomain-specific. Therefore, extending the system to any new\nlanguage-domain-task configuration requires (re)running an expensive and\nresource-intensive data annotation process. To mitigate the inherent data\nscarcity issue, current research on multilingual ToD assumes that sufficient\nEnglish-language annotated data are always available for particular tasks and\ndomains, and thus operates in a standard cross-lingual transfer setup. In this\nwork, we depart from this often unrealistic assumption. We examine challenging\nscenarios where such transfer-enabling English annotated data cannot be\nguaranteed, and focus on bootstrapping multilingual data-efficient slot\nlabelers in transfer-free scenarios directly in the target languages without\nany English-ready data. We propose a two-stage slot labeling approach (termed\nTWOSL) which transforms standard multilingual sentence encoders into effective\nslot labelers. In Stage 1, relying on SL-adapted contrastive learning with only\na handful of SL-annotated examples, we turn sentence encoders into\ntask-specific span encoders. In Stage 2, we recast SL from a token\nclassification into a simpler, less data-intensive span classification task.\nOur results on two standard multilingual TOD datasets and across diverse\nlanguages confirm the effectiveness and robustness of TWOSL. It is especially\neffective for the most challenging transfer-free few-shot setups, paving the\nway for quick and data-efficient bootstrapping of multilingual slot labelers\nfor ToD.\n","authors":["Evgeniia Razumovskaia","Ivan Vulić","Anna Korhonen"],"pdf_url":"https://arxiv.org/pdf/2305.13528v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.06771v1","updated":"2023-11-12T08:21:43Z","published":"2023-11-12T08:21:43Z","title":"Learning Globally Optimized Language Structure via Adversarial Training","summary":"  Recent work has explored integrating autoregressive language models with\nenergy-based models (EBMs) to enhance text generation capabilities. However,\nlearning effective EBMs for text is challenged by the discrete nature of\nlanguage. This work proposes an adversarial training strategy to address\nlimitations in prior efforts. Specifically, an iterative adversarial attack\nalgorithm is presented to generate negative samples for training the EBM by\nperturbing text from the autoregressive model. This aims to enable the EBM to\nsuppress spurious modes outside the support of the data distribution.\nExperiments on an arithmetic sequence generation task demonstrate that the\nproposed adversarial training approach can substantially enhance the quality of\ngenerated sequences compared to prior methods. The results highlight the\npromise of adversarial techniques to improve discrete EBM training. Key\ncontributions include: (1) an adversarial attack strategy tailored to text to\ngenerate negative samples, circumventing MCMC limitations; (2) an adversarial\ntraining algorithm for EBMs leveraging these attacks; (3) empirical validation\nof performance improvements on a sequence generation task.\n","authors":["Xuwang Yin"],"pdf_url":"https://arxiv.org/pdf/2311.06771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01305v3","updated":"2023-11-12T07:54:09Z","published":"2023-11-02T15:18:22Z","title":"AWEQ: Post-Training Quantization with Activation-Weight Equalization for\n  Large Language Models","summary":"  Large language models(LLMs) exhibit excellent performance across a variety of\ntasks, but they come with significant computational and storage costs.\nQuantizing these models is an effective way to alleviate this issue. However,\nexisting methods struggle to strike a balance between model accuracy and\nhardware efficiency. This is where we introduce AWEQ, a post-training method\nthat requires no additional training overhead. AWEQ excels in both\nultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.\nThere is an observation that weight quantization is less challenging than\nactivation quantization. AWEQ transfers the difficulty of activation\nquantization to weights using channel equalization, achieving a balance between\nthe quantization difficulties of both, and thereby maximizing performance. We\nhave further refined the equalization method to mitigate quantization bias\nerror, ensuring the robustness of the model. Extensive experiments on popular\nmodels such as LLaMA and OPT demonstrate that AWEQ outperforms all existing\npost-training quantization methods for large models.\n","authors":["Baisong Li","Xingwang Wang","Haixiao Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01305v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06761v1","updated":"2023-11-12T07:37:24Z","published":"2023-11-12T07:37:24Z","title":"Learning Knowledge-Enhanced Contextual Language Representations for\n  Domain Natural Language Understanding","summary":"  Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the\nperformance of various downstream NLP tasks by injecting knowledge facts from\nlarge-scale Knowledge Graphs (KGs). However, existing methods for pre-training\nKEPLMs with relational triples are difficult to be adapted to close domains due\nto the lack of sufficient domain graph semantics. In this paper, we propose a\nKnowledge-enhanced lANGuAge Representation learning framework for various\nclOsed dOmains (KANGAROO) via capturing the implicit graph structure among the\nentities. Specifically, since the entity coverage rates of closed-domain KGs\ncan be relatively low and may exhibit the global sparsity phenomenon for\nknowledge injection, we consider not only the shallow relational\nrepresentations of triples but also the hyperbolic embeddings of deep\nhierarchical entity-class structures for effective knowledge fusion.Moreover,\nas two closed-domain entities under the same entity-class often have locally\ndense neighbor subgraphs counted by max point biconnected component, we further\npropose a data augmentation strategy based on contrastive learning over\nsubgraphs to construct hard negative samples of higher quality. It makes the\nunderlying KELPMs better distinguish the semantics of these neighboring\nentities to further complement the global semantic sparsity. In the\nexperiments, we evaluate KANGAROO over various knowledge-aware and general NLP\ntasks in both full and few-shot learning settings, outperforming various KEPLM\ntraining paradigms performance in closed-domains significantly.\n","authors":["Ruyao Xu","Taolin Zhang","Chengyu Wang","Zhongjie Duan","Cen Chen","Minghui Qiu","Dawei Cheng","Xiaofeng He","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2311.06761v1.pdf","comment":"emnlp 2023"},{"id":"http://arxiv.org/abs/2311.06758v1","updated":"2023-11-12T07:20:37Z","published":"2023-11-12T07:20:37Z","title":"Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for\n  Cross-Lingual Machine Reading Comprehension","summary":"  In cross-lingual language understanding, machine translation is often\nutilized to enhance the transferability of models across languages, either by\ntranslating the training data from the source language to the target, or from\nthe target to the source to aid inference. However, in cross-lingual machine\nreading comprehension (MRC), it is difficult to perform a deep level of\nassistance to enhance cross-lingual transfer because of the variation of answer\nspan positions in different languages. In this paper, we propose X-STA, a new\napproach for cross-lingual MRC. Specifically, we leverage an attentive teacher\nto subtly transfer the answer spans of the source language to the answer output\nspace of the target. A Gradient-Disentangled Knowledge Sharing technique is\nproposed as an improved cross-attention block. In addition, we force the model\nto learn semantic alignments from multiple granularities and calibrate the\nmodel outputs with teacher guidance to enhance cross-lingual transferability.\nExperiments on three multi-lingual MRC datasets show the effectiveness of our\nmethod, outperforming state-of-the-art approaches.\n","authors":["Tingfeng Cao","Chengyu Wang","Chuanqi Tan","Jun Huang","Jinhui Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.06758v1.pdf","comment":"emnlp 2023"},{"id":"http://arxiv.org/abs/2311.06754v1","updated":"2023-11-12T06:56:21Z","published":"2023-11-12T06:56:21Z","title":"From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with\n  Small Language Models","summary":"  Reasoning is a distinctive human capacity, enabling us to address complex\nproblems by breaking them down into a series of manageable cognitive steps.\nYet, complex logical reasoning is still cumbersome for language models. Based\non the dual process theory in cognitive science, we are the first to unravel\nthe cognitive reasoning abilities of language models. Our framework employs an\niterative methodology to construct a Cognitive Tree (CogTree). The root node of\nthis tree represents the initial query, while the leaf nodes consist of\nstraightforward questions that can be answered directly. This construction\ninvolves two main components: the implicit extraction module (referred to as\nthe intuitive system) and the explicit reasoning module (referred to as the\nreflective system). The intuitive system rapidly generates multiple responses\nby utilizing in-context examples, while the reflective system scores these\nresponses using comparative learning. The scores guide the intuitive system in\nits subsequent generation step. Our experimental results on two popular and\nchallenging reasoning tasks indicate that it is possible to achieve a\nperformance level comparable to that of GPT-3.5 (with 175B parameters), using a\nsignificantly smaller language model that contains fewer parameters (<=7B) than\n5% of GPT-3.5.\n","authors":["Junbing Yan","Chengyu Wang","Taolin Zhang","Xiaofeng He","Jun Huang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.06754v1.pdf","comment":"emnlp 2023"},{"id":"http://arxiv.org/abs/2311.06753v1","updated":"2023-11-12T06:56:14Z","published":"2023-11-12T06:56:14Z","title":"Towards General-Purpose Speech Abilities for Large Language Models Using\n  Unpaired Data","summary":"  In this work, we extend the instruction-tuned Llama-2 model with end-to-end\ngeneral-purpose speech processing and reasoning abilities while maintaining the\nwide range of LLM capabilities, without using any carefully curated paired\ndata. The proposed model can utilize audio prompts as a replacement for text\nand sustain a conversation. Such a model also has extended cross-modal\ncapabilities such as being able to perform speech question answering, speech\ntranslation, and audio summarization amongst many other closed and open-domain\ntasks. This is unlike prior approaches in speech, in which LLMs are extended to\nhandle audio for a limited number of pre-designated tasks. Experiments show\nthat our end-to-end approach is on par with or outperforms a cascaded system\n(speech recognizer + LLM) in terms of modeling the response to a prompt.\nFurthermore, unlike a cascade, our approach shows the ability to interchange\ntext and audio modalities and utilize the prior context in a conversation to\nprovide better results.\n","authors":["Yassir Fathullah","Chunyang Wu","Egor Lakomkin","Junteng Jia","Yuan Shangguan","Jay Mahadeokar","Ozlem Kalinli","Christian Fuegen","Mike Seltzer"],"pdf_url":"https://arxiv.org/pdf/2311.06753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06752v1","updated":"2023-11-12T06:39:00Z","published":"2023-11-12T06:39:00Z","title":"BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image\n  Synthesis","summary":"  Recently, diffusion-based deep generative models (e.g., Stable Diffusion)\nhave shown impressive results in text-to-image synthesis. However, current\ntext-to-image models often require multiple passes of prompt engineering by\nhumans in order to produce satisfactory results for real-world applications. We\npropose BeautifulPrompt, a deep generative model to produce high-quality\nprompts from very simple raw descriptions, which enables diffusion-based models\nto generate more beautiful images. In our work, we first fine-tuned the\nBeautifulPrompt model over low-quality and high-quality collecting prompt\npairs. Then, to ensure that our generated prompts can generate more beautiful\nimages, we further propose a Reinforcement Learning with Visual AI Feedback\ntechnique to fine-tune our model to maximize the reward values of the generated\nprompts, where the reward values are calculated based on the PickScore and the\nAesthetic Scores. Our results demonstrate that learning from visual AI feedback\npromises the potential to improve the quality of generated prompts and images\nsignificantly. We further showcase the integration of BeautifulPrompt to a\ncloud-native AI platform to provide better text-to-image generation service in\nthe cloud.\n","authors":["Tingfeng Cao","Chengyu Wang","Bingyan Liu","Ziheng Wu","Jinhui Zhu","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2311.06752v1.pdf","comment":"emnlp 2023"},{"id":"http://arxiv.org/abs/2307.03025v3","updated":"2023-11-12T06:25:32Z","published":"2023-07-06T14:42:01Z","title":"Style Over Substance: Evaluation Biases for Large Language Models","summary":"  As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nRanking the relative performance of LLMs based on Elo ratings, according to\nhuman judgment, is gaining more popularity. However, the extent to which humans\nand LLMs are capable evaluators remains uncertain. This study investigates the\nbehavior of crowd-sourced and expert annotators, as well as LLMs, when\ncomparing outputs from different models. To achieve this, we curate a dataset\nof intentionally flawed machine-generated answers. Our findings reveal a\nconcerning bias in the evaluation process, as answers with factual errors are\nrated more favorably than answers that are too short or contained grammatical\nerrors. To address this issue, we propose independently evaluating\nmachine-generated text across multiple dimensions, rather than merging all the\nevaluation aspects into a single score. We instantiate this idea with the Elo\nrating system, resulting in the Multi-Elo Rating System (MERS). Empirical\nresults from our study reveal that this proposed approach significantly\nenhances the quality of LLM-based evaluations, particularly in terms of factual\naccuracy. However, there is no significant improvement in crowd-sourced-based\nevaluations, indicating the need for further investigation.\n","authors":["Minghao Wu","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2307.03025v3.pdf","comment":"Work in progress, 15 pages, 5 tables, 12 figures"},{"id":"http://arxiv.org/abs/2308.06744v3","updated":"2023-11-12T05:30:17Z","published":"2023-08-13T11:07:55Z","title":"Token-Scaled Logit Distillation for Ternary Weight Generative Language\n  Models","summary":"  Generative Language Models (GLMs) have shown impressive performance in tasks\nsuch as text generation, understanding, and reasoning. However, the large model\nsize poses challenges for practical deployment. To solve this problem,\nQuantization-Aware Training (QAT) has become increasingly popular. However,\ncurrent QAT methods for generative models have resulted in a noticeable loss of\naccuracy. To counteract this issue, we propose a novel knowledge distillation\nmethod specifically designed for GLMs. Our method, called token-scaled logit\ndistillation, prevents overfitting and provides superior learning from the\nteacher model and ground truth. This research marks the first evaluation of\nternary weight quantization-aware training of large-scale GLMs with less than\n1.0 degradation in perplexity and achieves enhanced accuracy in tasks like\ncommon-sense QA and arithmetic reasoning as well as natural language\nunderstanding. Our code is available at https://github.com/aiha-lab/TSLD.\n","authors":["Minsoo Kim","Sihwa Lee","Janghwan Lee","Sukjin Hong","Du-Seong Chang","Wonyong Sung","Jungwook Choi"],"pdf_url":"https://arxiv.org/pdf/2308.06744v3.pdf","comment":"NeurIPS 2023 Camera Ready"},{"id":"http://arxiv.org/abs/2311.06737v1","updated":"2023-11-12T05:20:20Z","published":"2023-11-12T05:20:20Z","title":"Detecting and Correcting Hate Speech in Multimodal Memes with Large\n  Visual Language Model","summary":"  Recently, large language models (LLMs) have taken the spotlight in natural\nlanguage processing. Further, integrating LLMs with vision enables the users to\nexplore more emergent abilities in multimodality. Visual language models\n(VLMs), such as LLaVA, Flamingo, or GPT-4, have demonstrated impressive\nperformance on various visio-linguistic tasks. Consequently, there are enormous\napplications of large models that could be potentially used on social media\nplatforms. Despite that, there is a lack of related work on detecting or\ncorrecting hateful memes with VLMs. In this work, we study the ability of VLMs\non hateful meme detection and hateful meme correction tasks with zero-shot\nprompting. From our empirical experiments, we show the effectiveness of the\npretrained LLaVA model and discuss its strengths and weaknesses in these tasks.\n","authors":["Minh-Hao Van","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2311.06737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07336v2","updated":"2023-11-12T05:14:30Z","published":"2023-08-11T13:15:35Z","title":"Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic","summary":"  We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.\n","authors":["Terufumi Morishita","Gaku Morio","Atsuki Yamaguchi","Yasuhiro Sogawa"],"pdf_url":"https://arxiv.org/pdf/2308.07336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06736v1","updated":"2023-11-12T05:12:49Z","published":"2023-11-12T05:12:49Z","title":"Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof\n  Generation with Contrastive Stepwise Decoding","summary":"  Logical reasoning remains a pivotal component within the realm of artificial\nintelligence. The recent evolution of large language models (LLMs) has marked\nsignificant progress in this domain. The adoption of strategies like\nchain-of-thought (CoT) has enhanced the performance of LLMs across diverse\nreasoning tasks. Nonetheless, logical reasoning that involves proof planning,\nspecifically those that necessitate the validation of explanation accuracy,\ncontinues to present stumbling blocks. In this study, we first evaluate the\nefficacy of LLMs with advanced CoT strategies concerning such tasks. Our\nanalysis reveals that LLMs still struggle to navigate complex reasoning chains,\nwhich demand the meticulous linkage of premises to derive a cogent conclusion.\nTo address this issue, we finetune a smaller-scale language model, equipping it\nto decompose proof objectives into more manageable subgoals. We also introduce\ncontrastive decoding to stepwise proof generation, making use of negative\nreasoning paths to strengthen the model's capacity for logical deduction.\nExperiments on EntailmentBank underscore the success of our method in\naugmenting the proof planning abilities of language models.\n","authors":["Ying Su","Xiaojin Fu","Mingwen Liu","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2311.06736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01732v2","updated":"2023-11-12T04:28:43Z","published":"2023-11-03T05:55:32Z","title":"Proto-lm: A Prototypical Network-Based Framework for Built-in\n  Interpretability in Large Language Models","summary":"  Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), but their lack of interpretability has been a major\nconcern. Current methods for interpreting LLMs are post hoc, applied after\ninference time, and have limitations such as their focus on low-level features\nand lack of explainability at higher level text units. In this work, we\nintroduce proto-lm, a prototypical network-based white-box framework that\nallows LLMs to learn immediately interpretable embeddings during the\nfine-tuning stage while maintaining competitive performance. Our method's\napplicability and interpretability are demonstrated through experiments on a\nwide range of NLP tasks, and our results indicate a new possibility of creating\ninterpretable models without sacrificing performance. This novel approach to\ninterpretability in LLMs can pave the way for more interpretable models without\nthe need to sacrifice performance.\n","authors":["Sean Xie","Soroush Vosoughi","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2311.01732v2.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.06729v1","updated":"2023-11-12T04:23:33Z","published":"2023-11-12T04:23:33Z","title":"Comprehending Lexical and Affective Ontologies in the Demographically\n  Diverse Spatial Social Media Discourse","summary":"  This study aims to comprehend linguistic and socio-demographic features,\nencompassing English language styles, conveyed sentiments, and lexical\ndiversity within spatial online social media review data. To this end, we\nundertake a case study that scrutinizes reviews composed by two distinct and\ndemographically diverse groups. Our analysis entails the extraction and\nexamination of various statistical, grammatical, and sentimental features from\nthese two groups. Subsequently, we leverage these features with machine\nlearning (ML) classifiers to discern their potential in effectively\ndifferentiating between the groups. Our investigation unveils substantial\ndisparities in certain linguistic attributes between the two groups. When\nintegrated into ML classifiers, these attributes exhibit a marked efficacy in\ndistinguishing the groups, yielding a macro F1 score of approximately 0.85.\nFurthermore, we conduct a comparative evaluation of these linguistic features\nwith word n-gram-based lexical features in discerning demographically diverse\nreview data. As expected, the n-gram lexical features, coupled with fine-tuned\ntransformer-based models, show superior performance, attaining accuracies\nsurpassing 95\\% and macro F1 scores exceeding 0.96. Our meticulous analysis and\ncomprehensive evaluations substantiate the efficacy of linguistic and\nsentimental features in effectively discerning demographically diverse review\ndata. The findings of this study provide valuable guidelines for future\nresearch endeavors concerning the analysis of demographic patterns in textual\ncontent across various social media platforms.\n","authors":["Salim Sazzed"],"pdf_url":"https://arxiv.org/pdf/2311.06729v1.pdf","comment":"Accepted in 22nd IEEE International Conference on Machine Learning\n  and Applications (ICMLA), 2023"},{"id":"http://arxiv.org/abs/2309.00312v3","updated":"2023-11-12T03:53:50Z","published":"2023-09-01T07:53:28Z","title":"Insights Into the Nutritional Prevention of Macular Degeneration based\n  on a Comparative Topic Modeling Approach","summary":"  Topic modeling and text mining are subsets of Natural Language Processing\n(NLP) with relevance for conducting meta-analysis (MA) and systematic review\n(SR). For evidence synthesis, the above NLP methods are conventionally used for\ntopic-specific literature searches or extracting values from reports to\nautomate essential phases of SR and MA. Instead, this work proposes a\ncomparative topic modeling approach to analyze reports of contradictory results\non the same general research question. Specifically, the objective is to\nidentify topics exhibiting distinct associations with significant results for\nan outcome of interest by ranking them according to their proportional\noccurrence in (and consistency of distribution across) reports of significant\neffects. The proposed method was tested on broad-scope studies addressing\nwhether supplemental nutritional compounds significantly benefit macular\ndegeneration (MD). Six compounds were identified as having a particular\nassociation with reports of significant results for benefiting MD. Four of\nthese were further supported in terms of effectiveness upon conducting a\nfollow-up literature search for validation (omega-3 fatty acids, copper,\nzeaxanthin, and nitrates). The two not supported by the follow-up literature\nsearch (niacin and molybdenum) also had scores in the lowest range under the\nproposed scoring system, suggesting that the proposed methods score for a given\ntopic may be a viable proxy for its degree of association with the outcome of\ninterest and can be helpful in the search for potentially causal relationships.\nThese results underpin the proposed methods potential to add specificity in\nunderstanding effects from broad-scope reports, elucidate topics of interest\nfor future research, and guide evidence synthesis in a systematic and scalable\nway. All of this is accomplished while yielding valuable insights into the\nprevention of MD.\n","authors":["Lucas Cassiel Jacaruso"],"pdf_url":"https://arxiv.org/pdf/2309.00312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06724v1","updated":"2023-11-12T03:51:38Z","published":"2023-11-12T03:51:38Z","title":"Controllable Topic-Focused Abstractive Summarization","summary":"  Controlled abstractive summarization focuses on producing condensed versions\nof a source article to cover specific aspects by shifting the distribution of\ngenerated text towards a desired style, e.g., a set of topics. Subsequently,\nthe resulting summaries may be tailored to user-defined requirements. This\npaper presents a new Transformer-based architecture capable of producing\ntopic-focused summaries. The architecture modifies the cross-attention\nmechanism of the Transformer to bring topic-focus control to the generation\nprocess while not adding any further parameters to the model. We show that our\nmodel sets a new state of the art on the NEWTS dataset in terms of\ntopic-focused abstractive summarization as well as a topic-prevalence score.\nMoreover, we show via extensive experiments that our proposed topical\ncross-attention mechanism can be plugged into various Transformer models, such\nas BART and T5, improving their performance on the CNN/Dailymail and XSum\nbenchmark datasets for abstractive summarization. This is achieved via\nfine-tuning, without requiring training from scratch. Finally, we show through\nhuman evaluation that our model generates more faithful summaries outperforming\nthe state-of-the-art Frost model.\n","authors":["Seyed Ali Bahrainian","Martin Jaggi","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2311.06724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06720v1","updated":"2023-11-12T03:25:34Z","published":"2023-11-12T03:25:34Z","title":"Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small\n  Scorer","summary":"  Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in\nmulti-tasking under a unified instruction-following paradigm, where they also\nexhibit remarkable generalization abilities to unseen tasks. Despite their\nimpressive performance, these LLMs, with sizes ranging from several billion to\nhundreds of billions of parameters, demand substantial computational resources,\nmaking their training and inference expensive and inefficient. Furthermore,\nadapting these models to downstream applications, particularly complex tasks,\nis often unfeasible due to the extensive hardware requirements for finetuning,\neven when utilizing parameter-efficient approaches such as prompt tuning.\nAdditionally, the most powerful multi-task LLMs, such as OPT-IML-175B and\nFLAN-PaLM-540B, are not publicly accessible, severely limiting their\ncustomization potential. To address these challenges, we introduce a pretrained\nsmall scorer, Cappy, designed to enhance the performance and efficiency of\nmulti-task LLMs. With merely 360 million parameters, Cappy functions either\nindependently on classification tasks or serve as an auxiliary component for\nLLMs, boosting their performance. Moreover, Cappy enables efficiently\nintegrating downstream supervision without requiring LLM finetuning nor the\naccess to their parameters. Our experiments demonstrate that, when working\nindependently on 11 language understanding tasks from PromptSource, Cappy\noutperforms LLMs that are several orders of magnitude larger. Besides, on 45\ncomplex tasks from BIG-Bench, Cappy boosts the performance of the advanced\nmulti-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to\ncooperate with other LLM adaptations, including finetuning and in-context\nlearning, offering additional performance enhancement.\n","authors":["Bowen Tan","Yun Zhu","Lijuan Liu","Eric Xing","Zhiting Hu","Jindong Chen"],"pdf_url":"https://arxiv.org/pdf/2311.06720v1.pdf","comment":"In proceedings of NeurIPS 2023; Code and model available at\n  https://github.com/tanyuqian/cappy and\n  https://huggingface.co/btan2/cappy-large, respectively"},{"id":"http://arxiv.org/abs/2311.06714v1","updated":"2023-11-12T02:54:11Z","published":"2023-11-12T02:54:11Z","title":"What factors influence the popularity of user-generated text in the\n  creative domain? A case study of book reviews","summary":"  This study investigates a range of psychological, lexical, semantic, and\nreadability features of book reviews to elucidate the factors underlying their\nperceived popularity. To this end, we conduct statistical analyses of various\nfeatures, including the types and frequency of opinion and emotion-conveying\nterms, connectives, character mentions, word uniqueness, commonness, and\nsentence structure, among others. Additionally, we utilize two readability\ntests to explore whether reading ease is positively associated with review\npopularity. Finally, we employ traditional machine learning classifiers and\ntransformer-based fine-tuned language models with n-gram features to\nautomatically determine review popularity. Our findings indicate that, with the\nexception of a few features (e.g., review length, emotions, and word\nuniqueness), most attributes do not exhibit significant differences between\npopular and non-popular review groups. Furthermore, the poor performance of\nmachine learning classifiers using the word n-gram feature highlights the\nchallenges associated with determining popularity in creative domains. Overall,\nour study provides insights into the factors underlying review popularity and\nhighlights the need for further research in this area, particularly in the\ncreative realm.\n","authors":["Salim Sazzed"],"pdf_url":"https://arxiv.org/pdf/2311.06714v1.pdf","comment":"Accepted in 22nd IEEE International Conference on Machine Learning\n  and Applications (ICMLA), 2023"},{"id":"http://arxiv.org/abs/2307.12166v2","updated":"2023-11-12T01:26:48Z","published":"2023-07-22T21:00:14Z","title":"The Imitation Game: Detecting Human and AI-Generated Texts in the Era of\n  ChatGPT and BARD","summary":"  The potential of artificial intelligence (AI)-based large language models\n(LLMs) holds considerable promise in revolutionizing education, research, and\npractice. However, distinguishing between human-written and AI-generated text\nhas become a significant task. This paper presents a comparative study,\nintroducing a novel dataset of human-written and LLM-generated texts in\ndifferent genres: essays, stories, poetry, and Python code. We employ several\nmachine learning models to classify the texts. Results demonstrate the efficacy\nof these models in discerning between human and AI-generated text, despite the\ndataset's limited sample size. However, the task becomes more challenging when\nclassifying GPT-generated text, particularly in story writing. The results\nindicate that the models exhibit superior performance in binary classification\ntasks, such as distinguishing human-generated text from a specific LLM,\ncompared to the more complex multiclass tasks that involve discerning among\nhuman-generated and multiple LLMs. Our findings provide insightful implications\nfor AI text detection while our dataset paves the way for future research in\nthis evolving area.\n","authors":["Kadhim Hayawi","Sakib Shahriar","Sujith Samuel Mathew"],"pdf_url":"https://arxiv.org/pdf/2307.12166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06697v1","updated":"2023-11-12T00:25:25Z","published":"2023-11-12T00:25:25Z","title":"Trusted Source Alignment in Large Language Models","summary":"  Large language models (LLMs) are trained on web-scale corpora that inevitably\ninclude contradictory factual information from sources of varying reliability.\nIn this paper, we propose measuring an LLM property called trusted source\nalignment (TSA): the model's propensity to align with content produced by\ntrusted publishers in the face of uncertainty or controversy. We present\nFactCheckQA, a TSA evaluation dataset based on a corpus of fact checking\narticles. We describe a simple protocol for evaluating TSA and offer a detailed\nanalysis of design considerations including response extraction, claim\ncontextualization, and bias in prompt formulation. Applying the protocol to\nPaLM-2, we find that as we scale up the model size, the model performance on\nFactCheckQA improves from near-random to up to 80% balanced accuracy in\naligning with trusted sources.\n","authors":["Vasilisa Bashlovkina","Zhaobin Kuang","Riley Matthews","Edward Clifford","Yennie Jun","William W. Cohen","Simon Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2311.06697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06696v1","updated":"2023-11-12T00:23:37Z","published":"2023-11-12T00:23:37Z","title":"Simple and Effective Input Reformulations for Translation","summary":"  Foundation language models learn from their finetuning input context in\ndifferent ways. In this paper, we reformulate inputs during finetuning for\nchallenging translation tasks, leveraging model strengths from pretraining in\nnovel ways to improve downstream performance. These reformulations are simple\ndata level modifications, require no additional collection of training data or\nmodification of data at inference time. They can be applied either on single\nlanguage pair translation tasks or massively multilingual translation tasks.\nExperiments with these techniques demonstrate significant performance\nimprovements up to $\\textbf{3.5 chrF++ on the Flores200 translation\nbenchmark}$. We hope our research accessibly improves finetuning data\nefficiency, enabling more effective training to scalably improve\nstate-of-the-art performance. Our code is released\n$\\href{https://github.com/bri25yu/LanguageModelExperimentation}{here}.$\n","authors":["Brian Yu","Hansen Lillemark","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2311.06696v1.pdf","comment":"13 pages, 6 figures. To be published in Empirical Methods in Natural\n  Language Processing (Main) 2023"},{"id":"http://arxiv.org/abs/2311.06694v1","updated":"2023-11-12T00:21:58Z","published":"2023-11-12T00:21:58Z","title":"Comparative Multi-View Language Grounding","summary":"  In this work, we consider the task of resolving object referents when given a\ncomparative language description. We present a Multi-view Approach to Grounding\nin Context (MAGiC) that leverages transformers to pragmatically reason over\nboth objects given multiple image views and a language description. In contrast\nto past efforts that attempt to connect vision and language for this task\nwithout fully considering the resulting referential context, MAGiC makes use of\nthe comparative information by jointly reasoning over multiple views of both\nobject referent candidates and the referring language expression. We present an\nanalysis demonstrating that comparative reasoning contributes to SOTA\nperformance on the SNARE object reference task.\n","authors":["Chancharik Mitra","Abrar Anwar","Rodolfo Corona","Dan Klein","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2311.06694v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2311.06978v1","updated":"2023-11-12T22:42:34Z","published":"2023-11-12T22:42:34Z","title":"Augmented Bridge Matching","summary":"  Flow and bridge matching are a novel class of processes which encompass\ndiffusion models. One of the main aspect of their increased flexibility is that\nthese models can interpolate between arbitrary data distributions i.e. they\ngeneralize beyond generative modeling and can be applied to learning stochastic\n(and deterministic) processes of arbitrary transfer tasks between two given\ndistributions. In this paper, we highlight that while flow and bridge matching\nprocesses preserve the information of the marginal distributions, they do\n\\emph{not} necessarily preserve the coupling information unless additional,\nstronger optimality conditions are met. This can be problematic if one aims at\npreserving the original empirical pairing. We show that a simple modification\nof the matching process recovers this coupling by augmenting the velocity field\n(or drift) with the information of the initial sample point. Doing so, we lose\nthe Markovian property of the process but preserve the coupling information\nbetween distributions. We illustrate the efficiency of our augmentation in\nlearning mixture of image translation tasks.\n","authors":["Valentin De Bortoli","Guan-Horng Liu","Tianrong Chen","Evangelos A. Theodorou","Weilie Nie"],"pdf_url":"https://arxiv.org/pdf/2311.06978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06976v1","updated":"2023-11-12T22:28:19Z","published":"2023-11-12T22:28:19Z","title":"CD-COCO: A Versatile Complex Distorted COCO Database for\n  Scene-Context-Aware Computer Vision","summary":"  The recent development of deep learning methods applied to vision has enabled\ntheir increasing integration into real-world applications to perform complex\nComputer Vision (CV) tasks. However, image acquisition conditions have a major\nimpact on the performance of high-level image processing. A possible solution\nto overcome these limitations is to artificially augment the training databases\nor to design deep learning models that are robust to signal distortions. We opt\nhere for the first solution by enriching the database with complex and\nrealistic distortions which were ignored until now in the existing databases.\nTo this end, we built a new versatile database derived from the well-known\nMS-COCO database to which we applied local and global photo-realistic\ndistortions. These new local distortions are generated by considering the scene\ncontext of the images that guarantees a high level of photo-realism.\nDistortions are generated by exploiting the depth information of the objects in\nthe scene as well as their semantics. This guarantees a high level of\nphoto-realism and allows to explore real scenarios ignored in conventional\ndatabases dedicated to various CV applications. Our versatile database offers\nan efficient solution to improve the robustness of various CV tasks such as\nObject Detection (OD), scene segmentation, and distortion-type classification\nmethods. The image database, scene classification index, and distortion\ngeneration codes are publicly available\n\\footnote{\\url{https://github.com/Aymanbegh/CD-COCO}}\n","authors":["Ayman Beghdadi","Azeddine Beghdadi","Malik Mallem","Lotfi Beji","Faouzi Alaya Cheikh"],"pdf_url":"https://arxiv.org/pdf/2311.06976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09447v3","updated":"2023-11-12T21:28:47Z","published":"2023-03-16T16:23:13Z","title":"Steering Prototypes with Prompt-tuning for Rehearsal-free Continual\n  Learning","summary":"  In the context of continual learning, prototypes-as representative class\nembeddings-offer advantages in memory conservation and the mitigation of\ncatastrophic forgetting. However, challenges related to semantic drift and\nprototype interference persist. In this study, we introduce the Contrastive\nPrototypical Prompt (CPP) approach. Through task-specific prompt-tuning,\nunderpinned by a contrastive learning objective, we effectively address both\naforementioned challenges. Our evaluations on four challenging\nclass-incremental benchmarks reveal that CPP achieves a significant 4% to 6%\nimprovement over state-of-the-art methods. Importantly, CPP operates without a\nrehearsal buffer and narrows the performance divergence between continual and\noffline joint-learning, suggesting an innovative scheme for Transformer-based\ncontinual learning systems.\n","authors":["Zhuowei Li","Long Zhao","Zizhao Zhang","Han Zhang","Di Liu","Ting Liu","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2303.09447v3.pdf","comment":"Accept to WACV 2024. Code is available at\n  https://github.com/LzVv123456/Contrastive-Prototypical-Prompt"},{"id":"http://arxiv.org/abs/2311.06964v1","updated":"2023-11-12T21:07:04Z","published":"2023-11-12T21:07:04Z","title":"Adaptive recurrent vision performs zero-shot computation scaling to\n  unseen difficulty levels","summary":"  Humans solving algorithmic (or) reasoning problems typically exhibit solution\ntimes that grow as a function of problem difficulty. Adaptive recurrent neural\nnetworks have been shown to exhibit this property for various\nlanguage-processing tasks. However, little work has been performed to assess\nwhether such adaptive computation can also enable vision models to extrapolate\nsolutions beyond their training distribution's difficulty level, with prior\nwork focusing on very simple tasks. In this study, we investigate a critical\nfunctional role of such adaptive processing using recurrent neural networks: to\ndynamically scale computational resources conditional on input requirements\nthat allow for zero-shot generalization to novel difficulty levels not seen\nduring training using two challenging visual reasoning tasks: PathFinder and\nMazes. We combine convolutional recurrent neural networks (ConvRNNs) with a\nlearnable halting mechanism based on Graves (2016). We explore various\nimplementations of such adaptive ConvRNNs (AdRNNs) ranging from tying weights\nacross layers to more sophisticated biologically inspired recurrent networks\nthat possess lateral connections and gating. We show that 1) AdRNNs learn to\ndynamically halt processing early (or late) to solve easier (or harder)\nproblems, 2) these RNNs zero-shot generalize to more difficult problem settings\nnot shown during training by dynamically increasing the number of recurrent\niterations at test time. Our study provides modeling evidence supporting the\nhypothesis that recurrent processing enables the functional advantage of\nadaptively allocating compute resources conditional on input requirements and\nhence allowing generalization to harder difficulty levels of a visual reasoning\nproblem without training.\n","authors":["Vijay Veerabadran","Srinivas Ravishankar","Yuan Tang","Ritik Raina","Virginia R. de Sa"],"pdf_url":"https://arxiv.org/pdf/2311.06964v1.pdf","comment":"37th Conference on Neural Information Processing Systems (NeurIPS\n  2023)"},{"id":"http://arxiv.org/abs/2311.06956v1","updated":"2023-11-12T20:46:54Z","published":"2023-11-12T20:46:54Z","title":"SegReg: Segmenting OARs by Registering MR Images and CT Annotations","summary":"  Organ at risk (OAR) segmentation is a critical process in radiotherapy\ntreatment planning such as head and neck tumors. Nevertheless, in clinical\npractice, radiation oncologists predominantly perform OAR segmentations\nmanually on CT scans. This manual process is highly time-consuming and\nexpensive, limiting the number of patients who can receive timely radiotherapy.\nAdditionally, CT scans offer lower soft-tissue contrast compared to MRI.\nDespite MRI providing superior soft-tissue visualization, its time-consuming\nnature makes it infeasible for real-time treatment planning. To address these\nchallenges, we propose a method called SegReg, which utilizes Elastic Symmetric\nNormalization for registering MRI to perform OAR segmentation. SegReg\noutperforms the CT-only baseline by 16.78% in mDSC and 18.77% in mIoU, showing\nthat it effectively combines the geometric accuracy of CT with the superior\nsoft-tissue contrast of MRI, making accurate automated OAR segmentation for\nclinical practice become possible.\n","authors":["Zeyu Zhang","Xuyin Qi","Bowen Zhang","Biao Wu","Hien Le","Bora Jeong","Minh-Son To","Richard Hartley"],"pdf_url":"https://arxiv.org/pdf/2311.06956v1.pdf","comment":"Contact: steve.zeyu.zhang@outlook.com"},{"id":"http://arxiv.org/abs/2305.14053v2","updated":"2023-11-12T20:17:47Z","published":"2023-05-23T13:32:19Z","title":"Parts of Speech-Grounded Subspaces in Vision-Language Models","summary":"  Latent image representations arising from vision-language models have proved\nimmensely useful for a variety of downstream tasks. However, their utility is\nlimited by their entanglement with respect to different visual attributes. For\ninstance, recent work has shown that CLIP image representations are often\nbiased toward specific visual properties (such as objects or actions) in an\nunpredictable manner. In this paper, we propose to separate representations of\nthe different visual modalities in CLIP's joint vision-language space by\nleveraging the association between parts of speech and specific visual modes of\nvariation (e.g. nouns relate to objects, adjectives describe appearance). This\nis achieved by formulating an appropriate component analysis model that learns\nsubspaces capturing variability corresponding to a specific part of speech,\nwhile jointly minimising variability to the rest. Such a subspace yields\ndisentangled representations of the different visual properties of an image or\ntext in closed form while respecting the underlying geometry of the manifold on\nwhich the representations lie. What's more, we show the proposed model\nadditionally facilitates learning subspaces corresponding to specific visual\nappearances (e.g. artists' painting styles), which enables the selective\nremoval of entire visual themes from CLIP-based text-to-image synthesis. We\nvalidate the model both qualitatively, by visualising the subspace projections\nwith a text-to-image model and by preventing the imitation of artists' styles,\nand quantitatively, through class invariance metrics and improvements to\nbaseline zero-shot classification.\n","authors":["James Oldfield","Christos Tzelepis","Yannis Panagakis","Mihalis A. Nicolaou","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2305.14053v2.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.06930v1","updated":"2023-11-12T19:06:33Z","published":"2023-11-12T19:06:33Z","title":"Video-based sympathetic arousal assessment via peripheral blood flow\n  estimation","summary":"  Electrodermal activity (EDA) is considered a standard marker of sympathetic\nactivity. However, traditional EDA measurement requires electrodes in steady\ncontact with the skin. Can sympathetic arousal be measured using only an\noptical sensor, such as an RGB camera? This paper presents a novel approach to\ninfer sympathetic arousal by measuring the peripheral blood flow on the face or\nhand optically. We contribute a self-recorded dataset of 21 participants,\ncomprising synchronized videos of participants' faces and palms and\ngold-standard EDA and photoplethysmography (PPG) signals. Our results show that\nwe can measure peripheral sympathetic responses that closely correlate with the\nground truth EDA. We obtain median correlations of 0.57 to 0.63 between our\ninferred signals and the ground truth EDA using only videos of the\nparticipants' palms or foreheads or PPG signals from the foreheads or fingers.\nWe also show that sympathetic arousal is best inferred from the forehead,\nfinger, or palm.\n","authors":["Bjoern Braun","Daniel McDuff","Tadas Baltrusaitis","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2311.06930v1.pdf","comment":"Accepted and to be published at Biomedical Optics Express"},{"id":"http://arxiv.org/abs/2305.08946v4","updated":"2023-11-12T18:41:21Z","published":"2023-05-15T18:35:47Z","title":"Image Matching by Bare Homography","summary":"  This paper presents Slime, a novel non-deep image matching framework which\nmodels the scene as rough local overlapping planes. This intermediate\nrepresentation sits in-between the local affine approximation of the keypoint\npatches and the global matching based on both spatial and similarity\nconstraints, providing a progressive pruning of the correspondences, as planes\nare easier to handle with respect to general scenes.\n  Slime decomposes the images into overlapping regions at different scales and\ncomputes loose planar homographies. Planes are mutually extended by compatible\nmatches and the images are split into fixed tiles, with only the best\nhomographies retained for each pair of tiles. Stable matches are identified\naccording to the consensus of the admissible stereo configurations provided by\npairwise homographies. Within tiles, the rough planes are then merged according\nto their overlap in terms of matches and further consistent correspondences are\nextracted.\n  The whole process only involves homography constraints. As a result, both the\ncoverage and the stability of correct matches over the scene are amplified,\ntogether with the ability to spot matches in challenging scenes, allowing\ntraditional hybrid matching pipelines to make up lost ground against recent\nend-to-end deep matching methods.\n  In addition, the paper gives a thorough comparative analysis of recent\nstate-of-the-art in image matching represented by end-to-end deep networks and\nhybrid pipelines. The evaluation considers both planar and non-planar scenes,\ntaking into account critical and challenging scenarios including abrupt\ntemporal image changes and strong variations in relative image rotations.\nAccording to this analysis, although the impressive progress done in this\nfield, there is still a wide room for improvements to be investigated in future\nresearch.\n","authors":["Fabio Bellavia"],"pdf_url":"https://arxiv.org/pdf/2305.08946v4.pdf","comment":"major revision update - fixed bars in Fig. 10 and further typos"},{"id":"http://arxiv.org/abs/2307.00562v4","updated":"2023-11-12T18:17:02Z","published":"2023-07-02T13:03:39Z","title":"A MIL Approach for Anomaly Detection in Surveillance Videos from\n  Multiple Camera Views","summary":"  Occlusion and clutter are two scene states that make it difficult to detect\nanomalies in surveillance video. Furthermore, anomaly events are rare and, as a\nconsequence, class imbalance and lack of labeled anomaly data are also key\nfeatures of this task. Therefore, weakly supervised methods are heavily\nresearched for this application. In this paper, we tackle these typical\nproblems of anomaly detection in surveillance video by combining Multiple\nInstance Learning (MIL) to deal with the lack of labels and Multiple Camera\nViews (MC) to reduce occlusion and clutter effects. In the resulting MC-MIL\nalgorithm we apply a multiple camera combined loss function to train a\nregression network with Sultani's MIL ranking function. To evaluate the MC-MIL\nalgorithm first proposed here, the multiple camera PETS-2009 benchmark dataset\nwas re-labeled for the anomaly detection task from multiple camera views. The\nresult shows a significant performance improvement in F1 score compared to the\nsingle-camera configuration.\n","authors":["Silas Santiago Lopes Pereira","José Everardo Bessa Maia"],"pdf_url":"https://arxiv.org/pdf/2307.00562v4.pdf","comment":"8 Pages, 4 Figures"},{"id":"http://arxiv.org/abs/2212.11146v4","updated":"2023-11-12T18:04:24Z","published":"2022-12-13T12:42:12Z","title":"The Challenges of HTR Model Training: Feedback from the Project Donner\n  le gout de l'archive a l'ere numerique","summary":"  The arrival of handwriting recognition technologies offers new possibilities\nfor research in heritage studies. However, it is now necessary to reflect on\nthe experiences and the practices developed by research teams. Our use of the\nTranskribus platform since 2018 has led us to search for the most significant\nways to improve the performance of our handwritten text recognition (HTR)\nmodels which are made to transcribe French handwriting dating from the 17th\ncentury. This article therefore reports on the impacts of creating transcribing\nprotocols, using the language model at full scale and determining the best way\nto use base models in order to help increase the performance of HTR models.\nCombining all of these elements can indeed increase the performance of a single\nmodel by more than 20% (reaching a Character Error Rate below 5%). This article\nalso discusses some challenges regarding the collaborative nature of HTR\nplatforms such as Transkribus and the way researchers can share their data\ngenerated in the process of creating or training handwritten text recognition\nmodels.\n","authors":["Beatrice Couture","Farah Verret","Maxime Gohier","Dominique Deslandres"],"pdf_url":"https://arxiv.org/pdf/2212.11146v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00567v2","updated":"2023-11-12T17:42:07Z","published":"2023-11-01T15:07:39Z","title":"A Robust Deep Learning Method with Uncertainty Estimation for the\n  Pathological Classification of Renal Cell Carcinoma based on CT Images","summary":"  Objectives To develop and validate a deep learning-based diagnostic model\nincorporating uncertainty estimation so as to facilitate radiologists in the\npreoperative differentiation of the pathological subtypes of renal cell\ncarcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients,\npathologically proven RCC, were retrospectively collected from Center 1. By\nusing five-fold cross-validation, a deep learning model incorporating\nuncertainty estimation was developed to classify RCC subtypes into clear cell\nRCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external\nvalidation set of 78 patients from Center 2 further evaluated the model's\nperformance. Results In the five-fold cross-validation, the model's area under\nthe receiver operating characteristic curve (AUC) for the classification of\nccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI:\n0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external\nvalidation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI:\n0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC,\nrespectively. Conclusions The developed deep learning model demonstrated robust\nperformance in predicting the pathological subtypes of RCC, while the\nincorporated uncertainty emphasized the importance of understanding model\nconfidence, which is crucial for assisting clinical decision-making for\npatients with renal tumors. Clinical relevance statement Our deep learning\napproach, integrated with uncertainty estimation, offers clinicians a dual\nadvantage: accurate RCC subtype predictions complemented by diagnostic\nconfidence references, promoting informed decision-making for patients with\nRCC.\n","authors":["Ni Yao","Hang Hu","Kaicong Chen","Chen Zhao","Yuan Guo","Boya Li","Jiaofen Nan","Yanting Li","Chuang Han","Fubao Zhu","Weihua Zhou","Li Tian"],"pdf_url":"https://arxiv.org/pdf/2311.00567v2.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.06892v1","updated":"2023-11-12T16:39:02Z","published":"2023-11-12T16:39:02Z","title":"Setting a Baseline for long-shot real-time Player and Ball detection in\n  Soccer Videos","summary":"  Players and ball detection are among the first required steps on a football\nanalytics platform. Until recently, the existing open datasets on which the\nevaluations of most models were based, were not sufficient. In this work, we\npoint out their weaknesses, and with the advent of the SoccerNet v3, we propose\nand deliver to the community an edited part of its dataset, in YOLO normalized\nannotation format for training and evaluation. The code of the methods and\nmetrics are provided so that they can be used as a benchmark in future\ncomparisons. The recent YOLO8n model proves better than FootAndBall in\nlong-shot real-time detection of the ball and players on football fields.\n","authors":["Konstantinos Moutselos","Ilias Maglogiannis"],"pdf_url":"https://arxiv.org/pdf/2311.06892v1.pdf","comment":"6 pages, 4 figures, 1 table. 14th International Conference on\n  Information,Intelligence, Systems and Applications (IISA 2023) , Thessaly,\n  Volos, Greece, 10-12 July 2023"},{"id":"http://arxiv.org/abs/2308.01412v2","updated":"2023-11-12T15:53:04Z","published":"2023-08-02T20:16:13Z","title":"Achieving state-of-the-art performance in the Medical\n  Out-of-Distribution (MOOD) challenge using plausible synthetic anomalies","summary":"  The detection and localization of anomalies is one important medical image\nanalysis task. Most commonly, Computer Vision anomaly detection approaches rely\non manual annotations that are both time consuming and expensive to obtain.\nUnsupervised anomaly detection, or Out-of-Distribution detection, aims at\nidentifying anomalous samples relying only on unannotated samples considered\nnormal. In this study we present a new unsupervised anomaly detection method.\nOur method builds upon the self-supervised strategy consisting on training a\nsegmentation network to identify local synthetic anomalies. Our contributions\nimprove the synthetic anomaly generation process, making synthetic anomalies\nmore heterogeneous and challenging by 1) using complex random shapes and 2)\nsmoothing the edges of synthetic anomalies so networks cannot rely on the high\ngradient between image and synthetic anomalies. In our implementation we\nadopted standard practices in 3D medical image segmentation, including 3D U-Net\narchitecture, patch-wise training and model ensembling. Our method was\nevaluated using a validation set with different types of synthetic anomalies.\nOur experiments show that our method improved substantially the baseline method\nperformance. Additionally, we evaluated our method by participating in the\nMedical Out-of-Distribution (MOOD) Challenge held at MICCAI in 2022 and\nachieved first position in both sample-wise and pixel-wise tasks. Our\nexperiments and results in the latest MOOD challenge show that our simple yet\neffective approach can substantially improve the performance of\nOut-of-Distribution detection techniques which rely on synthetic anomalies.\n","authors":["Sergio Naval Marimont","Giacomo Tarroni"],"pdf_url":"https://arxiv.org/pdf/2308.01412v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.06868v1","updated":"2023-11-12T14:58:11Z","published":"2023-11-12T14:58:11Z","title":"Concept-wise Fine-tuning Matters in Preventing Negative Transfer","summary":"  A multitude of prevalent pre-trained models mark a major milestone in the\ndevelopment of artificial intelligence, while fine-tuning has been a common\npractice that enables pretrained models to figure prominently in a wide array\nof target datasets. Our empirical results reveal that off-the-shelf finetuning\ntechniques are far from adequate to mitigate negative transfer caused by two\ntypes of underperforming features in a pre-trained model, including rare\nfeatures and spuriously correlated features. Rooted in structural causal models\nof predictions after fine-tuning, we propose a Concept-wise fine-tuning\n(Concept-Tuning) approach which refines feature representations in the level of\npatches with each patch encoding a concept. Concept-Tuning minimizes the\nnegative impacts of rare features and spuriously correlated features by (1)\nmaximizing the mutual information between examples in the same category with\nregard to a slice of rare features (a patch) and (2) applying front-door\nadjustment via attention neural networks in channels and feature slices\n(patches). The proposed Concept-Tuning consistently and significantly (by up to\n4.76%) improves prior state-of-the-art fine-tuning methods on eleven datasets,\ndiverse pre-training strategies (supervised and self-supervised ones), various\nnetwork architectures, and sample sizes in a target dataset.\n","authors":["Yunqiao Yang","Long-Kai Huang","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2311.06868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06855v1","updated":"2023-11-12T14:12:19Z","published":"2023-11-12T14:12:19Z","title":"DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial\n  Training","summary":"  This paper focuses on the DialFRED task, which is the task of embodied\ninstruction following in a setting where an agent can actively ask questions\nabout the task. To address this task, we propose DialMAT. DialMAT introduces\nMoment-based Adversarial Training, which incorporates adversarial perturbations\ninto the latent space of language, image, and action. Additionally, it\nintroduces a crossmodal parallel feature extraction mechanism that applies\nfoundation models to both language and image. We evaluated our model using a\ndataset constructed from the DialFRED dataset and demonstrated superior\nperformance compared to the baseline method in terms of success rate and path\nweighted success rate. The model secured the top position in the DialFRED\nChallenge, which took place at the CVPR 2023 Embodied AI workshop.\n","authors":["Kanta Kaneda","Ryosuke Korekata","Yuiga Wada","Shunya Nagashima","Motonari Kambara","Yui Iioka","Haruka Matsuo","Yuto Imai","Takayuki Nishimura","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2311.06855v1.pdf","comment":"Accepted for presentation at Fourth Annual Embodied AI Workshop at\n  CVPR"},{"id":"http://arxiv.org/abs/2311.06852v1","updated":"2023-11-12T14:05:09Z","published":"2023-11-12T14:05:09Z","title":"Contrastive Learning of View-Invariant Representations for Facial\n  Expressions Recognition","summary":"  Although there has been much progress in the area of facial expression\nrecognition (FER), most existing methods suffer when presented with images that\nhave been captured from viewing angles that are non-frontal and substantially\ndifferent from those used in the training process. In this paper, we propose\nViewFX, a novel view-invariant FER framework based on contrastive learning,\ncapable of accurately classifying facial expressions regardless of the input\nviewing angles during inference. ViewFX learns view-invariant features of\nexpression using a proposed self-supervised contrastive loss which brings\ntogether different views of the same subject with a particular expression in\nthe embedding space. We also introduce a supervised contrastive loss to push\nthe learnt view-invariant features of each expression away from other\nexpressions. Since facial expressions are often distinguished with very subtle\ndifferences in the learned feature space, we incorporate the Barlow twins loss\nto reduce the redundancy and correlations of the representations in the learned\nrepresentations. The proposed method is a substantial extension of our\npreviously proposed CL-MEx, which only had a self-supervised loss. We test the\nproposed framework on two public multi-view facial expression recognition\ndatasets, KDEF and DDCF. The experiments demonstrate that our approach\noutperforms previous works in the area and sets a new state-of-the-art for both\ndatasets while showing considerably less sensitivity to challenging angles and\nthe number of output labels used for training. We also perform detailed\nsensitivity and ablation experiments to evaluate the impact of different\ncomponents of our model as well as its sensitivity to different parameters.\n","authors":["Shuvendu Roy","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2311.06852v1.pdf","comment":"Accepted in ACM Transactions on Multimedia Computing, Communications,\n  and Applications"},{"id":"http://arxiv.org/abs/2311.06845v1","updated":"2023-11-12T13:35:25Z","published":"2023-11-12T13:35:25Z","title":"Sampler Scheduler for Diffusion Models","summary":"  Diffusion modeling (DM) has high-quality generative performance, and the\nsampling problem is an important part of the DM performance. Thanks to\nefficient differential equation solvers, the sampling speed can be reduced\nwhile higher sampling quality is guaranteed. However, currently, there is a\ncontradiction in samplers for diffusion-based generative models: the mainstream\nsampler choices are diverse, each with its own characteristics in terms of\nperformance. However, only a single sampler algorithm can be specified on all\nsampling steps in the generative process. This often makes one torn between\nsampler choices; in other words, it makes it difficult to fully utilize the\nadvantages of each sampler. In this paper, we propose the feasibility of using\ndifferent samplers (ODE/SDE) on different sampling steps of the same sampling\nprocess based on analyzing and generalizing the updating formulas of each\nmainstream sampler, and experimentally demonstrate that such a multi-sampler\nscheduling improves the sampling results to some extent. In particular, we also\nverify that the combination of using SDE in the early sampling steps and ODE in\nthe later sampling steps solves the inherent problems previously caused by\nusing both singly. We show that our design changes improve the sampling\nefficiency and quality in previous work. For instance, when Number of Function\nEvaluations (NFE) = 24, the ODE Sampler Scheduler achieves a FID score of 1.91\non the CIFAR-10 dataset, compared to 2.02 for DPM++ 2M, 1.97 for DPM2, and\n11.90 for Heun for the same NFE. Meanwhile the Sampler Scheduler with the\ncombined scheduling of SDE and ODE reaches 1.899, compared to 18.63 for Euler\na, 3.14 for DPM2 a and 23.14 for DPM++ SDE.\n","authors":["Zitong Cheng"],"pdf_url":"https://arxiv.org/pdf/2311.06845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04678v2","updated":"2023-11-12T13:31:14Z","published":"2023-11-08T13:35:08Z","title":"Weakly supervised cross-modal learning in high-content screening","summary":"  With the surge in available data from various modalities, there is a growing\nneed to bridge the gap between different data types. In this work, we introduce\na novel approach to learn cross-modal representations between image data and\nmolecular representations for drug discovery. We propose EMM and IMM, two\ninnovative loss functions built on top of CLIP that leverage weak supervision\nand cross sites replicates in High-Content Screening. Evaluating our model\nagainst known baseline on cross-modal retrieval, we show that our proposed\napproach allows to learn better representations and mitigate batch effect. In\naddition, we also present a preprocessing method for the JUMP-CP dataset that\neffectively reduce the required space from 85Tb to a mere usable 7Tb size,\nstill retaining all perturbations and most of the information content.\n","authors":["Watkinson Gabriel","Cohen Ethan","Bourriez Nicolas","Bendidi Ihab","Bollot Guillaume","Genovesio Auguste"],"pdf_url":"https://arxiv.org/pdf/2311.04678v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06834v1","updated":"2023-11-12T13:19:00Z","published":"2023-11-12T13:19:00Z","title":"Osteoporosis Prediction from Hand and Wrist X-rays using Image\n  Segmentation and Self-Supervised Learning","summary":"  Osteoporosis is a widespread and chronic metabolic bone disease that often\nremains undiagnosed and untreated due to limited access to bone mineral density\n(BMD) tests like Dual-energy X-ray absorptiometry (DXA). In response to this\nchallenge, current advancements are pivoting towards detecting osteoporosis by\nexamining alternative indicators from peripheral bone areas, with the goal of\nincreasing screening rates without added expenses or time. In this paper, we\npresent a method to predict osteoporosis using hand and wrist X-ray images,\nwhich are both widely accessible and affordable, though their link to DXA-based\ndata is not thoroughly explored. Initially, our method segments the ulnar,\nradius, and metacarpal bones using a foundational model for image segmentation.\nThen, we use a self-supervised learning approach to extract meaningful\nrepresentations without the need for explicit labels, and move on to classify\nosteoporosis in a supervised manner. Our method is evaluated on a dataset with\n192 individuals, cross-referencing their verified osteoporosis conditions\nagainst the standard DXA test. With a notable classification score (AUC=0.83),\nour model represents a pioneering effort in leveraging vision-based techniques\nfor osteoporosis identification from the peripheral skeleton sites.\n","authors":["Hyungeun Lee","Ung Hwang","Seungwon Yu","Chang-Hun Lee","Kijung Yoon"],"pdf_url":"https://arxiv.org/pdf/2311.06834v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 10 pages"},{"id":"http://arxiv.org/abs/2304.02848v2","updated":"2023-11-12T13:02:46Z","published":"2023-04-06T03:25:42Z","title":"Patch-aware Batch Normalization for Improving Cross-domain Robustness","summary":"  Despite the significant success of deep learning in computer vision tasks,\ncross-domain tasks still present a challenge in which the model's performance\nwill degrade when the training set and the test set follow different\ndistributions. Most existing methods employ adversarial learning or instance\nnormalization for achieving data augmentation to solve this task. In contrast,\nconsidering that the batch normalization (BN) layer may not be robust for\nunseen domains and there exist the differences between local patches of an\nimage, we propose a novel method called patch-aware batch normalization (PBN).\nTo be specific, we first split feature maps of a batch into non-overlapping\npatches along the spatial dimension, and then independently normalize each\npatch to jointly optimize the shared BN parameter at each iteration. By\nexploiting the differences between local patches of an image, our proposed PBN\ncan effectively enhance the robustness of the model's parameters. Besides,\nconsidering the statistics from each patch may be inaccurate due to their\nsmaller size compared to the global feature maps, we incorporate the globally\naccumulated statistics with the statistics from each batch to obtain the final\nstatistics for normalizing each patch. Since the proposed PBN can replace the\ntypical BN, it can be integrated into most existing state-of-the-art methods.\nExtensive experiments and analysis demonstrate the effectiveness of our PBN in\nmultiple computer vision tasks, including classification, object detection,\ninstance retrieval, and semantic segmentation.\n","authors":["Lei Qi","Dongjia Zhao","Yinghuan Shi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2304.02848v2.pdf","comment":"We are revising this paper"},{"id":"http://arxiv.org/abs/2311.06816v1","updated":"2023-11-12T11:41:07Z","published":"2023-11-12T11:41:07Z","title":"On original and latent space connectivity in deep neural networks","summary":"  We study whether inputs from the same class can be connected by a continuous\npath, in original or latent representation space, such that all points on the\npath are mapped by the neural network model to the same class. Understanding\nhow the neural network views its own input space and how the latent spaces are\nstructured has value for explainability and robustness. We show that paths,\nlinear or nonlinear, connecting same-class inputs exist in all cases studied.\n","authors":["Boyang Gu","Anastasia Borovykh"],"pdf_url":"https://arxiv.org/pdf/2311.06816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06798v1","updated":"2023-11-12T10:21:04Z","published":"2023-11-12T10:21:04Z","title":"MetaMix: Meta-state Precision Searcher for Mixed-precision Activation\n  Quantization","summary":"  Mixed-precision quantization of efficient networks often suffer from\nactivation instability encountered in the exploration of bit selections. To\naddress this problem, we propose a novel method called MetaMix which consists\nof bit selection and weight training phases. The bit selection phase iterates\ntwo steps, (1) the mixed-precision-aware weight update, and (2) the bit-search\ntraining with the fixed mixed-precision-aware weights, both of which combined\nreduce activation instability in mixed-precision quantization and contribute to\nfast and high-quality bit selection. The weight training phase exploits the\nweights and step sizes trained in the bit selection phase and fine-tunes them\nthereby offering fast training. Our experiments with efficient and\nhard-to-quantize networks, i.e., MobileNet v2 and v3, and ResNet-18 on ImageNet\nshow that our proposed method pushes the boundary of mixed-precision\nquantization, in terms of accuracy vs. operations, by outperforming both mixed-\nand single-precision SOTA methods.\n","authors":["Han-Byul Kim","Joo Hyung Lee","Sungjoo Yoo","Hong-Seok Kim"],"pdf_url":"https://arxiv.org/pdf/2311.06798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06797v1","updated":"2023-11-12T10:19:14Z","published":"2023-11-12T10:19:14Z","title":"Dual-Branch Reconstruction Network for Industrial Anomaly Detection with\n  RGB-D Data","summary":"  Unsupervised anomaly detection methods are at the forefront of industrial\nanomaly detection efforts and have made notable progress. Previous work\nprimarily used 2D information as input, but multi-modal industrial anomaly\ndetection based on 3D point clouds and RGB images is just beginning to emerge.\nThe regular approach involves utilizing large pre-trained models for feature\nrepresentation and storing them in memory banks. However, the above methods\nrequire a longer inference time and higher memory usage, which cannot meet the\nreal-time requirements of the industry. To overcome these issues, we propose a\nlightweight dual-branch reconstruction network(DBRN) based on RGB-D input,\nlearning the decision boundary between normal and abnormal examples. The\nrequirement for alignment between the two modalities is eliminated by using\ndepth maps instead of point cloud input. Furthermore, we introduce an\nimportance scoring module in the discriminative network to assist in fusing\nfeatures from these two modalities, thereby obtaining a comprehensive\ndiscriminative result. DBRN achieves 92.8% AUROC with high inference efficiency\non the MVTec 3D-AD dataset without large pre-trained models and memory banks.\n","authors":["Chenyang Bi","Yueyang Li","Haichi Luo"],"pdf_url":"https://arxiv.org/pdf/2311.06797v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.06796v1","updated":"2023-11-12T10:16:42Z","published":"2023-11-12T10:16:42Z","title":"Deep Perspective Transformation Based Vehicle Localization on Bird's Eye\n  View","summary":"  An accurate understanding of a self-driving vehicle's surrounding environment\nis crucial for its navigation system. To enhance the effectiveness of existing\nalgorithms and facilitate further research, it is essential to provide\ncomprehensive data to the routing system. Traditional approaches rely on\ninstalling multiple sensors to simulate the environment, leading to high costs\nand complexity. In this paper, we propose an alternative solution by generating\na top-down representation of the scene, enabling the extraction of distances\nand directions of other cars relative to the ego vehicle. We introduce a new\nsynthesized dataset that offers extensive information about the ego vehicle and\nits environment in each frame, providing valuable resources for similar\ndownstream tasks. Additionally, we present an architecture that transforms\nperspective view RGB images into bird's-eye-view maps with segmented\nsurrounding vehicles. This approach offers an efficient and cost-effective\nmethod for capturing crucial environmental information for self-driving cars.\nCode and dataset are available at\nhttps://github.com/IPM-HPC/Perspective-BEV-Transformer.\n","authors":["Abtin Mahyar","Hossein Motamednia","Dara Rahmati"],"pdf_url":"https://arxiv.org/pdf/2311.06796v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.15590v2","updated":"2023-11-12T10:10:24Z","published":"2023-10-24T07:54:39Z","title":"Facial Data Minimization: Shallow Model as Your Privacy Filter","summary":"  Face recognition service has been used in many fields and brings much\nconvenience to people. However, once the user's facial data is transmitted to a\nservice provider, the user will lose control of his/her private data. In recent\nyears, there exist various security and privacy issues due to the leakage of\nfacial data. Although many privacy-preserving methods have been proposed, they\nusually fail when they are not accessible to adversaries' strategies or\nauxiliary data. Hence, in this paper, by fully considering two cases of\nuploading facial images and facial features, which are very typical in face\nrecognition service systems, we proposed a data privacy minimization\ntransformation (PMT) method. This method can process the original facial data\nbased on the shallow model of authorized services to obtain the obfuscated\ndata. The obfuscated data can not only maintain satisfactory performance on\nauthorized models and restrict the performance on other unauthorized models but\nalso prevent original privacy data from leaking by AI methods and human visual\ntheft. Additionally, since a service provider may execute preprocessing\noperations on the received data, we also propose an enhanced perturbation\nmethod to improve the robustness of PMT. Besides, to authorize one facial image\nto multiple service models simultaneously, a multiple restriction mechanism is\nproposed to improve the scalability of PMT. Finally, we conduct extensive\nexperiments and evaluate the effectiveness of the proposed PMT in defending\nagainst face reconstruction, data abuse, and face attribute estimation attacks.\nThese experimental results demonstrate that PMT performs well in preventing\nfacial data abuse and privacy leakage while maintaining face recognition\naccuracy.\n","authors":["Yuwen Pu","Jiahao Chen","Jiayu Pan","Hao li","Diqun Yan","Xuhong Zhang","Shouling Ji"],"pdf_url":"https://arxiv.org/pdf/2310.15590v2.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2311.06792v1","updated":"2023-11-12T10:03:32Z","published":"2023-11-12T10:03:32Z","title":"IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion\n  Models","summary":"  We present a diffusion-based image morphing approach with\nperceptually-uniform sampling (IMPUS) that produces smooth, direct, and\nrealistic interpolations given an image pair. A latent diffusion model has\ndistinct conditional distributions and data embeddings for each of the two\nimages, especially when they are from different classes. To bridge this gap, we\ninterpolate in the locally linear and continuous text embedding space and\nGaussian latent space. We first optimize the endpoint text embeddings and then\nmap the images to the latent space using a probability flow ODE. Unlike\nexisting work that takes an indirect morphing path, we show that the model\nadaptation yields a direct path and suppresses ghosting artifacts in the\ninterpolated images. To achieve this, we propose an adaptive bottleneck\nconstraint based on a novel relative perceptual path diversity score that\nautomatically controls the bottleneck size and balances the diversity along the\npath with its directness. We also propose a perceptually-uniform sampling\ntechnique that enables visually smooth changes between the interpolated images.\nExtensive experiments validate that our IMPUS can achieve smooth, direct, and\nrealistic image morphing and be applied to other image generation tasks.\n","authors":["Zhaoyuan Yang","Zhengyang Yu","Zhiwei Xu","Jaskirat Singh","Jing Zhang","Dylan Campbell","Peter Tu","Richard Hartley"],"pdf_url":"https://arxiv.org/pdf/2311.06792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06791v1","updated":"2023-11-12T09:58:16Z","published":"2023-11-12T09:58:16Z","title":"InfMLLM: A Unified Framework for Visual-Language Tasks","summary":"  Large language models (LLMs) have proven their remarkable versatility in\nhandling a comprehensive range of language-centric applications. To expand\nLLMs' capabilities to a broader spectrum of modal inputs, multimodal large\nlanguage models (MLLMs) have attracted growing interest. This work delves into\nenabling LLMs to tackle more vision-language-related tasks, particularly image\ncaptioning, visual question answering (VQA,) and visual grounding. To this end,\nwe implemented a three-stage training scheme: starting with lightweight\nalignment pretraining, then moderate-weight multitask hybrid training, and\nfinally, LLM fine-tuning to improve instruction following capability.\nThroughout the training process, the requirements on GPU memory gradually\nincrease. To effectively manage the number of visual embeddings passed to the\nLLM while preserving their positional information, we introduce a\nstraightforward visual adapter module dubbed pool-adapter. Our experiments\ndemonstrate that preserving the positional information of visual embeddings\nthrough the pool-adapter is particularly beneficial for tasks like visual\ngrounding. We name our proposed approach InfMLLM and have evaluated it\nextensively on various benchmark datasets. Our results demonstrate that InfMLLM\nachieves either state-of-the-art (SOTA) performance or performance comparable\nto recent MLLMs. The code and model will be made open-source at:\n\\url{https://github.com/mightyzau/InfMLLM}.\n","authors":["Qiang Zhou","Zhibin Wang","Wei Chu","Yinghui Xu","Hao Li","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2311.06791v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2306.06874v4","updated":"2023-11-12T09:52:09Z","published":"2023-06-12T05:14:13Z","title":"VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion\n  Models","summary":"  Diffusion Models (DMs) are state-of-the-art generative models that learn a\nreversible corruption process from iterative noise addition and denoising. They\nare the backbone of many generative AI applications, such as text-to-image\nconditional generation. However, recent studies have shown that basic\nunconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a\ntype of output manipulation attack triggered by a maliciously embedded pattern\nat model input. This paper presents a unified backdoor attack framework\n(VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our\nframework covers mainstream unconditional and conditional DMs (denoising-based\nand score-based) and various training-free samplers for holistic evaluations.\nExperiments show that our unified framework facilitates the backdoor analysis\nof different DM configurations and provides new insights into caption-based\nbackdoor attacks on DMs. Our code is available on GitHub:\n\\url{https://github.com/IBM/villandiffusion}\n","authors":["Sheng-Yen Chou","Pin-Yu Chen","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2306.06874v4.pdf","comment":"Accepted by NeurIPS 2023, NeurIPS 2023 BUGS Workshop Oral"},{"id":"http://arxiv.org/abs/2311.06786v1","updated":"2023-11-12T09:23:40Z","published":"2023-11-12T09:23:40Z","title":"Explainability of Vision Transformers: A Comprehensive Review and New\n  Perspectives","summary":"  Transformers have had a significant impact on natural language processing and\nhave recently demonstrated their potential in computer vision. They have shown\npromising results over convolution neural networks in fundamental computer\nvision tasks. However, the scientific community has not fully grasped the inner\nworkings of vision transformers, nor the basis for their decision-making, which\nunderscores the importance of explainability methods. Understanding how these\nmodels arrive at their decisions not only improves their performance but also\nbuilds trust in AI systems. This study explores different explainability\nmethods proposed for visual transformers and presents a taxonomy for organizing\nthem according to their motivations, structures, and application scenarios. In\naddition, it provides a comprehensive review of evaluation criteria that can be\nused for comparing explanation results, as well as explainability tools and\nframeworks. Finally, the paper highlights essential but unexplored aspects that\ncan enhance the explainability of visual transformers, and promising research\ndirections are suggested for future investment.\n","authors":["Rojina Kashefi","Leili Barekatain","Mohammad Sabokrou","Fatemeh Aghaeipoor"],"pdf_url":"https://arxiv.org/pdf/2311.06786v1.pdf","comment":"20 pages,5 figures"},{"id":"http://arxiv.org/abs/2311.06783v1","updated":"2023-11-12T09:10:51Z","published":"2023-11-12T09:10:51Z","title":"Q-Instruct: Improving Low-level Visual Abilities for Multi-modality\n  Foundation Models","summary":"  Multi-modality foundation models, as represented by GPT-4V, have brought a\nnew paradigm for low-level visual perception and understanding tasks, that can\nrespond to a broad range of natural human instructions in a model. While\nexisting foundation models have shown exciting potentials on low-level visual\ntasks, their related abilities are still preliminary and need to be improved.\nIn order to enhance these models, we conduct a large-scale subjective\nexperiment collecting a vast number of real human feedbacks on low-level\nvision. Each feedback follows a pathway that starts with a detailed description\non the low-level visual appearance (*e.g. clarity, color, brightness* of an\nimage, and ends with an overall conclusion, with an average length of 45 words.\nThe constructed **Q-Pathway** dataset includes 58K detailed human feedbacks on\n18,973 images with diverse low-level appearance. Moreover, to enable foundation\nmodels to robustly respond to diverse types of questions, we design a\nGPT-participated conversion to process these feedbacks into diverse-format 200K\ninstruction-response pairs. Experimental results indicate that the\n**Q-Instruct** consistently elevates low-level perception and understanding\nabilities across several foundational models. We anticipate that our datasets\ncan pave the way for a future that general intelligence can perceive,\nunderstand low-level visual appearance and evaluate visual quality like a\nhuman. Our dataset, model zoo, and demo is published at:\nhttps://q-future.github.io/Q-Instruct.\n","authors":["Haoning Wu","Zicheng Zhang","Erli Zhang","Chaofeng Chen","Liang Liao","Annan Wang","Kaixin Xu","Chunyi Li","Jingwen Hou","Guangtao Zhai","Geng Xue","Wenxiu Sun","Qiong Yan","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2311.06783v1.pdf","comment":"16 pages, 11 figures, page 12-16 as appendix"},{"id":"http://arxiv.org/abs/2311.06772v1","updated":"2023-11-12T08:29:41Z","published":"2023-11-12T08:29:41Z","title":"ChatAnything: Facetime Chat with LLM-Enhanced Personas","summary":"  In this technical report, we target generating anthropomorphized personas for\nLLM-based characters in an online manner, including visual appearance,\npersonality and tones, with only text descriptions. To achieve this, we first\nleverage the in-context learning capability of LLMs for personality generation\nby carefully designing a set of system prompts. We then propose two novel\nconcepts: the mixture of voices (MoV) and the mixture of diffusers (MoD) for\ndiverse voice and appearance generation. For MoV, we utilize the text-to-speech\n(TTS) algorithms with a variety of pre-defined tones and select the most\nmatching one based on the user-provided text description automatically. For\nMoD, we combine the recent popular text-to-image generation techniques and\ntalking head algorithms to streamline the process of generating talking\nobjects. We termed the whole framework as ChatAnything. With it, users could be\nable to animate anything with any personas that are anthropomorphic using just\na few text inputs. However, we have observed that the anthropomorphic objects\nproduced by current generative models are often undetectable by pre-trained\nface landmark detectors, leading to failure of the face motion generation, even\nif these faces possess human-like appearances because those images are nearly\nseen during the training (e.g., OOD samples). To address this issue, we\nincorporate pixel-level guidance to infuse human face landmarks during the\nimage generation phase. To benchmark these metrics, we have built an evaluation\ndataset. Based on it, we verify that the detection rate of the face landmark is\nsignificantly increased from 57.0% to 92.5% thus allowing automatic face\nanimation based on generated speech content. The code and more results can be\nfound at https://chatanything.github.io/.\n","authors":["Yilin Zhao","Xinbin Yuan","Shanghua Gao","Zhijie Lin","Qibin Hou","Jiashi Feng","Daquan Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.06772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09338v3","updated":"2023-11-12T07:13:58Z","published":"2023-06-15T17:59:27Z","title":"Understanding Optimization of Deep Learning via Jacobian Matrix and\n  Lipschitz Constant","summary":"  This article provides a comprehensive understanding of optimization in deep\nlearning, with a primary focus on the challenges of gradient vanishing and\ngradient exploding, which normally lead to diminished model representational\nability and training instability, respectively. We analyze these two challenges\nthrough several strategic measures, including the improvement of gradient flow\nand the imposition of constraints on a network's Lipschitz constant. To help\nunderstand the current optimization methodologies, we categorize them into two\nclasses: explicit optimization and implicit optimization. Explicit optimization\nmethods involve direct manipulation of optimizer parameters, including weight,\ngradient, learning rate, and weight decay. Implicit optimization methods, by\ncontrast, focus on improving the overall landscape of a network by enhancing\nits modules, such as residual shortcuts, normalization methods, attention\nmechanisms, and activations. In this article, we provide an in-depth analysis\nof these two optimization classes and undertake a thorough examination of the\nJacobian matrices and the Lipschitz constants of many widely used deep learning\nmodules, highlighting existing issues as well as potential improvements.\nMoreover, we also conduct a series of analytical experiments to substantiate\nour theoretical discussions. This article does not aim to propose a new\noptimizer or network. Rather, our intention is to present a comprehensive\nunderstanding of optimization in deep learning. We hope that this article will\nassist readers in gaining a deeper insight in this field and encourages the\ndevelopment of more robust, efficient, and high-performing models.\n","authors":["Xianbiao Qi","Jianan Wang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.09338v3.pdf","comment":"International Digital Economy Academy (IDEA)"},{"id":"http://arxiv.org/abs/2311.02877v3","updated":"2023-11-12T06:35:27Z","published":"2023-11-06T05:14:24Z","title":"Inner-IoU: More Effective Intersection over Union Loss with Auxiliary\n  Bounding Box","summary":"  With the rapid development of detectors, Bounding Box Regression (BBR) loss\nfunction has constantly updated and optimized. However, the existing IoU-based\nBBR still focus on accelerating convergence by adding new loss terms, ignoring\nthe limitations of IoU loss term itself. Although theoretically IoU loss can\neffectively describe the state of bounding box regression,in practical\napplications, it cannot adjust itself according to different detectors and\ndetection tasks, and does not have strong generalization. Based on the above,\nwe first analyzed the BBR model and concluded that distinguishing different\nregression samples and using different scales of auxiliary bounding boxes to\ncalculate losses can effectively accelerate the bounding box regression\nprocess. For high IoU samples, using smaller auxiliary bounding boxes to\ncalculate losses can accelerate convergence, while larger auxiliary bounding\nboxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which\ncalculates IoU loss through auxiliary bounding boxes. For different datasets\nand detectors, we introduce a scaling factor ratio to control the scale size of\nthe auxiliary bounding boxes for calculating losses. Finally, integrate\nInner-IoU into the existing IoU-based loss functions for simulation and\ncomparative experiments. The experiment result demonstrate a further\nenhancement in detection performance with the utilization of the method\nproposed in this paper, verifying the effectiveness and generalization ability\nof Inner-IoU loss. Code is available at https://github.com/Instinct323/wiou.\n","authors":["Hao Zhang","Cong Xu","Shuaijie Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.02877v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08233v2","updated":"2023-11-12T06:24:25Z","published":"2023-06-14T04:11:38Z","title":"OT-Net: A Reusable Neural Optimal Transport Solver","summary":"  With the widespread application of optimal transport (OT), its calculation\nbecomes essential, and various algorithms have emerged. However, the existing\nmethods either have low efficiency or cannot represent discontinuous maps. A\nnovel reusable neural OT solver OT-Net is thus presented, which first learns\nBrenier's height representation via the neural network to obtain its potential,\nand then gained the OT map by computing the gradient of the potential. The\nalgorithm has two merits, 1) it can easily represent discontinuous maps, which\nallows it to match any target distribution with discontinuous supports and\nachieve sharp boundaries. This can well eliminate mode collapse in the\ngenerated models. 2) The OT map can be calculated straightly by the proposed\nalgorithm when new target samples are added, which greatly improves the\nefficiency and reusability of the map. Moreover, the theoretical error bound of\nthe algorithm is analyzed, and we have demonstrated the empirical success of\nour approach in image generation, color transfer, and domain adaptation.\n","authors":["Zezeng Li","Shenghao Li","Lianbao Jin","Na Lei","Zhongxuan Luo"],"pdf_url":"https://arxiv.org/pdf/2306.08233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06749v1","updated":"2023-11-12T06:23:33Z","published":"2023-11-12T06:23:33Z","title":"Aggregate, Decompose, and Fine-Tune: A Simple Yet Effective\n  Factor-Tuning Method for Vision Transformer","summary":"  Recent advancements have illuminated the efficacy of some\ntensorization-decomposition Parameter-Efficient Fine-Tuning methods like LoRA\nand FacT in the context of Vision Transformers (ViT). However, these methods\ngrapple with the challenges of inadequately addressing inner- and cross-layer\nredundancy. To tackle this issue, we introduce EFfective Factor-Tuning (EFFT),\na simple yet effective fine-tuning method. Within the VTAB-1K dataset, our EFFT\nsurpasses all baselines, attaining state-of-the-art performance with a\ncategorical average of 75.9% in top-1 accuracy with only 0.28% of the\nparameters for full fine-tuning. Considering the simplicity and efficacy of\nEFFT, it holds the potential to serve as a foundational benchmark. The code and\nmodel are now available at\nhttps://github.com/Dongping-Chen/EFFT-EFfective-Factor-Tuning.\n","authors":["Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2311.06749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.03461v3","updated":"2023-11-12T06:12:02Z","published":"2022-10-07T11:16:36Z","title":"FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using\n  Style Representations","summary":"  In recent years, language-driven artistic style transfer has emerged as a new\ntype of style transfer technique, eliminating the need for a reference style\nimage by using natural language descriptions of the style. The first model to\nachieve this, called CLIPstyler, has demonstrated impressive stylisation\nresults. However, its lengthy optimisation procedure at runtime for each query\nlimits its suitability for many practical applications. In this work, we\npresent FastCLIPstyler, a generalised text-based image style transfer model\ncapable of stylising images in a single forward pass for arbitrary text inputs.\nFurthermore, we introduce EdgeCLIPstyler, a lightweight model designed for\ncompatibility with resource-constrained devices. Through quantitative and\nqualitative comparisons with state-of-the-art approaches, we demonstrate that\nour models achieve superior stylisation quality based on measurable metrics\nwhile offering significantly improved runtime efficiency, particularly on edge\ndevices.\n","authors":["Ananda Padhmanabhan Suresh","Sanjana Jain","Pavit Noinongyao","Ankush Ganguly"],"pdf_url":"https://arxiv.org/pdf/2210.03461v3.pdf","comment":"Accepted at the 2024 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2024)"},{"id":"http://arxiv.org/abs/2311.06746v1","updated":"2023-11-12T05:57:56Z","published":"2023-11-12T05:57:56Z","title":"Two Stream Scene Understanding on Graph Embedding","summary":"  The paper presents a novel two-stream network architecture for enhancing\nscene understanding in computer vision. This architecture utilizes a graph\nfeature stream and an image feature stream, aiming to merge the strengths of\nboth modalities for improved performance in image classification and scene\ngraph generation tasks. The graph feature stream network comprises a\nsegmentation structure, scene graph generation, and a graph representation\nmodule. The segmentation structure employs the UPSNet architecture with a\nbackbone that can be a residual network, Vit, or Swin Transformer. The scene\ngraph generation component focuses on extracting object labels and neighborhood\nrelationships from the semantic map to create a scene graph. Graph\nConvolutional Networks (GCN), GraphSAGE, and Graph Attention Networks (GAT) are\nemployed for graph representation, with an emphasis on capturing node features\nand their interconnections. The image feature stream network, on the other\nhand, focuses on image classification through the use of Vision Transformer and\nSwin Transformer models. The two streams are fused using various data fusion\nmethods. This fusion is designed to leverage the complementary strengths of\ngraph-based and image-based features.Experiments conducted on the ADE20K\ndataset demonstrate the effectiveness of the proposed two-stream network in\nimproving image classification accuracy compared to conventional methods. This\nresearch provides a significant contribution to the field of computer vision,\nparticularly in the areas of scene understanding and image classification, by\neffectively combining graph-based and image-based approaches.\n","authors":["Wenkai Yang","Wenyuan Sun","Runxaing Huang"],"pdf_url":"https://arxiv.org/pdf/2311.06746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16045v2","updated":"2023-11-12T05:33:38Z","published":"2023-06-28T09:28:33Z","title":"OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection","summary":"  Since the strong comorbid similarity in NDDs, such as attention-deficit\nhyperactivity disorder, can interfere with the accurate diagnosis of autism\nspectrum disorder (ASD), identifying unknown classes is extremely crucial and\nchallenging from NDDs. We design a novel open set recognition framework for\nASD-aided diagnosis (OpenNDD), which trains a model by combining autoencoder\nand adversarial reciprocal points learning to distinguish in-distribution and\nout-of-distribution categories as well as identify ASD accurately. Considering\nthe strong similarities between NDDs, we present a joint scaling method by\nMin-Max scaling combined with Standardization (MMS) to increase the differences\nbetween classes for better distinguishing unknown NDDs. We conduct the\nexperiments in the hybrid datasets from Autism Brain Imaging Data Exchange I\n(ABIDE I) and THE ADHD-200 SAMPLE (ADHD-200) with 791 samples from four sites\nand the results demonstrate the superiority on various metrics. Our OpenNDD\nachieves promising performance, where the accuracy is 77.38%, AUROC is 75.53%\nand the open set classification rate is as high as 59.43%.\n","authors":["Jiaming Yu","Zihao Guan","Xinyue Chang","Shujie Liu","Zhenshan Shi","Xiumei Liu","Changcai Yang","Riqing Chen","Lanyan Xue","Lifang Wei"],"pdf_url":"https://arxiv.org/pdf/2306.16045v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2302.11728v3","updated":"2023-11-12T03:46:41Z","published":"2023-02-23T01:27:57Z","title":"A Convolutional-Transformer Network for Crack Segmentation with Boundary\n  Awareness","summary":"  Cracks play a crucial role in assessing the safety and durability of\nmanufactured buildings. However, the long and sharp topological features and\ncomplex background of cracks make the task of crack segmentation extremely\nchallenging. In this paper, we propose a novel convolutional-transformer\nnetwork based on encoder-decoder architecture to solve this challenge.\nParticularly, we designed a Dilated Residual Block (DRB) and a Boundary\nAwareness Module (BAM). The DRB pays attention to the local detail of cracks\nand adjusts the feature dimension for other blocks as needed. And the BAM\nlearns the boundary features from the dilated crack label. Furthermore, the DRB\nis combined with a lightweight transformer that captures global information to\nserve as an effective encoder. Experimental results show that the proposed\nnetwork performs better than state-of-the-art algorithms on two typical\ndatasets. Datasets, code, and trained models are available for research at\nhttps://github.com/HqiTao/CT-crackseg.\n","authors":["Huaqi Tao","Bingxi Liu","Jinqiang Cui","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.11728v3.pdf","comment":"Accepted to ICIP 2023"},{"id":"http://arxiv.org/abs/2209.08355v4","updated":"2023-11-12T03:34:57Z","published":"2022-09-17T15:47:01Z","title":"Towards Connectivity-Aware Pulmonary Airway Segmentation","summary":"  Detailed pulmonary airway segmentation is a clinically important task for\nendobronchial intervention and treatment of peripheral located lung cancer\nlesions. Convolutional Neural Networks (CNNs) are promising tools for medical\nimage analysis but have been performing poorly for cases when existing a\nsignificant imbalanced feature distribution, which is true for the airway data\nas the trachea and principal bronchi dominate most of the voxels whereas the\nlobar bronchi and distal segmental bronchi occupy a small proportion. In this\npaper, we propose a Differentiable Topology-Preserved Distance Transform\n(DTPDT) framework to improve the performance of airway segmentation. A\nTopology-Preserved Surrogate (TPS) learning strategy is first proposed to\nbalance the training progress within-class distribution. Furthermore, a\nConvolutional Distance Transform (CDT) is designed to identify the breakage\nphenomenon with superior sensitivity and minimize the variation of the distance\nmap between the predictionand ground-truth. The proposed method is validated\nwith the publically available reference airway segmentation datasets. The\ndetected rate of branch and length on public EXACT'09 and BAS datasets are\n82.1%/79.6% and 96.5%/91.5% respectively, demonstrating the reliability and\nefficiency of the method in terms of improving the topology completeness of the\nsegmentation performance while maintaining the overall topology accuracy.\n","authors":["Minghui Zhang","Guang-Zhong Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2209.08355v4.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2307.01097v5","updated":"2023-11-12T03:20:16Z","published":"2023-07-03T15:19:17Z","title":"MVDiffusion: Enabling Holistic Multi-view Image Generation with\n  Correspondence-Aware Diffusion","summary":"  This paper introduces MVDiffusion, a simple yet effective method for\ngenerating consistent multi-view images from text prompts given pixel-to-pixel\ncorrespondences (e.g., perspective crops from a panorama or multi-view images\ngiven depth maps and poses). Unlike prior methods that rely on iterative image\nwarping and inpainting, MVDiffusion simultaneously generates all images with a\nglobal awareness, effectively addressing the prevalent error accumulation\nissue. At its core, MVDiffusion processes perspective images in parallel with a\npre-trained text-to-image diffusion model, while integrating novel\ncorrespondence-aware attention layers to facilitate cross-view interactions.\nFor panorama generation, while only trained with 10k panoramas, MVDiffusion is\nable to generate high-resolution photorealistic images for arbitrary texts or\nextrapolate one perspective image to a 360-degree view. For multi-view\ndepth-to-image generation, MVDiffusion demonstrates state-of-the-art\nperformance for texturing a scene mesh. The project page is at\nhttps://mvdiffusion.github.io/.\n","authors":["Shitao Tang","Fuyang Zhang","Jiacheng Chen","Peng Wang","Yasutaka Furukawa"],"pdf_url":"https://arxiv.org/pdf/2307.01097v5.pdf","comment":"Project page, https://mvdiffusion.github.io, camera ready"},{"id":"http://arxiv.org/abs/2307.02881v5","updated":"2023-11-12T01:52:35Z","published":"2023-07-06T09:36:45Z","title":"Probabilistic and Semantic Descriptions of Image Manifolds and Their\n  Applications","summary":"  This paper begins with a description of methods for estimating image\nprobability density functions that reflects the observation that such data is\nusually constrained to lie in restricted regions of the high-dimensional image\nspace-not every pattern of pixels is an image. It is common to say that images\nlie on a lower-dimensional manifold in the high-dimensional space. However, it\nis not the case that all points on the manifold have an equal probability of\nbeing images. Images are unevenly distributed on the manifold, and our task is\nto devise ways to model this distribution as a probability distribution. We\ntherefore consider popular generative models. For our purposes,\ngenerative/probabilistic models should have the properties of 1) sample\ngeneration: the possibility to sample from this distribution with the modelled\ndensity function, and 2) probability computation: given a previously unseen\nsample from the dataset of interest, one should be able to compute its\nprobability, at least up to a normalising constant. To this end, we investigate\nthe use of methods such as normalising flow and diffusion models. We then show\nhow semantic interpretations are used to describe points on the manifold. To\nachieve this, we consider an emergent language framework that uses variational\nencoders for a disentangled representation of points that reside on a given\nmanifold. Trajectories between points on a manifold can then be described as\nevolving semantic descriptions. We also show that such probabilistic\ndescriptions (bounded) can be used to improve semantic consistency by\nconstructing defences against adversarial attacks. We evaluate our methods with\nimproved semantic robustness and OoD detection capability, explainable and\neditable semantic interpolation, and improved classification accuracy under\npatch attacks. We also discuss the limitation in diffusion models.\n","authors":["Peter Tu","Zhaoyuan Yang","Richard Hartley","Zhiwei Xu","Jing Zhang","Yiwei Fu","Dylan Campbell","Jaskirat Singh","Tianyu Wang"],"pdf_url":"https://arxiv.org/pdf/2307.02881v5.pdf","comment":"26 pages, 17 figures, 1 table, accepted to Frontiers in Computer\n  Science, 2023"},{"id":"http://arxiv.org/abs/2311.06694v1","updated":"2023-11-12T00:21:58Z","published":"2023-11-12T00:21:58Z","title":"Comparative Multi-View Language Grounding","summary":"  In this work, we consider the task of resolving object referents when given a\ncomparative language description. We present a Multi-view Approach to Grounding\nin Context (MAGiC) that leverages transformers to pragmatically reason over\nboth objects given multiple image views and a language description. In contrast\nto past efforts that attempt to connect vision and language for this task\nwithout fully considering the resulting referential context, MAGiC makes use of\nthe comparative information by jointly reasoning over multiple views of both\nobject referent candidates and the referring language expression. We present an\nanalysis demonstrating that comparative reasoning contributes to SOTA\nperformance on the SNARE object reference task.\n","authors":["Chancharik Mitra","Abrar Anwar","Rodolfo Corona","Dan Klein","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2311.06694v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.16452v2","updated":"2023-11-12T22:38:04Z","published":"2023-10-25T08:14:49Z","title":"Faithful Path Language Modelling for Explainable Recommendation over\n  Knowledge Graph","summary":"  Path reasoning methods over knowledge graphs have gained popularity for their\npotential to improve transparency in recommender systems. However, the\nresulting models still rely on pre-trained knowledge graph embeddings, fail to\nfully exploit the interdependence between entities and relations in the KG for\nrecommendation, and may generate inaccurate explanations. In this paper, we\nintroduce PEARLM, a novel approach that efficiently captures user behaviour and\nproduct-side knowledge through language modelling. With our approach, knowledge\ngraph embeddings are directly learned from paths over the KG by the language\nmodel, which also unifies entities and relations in the same optimisation\nspace. Constraints on the sequence decoding additionally guarantee path\nfaithfulness with respect to the KG. Experiments on two datasets show the\neffectiveness of our approach compared to state-of-the-art baselines. Source\ncode and datasets: AVAILABLE AFTER GETTING ACCEPTED.\n","authors":["Giacomo Balloccu","Ludovico Boratto","Christian Cancedda","Gianni Fenu","Mirko Marras"],"pdf_url":"https://arxiv.org/pdf/2310.16452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06794v1","updated":"2023-11-12T10:07:03Z","published":"2023-11-12T10:07:03Z","title":"CL-Flow:Strengthening the Normalizing Flows by Contrastive Learning for\n  Better Anomaly Detection","summary":"  In the anomaly detection field, the scarcity of anomalous samples has\ndirected the current research emphasis towards unsupervised anomaly detection.\nWhile these unsupervised anomaly detection methods offer convenience, they also\noverlook the crucial prior information embedded within anomalous samples.\nMoreover, among numerous deep learning methods, supervised methods generally\nexhibit superior performance compared to unsupervised methods. Considering the\nreasons mentioned above, we propose a self-supervised anomaly detection\napproach that combines contrastive learning with 2D-Flow to achieve more\nprecise detection outcomes and expedited inference processes. On one hand, we\nintroduce a novel approach to anomaly synthesis, yielding anomalous samples in\naccordance with authentic industrial scenarios, alongside their surrogate\nannotations. On the other hand, having obtained a substantial number of\nanomalous samples, we enhance the 2D-Flow framework by incorporating\ncontrastive learning, leveraging diverse proxy tasks to fine-tune the network.\nOur approach enables the network to learn more precise mapping relationships\nfrom self-generated labels while retaining the lightweight characteristics of\nthe 2D-Flow. Compared to mainstream unsupervised approaches, our\nself-supervised method demonstrates superior detection accuracy, fewer\nadditional model parameters, and faster inference speed. Furthermore, the\nentire training and inference process is end-to-end. Our approach showcases new\nstate-of-the-art results, achieving a performance of 99.6\\% in image-level\nAUROC on the MVTecAD dataset and 96.8\\% in image-level AUROC on the BTAD\ndataset.\n","authors":["Shunfeng Wang","Yueyang Li","Haichi Luo","Chenyang Bi"],"pdf_url":"https://arxiv.org/pdf/2311.06794v1.pdf","comment":"6 pages,6 figures"},{"id":"http://arxiv.org/abs/2311.06777v1","updated":"2023-11-12T08:46:07Z","published":"2023-11-12T08:46:07Z","title":"Alleviating Behavior Data Imbalance for Multi-Behavior Graph\n  Collaborative Filtering","summary":"  Graph collaborative filtering, which learns user and item representations\nthrough message propagation over the user-item interaction graph, has been\nshown to effectively enhance recommendation performance. However, most current\ngraph collaborative filtering models mainly construct the interaction graph on\na single behavior domain (e.g. click), even though users exhibit various types\nof behaviors on real-world platforms, including actions like click, cart, and\npurchase. Furthermore, due to variations in user engagement, there exists an\nimbalance in the scale of different types of behaviors. For instance, users may\nclick and view multiple items but only make selective purchases from a small\nsubset of them. How to alleviate the behavior imbalance problem and utilize\ninformation from the multiple behavior graphs concurrently to improve the\ntarget behavior conversion (e.g. purchase) remains underexplored. To this end,\nwe propose IMGCF, a simple but effective model to alleviate behavior data\nimbalance for multi-behavior graph collaborative filtering. Specifically, IMGCF\nutilizes a multi-task learning framework for collaborative filtering on\nmulti-behavior graphs. Then, to mitigate the data imbalance issue, IMGCF\nimproves representation learning on the sparse behavior by leveraging\nrepresentations learned from the behavior domain with abundant data volumes.\nExperiments on two widely-used multi-behavior datasets demonstrate the\neffectiveness of IMGCF.\n","authors":["Yijie Zhang","Yuanchen Bei","Shiqi Yang","Hao Chen","Zhiqing Li","Lijia Chen","Feiran Huang"],"pdf_url":"https://arxiv.org/pdf/2311.06777v1.pdf","comment":"accepted by ICDM2023 Workshop"},{"id":"http://arxiv.org/abs/2311.06714v1","updated":"2023-11-12T02:54:11Z","published":"2023-11-12T02:54:11Z","title":"What factors influence the popularity of user-generated text in the\n  creative domain? A case study of book reviews","summary":"  This study investigates a range of psychological, lexical, semantic, and\nreadability features of book reviews to elucidate the factors underlying their\nperceived popularity. To this end, we conduct statistical analyses of various\nfeatures, including the types and frequency of opinion and emotion-conveying\nterms, connectives, character mentions, word uniqueness, commonness, and\nsentence structure, among others. Additionally, we utilize two readability\ntests to explore whether reading ease is positively associated with review\npopularity. Finally, we employ traditional machine learning classifiers and\ntransformer-based fine-tuned language models with n-gram features to\nautomatically determine review popularity. Our findings indicate that, with the\nexception of a few features (e.g., review length, emotions, and word\nuniqueness), most attributes do not exhibit significant differences between\npopular and non-popular review groups. Furthermore, the poor performance of\nmachine learning classifiers using the word n-gram feature highlights the\nchallenges associated with determining popularity in creative domains. Overall,\nour study provides insights into the factors underlying review popularity and\nhighlights the need for further research in this area, particularly in the\ncreative realm.\n","authors":["Salim Sazzed"],"pdf_url":"https://arxiv.org/pdf/2311.06714v1.pdf","comment":"Accepted in 22nd IEEE International Conference on Machine Learning\n  and Applications (ICMLA), 2023"},{"id":"http://arxiv.org/abs/2311.03758v2","updated":"2023-11-12T02:36:54Z","published":"2023-11-07T06:48:47Z","title":"Large Language Model based Long-tail Query Rewriting in Taobao Search","summary":"  In the realm of e-commerce search, the significance of semantic matching\ncannot be overstated, as it directly impacts both user experience and company\nrevenue. Along this line, query rewriting, serving as an important technique to\nbridge the semantic gaps inherent in the semantic matching process, has\nattached wide attention from the industry and academia. However, existing query\nrewriting methods often struggle to effectively optimize long-tail queries and\nalleviate the phenomenon of \"few-recall\" caused by semantic gap. In this paper,\nwe present BEQUE, a comprehensive framework that Bridges the sEmantic gap for\nlong-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction\nsupervised fine tuning (SFT), offline feedback, and objective alignment. We\nfirst construct a rewriting dataset based on rejection sampling and auxiliary\ntasks mixing to fine-tune our large language model (LLM) in a supervised\nfashion. Subsequently, with the well-trained LLM, we employ beam search to\ngenerate multiple candidate rewrites, and feed them into Taobao offline system\nto obtain the partial order. Leveraging the partial order of rewrites, we\nintroduce a contrastive learning method to highlight the distinctions between\nrewrites, and align the model with the Taobao online objectives. Offline\nexperiments prove the effectiveness of our method in bridging semantic gap.\nOnline A/B tests reveal that our method can significantly boost gross\nmerchandise volume (GMV), number of transaction (#Trans) and unique visitor\n(UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most\npopular online shopping platforms in China, since October 2023.\n","authors":["Wenjun Peng","Guiyang Li","Yue Jiang","Zilong Wang","Dan Ou","Xiaoyi Zeng","Derong Xu"," Tongxu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2311.03758v2.pdf","comment":"WWW Industry Under Review"},{"id":"http://arxiv.org/abs/2204.00281v2","updated":"2023-11-12T02:05:16Z","published":"2022-04-01T08:30:06Z","title":"i-Razor: A Differentiable Neural Input Razor for Feature Selection and\n  Dimension Search in DNN-Based Recommender Systems","summary":"  Input features play a crucial role in DNN-based recommender systems with\nthousands of categorical and continuous fields from users, items, contexts, and\ninteractions. Noisy features and inappropriate embedding dimension assignments\ncan deteriorate the performance of recommender systems and introduce\nunnecessary complexity in model training and online serving. Optimizing the\ninput configuration of DNN models, including feature selection and embedding\ndimension assignment, has become one of the essential topics in feature\nengineering. However, in existing industrial practices, feature selection and\ndimension search are optimized sequentially, i.e., feature selection is\nperformed first, followed by dimension search to determine the optimal\ndimension size for each selected feature. Such a sequential optimization\nmechanism increases training costs and risks generating suboptimal input\nconfigurations. To address this problem, we propose a differentiable neural\ninput razor (i-Razor) that enables joint optimization of feature selection and\ndimension search. Concretely, we introduce an end-to-end differentiable model\nto learn the relative importance of different embedding regions of each\nfeature. Furthermore, a flexible pruning algorithm is proposed to achieve\nfeature filtering and dimension derivation simultaneously. Extensive\nexperiments on two large-scale public datasets in the Click-Through-Rate (CTR)\nprediction task demonstrate the efficacy and superiority of i-Razor in\nbalancing model complexity and performance.\n","authors":["Yao Yao","Bin Liu","Haoxun He","Dakui Sheng","Ke Wang","Li Xiao","Huanhuan Cao"],"pdf_url":"https://arxiv.org/pdf/2204.00281v2.pdf","comment":"Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2306.10532v3","updated":"2023-11-12T01:50:32Z","published":"2023-06-18T11:51:39Z","title":"Personalized Elastic Embedding Learning for On-Device Recommendation","summary":"  To address privacy concerns and reduce network latency, there has been a\nrecent trend of compressing cumbersome recommendation models trained on the\ncloud and deploying compact recommender models to resource-limited devices for\nthe real-time recommendation. Existing solutions generally overlook device\nheterogeneity and user heterogeneity. They require devices with the same budget\nto share the same model and assume the available device resources (e.g.,\nmemory) are constant, which is not reflective of reality. Considering device\nand user heterogeneities as well as dynamic resource constraints, this paper\nproposes a Personalized Elastic Embedding Learning framework (PEEL) for the\non-device recommendation, which generates Personalized Elastic Embeddings\n(PEEs) for devices with various memory budgets in a once-for-all manner,\nadapting to new or dynamic budgets, and addressing user preference diversity by\nassigning personalized embeddings for different groups of users. Specifically,\nit pretrains a global embedding table with collected user-item interaction\ninstances and clusters users into groups. Then, it refines the embedding tables\nwith local interaction instances within each group. PEEs are generated from the\ngroup-wise embedding blocks and their weights that indicate the contribution of\neach embedding block to the local recommendation performance. Given a memory\nbudget, PEEL efficiently generates PEEs by selecting embedding blocks with the\nlargest weights, making it adaptable to dynamic memory budgets on devices.\nFurthermore, a diversity-driven regularizer is implemented to encourage the\nexpressiveness of embedding blocks, and a controller is utilized to optimize\nthe weights. Extensive experiments are conducted on two public datasets, and\nthe results show that PEEL yields superior performance on devices with\nheterogeneous and dynamic memory budgets.\n","authors":["Ruiqi Zheng","Liang Qu","Tong Chen","Kai Zheng","Yuhui Shi","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2306.10532v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07619v1","updated":"2023-11-12T15:32:57Z","published":"2023-11-12T15:32:57Z","title":"Modeling User Viewing Flow using Large Language Models for Article\n  Recommendation","summary":"  This paper proposes the User Viewing Flow Modeling (SINGLE) method for the\narticle recommendation task, which models the user constant preference and\ninstant interest from user-clicked articles. Specifically, we employ a user\nconstant viewing flow modeling method to summarize the user's general interest\nto recommend articles. We utilize Large Language Models (LLMs) to capture\nconstant user preferences from previously clicked articles, such as skills and\npositions. Then we design the user instant viewing flow modeling method to\nbuild interactions between user-clicked article history and candidate articles.\nIt attentively reads the representations of user-clicked articles and aims to\nlearn the user's different interest views to match the candidate article. Our\nexperimental results on the Alibaba Technology Association (ATA) website show\nthe advantage of SINGLE, which achieves 2.4% improvements over previous\nbaseline models in the online A/B test. Our further analyses illustrate that\nSINGLE has the ability to build a more tailored recommendation system by\nmimicking different article viewing behaviors of users and recommending more\nappropriate and diverse articles to match user interests.\n","authors":["Zhenghao Liu","Zulong Chen","Moufeng Zhang","Shaoyang Duan","Hong Wen","Liangyue Li","Nan Li","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2311.07619v1.pdf","comment":"8 pagese"},{"id":"http://arxiv.org/abs/2311.07617v1","updated":"2023-11-12T07:45:35Z","published":"2023-11-12T07:45:35Z","title":"CLAMP: A Contrastive Language And Molecule Pre-training Network","summary":"  This paper highlights a shift in how to approach material generation. Instead\nof material-to-material, we propose a language-to-material generation\narchitecture that utilizes millions of untapped data points. Using a web\nscraper to collect crystal text pairs from open-source research papers, a\ncontrastive model can be trained using a convolutional graph neural network\nencoder and a language encoder. This would allow unsupervised zero-shot\nclassification which can be trained by taking advantage of linguistic\nstructure. Without any specific training data, an ~82\\% accuracy was achieved\nand ~75\\% accuracy for photocatalyst prediction with an extremely small\ndataset. This novel network could ideally be cross-applied to any reaction that\ncan be described via text, opening completely new methods to think about 3D\nchemical framework generation. In the full experiment diffusion models would\nlikely be incorporated to fully exploit the latent space.\n","authors":["Neel Redkar"],"pdf_url":"https://arxiv.org/pdf/2311.07617v1.pdf","comment":"3 pages, 1 figure, Presenting @ NeurIPS23 & Workshop - source @\n  https://github.com/neelr/clamp - dataset @\n  https://www.kaggle.com/datasets/programgeek01/cif-summary-data"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.06978v1","updated":"2023-11-12T22:42:34Z","published":"2023-11-12T22:42:34Z","title":"Augmented Bridge Matching","summary":"  Flow and bridge matching are a novel class of processes which encompass\ndiffusion models. One of the main aspect of their increased flexibility is that\nthese models can interpolate between arbitrary data distributions i.e. they\ngeneralize beyond generative modeling and can be applied to learning stochastic\n(and deterministic) processes of arbitrary transfer tasks between two given\ndistributions. In this paper, we highlight that while flow and bridge matching\nprocesses preserve the information of the marginal distributions, they do\n\\emph{not} necessarily preserve the coupling information unless additional,\nstronger optimality conditions are met. This can be problematic if one aims at\npreserving the original empirical pairing. We show that a simple modification\nof the matching process recovers this coupling by augmenting the velocity field\n(or drift) with the information of the initial sample point. Doing so, we lose\nthe Markovian property of the process but preserve the coupling information\nbetween distributions. We illustrate the efficiency of our augmentation in\nlearning mixture of image translation tasks.\n","authors":["Valentin De Bortoli","Guan-Horng Liu","Tianrong Chen","Evangelos A. Theodorou","Weilie Nie"],"pdf_url":"https://arxiv.org/pdf/2311.06978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16452v2","updated":"2023-11-12T22:38:04Z","published":"2023-10-25T08:14:49Z","title":"Faithful Path Language Modelling for Explainable Recommendation over\n  Knowledge Graph","summary":"  Path reasoning methods over knowledge graphs have gained popularity for their\npotential to improve transparency in recommender systems. However, the\nresulting models still rely on pre-trained knowledge graph embeddings, fail to\nfully exploit the interdependence between entities and relations in the KG for\nrecommendation, and may generate inaccurate explanations. In this paper, we\nintroduce PEARLM, a novel approach that efficiently captures user behaviour and\nproduct-side knowledge through language modelling. With our approach, knowledge\ngraph embeddings are directly learned from paths over the KG by the language\nmodel, which also unifies entities and relations in the same optimisation\nspace. Constraints on the sequence decoding additionally guarantee path\nfaithfulness with respect to the KG. Experiments on two datasets show the\neffectiveness of our approach compared to state-of-the-art baselines. Source\ncode and datasets: AVAILABLE AFTER GETTING ACCEPTED.\n","authors":["Giacomo Balloccu","Ludovico Boratto","Christian Cancedda","Gianni Fenu","Mirko Marras"],"pdf_url":"https://arxiv.org/pdf/2310.16452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06973v1","updated":"2023-11-12T22:01:34Z","published":"2023-11-12T22:01:34Z","title":"Analytical Verification of Deep Neural Network Performance for\n  Time-Synchronized Distribution System State Estimation","summary":"  Recently, we demonstrated success of a time-synchronized state estimator\nusing deep neural networks (DNNs) for real-time unobservable distribution\nsystems. In this letter, we provide analytical bounds on the performance of\nthat state estimator as a function of perturbations in the input measurements.\nIt has already been shown that evaluating performance based on only the test\ndataset might not effectively indicate a trained DNN's ability to handle input\nperturbations. As such, we analytically verify robustness and trustworthiness\nof DNNs to input perturbations by treating them as mixed-integer linear\nprogramming (MILP) problems. The ability of batch normalization in addressing\nthe scalability limitations of the MILP formulation is also highlighted. The\nframework is validated by performing time-synchronized distribution system\nstate estimation for a modified IEEE 34-node system and a real-world large\ndistribution system, both of which are incompletely observed by micro-phasor\nmeasurement units.\n","authors":["Behrouz Azimian","Shiva Moshtagh","Anamitra Pal","Shanshan Ma"],"pdf_url":"https://arxiv.org/pdf/2311.06973v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2311.06972v1","updated":"2023-11-12T21:54:53Z","published":"2023-11-12T21:54:53Z","title":"An Expandable Machine Learning-Optimization Framework to Sequential\n  Decision-Making","summary":"  We present an integrated prediction-optimization (PredOpt) framework to\nefficiently solve sequential decision-making problems by predicting the values\nof binary decision variables in an optimal solution. We address the key issues\nof sequential dependence, infeasibility, and generalization in machine learning\n(ML) to make predictions for optimal solutions to combinatorial problems. The\nsequential nature of the combinatorial optimization problems considered is\ncaptured with recurrent neural networks and a sliding-attention window. We\nintegrate an attention-based encoder-decoder neural network architecture with\nan infeasibility-elimination and generalization framework to learn high-quality\nfeasible solutions to time-dependent optimization problems. In this framework,\nthe required level of predictions is optimized to eliminate the infeasibility\nof the ML predictions. These predictions are then fixed in mixed-integer\nprogramming (MIP) problems to solve them quickly with the aid of a commercial\nsolver. We demonstrate our approach to tackling the two well-known dynamic\nNP-Hard optimization problems: multi-item capacitated lot-sizing (MCLSP) and\nmulti-dimensional knapsack (MSMK). Our results show that models trained on\nshorter and smaller-dimensional instances can be successfully used to predict\nlonger and larger-dimensional problems. The solution time can be reduced by\nthree orders of magnitude with an average optimality gap below 0.1%. We compare\nPredOpt with various specially designed heuristics and show that our framework\noutperforms them. PredOpt can be advantageous for solving dynamic MIP problems\nthat need to be solved instantly and repetitively.\n","authors":["Dogacan Yilmaz","İ. Esra Büyüktahtakın"],"pdf_url":"https://arxiv.org/pdf/2311.06972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09447v3","updated":"2023-11-12T21:28:47Z","published":"2023-03-16T16:23:13Z","title":"Steering Prototypes with Prompt-tuning for Rehearsal-free Continual\n  Learning","summary":"  In the context of continual learning, prototypes-as representative class\nembeddings-offer advantages in memory conservation and the mitigation of\ncatastrophic forgetting. However, challenges related to semantic drift and\nprototype interference persist. In this study, we introduce the Contrastive\nPrototypical Prompt (CPP) approach. Through task-specific prompt-tuning,\nunderpinned by a contrastive learning objective, we effectively address both\naforementioned challenges. Our evaluations on four challenging\nclass-incremental benchmarks reveal that CPP achieves a significant 4% to 6%\nimprovement over state-of-the-art methods. Importantly, CPP operates without a\nrehearsal buffer and narrows the performance divergence between continual and\noffline joint-learning, suggesting an innovative scheme for Transformer-based\ncontinual learning systems.\n","authors":["Zhuowei Li","Long Zhao","Zizhao Zhang","Han Zhang","Di Liu","Ting Liu","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2303.09447v3.pdf","comment":"Accept to WACV 2024. Code is available at\n  https://github.com/LzVv123456/Contrastive-Prototypical-Prompt"},{"id":"http://arxiv.org/abs/2311.06968v1","updated":"2023-11-12T21:25:56Z","published":"2023-11-12T21:25:56Z","title":"Physics-Informed Data Denoising for Real-Life Sensing Systems","summary":"  Sensors measuring real-life physical processes are ubiquitous in today's\ninterconnected world. These sensors inherently bear noise that often adversely\naffects performance and reliability of the systems they support. Classic\nfiltering-based approaches introduce strong assumptions on the time or\nfrequency characteristics of sensory measurements, while learning-based\ndenoising approaches typically rely on using ground truth clean data to train a\ndenoising model, which is often challenging or prohibitive to obtain for many\nreal-world applications. We observe that in many scenarios, the relationships\nbetween different sensor measurements (e.g., location and acceleration) are\nanalytically described by laws of physics (e.g., second-order differential\nequation). By incorporating such physics constraints, we can guide the\ndenoising process to improve even in the absence of ground truth data. In light\nof this, we design a physics-informed denoising model that leverages the\ninherent algebraic relationships between different measurements governed by the\nunderlying physics. By obviating the need for ground truth clean data, our\nmethod offers a practical denoising solution for real-world applications. We\nconducted experiments in various domains, including inertial navigation, CO2\nmonitoring, and HVAC control, and achieved state-of-the-art performance\ncompared with existing denoising methods. Our method can denoise data in real\ntime (4ms for a sequence of 1s) for low-cost noisy sensors and produces results\nthat closely align with those from high-precision, high-cost alternatives,\nleading to an efficient, cost-effective approach for more accurate sensor-based\nsystems.\n","authors":["Xiyuan Zhang","Xiaohan Fu","Diyan Teng","Chengyu Dong","Keerthivasan Vijayakumar","Jiayun Zhang","Ranak Roy Chowdhury","Junsheng Han","Dezhi Hong","Rashmi Kulkarni","Jingbo Shang","Rajesh Gupta"],"pdf_url":"https://arxiv.org/pdf/2311.06968v1.pdf","comment":"SenSys 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2311.06783v1","updated":"2023-11-12T09:10:51Z","published":"2023-11-12T09:10:51Z","title":"Q-Instruct: Improving Low-level Visual Abilities for Multi-modality\n  Foundation Models","summary":"  Multi-modality foundation models, as represented by GPT-4V, have brought a\nnew paradigm for low-level visual perception and understanding tasks, that can\nrespond to a broad range of natural human instructions in a model. While\nexisting foundation models have shown exciting potentials on low-level visual\ntasks, their related abilities are still preliminary and need to be improved.\nIn order to enhance these models, we conduct a large-scale subjective\nexperiment collecting a vast number of real human feedbacks on low-level\nvision. Each feedback follows a pathway that starts with a detailed description\non the low-level visual appearance (*e.g. clarity, color, brightness* of an\nimage, and ends with an overall conclusion, with an average length of 45 words.\nThe constructed **Q-Pathway** dataset includes 58K detailed human feedbacks on\n18,973 images with diverse low-level appearance. Moreover, to enable foundation\nmodels to robustly respond to diverse types of questions, we design a\nGPT-participated conversion to process these feedbacks into diverse-format 200K\ninstruction-response pairs. Experimental results indicate that the\n**Q-Instruct** consistently elevates low-level perception and understanding\nabilities across several foundational models. We anticipate that our datasets\ncan pave the way for a future that general intelligence can perceive,\nunderstand low-level visual appearance and evaluate visual quality like a\nhuman. Our dataset, model zoo, and demo is published at:\nhttps://q-future.github.io/Q-Instruct.\n","authors":["Haoning Wu","Zicheng Zhang","Erli Zhang","Chaofeng Chen","Liang Liao","Annan Wang","Kaixin Xu","Chunyi Li","Jingwen Hou","Guangtao Zhai","Geng Xue","Wenxiu Sun","Qiong Yan","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2311.06783v1.pdf","comment":"16 pages, 11 figures, page 12-16 as appendix"}]},"2023-11-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.08402v1","updated":"2023-11-14T18:59:24Z","published":"2023-11-14T18:59:24Z","title":"Retrieve and Copy: Scaling ASR Personalization to Large Catalogs","summary":"  Personalization of automatic speech recognition (ASR) models is a widely\nstudied topic because of its many practical applications. Most recently,\nattention-based contextual biasing techniques are used to improve the\nrecognition of rare words and domain specific entities. However, due to\nperformance constraints, the biasing is often limited to a few thousand\nentities, restricting real-world usability. To address this, we first propose a\n\"Retrieve and Copy\" mechanism to improve latency while retaining the accuracy\neven when scaled to a large catalog. We also propose a training strategy to\novercome the degradation in recall at such scale due to an increased number of\nconfusing entities. Overall, our approach achieves up to 6% more Word Error\nRate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a\nstrong baseline. Our method also allows for large catalog sizes of up to 20K\nwithout significantly affecting WER and F1-scores, while achieving at least 20%\ninference speedup per acoustic frame.\n","authors":["Sai Muralidhar Jayanthi","Devang Kulshreshtha","Saket Dingliwal","Srikanth Ronanki","Sravan Bodapati"],"pdf_url":"https://arxiv.org/pdf/2311.08402v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.08401v1","updated":"2023-11-14T18:59:15Z","published":"2023-11-14T18:59:15Z","title":"Fine-tuning Language Models for Factuality","summary":"  The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.\n","authors":["Katherine Tian","Eric Mitchell","Huaxiu Yao","Christopher D. Manning","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2311.08401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08398v1","updated":"2023-11-14T18:57:15Z","published":"2023-11-14T18:57:15Z","title":"Are Large Language Models Temporally Grounded?","summary":"  Are Large language models (LLMs) temporally grounded? Since LLMs cannot\nperceive and interact with the environment, it is impossible to answer this\nquestion directly. Instead, we provide LLMs with textual narratives and probe\nthem with respect to their common-sense knowledge of the structure and duration\nof events, their ability to order events along a timeline, and self-consistency\nwithin their temporal model (e.g., temporal relations such as after and before\nare mutually exclusive for any pair of events). We evaluate state-of-the-art\nLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\nGenerally, we find that LLMs lag significantly behind both human performance as\nwell as small-scale, specialised LMs. In-context learning, instruction tuning,\nand chain-of-thought prompting reduce this gap only to a limited degree.\nCrucially, LLMs struggle the most with self-consistency, displaying incoherent\nbehaviour in at least 27.23% of their predictions. Contrary to expectations, we\nalso find that scaling the model size does not guarantee positive gains in\nperformance. To explain these results, we study the sources from which LLMs may\ngather temporal information: we find that sentence ordering in unlabelled\ntexts, available during pre-training, is only weakly correlated with event\nordering. Moreover, public instruction tuning mixtures contain few temporal\ntasks. Hence, we conclude that current LLMs lack a consistent temporal model of\ntextual narratives. Code, datasets, and LLM outputs are available at\nhttps://github.com/yfqiu-nlp/temporal-llms.\n","authors":["Yifu Qiu","Zheng Zhao","Yftah Ziser","Anna Korhonen","Edoardo M. Ponti","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2311.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08396v1","updated":"2023-11-14T18:55:48Z","published":"2023-11-14T18:55:48Z","title":"Zero-shot audio captioning with audio-language model guidance and audio\n  context keywords","summary":"  Zero-shot audio captioning aims at automatically generating descriptive\ntextual captions for audio content without prior training for this task.\nDifferent from speech recognition which translates audio content that contains\nspoken language into text, audio captioning is commonly concerned with ambient\nsounds, or sounds produced by a human performing an action. Inspired by\nzero-shot image captioning methods, we propose ZerAuCap, a novel framework for\nsummarising such general audio signals in a text caption without requiring\ntask-specific training. In particular, our framework exploits a pre-trained\nlarge language model (LLM) for generating the text which is guided by a\npre-trained audio-language model to produce captions that describe the audio\ncontent. Additionally, we use audio context keywords that prompt the language\nmodel to generate text that is broadly relevant to sounds. Our proposed\nframework achieves state-of-the-art results in zero-shot audio captioning on\nthe AudioCaps and Clotho datasets. Our code is available at\nhttps://github.com/ExplainableML/ZerAuCap.\n","authors":["Leonard Salewski","Stefan Fauth","A. Sophia Koepke","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2311.08396v1.pdf","comment":"NeurIPS 2023 - Machine Learning for Audio Workshop (Oral)"},{"id":"http://arxiv.org/abs/2311.08391v1","updated":"2023-11-14T18:52:09Z","published":"2023-11-14T18:52:09Z","title":"A Material Lens on Coloniality in NLP","summary":"  Coloniality, the continuation of colonial harms beyond \"official\"\ncolonization, has pervasive effects across society and scientific fields.\nNatural Language Processing (NLP) is no exception to this broad phenomenon. In\nthis work, we argue that coloniality is implicitly embedded in and amplified by\nNLP data, algorithms, and software. We formalize this analysis using\nActor-Network Theory (ANT): an approach to understanding social phenomena\nthrough the network of relationships between human stakeholders and technology.\nWe use our Actor-Network to guide a quantitative survey of the geography of\ndifferent phases of NLP research, providing evidence that inequality along\ncolonial boundaries increases as NLP builds on itself. Based on this, we argue\nthat combating coloniality in NLP requires not only changing current values but\nalso active work to remove the accumulation of colonial ideals in our\nfoundational data and algorithms.\n","authors":["William Held","Camille Harris","Michael Best","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2311.08391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08390v1","updated":"2023-11-14T18:51:38Z","published":"2023-11-14T18:51:38Z","title":"On What Basis? Predicting Text Preference Via Structured Comparative\n  Reasoning","summary":"  Comparative reasoning plays a crucial role in text preference prediction;\nhowever, large language models (LLMs) often demonstrate inconsistencies in\ntheir reasoning. While approaches like Chain-of-Thought improve accuracy in\nmany other settings, they struggle to consistently distinguish the similarities\nand differences of complex texts. We introduce SC, a prompting approach that\npredicts text preferences by generating structured intermediate comparisons. SC\nbegins by proposing aspects of comparison, followed by generating textual\ncomparisons under each aspect. We select consistent comparisons with a pairwise\nconsistency comparator that ensures each aspect's comparisons clearly\ndistinguish differences between texts, significantly reducing hallucination and\nimproving consistency. Our comprehensive evaluations across various NLP tasks,\nincluding summarization, retrieval, and automatic rating, demonstrate that SC\nequips LLMs to achieve state-of-the-art performance in text preference\nprediction.\n","authors":["Jing Nathan Yan","Tianqi Liu","Justin T Chiu","Jiaming Shen","Zhen Qin","Yue Yu","Yao Zhao","Charu Lakshmanan","Yair Kurzion","Alexander M. Rush","Jialu Liu","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2311.08390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08389v1","updated":"2023-11-14T18:50:51Z","published":"2023-11-14T18:50:51Z","title":"TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer","summary":"  Text style is highly abstract, as it encompasses various aspects of a\nspeaker's characteristics, habits, logical thinking, and the content they\nexpress. However, previous text-style transfer tasks have primarily focused on\ndata-driven approaches, lacking in-depth analysis and research from the\nperspectives of linguistics and cognitive science. In this paper, we introduce\na novel task called Text Speech-Style Transfer (TSST). The main objective is to\nfurther explore topics related to human cognition, such as personality and\nemotion, based on the capabilities of existing LLMs. Considering the objective\nof our task and the distinctive characteristics of oral speech in real-life\nscenarios, we trained multi-dimension (i.e. filler words, vividness,\ninteractivity, emotionality) evaluation models for the TSST and validated their\ncorrelation with human assessments. We thoroughly analyze the performance of\nseveral large language models (LLMs) and identify areas where further\nimprovement is needed. Moreover, driven by our evaluation models, we have\nreleased a new corpus that improves the capabilities of LLMs in generating text\nwith speech-style characteristics. In summary, we present the TSST task, a new\nbenchmark for style transfer and emphasizing human-oriented evaluation,\nexploring and advancing the performance of current LLMs.\n","authors":["Huashan Sun","Yixiao Wu","Yinghao Li","Jiawei Li","Yizhe Yang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2311.08389v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2311.08385v1","updated":"2023-11-14T18:48:27Z","published":"2023-11-14T18:48:27Z","title":"ChOiRe: Characterizing and Predicting Human Opinions with Chain of\n  Opinion Reasoning","summary":"  Aligning language models (LMs) with human opinion is challenging yet vital to\nenhance their grasp of human values, preferences, and beliefs. We present\nChOiRe, a four-step solution framework to predict human opinion that\ndifferentiates between the user explicit personae (i.e. demographic or\nideological attributes) that are manually declared and implicit personae\ninferred from user historical opinions. Specifically, it consists of (i) an LM\nanalyzing the user explicit personae to filter out irrelevant attributes; (ii)\nthe LM ranking the implicit persona opinions into a preferential list; (iii)\nChain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the\nexplicit personae and the most relevant implicit personae to perform opinion\nprediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with\nincreasingly larger lists of implicit personae to overcome insufficient\npersonae information to infer a final result. ChOiRe achieves new\nstate-of-the-art effectiveness with limited inference calls, improving previous\nLLM-based techniques significantly by 3.22%.\n","authors":["Xuan Long Do","Kenji Kawaguchi","Min Yen Kan","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08385v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2311.08380v1","updated":"2023-11-14T18:43:51Z","published":"2023-11-14T18:43:51Z","title":"Direct Preference Optimization for Neural Machine Translation with\n  Minimum Bayes Risk Decoding","summary":"  Minimum Bayes Risk (MBR) decoding can significantly improve translation\nperformance of Multilingual Large Language Models (MLLMs). However, MBR\ndecoding is computationally expensive and in this paper, we show how recently\ndeveloped Reinforcement Learning (RL) technique, Direct Preference Optimization\n(DPO) can be used to fine-tune MLLMs so that we get the gains from MBR without\nthe additional computation in inference. Our fine-tuned models have\nsignificantly improved performance on multiple NMT test sets compared to base\nMLLMs without preference optimization. Our method boosts the translation\nperformance of MLLMs using relatively small monolingual fine-tuning sets.\n","authors":["Guangyu Yang","Jinghong Chen","Weizhe Lin","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2311.08380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08377v1","updated":"2023-11-14T18:41:54Z","published":"2023-11-14T18:41:54Z","title":"Learning to Filter Context for Retrieval-Augmented Generation","summary":"  On-the-fly retrieval of relevant knowledge has proven an essential element of\nreliable systems for tasks such as open-domain question answering and fact\nverification. However, because retrieval systems are not perfect, generation\nmodels are required to generate outputs given partially or entirely irrelevant\npassages. This can cause over- or under-reliance on context, and result in\nproblems in the generated output such as hallucinations. To alleviate these\nproblems, we propose FILCO, a method that improves the quality of the context\nprovided to the generator by (1) identifying useful context based on lexical\nand information-theoretic approaches, and (2) training context filtering models\nthat can filter retrieved contexts at test time. We experiment on six\nknowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our\nmethod outperforms existing approaches on extractive question answering (QA),\ncomplex multi-hop and long-form QA, fact verification, and dialog generation\ntasks. FILCO effectively improves the quality of context, whether or not it\nsupports the canonical output.\n","authors":["Zhiruo Wang","Jun Araki","Zhengbao Jiang","Md Rizwan Parvez","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2311.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08374v1","updated":"2023-11-14T18:40:42Z","published":"2023-11-14T18:40:42Z","title":"A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts","summary":"  In the realm of text manipulation and linguistic transformation, the question\nof authorship has always been a subject of fascination and philosophical\ninquiry. Much like the \\textbf{Ship of Theseus paradox}, which ponders whether\na ship remains the same when each of its original planks is replaced, our\nresearch delves into an intriguing question: \\textit{Does a text retain its\noriginal authorship when it undergoes numerous paraphrasing iterations?}\nSpecifically, since Large Language Models (LLMs) have demonstrated remarkable\nproficiency in the generation of both original content and the modification of\nhuman-authored texts, a pivotal question emerges concerning the determination\nof authorship in instances where LLMs or similar paraphrasing tools are\nemployed to rephrase the text. This inquiry revolves around \\textit{whether\nauthorship should be attributed to the original human author or the AI-powered\ntool, given the tool's independent capacity to produce text that closely\nresembles human-generated content.} Therefore, we embark on a philosophical\nvoyage through the seas of language and authorship to unravel this intricate\npuzzle.\n","authors":["Nafis Irtiza Tripto","Saranya Venkatraman","Dominik Macko","Robert Moro","Ivan Srba","Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2311.08374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08370v1","updated":"2023-11-14T18:33:43Z","published":"2023-11-14T18:33:43Z","title":"SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in\n  Large Language Models","summary":"  The past year has seen rapid acceleration in the development of large\nlanguage models (LLMs). For many tasks, there is now a wide range of\nopen-source and open-access LLMs that are viable alternatives to proprietary\nmodels like ChatGPT. Without proper steering and safeguards, however, LLMs will\nreadily follow malicious instructions, provide unsafe advice, and generate\ntoxic content. This is a critical safety risk for businesses and developers. We\nintroduce SimpleSafetyTests as a new test suite for rapidly and systematically\nidentifying such critical safety risks. The test suite comprises 100 test\nprompts across five harm areas that LLMs, for the vast majority of\napplications, should refuse to comply with. We test 11 popular open LLMs and\nfind critical safety weaknesses in several of them. While some LLMs do not give\na single unsafe response, most models we test respond unsafely on more than 20%\nof cases, with over 50% unsafe responses in the extreme. Prepending a\nsafety-emphasising system prompt substantially reduces the occurrence of unsafe\nresponses, but does not completely stop them from happening. We recommend that\ndevelopers use such system prompts as a first line of defence against critical\nsafety risks.\n","authors":["Bertie Vidgen","Hannah Rose Kirk","Rebecca Qian","Nino Scherrer","Anand Kannappan","Scott A. Hale","Paul Röttger"],"pdf_url":"https://arxiv.org/pdf/2311.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08369v1","updated":"2023-11-14T18:32:52Z","published":"2023-11-14T18:32:52Z","title":"How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection","summary":"  Against the misuse (e.g., plagiarism or spreading misinformation) of Large\nLanguage Models (LLMs), many recent works have presented LLM-generated-text\ndetectors with promising detection performance. Spotlighting a situation where\nusers instruct LLMs to generate texts (e.g., essay writing), there are various\nways to write the instruction (e.g., what task-oriented constraint to include).\nIn this paper, we discover that even a task-oriented constraint in instruction\ncan cause the inconsistent performance of current detectors to the generated\ntexts. Specifically, we focus on student essay writing as a realistic domain\nand manually create the task-oriented constraint for each factor on essay\nquality by Ke and Ng (2019). Our experiment shows that the detection\nperformance variance of the current detector on texts generated by instruction\nwith each task-oriented constraint is up to 20 times larger than the variance\ncaused by generating texts multiple times and paraphrasing the instruction. Our\nfinding calls for further research on developing robust detectors that can\ndetect such distributional shifts caused by a task-oriented constraint in the\ninstruction.\n","authors":["Ryuto Koike","Masahiro Kaneko","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2311.08369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14770v2","updated":"2023-11-14T18:30:28Z","published":"2023-05-24T06:19:14Z","title":"Using Natural Language Explanations to Rescale Human Judgments","summary":"  The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover crowdworker judgments. However, annotators' judgments for subjective tasks\ncan differ in many ways: they may have different qualitative judgments about an\nexample, and they may map those to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.\n","authors":["Manya Wadhwa","Jifan Chen","Junyi Jessy Li","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2305.14770v2.pdf","comment":"Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling"},{"id":"http://arxiv.org/abs/2212.04972v2","updated":"2023-11-14T18:06:48Z","published":"2022-12-09T16:35:14Z","title":"MOPRD: A multidisciplinary open peer review dataset","summary":"  Open peer review is a growing trend in academic publications. Public access\nto peer review data can benefit both the academic and publishing communities.\nIt also serves as a great support to studies on review comment generation and\nfurther to the realization of automated scholarly paper review. However, most\nof the existing peer review datasets do not provide data that cover the whole\npeer review process. Apart from this, their data are not diversified enough as\nthe data are mainly collected from the field of computer science. These two\ndrawbacks of the currently available peer review datasets need to be addressed\nto unlock more opportunities for related studies. In response, we construct\nMOPRD, a multidisciplinary open peer review dataset. This dataset consists of\npaper metadata, multiple version manuscripts, review comments, meta-reviews,\nauthor's rebuttal letters, and editorial decisions. Moreover, we propose a\nmodular guided review comment generation method based on MOPRD. Experiments\nshow that our method delivers better performance as indicated by both automatic\nmetrics and human evaluation. We also explore other potential applications of\nMOPRD, including meta-review generation, editorial decision prediction, author\nrebuttal generation, and scientometric analysis. MOPRD is a strong endorsement\nfor further studies in peer review-related research and other applications.\n","authors":["Jialiang Lin","Jiaxin Song","Zhangping Zhou","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2212.04972v2.pdf","comment":"Please cite the version of Neural Computing and Applications"},{"id":"http://arxiv.org/abs/2311.08360v1","updated":"2023-11-14T18:03:20Z","published":"2023-11-14T18:03:20Z","title":"The Transient Nature of Emergent In-Context Learning in Transformers","summary":"  Transformer neural networks can exhibit a surprising capacity for in-context\nlearning (ICL) despite not being explicitly trained for it. Prior work has\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\nexamining the distributional properties of training data. However, in each of\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\nemergence of ICL during transformer training is, in fact, often transient. We\ntrain transformers on synthetic data designed so that both ICL and in-weights\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\nfirst emerges, then disappears and gives way to IWL, all while the training\nloss decreases, indicating an asymptotic preference for IWL. The transient\nnature of ICL is observed in transformers across a range of model sizes and\ndatasets, raising the question of how much to \"overtrain\" transformers when\nseeking compact, cheaper-to-run models. We find that L2 regularization may\noffer a path to more persistent ICL that removes the need for early stopping\nbased on ICL-style validation tasks. Finally, we present initial evidence that\nICL transience may be caused by competition between ICL and IWL circuits.\n","authors":["Aaditya K. Singh","Stephanie C. Y. Chan","Ted Moskovitz","Erin Grant","Andrew M. Saxe","Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2311.08360v1.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2311.08349v1","updated":"2023-11-14T17:48:19Z","published":"2023-11-14T17:48:19Z","title":"Artificial Text Boundary Detection with Topological Data Analysis and\n  Sliding Window Techniques","summary":"  Due to the rapid development of text generation models, people increasingly\noften encounter texts that may start out as written by a human but then\ncontinue as machine-generated results of large language models. Detecting the\nboundary between human-written and machine-generated parts of such texts is a\nvery challenging problem that has not received much attention in literature. In\nthis work, we consider and compare a number of different approaches for this\nartificial text boundary detection problem, comparing several predictors over\nfeatures of different nature. We show that supervised fine-tuning of the\nRoBERTa model works well for this task in general but fails to generalize in\nimportant cross-domain and cross-generator settings, demonstrating a tendency\nto overfit to spurious properties of the data. Then, we propose novel\napproaches based on features extracted from a frozen language model's\nembeddings that are able to outperform both the human accuracy level and\npreviously considered baselines on the Real or Fake Text benchmark. Moreover,\nwe adapt perplexity-based approaches for the boundary detection task and\nanalyze their behaviour. We analyze the robustness of all proposed classifiers\nin cross-domain and cross-model settings, discovering important properties of\nthe data that can negatively influence the performance of artificial text\nboundary detection algorithms.\n","authors":["Laida Kushnareva","Tatiana Gaintseva","German Magai","Serguei Barannikov","Dmitry Abulkhanov","Kristian Kuznetsov","Irina Piontkovskaya","Sergey Nikolenko"],"pdf_url":"https://arxiv.org/pdf/2311.08349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08348v1","updated":"2023-11-14T17:45:50Z","published":"2023-11-14T17:45:50Z","title":"MC^2: A Multilingual Corpus of Minority Languages in China","summary":"  Large-scale corpora play a vital role in the construction of large language\nmodels (LLMs). However, existing LLMs exhibit limited abilities in\nunderstanding low-resource languages, including the minority languages in\nChina, due to a lack of training data. To improve the accessibility of these\nlanguages, we present MC^2, a Multilingual Corpus of Minority Languages in\nChina, which is the largest open-source corpus so far. It encompasses four\nunderrepresented languages, i.e., Tibetan, Uyghur, Kazakh in the Kazakh Arabic\nscript, and Mongolian in the traditional Mongolian script. Notably, two writing\nsystems in MC^2 are long neglected in previous corpora. As we identify serious\ncontamination in the low-resource language split in the existing multilingual\ncorpora, we propose a quality-centric solution for collecting MC^2,\nprioritizing quality and accuracy while enhancing representativeness and\ndiversity. By in-depth analysis, we demonstrate the new research challenges\nMC^2 brings, such as long-text modeling and multiplicity of writing systems. We\nhope MC^2 can help enhance the equity of the underrepresented languages in\nChina and provide a reliable data foundation for further research on\nlow-resource languages.\n","authors":["Chen Zhang","Mingxu Tao","Quzhe Huang","Jiuheng Lin","Zhibin Chen","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2311.08348v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2309.17157v3","updated":"2023-11-14T17:40:37Z","published":"2023-09-29T11:46:07Z","title":"LatticeGen: A Cooperative Framework which Hides Generated Text in a\n  Lattice for Privacy-Aware Generation on Cloud","summary":"  In the current user-server interaction paradigm of prompted generation with\nlarge language models (LLM) on cloud, the server fully controls the generation\nprocess, which leaves zero options for users who want to keep the generated\ntext to themselves. We propose LatticeGen, a cooperative framework in which the\nserver still handles most of the computation while the user controls the\nsampling operation. The key idea is that the true generated sequence is mixed\nwith noise tokens by the user and hidden in a noised lattice. Considering\npotential attacks from a hypothetically malicious server and how the user can\ndefend against it, we propose the repeated beam-search attack and the mixing\nnoise scheme. In our experiments we apply LatticeGen to protect both prompt and\ngeneration. It is shown that while the noised lattice degrades generation\nquality, LatticeGen successfully protects the true generation to a remarkable\ndegree under strong attacks (more than 50% of the semantic remains hidden as\nmeasured by BERTScore).\n","authors":["Mengke Zhang","Tianxing He","Tianle Wang","Lu Mi","Fatemehsadat Mireshghallah","Binyi Chen","Hao Wang","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2309.17157v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16445v3","updated":"2023-11-14T17:24:28Z","published":"2023-03-29T04:00:53Z","title":"Larger Probes Tell a Different Story: Extending Psycholinguistic\n  Datasets Via In-Context Learning","summary":"  Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.\n","authors":["Namrata Shivagunde","Vladislav Lialin","Anna Rumshisky"],"pdf_url":"https://arxiv.org/pdf/2303.16445v3.pdf","comment":"14 pages, 6 figures. Published as a conference paper at EMNLP 2023\n  (short). The datasets and code are available on this\n  $\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{URL}$"},{"id":"http://arxiv.org/abs/2311.08329v1","updated":"2023-11-14T17:18:08Z","published":"2023-11-14T17:18:08Z","title":"KTRL+F: Knowledge-Augmented In-Document Search","summary":"  We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. This task addresses following unique challenges for in-document search:\n1) utilizing knowledge outside the document for extended use of additional\ninformation about targets to bridge the semantic gap between the query and the\ntargets, and 2) balancing between real-time applicability with the performance.\nWe analyze various baselines in KTRL+F and find there are limitations of\nexisting models, such as hallucinations, low latency, or difficulties in\nleveraging external knowledge. Therefore we propose a Knowledge-Augmented\nPhrase Retrieval model that shows a promising balance between speed and\nperformance by simply augmenting external knowledge embedding in phrase\nembedding. Additionally, we conduct a user study to verify whether solving\nKTRL+F can enhance search experience of users. It demonstrates that even with\nour simple model users can reduce the time for searching with less queries and\nreduced extra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.\n","authors":["Hanseok Oh","Haebin Shin","Miyoung Ko","Hyunji Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.08329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02712v2","updated":"2023-11-14T17:12:36Z","published":"2022-12-06T02:33:47Z","title":"Improved Beam Search for Hallucination Mitigation in Abstractive\n  Summarization","summary":"  Advancement in large pretrained language models has significantly improved\ntheir performance for conditional language generation tasks including\nsummarization albeit with hallucinations. To reduce hallucinations,\nconventional methods proposed improving beam search or using a fact checker as\na postprocessing step. In this paper, we investigate the use of the Natural\nLanguage Inference (NLI) entailment metric to detect and prevent hallucinations\nin summary generation. We propose an NLI-assisted beam re-ranking mechanism by\ncomputing entailment probability scores between the input context and\nsummarization model-generated beams during saliency-enhanced greedy decoding.\nMoreover, a diversity metric is introduced to compare its effectiveness against\nvanilla beam search. Our proposed algorithm significantly outperforms vanilla\nbeam decoding on XSum and CNN/DM datasets.\n","authors":["Arvind Krishna Sridhar","Erik Visser"],"pdf_url":"https://arxiv.org/pdf/2212.02712v2.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.08324v1","updated":"2023-11-14T17:09:43Z","published":"2023-11-14T17:09:43Z","title":"Anti-LM Decoding for Zero-shot In-context Machine Translation","summary":"  Zero-shot In-context learning is the phenomenon where models can perform the\ntask simply given the instructions. However, pre-trained large language models\nare known to be poorly calibrated for this task. One of the most effective\napproaches to handling this bias is to adopt a contrastive decoding objective,\nwhich accounts for the prior probability of generating the next token by\nconditioning on some context. This work introduces an Anti-Language Model\nobjective with a decay factor designed to address the weaknesses of In-context\nMachine Translation. We conduct our experiments across 3 model types and sizes,\n3 language directions, and for both greedy decoding and beam search ($B=5$).\nThe proposed method outperforms other state-of-art decoding objectives, with up\nto $20$ BLEU point improvement from the default objective observed in some\nsettings.\n","authors":["Suzanna Sia","Alexandra DeLucia","Kevin Duh"],"pdf_url":"https://arxiv.org/pdf/2311.08324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08323v1","updated":"2023-11-14T17:09:07Z","published":"2023-11-14T17:09:07Z","title":"Open-vocabulary keyword spotting in any language through multilingual\n  contrastive speech-phoneme pretraining","summary":"  In this paper, we introduce a massively multilingual speech corpora with\nfine-grained phonemic transcriptions, encompassing more than 115 languages from\ndiverse language families. Based on this multilingual dataset, we propose\nCLAP-IPA, a multilingual phoneme-speech contrastive embedding model capable of\nopen-vocabulary matching between speech signals and phonemically transcribed\nkeywords or arbitrary phrases. The proposed model has been tested on two\nfieldwork speech corpora in 97 unseen languages, exhibiting strong\ngeneralizability across languages. Comparison with a text-based model shows\nthat using phonemes as modeling units enables much better crosslinguistic\ngeneralization than orthographic texts.\n","authors":["Jian Zhu","Farhan Samir","Changbing Yang","Jahurul Islam"],"pdf_url":"https://arxiv.org/pdf/2311.08323v1.pdf","comment":"Preprint; Work in Progress"},{"id":"http://arxiv.org/abs/2212.00768v3","updated":"2023-11-14T16:52:48Z","published":"2022-12-01T18:53:06Z","title":"Simplifying and Understanding State Space Models with Diagonal Linear\n  RNNs","summary":"  Sequence models based on linear state spaces (SSMs) have recently emerged as\na promising choice of architecture for modeling long range dependencies across\nvarious modalities. However, they invariably rely on discretization of a\ncontinuous state space, which complicates their presentation and understanding.\nIn this work, we dispose of the discretization step, and propose a model based\non vanilla Diagonal Linear RNNs ($\\mathrm{DLR}$). We empirically show that,\ndespite being conceptually much simpler, $\\mathrm{DLR}$ is as performant as\npreviously-proposed SSMs on a variety of tasks and benchmarks including Long\nRange Arena and raw speech classification. Moreover, we characterize the\nexpressivity of SSMs (including $\\mathrm{DLR}$) and attention-based models via\na suite of $13$ synthetic sequence-to-sequence tasks involving interactions\nover tens of thousands of tokens, ranging from simple operations, such as\nshifting an input sequence, to detecting co-dependent visual features over long\nspatial ranges in flattened images. We find that while SSMs report near-perfect\nperformance on tasks that can be modeled via $\\textit{few}$ convolutional\nkernels, they struggle on tasks requiring $\\textit{many}$ such kernels and\nespecially when the desired sequence manipulation is\n$\\textit{context-dependent}$. Despite these limitations, $\\mathrm{DLR}$ reaches\nhigh performance on two higher-order reasoning tasks $\\mathrm{ListOpsSubTrees}$\nand $\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{256}$ with input lengths\n$8K$ and $65K$ respectively, and gives encouraging performance on\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{512}$ with input length $262K$\nfor which attention is not a viable choice.\n","authors":["Ankit Gupta","Harsh Mehta","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2212.00768v3.pdf","comment":"added Long Range Arena, language modeling with mixture of experts"},{"id":"http://arxiv.org/abs/2311.08306v1","updated":"2023-11-14T16:49:33Z","published":"2023-11-14T16:49:33Z","title":"On-the-Fly Fusion of Large Language Models and Machine Translation","summary":"  We propose the on-the-fly ensembling of a machine translation model with an\nLLM, prompted on the same task and input. We perform experiments on 4 language\npairs (both directions) with varying data amounts. We find that a slightly\nweaker-at-translation LLM can improve translations of a NMT model, and\nensembling with an LLM can produce better translations than ensembling two\nstronger MT models. We combine our method with various techniques from LLM\nprompting, such as in context learning and translation context.\n","authors":["Hieu Hoang","Huda Khayrallah","Marcin Junczys-Dowmunt"],"pdf_url":"https://arxiv.org/pdf/2311.08306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08303v1","updated":"2023-11-14T16:46:15Z","published":"2023-11-14T16:46:15Z","title":"Extrinsically-Focused Evaluation of Omissions in Medical Summarization","summary":"  The goal of automated summarization techniques (Paice, 1990; Kupiec et al,\n1995) is to condense text by focusing on the most critical information.\nGenerative large language models (LLMs) have shown to be robust summarizers,\nyet traditional metrics struggle to capture resulting performance (Goyal et al,\n2022) in more powerful LLMs. In safety-critical domains such as medicine, more\nrigorous evaluation is required, especially given the potential for LLMs to\nomit important information in the resulting summary. We propose MED-OMIT, a new\nomission benchmark for medical summarization. Given a doctor-patient\nconversation and a generated summary, MED-OMIT categorizes the chat into a set\nof facts and identifies which are omitted from the summary. We further propose\nto determine fact importance by simulating the impact of each fact on a\ndownstream clinical task: differential diagnosis (DDx) generation. MED-OMIT\nleverages LLM prompt-based approaches which categorize the importance of facts\nand cluster them as supporting or negating evidence to the diagnosis. We\nevaluate MED-OMIT on a publicly-released dataset of patient-doctor\nconversations and find that MED-OMIT captures omissions better than alternative\nmetrics.\n","authors":["Elliot Schumacher","Daniel Rosenthal","Varun Nair","Luladay Price","Geoffrey Tso","Anitha Kannan"],"pdf_url":"https://arxiv.org/pdf/2311.08303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08300v1","updated":"2023-11-14T16:44:33Z","published":"2023-11-14T16:44:33Z","title":"Workflow-Guided Response Generation for Task-Oriented Dialogue","summary":"  Task-oriented dialogue (TOD) systems aim to achieve specific goals through\ninteractive dialogue. Such tasks usually involve following specific workflows,\ni.e. executing a sequence of actions in a particular order. While prior work\nhas focused on supervised learning methods to condition on past actions, they\ndo not explicitly optimize for compliance to a desired workflow. In this paper,\nwe propose a novel framework based on reinforcement learning (RL) to generate\ndialogue responses that are aligned with a given workflow. Our framework\nconsists of ComplianceScorer, a metric designed to evaluate how well a\ngenerated response executes the specified action, combined with an RL\nopimization process that utilizes an interactive sampling technique. We\nevaluate our approach on two TOD datasets, Action-Based Conversations Dataset\n(ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range of\nautomated and human evaluation metrics. Our findings indicate that our RL-based\nframework outperforms baselines and is effective at enerating responses that\nboth comply with the intended workflows while being expressed in a natural and\nfluent manner.\n","authors":["Do June Min","Paloma Sodhi","Ramya Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2311.08300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08299v1","updated":"2023-11-14T16:44:16Z","published":"2023-11-14T16:44:16Z","title":"VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing","summary":"  Reflective listening is a fundamental skill that counselors must acquire to\nachieve proficiency in motivational interviewing (MI). It involves responding\nin a manner that acknowledges and explores the meaning of what the client has\nexpressed in the conversation. In this work, we introduce the task of\ncounseling response rewriting, which transforms non-reflective statements into\nreflective responses. We introduce VERVE, a template-based rewriting system\nwith paraphrase-augmented training and adaptive template updating. VERVE first\ncreates a template by identifying and filtering out tokens that are not\nrelevant to reflections and constructs a reflective response using the\ntemplate. Paraphrase-augmented training allows the model to learn less-strict\nfillings of masked spans, and adaptive template updating helps discover\neffective templates for rewriting without significantly removing the original\ncontent. Using both automatic and human evaluations, we compare our method\nagainst text rewriting baselines and show that our framework is effective in\nturning non-reflective statements into more reflective responses while\nachieving a good content preservation-reflection style trade-off.\n","authors":["Do June Min","Verónica Pérez-Rosas","Kenneth Resnicow","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2311.08299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08298v1","updated":"2023-11-14T16:43:29Z","published":"2023-11-14T16:43:29Z","title":"A Survey of Language Model Confidence Estimation and Calibration","summary":"  Language models (LMs) have demonstrated remarkable capabilities across a wide\nrange of tasks in various domains. Despite their impressive performance, the\nreliability of their output is concerning and questionable regarding the demand\nfor AI safety. Assessing the confidence of LM predictions and calibrating them\nacross different tasks with the aim to align LM confidence with accuracy can\nhelp mitigate risks and enable LMs to make better decisions. There have been\nvarious works in this respect, but there has been no comprehensive overview of\nthis important research area. The present survey aims to bridge this gap. In\nparticular, we discuss methods and techniques for LM confidence estimation and\ncalibration, encompassing different LMs and various tasks. We further outline\nthe challenges of estimating the confidence for large language models and we\nsuggest some promising directions for future work.\n","authors":["Jiahui Geng","Fengyu Cai","Yuxia Wang","Heinz Koeppl","Preslav Nakov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2311.08298v1.pdf","comment":"16 pages, 1 page, 1 table"},{"id":"http://arxiv.org/abs/2307.10485v2","updated":"2023-11-14T16:34:00Z","published":"2023-07-19T22:43:57Z","title":"FinGPT: Democratizing Internet-scale Data for Financial Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating human-like texts, which may potentially\nrevolutionize the finance industry. However, existing LLMs often fall short in\nthe financial field, which is mainly attributed to the disparities between\ngeneral text data and financial text data. Unfortunately, there is only a\nlimited number of financial text datasets available, and BloombergGPT, the\nfirst financial LLM (FinLLM), is close-sourced (only the training logs were\nreleased). In light of this, we aim to democratize Internet-scale financial\ndata for LLMs, which is an open challenge due to diverse data sources, low\nsignal-to-noise ratio, and high time-validity. To address the challenges, we\nintroduce an open-sourced and data-centric framework, Financial Generative\nPre-trained Transformer (FinGPT), that automates the collection and curation of\nreal-time financial data from 34 diverse sources on the Internet, providing\nresearchers and practitioners with accessible and transparent resources to\ndevelop their FinLLMs. Additionally, we propose a simple yet effective strategy\nfor fine-tuning FinLLM using the inherent feedback from the market, dubbed\nReinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank\nAdaptation (LoRA, QLoRA) method that enables users to customize their own\nFinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several\nFinGPT applications, including robo-advisor, sentiment analysis for algorithmic\ntrading, and low-code development. FinGPT aims to democratize FinLLMs,\nstimulate innovation, and unlock new opportunities in open finance. The codes\nhave been open-sourced.\n","authors":["Xiao-Yang Liu","Guoxuan Wang","Hongyang Yang","Daochen Zha"],"pdf_url":"https://arxiv.org/pdf/2307.10485v2.pdf","comment":"43 pages, 8 tables, and 2 figures"},{"id":"http://arxiv.org/abs/2311.08287v1","updated":"2023-11-14T16:30:36Z","published":"2023-11-14T16:30:36Z","title":"How Well Do Large Language Models Understand Syntax? An Evaluation by\n  Asking Natural Language Questions","summary":"  While recent advancements in large language models (LLMs) bring us closer to\nachieving artificial general intelligence, the question persists: Do LLMs truly\nunderstand language, or do they merely mimic comprehension through pattern\nrecognition? This study seeks to explore this question through the lens of\nsyntax, a crucial component of sentence comprehension. Adopting a natural\nlanguage question-answering (Q&A) scheme, we craft questions targeting nine\nsyntactic knowledge points that are most closely related to sentence\ncomprehension. Experiments conducted on 24 LLMs suggest that most have a\nlimited grasp of syntactic knowledge, exhibiting notable discrepancies across\ndifferent syntactic knowledge points. In particular, questions involving\nprepositional phrase attachment pose the greatest challenge, whereas those\nconcerning adjectival modifier and indirect object are relatively easier for\nLLMs to handle. Furthermore, a case study on the training dynamics of the LLMs\nreveals that the majority of syntactic knowledge is learned during the initial\nstages of training, hinting that simply increasing the number of training\ntokens may not be the `silver bullet' for improving the comprehension ability\nof LLMs.\n","authors":["Houquan Zhou","Yang Hou","Zhenghua Li","Xuebin Wang","Zhefeng Wang","Xinyu Duan","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08287v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08273v1","updated":"2023-11-14T16:11:23Z","published":"2023-11-14T16:11:23Z","title":"Examining Modularity in Multilingual LMs via Language-Specialized\n  Subnetworks","summary":"  Recent work has proposed explicitly inducing language-wise modularity in\nmultilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a\nmeans of better guiding cross-lingual sharing. In this work, we investigate (1)\nthe degree to which language-wise modularity naturally arises within models\nwith no special modularity interventions, and (2) how cross-lingual sharing and\ninterference differ between such models and those with explicit SFT-guided\nsubnetwork modularity. To quantify language specialization and cross-lingual\ninteraction, we use a Training Data Attribution method that estimates the\ndegree to which a model's predictions are influenced by in-language or\ncross-language training examples. Our results show that language-specialized\nsubnetworks do naturally arise, and that SFT, rather than always increasing\nmodularity, can decrease language specialization of subnetworks in favor of\nmore cross-lingual sharing.\n","authors":["Rochelle Choenni","Ekaterina Shutova","Dan Garrette"],"pdf_url":"https://arxiv.org/pdf/2311.08273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08268v1","updated":"2023-11-14T16:02:16Z","published":"2023-11-14T16:02:16Z","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily","summary":"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful\ncontent. Exploring jailbreak prompts can help to better reveal the weaknesses\nof LLMs and further steer us to secure them. Unfortunately, existing jailbreak\nmethods either suffer from intricate manual design or require optimization on\nanother white-box model, compromising generalization or jailbreak efficiency.\nIn this paper, we generalize jailbreak prompt attacks into two aspects: (1)\nPrompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,\nan automatic framework that leverages LLMs themselves to generate effective\njailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly\nimproves the attack success rate while greatly reducing the time cost compared\nto existing baselines. Our study also reveals the inadequacy of current defense\nmethods in safeguarding LLMs. Finally, we offer detailed analysis and\ndiscussion from the perspective of prompt execution priority on the failure of\nLLMs' defense. We hope that our research can catalyze both the academic\ncommunity and LLMs vendors towards the provision of safer and more regulated\nLarge Language Models.\n","authors":["Peng Ding","Jun Kuang","Dan Ma","Xuezhi Cao","Yunsen Xian","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08263v1","updated":"2023-11-14T15:56:18Z","published":"2023-11-14T15:56:18Z","title":"Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads\n  to Answers Faster","summary":"  In this work, we propose FastCoT, a model-agnostic framework based on\nparallel decoding without any further training of an auxiliary model or\nmodification to the LLM itself. FastCoT uses a size-varying context window\nwhose size changes with position to conduct parallel decoding and\nauto-regressive decoding simultaneously, thus fully utilizing GPU computation\nresources. In FastCoT, the parallel decoding part provides the LLM with a quick\nglance of the future composed of approximate tokens, which could lead to faster\nanswers compared to regular autoregressive decoding used by causal\ntransformers. We also provide an implementation of parallel decoding within\nLLM, which supports KV-cache generation and batch processing. Through extensive\nexperiments, we demonstrate that FastCoT saves inference time by nearly 20%\nwith only a negligible performance drop compared to the regular approach.\nAdditionally, we show that the context window size exhibits considerable\nrobustness for different tasks.\n","authors":["Hongxuan Zhang","Zhining Liu","Jiaqi Zheng","Chenyi Zhuang","Jinjie Gu","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07793v2","updated":"2023-11-14T15:51:18Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional carefully\ndesigned embedding-based and rule-based models dominate. The question remains\nopen of whether pre-trained LLMs can understand structured temporal relational\ndata and replace them as the foundation model for temporal relational\nforecasting. Therefore, we bring temporal knowledge forecasting into the\ngenerative setting. However, challenges occur in the huge chasms between\ncomplex temporal graph data structure and sequential natural expressions LLMs\ncan handle, and between the enormous data sizes of tKGs and heavy computation\ncosts of finetuning LLMs. To address these challenges, we propose a novel\nretrieval augmented generation framework that performs generative forecasting\non tKGs named GenTKG, which combines a temporal logical rule-based retrieval\nstrategy and lightweight parameter-efficient instruction tuning. Extensive\nexperiments have shown that GenTKG outperforms conventional methods of temporal\nrelational forecasting under low computation resources. GenTKG also highlights\nremarkable transferability with exceeding performance on unseen datasets\nwithout re-training. Our work reveals the huge potential of LLMs in the tKG\ndomain and opens a new frontier for generative forecasting on tKGs.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v2.pdf","comment":"8 pages, accepted to Temporal Graph Learning @ NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08252v1","updated":"2023-11-14T15:43:47Z","published":"2023-11-14T15:43:47Z","title":"REST: Retrieval-Based Speculative Decoding","summary":"  We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.\n","authors":["Zhenyu He","Zexuan Zhong","Tianle Cai","Jason D Lee","Di He"],"pdf_url":"https://arxiv.org/pdf/2311.08252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01420v2","updated":"2023-11-14T15:41:45Z","published":"2023-09-26T23:27:06Z","title":"Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring\n  Systems","summary":"  Conversational tutoring systems (CTSs) offer learning experiences driven by\nnatural language interaction. They are known to promote high levels of\ncognitive engagement and benefit learning outcomes, particularly in reasoning\ntasks. Nonetheless, the time and cost required to author CTS content is a major\nobstacle to widespread adoption. In this paper, we introduce a novel type of\nCTS that leverages the recent advances in large language models (LLMs) in two\nways: First, the system induces a tutoring script automatically from a lesson\ntext. Second, the system automates the script orchestration via two LLM-based\nagents (Ruffle&Riley) with the roles of a student and a professor in a\nlearning-by-teaching format. The system allows a free-form conversation that\nfollows the ITS-typical inner and outer loop structure. In an initial\nbetween-subject online user study (N = 100) comparing Ruffle&Riley to simpler\nQA chatbots and reading activity, we found no significant differences in\npost-test scores. Nonetheless, in the learning experience survey, Ruffle&Riley\nusers expressed higher ratings of understanding and remembering and further\nperceived the offered support as more helpful and the conversation as coherent.\nOur study provides insights for a new generation of scalable CTS technologies.\n","authors":["Robin Schmucker","Meng Xia","Amos Azaria","Tom Mitchell"],"pdf_url":"https://arxiv.org/pdf/2310.01420v2.pdf","comment":"NeurIPS'23 GAIED, Camera-ready"},{"id":"http://arxiv.org/abs/2311.08249v1","updated":"2023-11-14T15:37:19Z","published":"2023-11-14T15:37:19Z","title":"On Using Distribution-Based Compositionality Assessment to Evaluate\n  Compositional Generalisation in Machine Translation","summary":"  Compositional generalisation (CG), in NLP and in machine learning more\ngenerally, has been assessed mostly using artificial datasets. It is important\nto develop benchmarks to assess CG also in real-world natural language tasks in\norder to understand the abilities and limitations of systems deployed in the\nwild. To this end, our GenBench Collaborative Benchmarking Task submission\nutilises the distribution-based compositionality assessment (DBCA) framework to\nsplit the Europarl translation corpus into a training and a test set in such a\nway that the test set requires compositional generalisation capacity.\nSpecifically, the training and test sets have divergent distributions of\ndependency relations, testing NMT systems' capability of translating\ndependencies that they have not been trained on. This is a fully-automated\nprocedure to create natural language compositionality benchmarks, making it\nsimple and inexpensive to apply it further to other datasets and languages. The\ncode and data for the experiments is available at\nhttps://github.com/aalto-speech/dbca.\n","authors":["Anssi Moisio","Mathias Creutz","Mikko Kurimo"],"pdf_url":"https://arxiv.org/pdf/2311.08249v1.pdf","comment":"To appear at the GenBench Workshop at EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.08240v1","updated":"2023-11-14T15:21:49Z","published":"2023-11-14T15:21:49Z","title":"Investigating the Encoding of Words in BERT's Neurons using Feature\n  Textualization","summary":"  Pretrained language models (PLMs) form the basis of most state-of-the-art NLP\ntechnologies. Nevertheless, they are essentially black boxes: Humans do not\nhave a clear understanding of what knowledge is encoded in different parts of\nthe models, especially in individual neurons. The situation is different in\ncomputer vision, where feature visualization provides a decompositional\ninterpretability technique for neurons of vision models. Activation\nmaximization is used to synthesize inherently interpretable visual\nrepresentations of the information encoded in individual neurons. Our work is\ninspired by this but presents a cautionary tale on the interpretability of\nsingle neurons, based on the first large-scale attempt to adapt activation\nmaximization to NLP, and, more specifically, large PLMs. We propose feature\ntextualization, a technique to produce dense representations of neurons in the\nPLM word embedding space. We apply feature textualization to the BERT model\n(Devlin et al., 2019) to investigate whether the knowledge encoded in\nindividual neurons can be interpreted and symbolized. We find that the produced\nrepresentations can provide insights about the knowledge encoded in individual\nneurons, but that individual neurons do not represent clearcut symbolic units\nof language such as words. Additionally, we use feature textualization to\ninvestigate how many neurons are needed to encode words in BERT.\n","authors":["Tanja Baeumel","Soniya Vijayakumar","Josef van Genabith","Guenter Neumann","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2311.08240v1.pdf","comment":"To be published in 'BlackboxNLP 2023: The 6th Workshop on Analysing\n  and Interpreting Neural Networks for NLP'. Camera-ready version"},{"id":"http://arxiv.org/abs/2311.06595v2","updated":"2023-11-14T15:14:06Z","published":"2023-11-11T15:40:21Z","title":"From Classification to Generation: Insights into Crosslingual Retrieval\n  Augmented ICL","summary":"  The remarkable ability of Large Language Models (LLMs) to understand and\nfollow instructions has sometimes been limited by their in-context learning\n(ICL) performance in low-resource languages. To address this, we introduce a\nnovel approach that leverages cross-lingual retrieval-augmented in-context\nlearning (CREA-ICL). By extracting semantically similar prompts from\nhigh-resource languages, we aim to improve the zero-shot performance of\nmultilingual pre-trained language models (MPLMs) across diverse tasks. Though\nour approach yields steady improvements in classification tasks, it faces\nchallenges in generation tasks. Our evaluation offers insights into the\nperformance dynamics of retrieval-augmented in-context learning across both\nclassification and generation domains.\n","authors":["Xiaoqian Li","Ercong Nie","Sheng Liang"],"pdf_url":"https://arxiv.org/pdf/2311.06595v2.pdf","comment":"In The Workshop on Instruction Tuning and Instruction Following, held\n  in conjunction with The Conference on NeurIPS 2023, December 2023. arXiv\n  admin note: text overlap with arXiv:2311.00587"},{"id":"http://arxiv.org/abs/2311.08219v1","updated":"2023-11-14T14:56:33Z","published":"2023-11-14T14:56:33Z","title":"Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese\n  Spelling Correction","summary":"  ChatGPT has demonstrated impressive performance in various downstream tasks.\nHowever, in the Chinese Spelling Correction (CSC) task, we observe a\ndiscrepancy: while ChatGPT performs well under human evaluation, it scores\npoorly according to traditional metrics. We believe this inconsistency arises\nbecause the traditional metrics are not well-suited for evaluating generative\nmodels. Their overly strict length and phonics constraints may lead to\nunderestimating ChatGPT's correction capabilities. To better evaluate\ngenerative models in the CSC task, this paper proposes a new evaluation metric:\nEval-GCSC. By incorporating word-level and semantic similarity judgments, it\nrelaxes the stringent length and phonics constraints. Experimental results show\nthat Eval-GCSC closely aligns with human evaluations. Under this metric,\nChatGPT's performance is comparable to traditional token-level classification\nmodels (TCM), demonstrating its potential as a CSC tool. The source code and\nscripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.\n","authors":["Kunting Li","Yong Hu","Shaolei Wang","Hanhan Ma","Liang He","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08213v1","updated":"2023-11-14T14:49:46Z","published":"2023-11-14T14:49:46Z","title":"Unlock the Power: Competitive Distillation for Multi-Modal Large\n  Language Models","summary":"  Recently, multi-modal content generation has attracted lots of attention from\nresearchers by investigating the utilization of visual instruction tuning based\non large language models (LLMs). To enhance the performance and generalization\nability of such LLMs, the practice of distilling knowledge from pretrained\nmulti-modal models (a.k.a. teachers) to more compact multi-modal LLMs\n(students) has gained considerable interest. However, the prevailing paradigm\nof instructiontuning in multi-modal LLMs knowledge distillation is\nresource-intensive and unidirectional, neglecting the potential for mutual\nfeedback between the student and teacher models. Thus, we propose an innovative\nCompetitive Multi-modal Distillation framework (CoMD), which captures\nbidirectional feedback between teacher and student models and continually\nupdates the multi-modal capabilities that the student model has learned. It\ncomprises two stages: multi-modal pre-training and multi-modal competitive\ndistillation. The first stage pre-trains the student model on a large number of\nfiltered multi-modal datasets. The second stage facilitates a bidirectional\nknowledge transfer between the student and teacher models. Our experimental\nanalysis of diverse datasets shows that our knowledge transfer method\nconsistently improves the capabilities of the student model. Finally, the\n7B-sized student model after four distillations surpassed the current\nstate-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also\noutperforms other strong baselines in the zero-shot setting.\n","authors":["Xinwei Li","Li Lin","Shuai Wang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2311.08213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08206v1","updated":"2023-11-14T14:42:28Z","published":"2023-11-14T14:42:28Z","title":"Human-Centric Autonomous Systems With LLMs for User Command Reasoning","summary":"  The evolution of autonomous driving has made remarkable advancements in\nrecent years, evolving into a tangible reality. However, a human-centric\nlarge-scale adoption hinges on meeting a variety of multifaceted requirements.\nTo ensure that the autonomous system meets the user's intent, it is essential\nto accurately discern and interpret user commands, especially in complex or\nemergency situations. To this end, we propose to leverage the reasoning\ncapabilities of Large Language Models (LLMs) to infer system requirements from\nin-cabin users' commands. Through a series of experiments that include\ndifferent LLM models and prompt designs, we explore the few-shot multivariate\nbinary classification accuracy of system requirements from natural language\ntextual commands. We confirm the general ability of LLMs to understand and\nreason about prompts but underline that their effectiveness is conditioned on\nthe quality of both the LLM model and the design of appropriate sequential\nprompts. Code and models are public with the link\n\\url{https://github.com/KTH-RPL/DriveCmd_LLM}.\n","authors":["Yi Yang","Qingwen Zhang","Ci Li","Daniel Simões Marta","Nazre Batool","John Folkesson"],"pdf_url":"https://arxiv.org/pdf/2311.08206v1.pdf","comment":"6 pages, accepted by WACV LLVM-AD workshp, code\n  https://github.com/KTH-RPL/DriveCmd_LLM"},{"id":"http://arxiv.org/abs/2311.08195v1","updated":"2023-11-14T14:29:00Z","published":"2023-11-14T14:29:00Z","title":"Automated Fact-Checking in Dialogue: Are Specialized Models Needed?","summary":"  Prior research has shown that typical fact-checking models for stand-alone\nclaims struggle with claims made in dialogues. As a solution, fine-tuning these\nmodels on labelled dialogue data has been proposed. However, creating separate\nmodels for each use case is impractical, and we show that fine-tuning models\nfor dialogue results in poor performance on typical fact-checking. To overcome\nthis challenge, we present techniques that allow us to use the same models for\nboth dialogue and typical fact-checking. These mainly focus on retrieval\nadaptation and transforming conversational inputs so that they can be\naccurately predicted by models trained on stand-alone claims. We demonstrate\nthat a typical fact-checking model incorporating these techniques is\ncompetitive with state-of-the-art models fine-tuned for dialogue, while\nmaintaining its accuracy on stand-alone claims.\n","authors":["Eric Chamoun","Marzieh Saeidi","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2311.08195v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.08191v1","updated":"2023-11-14T14:24:36Z","published":"2023-11-14T14:24:36Z","title":"GEC-DePenD: Non-Autoregressive Grammatical Error Correction with\n  Decoupled Permutation and Decoding","summary":"  Grammatical error correction (GEC) is an important NLP task that is currently\nusually solved with autoregressive sequence-to-sequence models. However,\napproaches of this class are inherently slow due to one-by-one token\ngeneration, so non-autoregressive alternatives are needed. In this work, we\npropose a novel non-autoregressive approach to GEC that decouples the\narchitecture into a permutation network that outputs a self-attention weight\nmatrix that can be used in beam search to find the best permutation of input\ntokens (with auxiliary {ins} tokens) and a decoder network based on a\nstep-unrolled denoising autoencoder that fills in specific tokens. This allows\nus to find the token permutation after only one forward pass of the permutation\nnetwork, avoiding autoregressive constructions. We show that the resulting\nnetwork improves over previously known non-autoregressive methods for GEC and\nreaches the level of autoregressive methods that do not use language-specific\nsynthetic data generation methods. Our results are supported by a comprehensive\nexperimental validation on the ConLL-2014 and Write&Improve+LOCNESS datasets\nand an extensive ablation study that supports our architectural and algorithmic\nchoices.\n","authors":["Konstantin Yakovlev","Alexander Podolskiy","Andrey Bout","Sergey Nikolenko","Irina Piontkovskaya"],"pdf_url":"https://arxiv.org/pdf/2311.08191v1.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2311.08189v1","updated":"2023-11-14T14:22:47Z","published":"2023-11-14T14:22:47Z","title":"Unlocking Science: Novel Dataset and Benchmark for Cross-Modality\n  Scientific Information Extraction","summary":"  Extracting key information from scientific papers has the potential to help\nresearchers work more efficiently and accelerate the pace of scientific\nprogress. Over the last few years, research on Scientific Information\nExtraction (SciIE) witnessed the release of several new systems and benchmarks.\nHowever, existing paper-focused datasets mostly focus only on specific parts of\na manuscript (e.g., abstracts) and are single-modality (i.e., text- or\ntable-only), due to complex processing and expensive annotations. Moreover,\ncore information can be present in either text or tables or across both. To\nclose this gap in data availability and enable cross-modality IE, while\nalleviating labeling costs, we propose a semi-supervised pipeline for\nannotating entities in text, as well as entities and relations in tables, in an\niterative procedure. Based on this pipeline, we release novel resources for the\nscientific community, including a high-quality benchmark, a large-scale corpus,\nand a semi-supervised annotation pipeline. We further report the performance of\nstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.\nLastly, we explore the potential capability of large language models such as\nChatGPT for the current task. Our new dataset, results, and analysis validate\nthe effectiveness and efficiency of our semi-supervised pipeline, and we\ndiscuss its remaining limitations.\n","authors":["Yuhan Li","Jian Wu","Zhiwei Yu","Börje F. Karlsso","Wei Shen","Manabu Okumura","Chin-Yew Lin"],"pdf_url":"https://arxiv.org/pdf/2311.08189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08182v1","updated":"2023-11-14T14:10:40Z","published":"2023-11-14T14:10:40Z","title":"Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning","summary":"  Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.\n","authors":["Shengguang Wu","Keming Lu","Benfeng Xu","Junyang Lin","Qi Su","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07439v2","updated":"2023-11-14T14:01:46Z","published":"2023-11-13T16:15:20Z","title":"Investigating Multi-Pivot Ensembling with Massively Multilingual Machine\n  Translation Models","summary":"  Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. As an alternative, we propose MaxEns, a\ncombination strategy that is biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n","authors":["Alireza Mohammadshahi","Jannis Vamvas","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2311.07439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08166v1","updated":"2023-11-14T13:49:03Z","published":"2023-11-14T13:49:03Z","title":"MechAgents: Large language model multi-agent collaborations can solve\n  mechanics problems, generate new data, and integrate knowledge","summary":"  Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.\n","authors":["Bo Ni","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2311.08166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17940v2","updated":"2023-11-14T13:39:51Z","published":"2023-10-27T07:34:51Z","title":"Unified Segment-to-Segment Framework for Simultaneous Sequence\n  Generation","summary":"  Simultaneous sequence generation is a pivotal task for real-time scenarios,\nsuch as streaming speech recognition, simultaneous machine translation and\nsimultaneous speech translation, where the target sequence is generated while\nreceiving the source sequence. The crux of achieving high-quality generation\nwith low latency lies in identifying the optimal moments for generating,\naccomplished by learning a mapping between the source and target sequences.\nHowever, existing methods often rely on task-specific heuristics for different\nsequence types, limiting the model's capacity to adaptively learn the\nsource-target mapping and hindering the exploration of multi-task learning for\nvarious simultaneous tasks. In this paper, we propose a unified\nsegment-to-segment framework (Seg2Seg) for simultaneous sequence generation,\nwhich learns the mapping in an adaptive and unified manner. During the process\nof simultaneous generation, the model alternates between waiting for a source\nsegment and generating a target segment, making the segment serve as the\nnatural bridge between the source and target. To accomplish this, Seg2Seg\nintroduces a latent segment as the pivot between source to target and explores\nall potential source-target mappings via the proposed expectation training,\nthereby learning the optimal moments for generating. Experiments on multiple\nsimultaneous generation tasks demonstrate that Seg2Seg achieves\nstate-of-the-art performance and exhibits better generality across various\ntasks.\n","authors":["Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2310.17940v2.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08154v1","updated":"2023-11-14T13:30:54Z","published":"2023-11-14T13:30:54Z","title":"Ask One More Time: Self-Agreement Improves Reasoning of Language Models\n  in (Almost) All Scenarios","summary":"  Although chain-of-thought (CoT) prompting combined with language models has\nachieved encouraging results on complex reasoning tasks, the naive greedy\ndecoding used in CoT prompting usually causes the repetitiveness and local\noptimality. To address this shortcoming, ensemble-optimization tries to obtain\nmultiple reasoning paths to get the final answer assembly. However, current\nensemble-optimization methods either simply employ rule-based post-processing\nsuch as \\textit{self-consistency}, or train an additional model based on\nseveral task-related human annotations to select the best one among multiple\nreasoning paths, yet fail to generalize to realistic settings where the type of\ninput questions is unknown or the answer format of reasoning paths is unknown.\nTo avoid their limitations, we propose \\textbf{self-agreement}, a generalizable\nensemble-optimization method applying in almost all scenarios where the type of\ninput questions and the answer format of reasoning paths may be known or\nunknown. Self-agreement firstly samples from language model's decoder to\ngenerate a \\textit{diverse} set of reasoning paths, and subsequently prompts\nthe language model \\textit{one more time} to determine the optimal answer by\nselecting the most \\textit{agreed} answer among the sampled reasoning paths.\nSelf-agreement simultaneously achieves remarkable performance on six public\nreasoning benchmarks and superior generalization capabilities.\n","authors":["Lei Lin","Jiayi Fu","Pengli Liu","Junchen Wan","Fuzheng Zhang","Zhongyuan Wang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2311.08154v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.08152v1","updated":"2023-11-14T13:27:07Z","published":"2023-11-14T13:27:07Z","title":"Towards Reasoning in Large Language Models via Multi-Agent Peer Review\n  Collaboration","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in general\nnatural language processing tasks but often fall short in complex reasoning\ntasks. Recent studies have explored human-like problem-solving strategies, such\nas self-correct, to push further the boundary of single-model reasoning\nability. In this work, we let a single model \"step outside the box\" by engaging\nmultiple models to correct each other. We introduce a multi-agent collaboration\nstrategy that emulates the academic peer review process. Each agent\nindependently constructs its own solution, provides reviews on the solutions of\nothers, and assigns confidence levels to its reviews. Upon receiving peer\nreviews, agents revise their initial solutions. Extensive experiments on three\ndifferent types of reasoning tasks show that our collaboration approach\ndelivers superior accuracy across all ten datasets compared to existing\nmethods. Further study demonstrates the effectiveness of integrating confidence\nin the reviews for math reasoning, and suggests a promising direction for\nhuman-mimicking multi-agent collaboration process.\n","authors":["Zhenran Xu","Senbao Shi","Baotian Hu","Jindi Yu","Dongfang Li","Min Zhang","Yuxiang Wu"],"pdf_url":"https://arxiv.org/pdf/2311.08152v1.pdf","comment":"9 pages, 3 figures, 8 tables. Work in progress"},{"id":"http://arxiv.org/abs/2311.08147v1","updated":"2023-11-14T13:24:19Z","published":"2023-11-14T13:24:19Z","title":"RECALL: A Benchmark for LLMs Robustness against External Counterfactual\n  Knowledge","summary":"  LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.\n","authors":["Yi Liu","Lianzhe Huang","Shicheng Li","Sishuo Chen","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2311.08147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08143v1","updated":"2023-11-14T13:20:23Z","published":"2023-11-14T13:20:23Z","title":"Sinkhorn Transformations for Single-Query Postprocessing in Text-Video\n  Retrieval","summary":"  A recent trend in multimodal retrieval is related to postprocessing test set\nresults via the dual-softmax loss (DSL). While this approach can bring\nsignificant improvements, it usually presumes that an entire matrix of test\nsamples is available as DSL input. This work introduces a new postprocessing\napproach based on Sinkhorn transformations that outperforms DSL. Further, we\npropose a new postprocessing setting that does not require access to multiple\ntest queries. We show that our approach can significantly improve the results\nof state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thus\nachieving a new state-of-the-art on several standard text-video retrieval\ndatasets both with access to the entire test set and in the single-query\nsetting.\n","authors":["Konstantin Yakovlev","Gregory Polyakov","Ilseyar Alimova","Alexander Podolskiy","Andrey Bout","Sergey Nikolenko","Irina Piontkovskaya"],"pdf_url":"https://arxiv.org/pdf/2311.08143v1.pdf","comment":"SIGIR 2023"},{"id":"http://arxiv.org/abs/2311.07093v2","updated":"2023-11-14T13:09:51Z","published":"2023-11-13T05:45:55Z","title":"On the Effectiveness of ASR Representations in Real-world Noisy Speech\n  Emotion Recognition","summary":"  This paper proposes an efficient attempt to noisy speech emotion recognition\n(NSER). Conventional NSER approaches have proven effective in mitigating the\nimpact of artificial noise sources, such as white Gaussian noise, but are\nlimited to non-stationary noises in real-world environments due to their\ncomplexity and uncertainty. To overcome this limitation, we introduce a new\nmethod for NSER by adopting the automatic speech recognition (ASR) model as a\nnoise-robust feature extractor to eliminate non-vocal information in noisy\nspeech. We first obtain intermediate layer information from the ASR model as a\nfeature representation for emotional speech and then apply this representation\nfor the downstream NSER task. Our experimental results show that 1) the\nproposed method achieves better NSER performance compared with the conventional\nnoise reduction method, 2) outperforms self-supervised learning approaches, and\n3) even outperforms text-based approaches using ASR transcription or the ground\ntruth transcription of noisy speech.\n","authors":["Xiaohan Shi","Jiajun He","Xingfeng Li","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2311.07093v2.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2307.06440v4","updated":"2023-11-14T13:01:48Z","published":"2023-07-12T20:10:14Z","title":"No Train No Gain: Revisiting Efficient Training Algorithms For\n  Transformer-based Language Models","summary":"  The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n","authors":["Jean Kaddour","Oscar Key","Piotr Nawrot","Pasquale Minervini","Matt J. Kusner"],"pdf_url":"https://arxiv.org/pdf/2307.06440v4.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08123v1","updated":"2023-11-14T12:37:25Z","published":"2023-11-14T12:37:25Z","title":"Memory-efficient Stochastic methods for Memory-based Transformers","summary":"  Training Memory-based transformers can require a large amount of memory and\ncan be quite inefficient. We propose a novel two-phase training mechanism and a\nnovel regularization technique to improve the training efficiency of\nmemory-based transformers, which are often used for long-range context\nproblems. For our experiments, we consider transformer-XL as our baseline model\nwhich is one of memorybased transformer models. We show that our resultant\nmodel, Skip Cross-head TransformerXL, outperforms the baseline on character\nlevel language modeling task with similar parameters and outperforms the\nbaseline on word level language modelling task with almost 20% fewer\nparameters. Our proposed methods do not require any additional memory. We also\ndemonstrate the effectiveness of our regularization mechanism on BERT which\nshows similar performance with reduction in standard deviation of scores of\naround 30% on multiple GLUE tasks.\n","authors":["Vishwajit Kumar Vishnu","C. Chandra Sekhar"],"pdf_url":"https://arxiv.org/pdf/2311.08123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08117v1","updated":"2023-11-14T12:30:28Z","published":"2023-11-14T12:30:28Z","title":"Insights into Classifying and Mitigating LLMs' Hallucinations","summary":"  The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.\n","authors":["Alessandro Bruno","Pier Luigi Mazzeo","Aladine Chetouani","Marouane Tliba","Mohamed Amine Kerkouri"],"pdf_url":"https://arxiv.org/pdf/2311.08117v1.pdf","comment":"Accepted at AIxIA 2023"},{"id":"http://arxiv.org/abs/2311.08110v1","updated":"2023-11-14T12:14:54Z","published":"2023-11-14T12:14:54Z","title":"Improving hateful memes detection via learning hatefulness-aware\n  embedding space through retrieval-guided contrastive learning","summary":"  Hateful memes have emerged as a significant concern on the Internet. These\nmemes, which are a combination of image and text, often convey messages vastly\ndifferent from their individual meanings. Thus, detecting hateful memes\nrequires the system to jointly understand the visual and textual modalities.\nHowever, our investigation reveals that the embedding space of existing\nCLIP-based systems lacks sensitivity to subtle differences in memes that are\nvital for correct hatefulness classification. To address this issue, we propose\nconstructing a hatefulness-aware embedding space through retrieval-guided\ncontrastive training. Specifically, we add an auxiliary loss that utilizes hard\nnegative and pseudo-gold samples to train the embedding space. Our approach\nachieves state-of-the-art performance on the HatefulMemes dataset with an AUROC\nof 86.7. Notably, our approach outperforms much larger fine-tuned Large\nMultimodal Models like Flamingo and LLaVA. Finally, we demonstrate a\nretrieval-based hateful memes detection system, which is capable of making\nhatefulness classification based on data unseen in training from a database.\nThis allows developers to update the hateful memes detection system by simply\nadding new data without retraining, a desirable feature for real services in\nthe constantly-evolving landscape of hateful memes on the Internet.\n","authors":["Jingbiao Mei","Jinghong Chen","Weizhe Lin","Bill Byrne","Marcus Tomalin"],"pdf_url":"https://arxiv.org/pdf/2311.08110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08107v1","updated":"2023-11-14T12:12:25Z","published":"2023-11-14T12:12:25Z","title":"SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training\n  with Adversarial Remarks","summary":"  Large Language Models (LLMs) can justify or criticize their predictions\nthrough discussion with other models or humans, thereby enhancing their\nintrinsic understanding of instances. While proactive discussions enhance\nperformance, this approach is currently limited to the inference phase. In this\ncontext, we posit a hypothesis: learning interactive discussions during\ntraining can improve understanding for the instances in the training step and\nproficiency in logical/critical thinking ability and verbalized expression of\nthe model in the inference step. Our proposed SAIE training method involves\nboth supportive and adversarial discussions between the learner and partner\nmodels. The learner model receives a remark from the partner through the\ndiscussion, and the parameters of the learner model are then updated based on\nthis remark. That is, the teacher signal dynamically adjusts in response to the\nevolving model output throughout the training step. By bolstering the capacity\nfor discussion and comprehension of instances, our experiments across datasets,\nincluding GSM8K, CommonsenseQA, and MMLU, reveal that models fine-tuned with\nour method consistently surpass those trained with standard fine-tuning\ntechniques. Moreover, our approach demonstrates superior performance in\nmulti-agent inference scenarios, boosting the models' reasoning abilities at\nthe inference step.\n","authors":["Mengsay Loem","Masahiro Kaneko","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2311.08107v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.08106v1","updated":"2023-11-14T12:12:02Z","published":"2023-11-14T12:12:02Z","title":"Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language\n  Models","summary":"  In an ever-evolving world, the dynamic nature of knowledge presents\nchallenges for language models that are trained on static data, leading to\noutdated encoded information. However, real-world scenarios require models not\nonly to acquire new knowledge but also to overwrite outdated information into\nupdated ones. To address this under-explored issue, we introduce the temporally\nevolving question answering benchmark, EvolvingQA - a novel benchmark designed\nfor training and evaluating LMs on an evolving Wikipedia database, where the\nconstruction of our benchmark is automated with our pipeline using large\nlanguage models. Our benchmark incorporates question-answering as a downstream\ntask to emulate real-world applications. Through EvolvingQA, we uncover that\nexisting continual learning baselines have difficulty in updating and\nforgetting outdated knowledge. Our findings suggest that the models fail to\nlearn updated knowledge due to the small weight gradient. Furthermore, we\nelucidate that the models struggle mostly on providing numerical or temporal\nanswers to questions asking for updated knowledge. Our work aims to model the\ndynamic nature of real-world information, offering a robust measure for the\nevolution-adaptability of language models.\n","authors":["Yujin Kim","Jaehong Yoon","Seonghyeon Ye","Sung Ju Hwang","Se-young Yun"],"pdf_url":"https://arxiv.org/pdf/2311.08106v1.pdf","comment":"14 pages, 5 figures, 5 tables; accepted at NeurIPS Syntheticdata4ML\n  workshop, 2023"},{"id":"http://arxiv.org/abs/2311.08105v1","updated":"2023-11-14T12:05:45Z","published":"2023-11-14T12:05:45Z","title":"DiLoCo: Distributed Low-Communication Training of Language Models","summary":"  Large language models (LLM) have become a critical component in many\napplications of machine learning. However, standard approaches to training LLM\nrequire a large number of tightly interconnected accelerators, with devices\nexchanging gradients and other intermediate states at each optimization step.\nWhile it is difficult to build and maintain a single computing cluster hosting\nmany accelerators, it might be easier to find several computing clusters each\nhosting a smaller number of devices. In this work, we propose a distributed\noptimization algorithm, Distributed Low-Communication (DiLoCo), that enables\ntraining of language models on islands of devices that are poorly connected.\nThe approach is a variant of federated averaging, where the number of inner\nsteps is large, the inner optimizer is AdamW, and the outer optimizer is\nNesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8\nworkers performs as well as fully synchronous optimization while communicating\n500 times less. DiLoCo exhibits great robustness to the data distribution of\neach worker. It is also robust to resources becoming unavailable over time, and\nvice versa, it can seamlessly leverage resources that become available during\ntraining.\n","authors":["Arthur Douillard","Qixuan Feng","Andrei A. Rusu","Rachita Chhaparia","Yani Donchev","Adhiguna Kuncoro","Marc'Aurelio Ranzato","Arthur Szlam","Jiajun Shen"],"pdf_url":"https://arxiv.org/pdf/2311.08105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08103v1","updated":"2023-11-14T12:03:26Z","published":"2023-11-14T12:03:26Z","title":"Exploring Semi-supervised Hierarchical Stacked Encoder for Legal\n  Judgement Prediction","summary":"  Predicting the judgment of a legal case from its unannotated case facts is a\nchallenging task. The lengthy and non-uniform document structure poses an even\ngreater challenge in extracting information for decision prediction. In this\nwork, we explore and propose a two-level classification mechanism; both\nsupervised and unsupervised; by using domain-specific pre-trained BERT to\nextract information from long documents in terms of sentence embeddings further\nprocessing with transformer encoder layer and use unsupervised clustering to\nextract hidden labels from these embeddings to better predict a judgment of a\nlegal case. We conduct several experiments with this mechanism and see higher\nperformance gains than the previously proposed methods on the ILDC dataset. Our\nexperimental results also show the importance of domain-specific pre-training\nof Transformer Encoders in legal information processing.\n","authors":["Nishchal Prasad","Mohand Boughanem","Taoufiq Dkaki"],"pdf_url":"https://arxiv.org/pdf/2311.08103v1.pdf","comment":"Published in the 1st International Workshop on Legal Information\n  Retrieval at ECIR 2023, April 2nd 2023, Dublin, Ireland.\n  (https://tmr.liacs.nl/legalIR/)"},{"id":"http://arxiv.org/abs/2311.04589v2","updated":"2023-11-14T11:58:48Z","published":"2023-11-08T10:34:16Z","title":"TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models","summary":"  Despite Multi-modal Large Language Models (MM-LLMs) have made exciting\nstrides recently, they are still struggling to efficiently model the\ninteractions among multi-modal inputs and the generation in non-textual\nmodalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an\napproach to treat the input from any modality as a token sequence and learn a\njoint embedding space for all modalities. Specifically, for the input from any\nmodality, TEAL first discretizes it into a token sequence with the\noff-the-shelf tokenizer and embeds the token sequence into a joint embedding\nspace with a learnable embedding matrix. MM-LLMs just need to predict the\nmulti-modal tokens autoregressively as the textual LLMs do. Finally, the\ncorresponding de-tokenizer is applied to generate the output in each modality\nbased on the predicted token sequence. With the joint embedding space, TEAL\nenables the frozen LLMs to perform both understanding and generation tasks\ninvolving non-textual modalities, such as image and audio. Thus, the textual\nLLM can just work as an interface and maintain its high performance in textual\nunderstanding and generation. Experiments show that TEAL achieves substantial\nimprovements in multi-modal understanding, and implements a simple scheme for\nmulti-modal generations.\n","authors":["Zhen Yang","Yingxue Zhang","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.04589v2.pdf","comment":"Multi-modal, Large Language Models, Tokenizer, Understanding and\n  Generation"},{"id":"http://arxiv.org/abs/2311.08097v1","updated":"2023-11-14T11:49:43Z","published":"2023-11-14T11:49:43Z","title":"Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts","summary":"  Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large\nLanguage Models (LLMs), eliciting them to solve complex reasoning tasks\nstep-by-step. However, with the success of CoT methods, the ability to deliver\nmulti-step reasoning remains limited to English due to the imbalance in the\ndistribution of the pre-training data, making the other languages a barrier.\n  In this work, we propose a Cross-lingual multi-step reasoning approach,\naiming to align reasoning processes across different languages. In particular,\nour method, through a Self-consistent Cross-lingual prompting mechanism\ninspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths\nin different languages that, during the steps, lead to the final solution. Our\nexperimental evaluations show that our method significantly outperforms\nexisting prompting methods, reducing the number of interactions and achieving\nstate-of-the-art performance.\n","authors":["Leonardo Ranaldi","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2311.08097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03520v2","updated":"2023-11-14T11:46:15Z","published":"2023-05-05T13:50:04Z","title":"Context-Aware Semantic Similarity Measurement for Unsupervised Word\n  Sense Disambiguation","summary":"  The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.\n","authors":["Jorge Martinez-Gil"],"pdf_url":"https://arxiv.org/pdf/2305.03520v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2311.08093v1","updated":"2023-11-14T11:35:09Z","published":"2023-11-14T11:35:09Z","title":"Spot: A Natural Language Interface for Geospatial Searches in OSM","summary":"  Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to\nbe an invaluable resource for their work due to its extensive coverage and\nintricate details of various locations, which play a crucial role in\ninvestigating news scenes. Despite its value, OSM's complexity presents\nconsiderable accessibility and usability challenges, especially for those\nwithout a technical background. To address this, we introduce 'Spot', a\nuser-friendly natural language interface for querying OSM data. Spot utilizes a\nsemantic mapping from natural language to OSM tags, leveraging artificially\ngenerated sentence queries and a T5 transformer. This approach enables Spot to\nextract relevant information from user-input sentences and display candidate\nlocations matching the descriptions on a map. To foster collaboration and\nfuture advancement, all code and generated data is available as an open-source\nrepository.\n","authors":["Lynn Khellaf","Ipek Baris Schlicht","Julia Bayer","Ruben Bouwmeester","Tilman Miraß","Tilman Wagner"],"pdf_url":"https://arxiv.org/pdf/2311.08093v1.pdf","comment":"To be published in the Proceedings of the OSM Science 2023"},{"id":"http://arxiv.org/abs/2311.08089v1","updated":"2023-11-14T11:24:08Z","published":"2023-11-14T11:24:08Z","title":"Align after Pre-train: Improving Multilingual Generative Models with\n  Cross-lingual Alignment","summary":"  Multilingual generative models obtain remarkable cross-lingual capabilities\nthrough pre-training on large-scale corpora. However, they still exhibit a\nperformance bias toward high-resource languages, and learn isolated\ndistributions of sentence representations across languages. To bridge this gap,\nwe propose a simple yet effective alignment framework exploiting pairs of\ntranslation sentences. It aligns the internal sentence representations across\ndifferent languages via multilingual contrastive learning and aligns model\noutputs by answering prompts in different languages. Experimental results\ndemonstrate that even with less than 0.1 {\\textperthousand} of pre-training\ntokens, our alignment framework significantly boosts the cross-lingual\nabilities of generative models and mitigates the performance gap. Further\nanalysis reveals that it results in a better internal multilingual\nrepresentation distribution of multilingual models.\n","authors":["Chong Li","Shaonan Wang","Jiajun Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2311.08089v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.20410v2","updated":"2023-11-14T11:01:06Z","published":"2023-10-31T12:32:38Z","title":"FollowBench: A Multi-level Fine-grained Constraints Following Benchmark\n  for Large Language Models","summary":"  The ability to follow instructions is crucial for Large Language Models\n(LLMs) to handle various real-world applications. Existing benchmarks primarily\nfocus on evaluating pure response quality, rather than assessing whether the\nresponse follows constraints stated in the instruction. To fill this research\ngap, in this paper, we propose FollowBench, a Multi-level Fine-grained\nConstraints Following Benchmark for LLMs. FollowBench comprehensively includes\nfive different types (i.e., Content, Situation, Style, Format, and Example) of\nfine-grained constraints. To enable a precise constraint following estimation\non diverse difficulties, we introduce a Multi-level mechanism that\nincrementally adds a single constraint to the initial instruction at each\nincreased level. To assess whether LLMs' outputs have satisfied every\nindividual constraint, we propose to prompt strong LLMs with\nconstraint-evolution paths to handle challenging open-ended instructions. By\nevaluating ten closed-source and open-source popular LLMs on FollowBench, we\nhighlight the weaknesses of LLMs in instruction following and point towards\npotential avenues for future work. The data and code are publicly available at\nhttps://github.com/YJiangcm/FollowBench.\n","authors":["Yuxin Jiang","Yufei Wang","Xingshan Zeng","Wanjun Zhong","Liangyou Li","Fei Mi","Lifeng Shang","Xin Jiang","Qun Liu","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2310.20410v2.pdf","comment":"19 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2311.08057v1","updated":"2023-11-14T10:30:49Z","published":"2023-11-14T10:30:49Z","title":"Data and models for stance and premise detection in COVID-19 tweets:\n  insights from the Social Media Mining for Health (SMM4H) 2022 shared task","summary":"  The COVID-19 pandemic has sparked numerous discussions on social media\nplatforms, with users sharing their views on topics such as mask-wearing and\nvaccination. To facilitate the evaluation of neural models for stance detection\nand premise classification, we organized the Social Media Mining for Health\n(SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts\non three COVID-19-related topics: school closures, stay-at-home orders, and\nwearing masks. In this paper, we extend the previous work and present newly\ncollected data on vaccination from Twitter to assess the performance of models\non a different topic. To enhance the accuracy and effectiveness of our\nevaluation, we employed various strategies to aggregate tweet texts with\nclaims, including models with feature-level (early) fusion and dual-view\narchitectures from SMM4H 2022 leaderboard. Our primary objective was to create\na valuable dataset and perform an extensive experimental evaluation to support\nfuture research in argument mining in the health domain.\n","authors":["Vera Davydova","Huabin Yang","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2311.08057v1.pdf","comment":"This paper is under review in the Journal of Biomedical Informatics"},{"id":"http://arxiv.org/abs/2311.08045v1","updated":"2023-11-14T10:10:31Z","published":"2023-11-14T10:10:31Z","title":"Adversarial Preference Optimization","summary":"  Human preference alignment is a crucial training step to improve the\ninteraction quality of large language models (LLMs). Existing aligning methods\ndepend on manually annotated preference data to guide the LLM optimization\ndirections. However, in practice, continuously updating LLMs raises a\ndistribution gap between model-generated samples and human-preferred responses,\nwhich hinders model fine-tuning efficiency. To mitigate this issue, previous\nmethods require additional preference annotation on generated samples to adapt\nthe shifted distribution, which consumes a large amount of annotation\nresources. Targeting more efficient human preference optimization, we propose\nan adversarial preference optimization (APO) framework, where the LLM agent and\nthe preference model update alternatively via a min-max game. Without\nadditional annotation, our APO method can make a self-adaption to the\ngeneration distribution gap through the adversarial learning process. In\nexperiments, we empirically verify the effectiveness of APO in improving LLM's\nhelpfulness and harmlessness compared with rejection sampling baselines.\n","authors":["Pengyu Cheng","Yifan Yang","Jian Li","Yong Dai","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2311.08045v1.pdf","comment":"In process"},{"id":"http://arxiv.org/abs/2311.08011v1","updated":"2023-11-14T09:12:40Z","published":"2023-11-14T09:12:40Z","title":"Forgetting before Learning: Utilizing Parametric Arithmetic for\n  Knowledge Updating in Large Language Models","summary":"  Recently Large Language Models (LLMs) have demonstrated their amazing text\nunderstanding and generation capabilities. However, even stronger LLMs may\nstill learn incorrect knowledge from the training corpus, as well as some\nknowledge that is outdated over time. Direct secondary fine-tuning with data\ncontaining new knowledge may be ineffective in updating knowledge due to the\nconflict between old and new knowledge. In this paper, we propose a new\nparadigm for fine-tuning called F-Learning (Forgetting before Learning), which\nis based on parametric arithmetic to achieve forgetting of old knowledge and\nlearning of new knowledge. Experimental results on two publicly available\ndatasets demonstrate that our proposed F-Learning can obviously improve the\nknowledge updating performance of both full fine-tuning and LoRA fine-tuning.\nMoreover, we have also discovered that forgetting old knowledge by subtracting\nthe parameters of LoRA can achieve a similar effect to subtracting the\nparameters of full fine-tuning, and sometimes even surpass it significantly.\n","authors":["Shiwen Ni","Dingwei Chen","Chengming Li","Xiping Hu","Ruifeng Xu","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2311.08011v1.pdf","comment":"8 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.08010v1","updated":"2023-11-14T09:09:58Z","published":"2023-11-14T09:09:58Z","title":"Distantly-Supervised Named Entity Recognition with Uncertainty-aware\n  Teacher Learning and Student-student Collaborative Learning","summary":"  Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates\nthe burden of annotation, but meanwhile suffers from the label noise. Recent\nworks attempt to adopt the teacher-student framework to gradually refine the\ntraining labels and improve the overall robustness. However, we argue that\nthese teacher-student methods achieve limited performance because poor network\ncalibration produces incorrectly pseudo-labeled samples, leading to error\npropagation. Therefore, we attempt to mitigate this issue by proposing: (1)\nUncertainty-aware Teacher Learning that leverages the prediction uncertainty to\nguide the selection of pseudo-labels, avoiding the number of incorrect\npseudo-labels in the self-training stage. (2) Student-student Collaborative\nLearning that allows the transfer of reliable labels between two student\nnetworks instead of completely relying on all pseudo-labels from its teacher.\nMeanwhile, this approach allows a full exploration of mislabeled samples rather\nthan simply filtering unreliable pseudo-labeled samples. Extensive experimental\nresults on five DS-NER datasets demonstrate that our method is superior to\nstate-of-the-art teacher-student methods.\n","authors":["Helan Hu","Shuzheng Si","Haozhe Zhao","Shuang Zeng","Kaikai An","Zefan Cai","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2311.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08002v1","updated":"2023-11-14T08:57:01Z","published":"2023-11-14T08:57:01Z","title":"TempTabQA: Temporal Question Answering for Semi-Structured Tables","summary":"  Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.\n","authors":["Vivek Gupta","Pranshu Kandoi","Mahek Bhavesh Vora","Shuo Zhang","Yujie He","Ridho Reinanda","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2311.08002v1.pdf","comment":"EMNLP 2023(Main), 23 Figures, 32 Tables"},{"id":"http://arxiv.org/abs/2311.08001v1","updated":"2023-11-14T08:55:11Z","published":"2023-11-14T08:55:11Z","title":"A Comparative Analysis of the COVID-19 Infodemic in English and Chinese:\n  Insights from Social Media Textual Data","summary":"  The COVID-19 infodemic, characterized by the rapid spread of misinformation\nand unverified claims related to the pandemic, presents a significant\nchallenge. This paper presents a comparative analysis of the COVID-19 infodemic\nin the English and Chinese languages, utilizing textual data extracted from\nsocial media platforms. To ensure a balanced representation, two infodemic\ndatasets were created by augmenting previously collected social media textual\ndata. Through word frequency analysis, the thirty-five most frequently\noccurring infodemic words are identified, shedding light on prevalent\ndiscussions surrounding the infodemic. Moreover, topic clustering analysis\nuncovers thematic structures and provides a deeper understanding of primary\ntopics within each language context. Additionally, sentiment analysis enables\ncomprehension of the emotional tone associated with COVID-19 information on\nsocial media platforms in English and Chinese. This research contributes to a\nbetter understanding of the COVID-19 infodemic phenomenon and can guide the\ndevelopment of strategies to combat misinformation during public health crises\nacross different languages.\n","authors":["Jia Luo","Daiyun Peng","Lei Shi","Didier El Baz","Xinran Liu"],"pdf_url":"https://arxiv.org/pdf/2311.08001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07996v1","updated":"2023-11-14T08:51:00Z","published":"2023-11-14T08:51:00Z","title":"How Well Do Text Embedding Models Understand Syntax?","summary":"  Text embedding models have significantly contributed to advancements in\nnatural language processing by adeptly capturing semantic properties of textual\ndata. However, the ability of these models to generalize across a wide range of\nsyntactic contexts remains under-explored. In this paper, we first develop an\nevaluation set, named \\textbf{SR}, to scrutinize the capability for syntax\nunderstanding of text embedding models from two crucial syntactic aspects:\nStructural heuristics, and Relational understanding among concepts, as revealed\nby the performance gaps in previous studies. Our findings reveal that existing\ntext embedding models have not sufficiently addressed these syntactic\nunderstanding challenges, and such ineffectiveness becomes even more apparent\nwhen evaluated against existing benchmark datasets. Furthermore, we conduct\nrigorous analysis to unearth factors that lead to such limitations and examine\nwhy previous evaluations fail to detect such ineffectiveness. Lastly, we\npropose strategies to augment the generalization ability of text embedding\nmodels in diverse syntactic scenarios. This study serves to highlight the\nhurdles associated with syntactic generalization and provides pragmatic\nguidance for boosting model performance across varied syntactic contexts.\n","authors":["Yan Zhang","Zhaopeng Feng","Zhiyang Teng","Zuozhu Liu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2311.07996v1.pdf","comment":"Accepted to EMNLP-Findings 2023, datasets and code are released"},{"id":"http://arxiv.org/abs/2311.07989v1","updated":"2023-11-14T08:34:26Z","published":"2023-11-14T08:34:26Z","title":"A Survey on Language Models for Code","summary":"  In this work we systematically review the recent advancements in code\nprocessing with language models, covering 50+ models, 30+ evaluation tasks, and\n500 related works. We break down code processing models into general language\nmodels represented by the GPT family and specialized models that are\nspecifically pretrained on code, often with tailored objectives. We discuss the\nrelations and differences between these models, and highlight the historical\ntransition of code modeling from statistical models and RNNs to pretrained\nTransformers and LLMs, which is exactly the same course that had been taken by\nNLP. We also discuss code-specific features such as AST, CFG, and unit tests,\nalong with their application in training code language models, and identify key\nchallenges and potential future directions in this domain. We keep the survey\nopen and updated on github repository at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n","authors":["Ziyin Zhang","Chaoyu Chen","Bingchang Liu","Cong Liao","Zi Gong","Hang Yu","Jianguo Li","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07989v1.pdf","comment":"Repo is available at https://github.com/codefuse-ai/Awesome-Code-LLM"},{"id":"http://arxiv.org/abs/2311.07978v1","updated":"2023-11-14T08:10:14Z","published":"2023-11-14T08:10:14Z","title":"How good are Large Language Models on African Languages?","summary":"  Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on unseen tasks and\nlanguages. Additionally, they have been widely adopted as\nlanguage-model-as-a-service commercial APIs like GPT-4 API. However, their\nperformance on African languages is largely unknown. We present an analysis of\nthree popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks\n(news topic classification, sentiment classification, machine translation,\nquestion answering, and named entity recognition) across 30 African languages,\nspanning different language families and geographical regions. Our results\nsuggest that all LLMs produce below-par performance on African languages, and\nthere is a large gap in performance compared to high-resource languages like\nEnglish most tasks. We find that GPT-4 has an average or impressive performance\non classification tasks but very poor results on generative tasks like machine\ntranslation. Surprisingly, we find that mT0 had the best overall on\ncross-lingual QA, better than the state-of-the-art supervised model (i.e.\nfine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the\nworst performance due to its limited multilingual capabilities and\nEnglish-centric pre-training corpus. In general, our findings present a\ncall-to-action to ensure African languages are well represented in large\nlanguage models, given their growing popularity.\n","authors":["Jessica Ojo","Kelechi Ogueji","Pontus Stenetorp","David I. Adelani"],"pdf_url":"https://arxiv.org/pdf/2311.07978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05915v2","updated":"2023-11-14T07:49:56Z","published":"2023-11-10T08:01:23Z","title":"Fake Alignment: Are LLMs Really Aligned Well?","summary":"  The growing awareness of safety concerns in large language models (LLMs) has\nsparked considerable interest in the evaluation of safety within current\nresearch endeavors. This study investigates an interesting issue pertaining to\nthe evaluation of LLMs, namely the substantial discrepancy in performance\nbetween multiple-choice questions and open-ended questions. Inspired by\nresearch on jailbreak attack patterns, we argue this is caused by mismatched\ngeneralization. That is, the LLM does not have a comprehensive understanding of\nthe complex concept of safety. Instead, it only remembers what to answer for\nopen-ended safety questions, which makes it unable to solve other forms of\nsafety tests. We refer to this phenomenon as fake alignment and construct a\ncomparative benchmark to empirically verify its existence in LLMs. Such fake\nalignment renders previous evaluation protocols unreliable. To address this, we\nintroduce the Fake alIgNment Evaluation (FINE) framework and two novel\nmetrics--Consistency Score (CS) and Consistent Safety Score (CSS), which\njointly assess two complementary forms of evaluation to quantify fake alignment\nand obtain corrected performance estimates. Applying FINE to 14 widely-used\nLLMs reveals several models with purported safety are poorly aligned in\npractice. Our work highlights potential limitations in prevailing alignment\nmethodologies.\n","authors":["Yixu Wang","Yan Teng","Kexin Huang","Chengqi Lyu","Songyang Zhang","Wenwei Zhang","Xingjun Ma","Yu-Gang Jiang","Yu Qiao","Yingchun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.05544v3","updated":"2023-11-14T07:42:42Z","published":"2021-06-10T07:10:25Z","title":"CogAlign: Learning to Align Textual Neural Representations to Cognitive\n  Language Processing Signals","summary":"  Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.\n","authors":["Yuqi Ren","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2106.05544v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06513v2","updated":"2023-11-14T07:30:37Z","published":"2023-11-11T09:06:15Z","title":"Step by Step to Fairness: Attributing Societal Bias in Task-oriented\n  Dialogue Systems","summary":"  Recent works have shown considerable improvements in task-oriented dialogue\n(TOD) systems by utilizing pretrained large language models (LLMs) in an\nend-to-end manner. However, the biased behavior of each component in a TOD\nsystem and the error propagation issue in the end-to-end framework can lead to\nseriously biased TOD responses. Existing works of fairness only focus on the\ntotal bias of a system. In this paper, we propose a diagnosis method to\nattribute bias to each component of a TOD system. With the proposed attribution\nmethod, we can gain a deeper understanding of the sources of bias.\nAdditionally, researchers can mitigate biased model behavior at a more granular\nlevel. We conduct experiments to attribute the TOD system's bias toward three\ndemographic axes: gender, age, and race. Experimental results show that the\nbias of a TOD system usually comes from the response generation model.\n","authors":["Hsuan Su","Rebecca Qian","Chinnadhurai Sankar","Shahin Shayandeh","Shang-Tse Chen","Hung-yi Lee","Daniel M. Bikel"],"pdf_url":"https://arxiv.org/pdf/2311.06513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00566v2","updated":"2023-11-14T07:30:18Z","published":"2023-10-01T03:50:34Z","title":"Empowering Many, Biasing a Few: Generalist Credit Scoring through Large\n  Language Models","summary":"  In the financial industry, credit scoring is a fundamental element, shaping\naccess to credit and determining the terms of loans for individuals and\nbusinesses alike. Traditional credit scoring methods, however, often grapple\nwith challenges such as narrow knowledge scope and isolated evaluation of\ncredit tasks. Our work posits that Large Language Models (LLMs) have great\npotential for credit scoring tasks, with strong generalization ability across\nmultiple tasks. To systematically explore LLMs for credit scoring, we propose\nthe first open-source comprehensive framework. We curate a novel benchmark\ncovering 9 datasets with 14K samples, tailored for credit assessment and a\ncritical examination of potential biases within LLMs, and the novel instruction\ntuning data with over 45k samples. We then propose the first Credit and Risk\nAssessment Large Language Model (CALM) by instruction tuning, tailored to the\nnuanced demands of various financial risk assessment tasks. We evaluate CALM,\nand existing state-of-art (SOTA) open source and close source LLMs on the build\nbenchmark. Our empirical results illuminate the capability of LLMs to not only\nmatch but surpass conventional models, pointing towards a future where credit\nscoring can be more inclusive, comprehensive, and unbiased. We contribute to\nthe industry's transformation by sharing our pioneering instruction-tuning\ndatasets, credit and risk assessment LLM, and benchmarks with the research\ncommunity and the financial industry.\n","authors":["Duanyu Feng","Yongfu Dai","Jimin Huang","Yifang Zhang","Qianqian Xie","Weiguang Han","Alejandro Lopez-Lira","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.00566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07961v1","updated":"2023-11-14T07:26:32Z","published":"2023-11-14T07:26:32Z","title":"The ART of LLM Refinement: Ask, Refine, and Trust","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ngenerative abilities, but can they judge the quality of their own generations?\nA popular concept, referred to as self-refinement, postulates that LLMs can\ndetect and correct the errors in their generations when asked to do so.\nHowever, recent empirical evidence points in the opposite direction, suggesting\nthat LLMs often struggle to accurately identify errors when reasoning is\ninvolved. To address this, we propose a reasoning with refinement objective\ncalled ART: Ask, Refine, and Trust, which asks necessary questions to decide\nwhen an LLM should refine its output, and either affirm or withhold trust in\nits refinement by ranking the refinement and the initial prediction. On two\nmultistep reasoning tasks of mathematical word problems (GSM8K) and question\nanswering (StrategyQA), ART achieves a performance gain of +5 points over\nself-refinement baselines, while using a much smaller model as the decision\nmaker. We also demonstrate the benefit of using smaller models to make\nrefinement decisions as a cost-effective alternative to fine-tuning a larger\nmodel.\n","authors":["Kumar Shridhar","Koustuv Sinha","Andrew Cohen","Tianlu Wang","Ping Yu","Ram Pasunuru","Mrinmaya Sachan","Jason Weston","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2311.07961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07954v1","updated":"2023-11-14T07:13:10Z","published":"2023-11-14T07:13:10Z","title":"A Closer Look at the Self-Verification Abilities of Large Language\n  Models in Logical Reasoning","summary":"  Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.\n","authors":["Ruixin Hong","Hongming Zhang","Xinyu Pang","Dong Yu","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07954v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2310.19347v3","updated":"2023-11-14T06:55:56Z","published":"2023-10-30T08:40:16Z","title":"Improving Factual Consistency of Text Summarization by Adversarially\n  Decoupling Comprehension and Embellishment Abilities of LLMs","summary":"  Despite the recent progress in text summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose an\nadversarially DEcoupling method to disentangle the Comprehension and\nEmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based\nefficient training to cover the shortage of sensitivity for true and false in\nthe training process of LLMs. In this way, LLMs are less confused about\nembellishing and understanding; thus, they can execute the instructions more\naccurately and have enhanced abilities to distinguish hallucinations.\nExperimental results show that DECENT significantly improves the reliability of\ntext summarization based on LLMs.\n","authors":["Huawen Feng","Yan Fan","Xiong Liu","Ting-En Lin","Zekun Yao","Yuchuan Wu","Fei Huang","Yongbin Li","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2310.19347v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17490v3","updated":"2023-11-14T06:49:33Z","published":"2023-10-26T15:45:12Z","title":"Improving Zero-shot Reader by Reducing Distractions from Irrelevant\n  Documents in Open-Domain Question Answering","summary":"  Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.\n","authors":["Sukmin Cho","Jeongyeon Seo","Soyeong Jeong","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2310.17490v3.pdf","comment":"Findings of EMNLP 2023 Camera Ready"},{"id":"http://arxiv.org/abs/2311.07945v1","updated":"2023-11-14T06:45:31Z","published":"2023-11-14T06:45:31Z","title":"First Step Advantage: Importance of Starting Right in Multi-Step\n  Reasoning","summary":"  Large Language Models (LLMs) can solve complex reasoning tasks by generating\nrationales for their predictions. Distilling these capabilities into a smaller,\ncompact model can facilitate the creation of specialized, cost-effective models\ntailored for specific tasks. However, smaller models often face challenges in\ncomplex reasoning tasks and often deviate from the correct reasoning path. We\nshow that LLMs can guide smaller models and bring them back to the correct\nreasoning path only if they intervene at the right time. We show that smaller\nmodels fail to reason primarily due to their difficulty in initiating the\nprocess, and that guiding them in the right direction can lead to a performance\ngain of over 100%. We explore different model sizes and evaluate the benefits\nof providing guidance to improve reasoning in smaller models.\n","authors":["Kushal Jain","Kumar Shridhar"],"pdf_url":"https://arxiv.org/pdf/2311.07945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07941v1","updated":"2023-11-14T06:39:04Z","published":"2023-11-14T06:39:04Z","title":"Non-autoregressive Machine Translation with Probabilistic Context-free\n  Grammar","summary":"  Non-autoregressive Transformer(NAT) significantly accelerates the inference\nof neural machine translation. However, conventional NAT models suffer from\nlimited expression power and performance degradation compared to autoregressive\n(AT) models due to the assumption of conditional independence among target\ntokens. To address these limitations, we propose a novel approach called\nPCFG-NAT, which leverages a specially designed Probabilistic Context-Free\nGrammar (PCFG) to enhance the ability of NAT models to capture complex\ndependencies among output tokens. Experimental results on major machine\ntranslation benchmarks demonstrate that PCFG-NAT further narrows the gap in\ntranslation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a\ndeeper understanding of the generated sentences, addressing the lack of\nsatisfactory explainability in neural machine translation.Code is publicly\navailable at https://github.com/ictnlp/PCFG-NAT.\n","authors":["Shangtong Gui","Chenze Shao","Zhengrui Ma","Xishan Zhang","Yunji Chen","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2311.07941v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07930v1","updated":"2023-11-14T06:16:49Z","published":"2023-11-14T06:16:49Z","title":"It's All Relative! -- A Synthetic Query Generation Approach for\n  Improving Zero-Shot Relevance Prediction","summary":"  Recent developments in large language models (LLMs) have shown promise in\ntheir ability to generate synthetic query-document pairs by prompting with as\nfew as 8 demonstrations. This has enabled building better IR models, especially\nfor tasks with no training data readily available. Typically, such synthetic\nquery generation (QGen) approaches condition on an input context (e.g. a text\ndocument) and generate a query relevant to that context, or condition the QGen\nmodel additionally on the relevance label (e.g. relevant vs irrelevant) to\ngenerate queries across relevance buckets. However, we find that such QGen\napproaches are sub-optimal as they require the model to reason about the\ndesired label and the input from a handful of examples. In this work, we\npropose to reduce this burden of LLMs by generating queries simultaneously for\ndifferent labels. We hypothesize that instead of asking the model to generate,\nsay, an irrelevant query given an input context, asking the model to generate\nan irrelevant query relative to a relevant query is a much simpler task setup\nfor the model to reason about. Extensive experimentation across seven IR\ndatasets shows that synthetic queries generated in such a fashion translates to\na better downstream performance, suggesting that the generated queries are\nindeed of higher quality.\n","authors":["Aditi Chaudhary","Karthik Raman","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2311.07930v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2311.07362v2","updated":"2023-11-14T06:04:31Z","published":"2023-11-13T14:26:24Z","title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback\n  Guided Revision","summary":"  Large multimodal models (LMMs) suffer from multimodal hallucination, where\nthey provide incorrect responses misaligned with the given visual information.\nRecent works have conjectured that one of the reasons behind multimodal\nhallucination might be due to the vision encoder failing to ground on the image\nproperly. To mitigate this issue, we propose a novel approach that leverages\nself-feedback as visual cues. Building on this approach, we introduce Volcano,\na multimodal self-feedback guided revision model. Volcano generates natural\nlanguage feedback to its initial response based on the provided visual\ninformation and utilizes this feedback to self-revise its initial response.\nVolcano effectively reduces multimodal hallucination and achieves\nstate-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general\nmultimodal abilities and outperforms previous models on MM-Vet and MMBench.\nThrough a qualitative analysis, we show that Volcano's feedback is properly\ngrounded on the image than the initial response. This indicates that Volcano\ncan provide itself with richer visual information, helping alleviate multimodal\nhallucination. We publicly release Volcano models of 7B and 13B sizes along\nwith the data and code at https://github.com/kaistAI/Volcano.\n","authors":["Seongyun Lee","Sue Hyun Park","Yongrae Jo","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.07362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07925v1","updated":"2023-11-14T05:59:58Z","published":"2023-11-14T05:59:58Z","title":"Brain-Driven Representation Learning Based on Diffusion Model","summary":"  Interpreting EEG signals linked to spoken language presents a complex\nchallenge, given the data's intricate temporal and spatial attributes, as well\nas the various noise factors. Denoising diffusion probabilistic models (DDPMs),\nwhich have recently gained prominence in diverse areas for their capabilities\nin representation learning, are explored in our research as a means to address\nthis issue. Using DDPMs in conjunction with a conditional autoencoder, our new\napproach considerably outperforms traditional machine learning algorithms and\nestablished baseline models in accuracy. Our results highlight the potential of\nDDPMs as a sophisticated computational method for the analysis of\nspeech-related EEG signals. This could lead to significant advances in\nbrain-computer interfaces tailored for spoken communication.\n","authors":["Soowon Kim","Seo-Hyun Lee","Young-Eun Lee","Ji-Won Lee","Ji-Ha Park","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2311.07925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07919v1","updated":"2023-11-14T05:34:50Z","published":"2023-11-14T05:34:50Z","title":"Qwen-Audio: Advancing Universal Audio Understanding via Unified\n  Large-Scale Audio-Language Models","summary":"  Recently, instruction-following audio-language models have received broad\nattention for audio interaction with humans. However, the absence of\npre-trained audio models capable of handling diverse audio types and tasks has\nhindered progress in this field. Consequently, most existing works have only\nbeen able to support a limited range of interaction capabilities. In this\npaper, we develop the Qwen-Audio model and address this limitation by scaling\nup audio-language pre-training to cover over 30 tasks and various audio types,\nsuch as human speech, natural sounds, music, and songs, to facilitate universal\naudio understanding abilities. However, directly co-training all tasks and\ndatasets can lead to interference issues, as the textual labels associated with\ndifferent datasets exhibit considerable variations due to differences in task\nfocus, language, granularity of annotation, and text structure. To overcome the\none-to-many interference, we carefully design a multi-task training framework\nby conditioning on a sequence of hierarchical tags to the decoder for\nencouraging knowledge sharing and avoiding interference through shared and\nspecified tags respectively. Remarkably, Qwen-Audio achieves impressive\nperformance across diverse benchmark tasks without requiring any task-specific\nfine-tuning, surpassing its counterparts. Building upon the capabilities of\nQwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from\nvarious audios and text inputs, enabling multi-turn dialogues and supporting\nvarious audio-central scenarios.\n","authors":["Yunfei Chu","Jin Xu","Xiaohuan Zhou","Qian Yang","Shiliang Zhang","Zhijie Yan","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.07919v1.pdf","comment":"The code is released at https://github.com/QwenLM/Qwen-Audio"},{"id":"http://arxiv.org/abs/2311.07918v1","updated":"2023-11-14T05:30:43Z","published":"2023-11-14T05:30:43Z","title":"Automated title and abstract screening for scoping reviews using the\n  GPT-4 Large Language Model","summary":"  Scoping reviews, a type of literature review, require intensive human effort\nto screen large numbers of scholarly sources for their relevance to the review\nobjectives. This manuscript introduces GPTscreenR, a package for the R\nstatistical programming language that uses the GPT-4 Large Language Model (LLM)\nto automatically screen sources. The package makes use of the chain-of-thought\ntechnique with the goal of maximising performance on complex screening tasks.\nIn validation against consensus human reviewer decisions, GPTscreenR performed\nsimilarly to an alternative zero-shot technique, with a sensitivity of 71%,\nspecificity of 89%, and overall accuracy of 84%. Neither method achieved\nperfect accuracy nor human levels of intraobserver agreement. GPTscreenR\ndemonstrates the potential for LLMs to support scholarly work and provides a\nuser-friendly software framework that can be integrated into existing review\nprocesses.\n","authors":["David Wilkins"],"pdf_url":"https://arxiv.org/pdf/2311.07918v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.09299v2","updated":"2023-11-14T05:24:06Z","published":"2023-06-15T17:27:20Z","title":"Can Language Models Teach Weaker Agents? Teacher Explanations Improve\n  Students via Personalization","summary":"  A hallmark property of explainable AI models is the ability to teach other\nagents, communicating knowledge of how to perform a task. While Large Language\nModels perform complex reasoning by generating explanations for their\npredictions, it is unclear whether they also make good teachers for weaker\nagents. To address this, we consider a student-teacher framework between two\nLLM agents and study if, when, and how the teacher should intervene with\nnatural language explanations to improve the student's performance. Since\ncommunication is expensive, we define a budget such that the teacher only\ncommunicates explanations for a fraction of the data, after which the student\nshould perform well on its own. We decompose the teaching problem along four\naxes: (1) if teacher's test time intervention improve student predictions, (2)\nwhen it is worth explaining a data point, (3) how the teacher should\npersonalize explanations to better teach the student, and (4) if teacher\nexplanations also improve students on future unexplained data. We first show\nthat teacher LLMs can indeed intervene on student reasoning to improve their\nperformance. Next, inspired by the Theory of Mind abilities of effective\nteachers, we propose building two few-shot mental models of the student. The\nfirst model defines an Intervention Function that simulates the utility of an\nintervention, allowing the teacher to intervene when this utility is the\nhighest and improving student performance at lower budgets. The second model\nenables the teacher to personalize explanations for a particular student and\noutperform unpersonalized teachers. We also demonstrate that in multi-turn\ninteractions, teacher explanations generalize and learning from explained data\nimproves student performance on future unexplained data. Finally, we verify\nthat misaligned teachers can lower student performance to random chance by\nintentionally misleading them.\n","authors":["Swarnadeep Saha","Peter Hase","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2306.09299v2.pdf","comment":"NeurIPS 2023 (23 pages, 12 figures). Our code is available at\n  https://github.com/swarnaHub/ExplanationIntervention"},{"id":"http://arxiv.org/abs/2311.07914v1","updated":"2023-11-14T05:21:57Z","published":"2023-11-14T05:21:57Z","title":"Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey","summary":"  The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\nconduct a comprehensive review of these knowledge-graph-based knowledge\naugmentation techniques in LLMs, focusing on their efficacy in mitigating\nhallucinations. We systematically categorize these methods into three\noverarching groups, offering both methodological comparisons and empirical\nevaluations of their performance. Lastly, the paper explores the challenges\nassociated with these techniques and outlines potential avenues for future\nresearch in this emerging field.\n","authors":["Garima Agrawal","Tharindu Kumarage","Zeyad Alghami","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2311.07914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06453v2","updated":"2023-11-14T05:14:31Z","published":"2023-11-11T01:14:37Z","title":"DocGen: Generating Detailed Parameter Docstrings in Python","summary":"  Documentation debt hinders the effective utilization of open-source software.\nAlthough code summarization tools have been helpful for developers, most would\nprefer a detailed account of each parameter in a function rather than a\nhigh-level summary. However, generating such a summary is too intricate for a\nsingle generative model to produce reliably due to the lack of high-quality\ntraining data. Thus, we propose a multi-step approach that combines multiple\ntask-specific models, each adept at producing a specific section of a\ndocstring. The combination of these models ensures the inclusion of each\nsection in the final docstring. We compared the results from our approach with\nexisting generative models using both automatic metrics and a human-centred\nevaluation with 17 participating developers, which proves the superiority of\nour approach over existing methods.\n","authors":["Vatsal Venkatkrishna","Durga Shree Nagabushanam","Emmanuel Iko-Ojo Simon","Melina Vidoni"],"pdf_url":"https://arxiv.org/pdf/2311.06453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07911v1","updated":"2023-11-14T05:13:55Z","published":"2023-11-14T05:13:55Z","title":"Instruction-Following Evaluation for Large Language Models","summary":"  One core capability of Large Language Models (LLMs) is to follow natural\nlanguage instructions. However, the evaluation of such abilities is not\nstandardized: Human evaluations are expensive, slow, and not objectively\nreproducible, while LLM-based auto-evaluation is potentially biased or limited\nby the ability of the evaluator LLM. To overcome these issues, we introduce\nInstruction-Following Eval (IFEval) for large language models. IFEval is a\nstraightforward and easy-to-reproduce evaluation benchmark. It focuses on a set\nof \"verifiable instructions\" such as \"write in more than 400 words\" and\n\"mention the keyword of AI at least 3 times\". We identified 25 types of those\nverifiable instructions and constructed around 500 prompts, with each prompt\ncontaining one or more verifiable instructions. We show evaluation results of\ntwo widely available LLMs on the market. Our code and data can be found at\nhttps://github.com/google-research/google-research/tree/master/instruction_following_eval\n","authors":["Jeffrey Zhou","Tianjian Lu","Swaroop Mishra","Siddhartha Brahma","Sujoy Basu","Yi Luan","Denny Zhou","Le Hou"],"pdf_url":"https://arxiv.org/pdf/2311.07911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10874v2","updated":"2023-11-14T05:06:01Z","published":"2023-08-21T17:21:23Z","title":"Analyzing Transformer Dynamics as Movement through Embedding Space","summary":"  Transformer based language models exhibit intelligent behaviors such as\nunderstanding natural language, recognizing patterns, acquiring knowledge,\nreasoning, planning, reflecting and using tools. This paper explores how their\nunderlying mechanics give rise to intelligent behaviors. Towards that end, we\npropose framing Transformer dynamics as movement through embedding space.\nExamining Transformers through this perspective reveals key insights,\nestablishing a Theory of Transformers: 1) Intelligent behaviours map to paths\nin Embedding Space which, the Transformer random-walks through during\ninferencing. 2) LM training learns a probability distribution over all possible\npaths. `Intelligence' is learnt by assigning higher probabilities to paths\nrepresenting intelligent behaviors. No learning can take place in-context;\ncontext only narrows the subset of paths sampled during decoding. 5) The\nTransformer is a self-mapping composition function, folding a context sequence\ninto a context-vector such that it's proximity to a token-vector reflects its\nco-occurrence and conditioned probability. Thus, the physical arrangement of\nvectors in Embedding Space determines path probabilities. 6) Context vectors\nare composed by aggregating features of the sequence's tokens via a process we\ncall the encoding walk. Attention contributes a - potentially redundant -\nassociation-bias to this process. 7) This process is comprised of two principal\noperation types: filtering (data independent) and aggregation (data dependent).\nThis generalization unifies Transformers with other sequence models. Building\nupon this foundation, we formalize a popular semantic interpretation of\nembeddings into a ``concept-space theory'' and find some evidence of it's\nvalidity.\n","authors":["Sumeet S. Singh"],"pdf_url":"https://arxiv.org/pdf/2308.10874v2.pdf","comment":"V2. Rewrote abstract. Rewrote / re-organized the entire paper into a\n  more formal proposition/argument/result format. To shorten main paper length:\n  Wrote more compact text in general, moved \"negative self bias\" and \"encoder\n  v/s decoder walks\" sections to the appendix and packed figures. Styled as\n  TMLR"},{"id":"http://arxiv.org/abs/2311.07897v1","updated":"2023-11-14T04:10:40Z","published":"2023-11-14T04:10:40Z","title":"CPopQA: Ranking Cultural Concept Popularity by LLMs","summary":"  Prior work has demonstrated large language models' (LLMs) potential to\ndiscern statistical tendencies within their pre-training corpora. Despite that,\nmany examinations of LLMs' knowledge capacity focus on knowledge explicitly\nappearing in the training data or implicitly inferable from similar contexts.\nHow well an LLM captures the corpus-level statistical trends of concepts for\nreasoning, especially long-tail ones, is still underexplored. In this study, we\nintroduce a novel few-shot question-answering task (CPopQA) that examines LLMs'\nstatistical ranking abilities for long-tail cultural concepts (e.g., holidays),\nwith a specific focus on these concepts' popularity in the United States and\nthe United Kingdom, respectively. We curate a dataset containing 459 holidays\nacross 58 countries, generating a total of 6,000 QA testing pairs. Experiments\non four strong LLMs show that large models are capable of ranking long-tail\ncultural concepts regarding their statistical tendency. Notably, GPT-3.5\ndisplayed superior performance and exhibited its potential to identify\ngeo-cultural proximity across continents.\n","authors":["Ming Jiang","Mansi Joshi"],"pdf_url":"https://arxiv.org/pdf/2311.07897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17353v2","updated":"2023-11-14T04:03:18Z","published":"2023-05-27T03:21:36Z","title":"Complementary and Integrative Health Lexicon (CIHLex) and Entity\n  Recognition in the Literature","summary":"  Objective: Our study aimed to construct an exhaustive Complementary and\nIntegrative Health (CIH) Lexicon (CIHLex) to better represent the often\nunderrepresented physical and psychological CIH approaches in standard\nterminologies. We also intended to apply advanced Natural Language Processing\n(NLP) models such as Bidirectional Encoder Representations from Transformers\n(BERT) and GPT-3.5 Turbo for CIH named entity recognition, evaluating their\nperformance against established models like MetaMap and CLAMP. Materials and\nMethods: We constructed the CIHLex by integrating various resources, compiling\nand integrating data from biomedical literature and relevant knowledge bases.\nThe Lexicon encompasses 198 unique concepts with 1090 corresponding unique\nterms. We matched these concepts to the Unified Medical Language System (UMLS).\nAdditionally, we developed and utilized BERT models and compared their\nefficiency in CIH named entity recognition to that of other models such as\nMetaMap, CLAMP, and GPT3.5-turbo. Results: From the 198 unique concepts in\nCIHLex, 62.1% could be matched to at least one term in the UMLS. Moreover,\n75.7% of the mapped UMLS Concept Unique Identifiers (CUIs) were categorized as\n\"Therapeutic or Preventive Procedure.\" Among the models applied to CIH named\nentity recognition, BLUEBERT delivered the highest macro average F1-score of\n0.90, surpassing other models. Conclusion: Our CIHLex significantly augments\nrepresentation of CIH approaches in biomedical literature. Demonstrating the\nutility of advanced NLP models, BERT notably excelled in CIH entity\nrecognition. These results highlight promising strategies for enhancing\nstandardization and recognition of CIH terminology in biomedical contexts.\n","authors":["Huixue Zhou","Robin Austin","Sheng-Chieh Lu","Greg Silverman","Yuqi Zhou","Halil Kilicoglu","Hua Xu","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.17353v2.pdf","comment":"need to update the data"},{"id":"http://arxiv.org/abs/2311.07884v1","updated":"2023-11-14T03:38:55Z","published":"2023-11-14T03:38:55Z","title":"Fair Abstractive Summarization of Diverse Perspectives","summary":"  People from different social and demographic groups express diverse\nperspectives and conflicting opinions on a broad set of topics such as product\nreviews, healthcare, law, and politics. A fair summary should provide a\ncomprehensive coverage of diverse perspectives without underrepresenting\ncertain groups. However, current work in summarization metrics and Large\nLanguage Models (LLMs) evaluation has not explored fair abstractive\nsummarization. In this paper, we systematically investigate fair abstractive\nsummarization for user-generated data. We first formally define fairness in\nabstractive summarization as not underrepresenting perspectives of any groups\nof people and propose four reference-free automatic metrics measuring the\ndifferences between target and source perspectives. We evaluate five LLMs,\nincluding three GPT models, Alpaca, and Claude, on six datasets collected from\nsocial media, online reviews, and recorded transcripts. Experiments show that\nboth the model-generated and the human-written reference summaries suffer from\nlow fairness. We conduct a comprehensive analysis of the common factors\ninfluencing fairness and propose three simple but effective methods to\nalleviate unfair summarization. Our dataset and code are available at\nhttps://github.com/psunlpgroup/FairSumm.\n","authors":["Yusen Zhang","Nan Zhang","Yixin Liu","Alexander Fabbri","Junru Liu","Ryo Kamoi","Xiaoxin Lu","Caiming Xiong","Jieyu Zhao","Dragomir Radev","Kathleen McKeown","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07884v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.07879v1","updated":"2023-11-14T03:18:28Z","published":"2023-11-14T03:18:28Z","title":"Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting\n  Volunteer Content Moderators","summary":"  Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\n-- with the aim of lightening the load for moderators. Yet, it remains\nuncertain whether improvements on those tasks truly address the needs that\nmoderators have in accomplishing their work. In this paper, we surface the gaps\nbetween past research efforts that have aimed to provide automation for aspects\nof the content moderation task, and the needs of volunteer content moderators.\nTo do so, we conduct a model review on Hugging Face to reveal the availability\nof models to cover various moderation rules and guidelines. We further put\nstate-of-the-art LLMs to the test (GPT-4 and Llama-2), evaluating how well\nthese models perform in flagging violations of platform rules. Overall, we\nobserve a non-trivial gap, as missing developed models and LLMs exhibit low\nrecall on a significant portion of the rules.\n","authors":["Yang Trista Cao","Lovely-Frances Domingo","Sarah Ann Gilbert","Michelle Mazurek","Katie Shilton","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2311.07879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07336v3","updated":"2023-11-14T03:14:49Z","published":"2023-08-11T13:15:35Z","title":"Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic","summary":"  We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.\n","authors":["Terufumi Morishita","Gaku Morio","Atsuki Yamaguchi","Yasuhiro Sogawa"],"pdf_url":"https://arxiv.org/pdf/2308.07336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16147v2","updated":"2023-11-14T02:46:32Z","published":"2023-10-24T19:47:26Z","title":"PreWoMe: Exploiting Presuppositions as Working Memory for Long Form\n  Question Answering","summary":"  Information-seeking questions in long-form question answering (LFQA) often\nprove misleading due to ambiguity or false presupposition in the question.\nWhile many existing approaches handle misleading questions, they are tailored\nto limited questions, which are insufficient in a real-world setting with\nunpredictable input characteristics. In this work, we propose PreWoMe, a\nunified approach capable of handling any type of information-seeking question.\nThe key idea of PreWoMe involves extracting presuppositions in the question and\nexploiting them as working memory to generate feedback and action about the\nquestion. Our experiment shows that PreWoMe is effective not only in tackling\nmisleading questions but also in handling normal ones, thereby demonstrating\nthe effectiveness of leveraging presuppositions, feedback, and action for\nreal-world QA settings.\n","authors":["Wookje Han","Jinsol Park","Kyungjae Lee"],"pdf_url":"https://arxiv.org/pdf/2310.16147v2.pdf","comment":"11 pages 3 figures, Accepted to EMNLP 2023 (short)"},{"id":"http://arxiv.org/abs/2310.17784v2","updated":"2023-11-14T02:41:40Z","published":"2023-10-07T04:53:31Z","title":"Data-Centric Financial Large Language Models","summary":"  Large language models (LLMs) show promise for natural language tasks but\nstruggle when applied directly to complex domains like finance. LLMs have\ndifficulty reasoning about and integrating all relevant information. We propose\na data-centric approach to enable LLMs to better handle financial tasks. Our\nkey insight is that rather than overloading the LLM with everything at once, it\nis more effective to preprocess and pre-understand the data. We create a\nfinancial LLM (FLLM) using multitask prompt-based finetuning to achieve data\npre-processing and pre-understanding. However, labeled data is scarce for each\ntask. To overcome manual annotation costs, we employ abductive augmentation\nreasoning (AAR) to automatically generate training data by modifying the pseudo\nlabels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR\nsubstantially outperforms baseline financial LLMs designed for raw text,\nachieving state-of-the-art on financial analysis and interpretation tasks. We\nalso open source a new benchmark for financial analysis and interpretation. Our\nmethodology provides a promising path to unlock LLMs' potential for complex\nreal-world domains.\n","authors":["Zhixuan Chu","Huaiyu Guo","Xinyuan Zhou","Yijia Wang","Fei Yu","Hong Chen","Wanqing Xu","Xin Lu","Qing Cui","Longfei Li","Jun Zhou","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2310.17784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20689v2","updated":"2023-11-14T02:37:55Z","published":"2023-10-31T17:52:22Z","title":"Learning From Mistakes Makes LLM Better Reasoner","summary":"  Large language models (LLMs) recently exhibited remarkable reasoning\ncapabilities on solving math problems. To further improve this capability, this\nwork proposes Learning from Mistakes (LeMa), akin to human learning processes.\nConsider a human student who failed to solve a math problem, he will learn from\nwhat mistake he has made and how to correct it. Mimicking this error-driven\nlearning process, LeMa fine-tunes LLMs on mistake-correction data pairs\ngenerated by GPT-4. Specifically, we first collect inaccurate reasoning paths\nfrom various LLMs and then employ GPT-4 as a \"corrector\" to (1) identify the\nmistake step, (2) explain the reason for the mistake, and (3) correct the\nmistake and generate the final answer. Experimental results demonstrate the\neffectiveness of LeMa: across five backbone LLMs and two mathematical reasoning\ntasks, LeMa consistently improves the performance compared with fine-tuning on\nCoT data alone. Impressively, LeMa can also benefit specialized LLMs such as\nWizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on\nMATH. This surpasses the SOTA performance achieved by non-execution open-source\nmodels on these challenging tasks. Our code, data and models will be publicly\navailable at https://github.com/microsoft/LEMA.\n","authors":["Shengnan An","Zexiong Ma","Zeqi Lin","Nanning Zheng","Jian-Guang Lou","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.20689v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.14827v2","updated":"2023-11-14T02:23:01Z","published":"2023-05-24T07:34:32Z","title":"Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent\n  Classification","summary":"  Intent classification (IC) plays an important role in task-oriented dialogue\nsystems. However, IC models often generalize poorly when training without\nsufficient annotated examples for each user intent. We propose a novel\npre-training method for text encoders that uses contrastive learning with\nintent psuedo-labels to produce embeddings that are well-suited for IC tasks,\nreducing the need for manual annotations. By applying this pre-training\nstrategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is\ndesigned to align encodings of utterances with their intent names.\nSpecifically, we first train a tagger to identify key phrases within utterances\nthat are crucial for interpreting intents. We then use these extracted phrases\nto create examples for pre-training a text encoder in a contrastive manner. As\na result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the\nprevious state-of-the-art text encoder for the N-way zero- and one-shot\nsettings on four IC datasets.\n","authors":["Mujeen Sung","James Gung","Elman Mansimov","Nikolaos Pappas","Raphael Shu","Salvatore Romeo","Yi Zhang","Vittorio Castelli"],"pdf_url":"https://arxiv.org/pdf/2305.14827v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.07853v1","updated":"2023-11-14T02:09:10Z","published":"2023-11-14T02:09:10Z","title":"Learning Mutually Informed Representations for Characters and Subwords","summary":"  Most pretrained language models rely on subword tokenization, which processes\ntext as a sequence of subword tokens. However, different granularities of text,\nsuch as characters, subwords, and words, can contain different kinds of\ninformation. Previous studies have shown that incorporating multiple input\ngranularities improves model generalization, yet very few of them outputs\nuseful representations for each granularity. In this paper, we introduce the\nentanglement model, aiming to combine character and subword language models.\nInspired by vision-language models, our model treats characters and subwords as\nseparate modalities, and it generates mutually informed representations for\nboth granularities as output. We evaluate our model on text classification,\nnamed entity recognition, and POS-tagging tasks. Notably, the entanglement\nmodel outperforms its backbone language models, particularly in the presence of\nnoisy texts and low-resource languages. Furthermore, the entanglement model\neven outperforms larger pre-trained models on all English sequence labeling\ntasks and classification tasks. Our anonymized code is available at\nhttps://anonymous.4open.science/r/noisy-IE-A673\n","authors":["Yilin Wang","Xinyi Hu","Matthew R. Gormley"],"pdf_url":"https://arxiv.org/pdf/2311.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07850v1","updated":"2023-11-14T02:05:29Z","published":"2023-11-14T02:05:29Z","title":"Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA","summary":"  We present BYOKG, a universal question-answering (QA) system that can operate\non any knowledge graph (KG), requires no human-annotated training data, and can\nbe ready to use within a day -- attributes that are out-of-scope for current\nKGQA systems. BYOKG draws inspiration from the remarkable ability of humans to\ncomprehend information present in an unseen KG through exploration -- starting\nat random nodes, inspecting the labels of adjacent nodes and edges, and\ncombining them with their prior world knowledge. In BYOKG, exploration\nleverages an LLM-backed symbolic agent that generates a diverse set of\nquery-program exemplars, which are then used to ground a retrieval-augmented\nreasoning procedure to predict programs for arbitrary questions. BYOKG is\neffective over both small- and large-scale graphs, showing dramatic gains in QA\naccuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,\nrespectively. On GrailQA, we further show that our unsupervised BYOKG\noutperforms a supervised in-context learning method, demonstrating the\neffectiveness of exploration. Lastly, we find that performance of BYOKG\nreliably improves with continued exploration as well as improvements in the\nbase LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1\non a sub-sampled zero-shot split of GrailQA.\n","authors":["Dhruv Agarwal","Rajarshi Das","Sopan Khosla","Rashmi Gangadharaiah"],"pdf_url":"https://arxiv.org/pdf/2311.07850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06401v2","updated":"2023-11-14T02:01:23Z","published":"2023-11-10T21:32:34Z","title":"Autoregressive Language Models For Estimating the Entropy of Epic EHR\n  Audit Logs","summary":"  EHR audit logs are a highly granular stream of events that capture clinician\nactivities, and is a significant area of interest for research in\ncharacterizing clinician workflow on the electronic health record (EHR).\nExisting techniques to measure the complexity of workflow through EHR audit\nlogs (audit logs) involve time- or frequency-based cross-sectional aggregations\nthat are unable to capture the full complexity of a EHR session. We briefly\nevaluate the usage of transformer-based tabular language model (tabular LM) in\nmeasuring the entropy or disorderedness of action sequences within workflow and\nrelease the evaluated models publicly.\n","authors":["Benjamin C. Warner","Thomas Kannampallil","Seunghwan Kim"],"pdf_url":"https://arxiv.org/pdf/2311.06401v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 10 pages"},{"id":"http://arxiv.org/abs/2309.16039v3","updated":"2023-11-14T01:40:13Z","published":"2023-09-27T21:41:49Z","title":"Effective Long-Context Scaling of Foundation Models","summary":"  We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n","authors":["Wenhan Xiong","Jingyu Liu","Igor Molybog","Hejia Zhang","Prajjwal Bhargava","Rui Hou","Louis Martin","Rashi Rungta","Karthik Abinav Sankararaman","Barlas Oguz","Madian Khabsa","Han Fang","Yashar Mehdad","Sharan Narang","Kshitiz Malik","Angela Fan","Shruti Bhosale","Sergey Edunov","Mike Lewis","Sinong Wang","Hao Ma"],"pdf_url":"https://arxiv.org/pdf/2309.16039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07838v1","updated":"2023-11-14T01:38:02Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with corresponding supporting documents, which enables the user to\nflexibly verify the answer and makes it more trustworthy. Its evaluation not\nonly measures the correctness of the answer, but also the answer's\nverifiability, i.e., how well the answer is supported by the corresponding\ndocuments. In typical, verifiable generation adopts the retrieval-read\npipeline, which is divided into two stages: 1) retrieve relevant documents of\nthe question. 2) according to the documents, generate the corresponding answer.\nSince the retrieved documents can supplement knowledge for the LLM to generate\nthe answer and serve as evidence, the retrieval stage is essential for the\ncorrectness and verifiability of the answer. However, the widely used\nretrievers become the bottleneck of the entire pipeline and limit the overall\nperformance. They often have fewer parameters than the large language model and\nhave not been proven to scale well to the size of LLMs. Since the LLM passively\nreceives the retrieval result, if the retriever does not correctly find the\nsupporting documents, the LLM can not generate the correct and verifiable\nanswer, which overshadows the LLM's remarkable abilities. In this paper, we\npropose LLatrieval (Large Language Model Verified Retrieval), where the LLM\nupdates the retrieval result until it verifies that the retrieved documents can\nsupport answering the question. Thus, the LLM can iteratively provide feedback\nto retrieval and facilitate the retrieval result to sufficiently support\nverifiable generation. Experimental results show that our method significantly\noutperforms extensive baselines and achieves new state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07820v1","updated":"2023-11-14T00:43:33Z","published":"2023-11-14T00:43:33Z","title":"On the Analysis of Cross-Lingual Prompt Tuning for Decoder-based\n  Multilingual Model","summary":"  An exciting advancement in the field of multilingual models is the emergence\nof autoregressive models with zero- and few-shot capabilities, a phenomenon\nwidely reported in large-scale language models. To further improve model\nadaptation to cross-lingual tasks, another trend is to further fine-tune the\nlanguage models with either full fine-tuning or parameter-efficient tuning.\nHowever, the interaction between parameter-efficient fine-tuning (PEFT) and\ncross-lingual tasks in multilingual autoregressive models has yet to be\nstudied. Specifically, we lack an understanding of the role of linguistic\ndistributions in multilingual models in the effectiveness of token-based prompt\ntuning. To address this question, we conduct experiments comparing prompt\ntuning and fine-tuning on the decoder-based multilingual model, XGLM, with four\ncross-lingual tasks (XNLI, PAWS-X, POS, NER). According to our study, prompt\ntuning achieves on par or better performance over fine-tuning across all\nlanguages while updating at most 0.13\\% of the model parameters. Moreover, we\nempirically show that prompt tuning is more effective in enhancing the\nperformance of low-resource languages than fine-tuning. Our further analysis\nshows that the phenomenon is related to the tokenization scheme of the\nmultilingual model.\n","authors":["Nohil Park","Joonsuk Park","Kang Min Yoo","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2311.07820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06273v3","updated":"2023-11-14T00:20:20Z","published":"2023-03-11T01:19:01Z","title":"Consistency Analysis of ChatGPT","summary":"  ChatGPT has gained a huge popularity since its introduction. Its positive\naspects have been reported through many media platforms, and some analyses even\nshowed that ChatGPT achieved a decent grade in professional exams, adding extra\nsupport to the claim that AI can now assist and even replace humans in\nindustrial fields. Others, however, doubt its reliability and trustworthiness.\nThis paper investigates the trustworthiness of ChatGPT and GPT-4 regarding\nlogically consistent behaviour, focusing specifically on semantic consistency\nand the properties of negation, symmetric, and transitive consistency. Our\nfindings suggest that while both models appear to show an enhanced language\nunderstanding and reasoning ability, they still frequently fall short of\ngenerating logically consistent predictions. We also ascertain via experiments\nthat prompt designing, few-shot learning and employing larger large language\nmodels (LLMs) are unlikely to be the ultimate solution to resolve the\ninconsistency issue of LLMs.\n","authors":["Myeongjun Erik Jang","Thomas Lukasiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.06273v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2311.08598v1","updated":"2023-11-14T23:43:47Z","published":"2023-11-14T23:43:47Z","title":"DALA: A Distribution-Aware LoRA-Based Adversarial Attack against\n  Pre-trained Language Models","summary":"  Pre-trained language models (PLMs) that achieve success in applications are\nsusceptible to adversarial attack methods that are capable of generating\nadversarial examples with minor perturbations. Although recent attack methods\ncan achieve a relatively high attack success rate (ASR), our observation shows\nthat the generated adversarial examples have a different data distribution\ncompared with the original examples. Specifically, these adversarial examples\nexhibit lower confidence levels and higher distance to the training data\ndistribution. As a result, they are easy to detect using very simple detection\nmethods, diminishing the actual effectiveness of these attack methods. To solve\nthis problem, we propose a Distribution-Aware LoRA-based Adversarial Attack\n(DALA) method, which considers the distribution shift of adversarial examples\nto improve attack effectiveness under detection methods. We further design a\nnew evaluation metric NASR combining ASR and detection for the attack task. We\nconduct experiments on four widely-used datasets and validate the attack\neffectiveness on ASR and NASR of the adversarial examples generated by DALA on\nthe BERT-base model and the black-box LLaMA2-7b model.\n","authors":["Yibo Wang","Xiangjue Dong","James Caverlee","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2311.08598v1.pdf","comment":"First two authors contribute equally"},{"id":"http://arxiv.org/abs/2311.08596v1","updated":"2023-11-14T23:40:22Z","published":"2023-11-14T23:40:22Z","title":"Are You Sure? Challenging LLMs Leads to Performance Drops in The\n  FlipFlop Experiment","summary":"  The interactive nature of Large Language Models (LLMs) theoretically allows\nmodels to refine and improve their answers, yet systematic analysis of the\nmulti-turn behavior of LLMs remains limited. In this paper, we propose the\nFlipFlop experiment: in the first round of the conversation, an LLM responds to\na prompt containing a classification task. In a second round, the LLM is\nchallenged with a follow-up phrase like \"Are you sure?\", offering an\nopportunity for the model to reflect on its initial answer, and decide whether\nto confirm or flip its answer. A systematic study of nine LLMs on seven\nclassification tasks reveals that models flip their answers on average 46% of\nthe time and that all models see a deterioration of accuracy between their\nfirst and final prediction, with an average drop of 17%. The FlipFlop\nexperiment illustrates the universality of sycophantic behavior in LLMs and\nprovides a robust framework to analyze model behavior and evaluate potential\nsolutions.\n","authors":["Philippe Laban","Lidiya Murakhovs'ka","Caiming Xiong","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2311.08596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08593v1","updated":"2023-11-14T23:28:36Z","published":"2023-11-14T23:28:36Z","title":"ACID: Abstractive, Content-Based IDs for Document Retrieval with\n  Language Models","summary":"  Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a new approach\nfor end-to-end document retrieval that directly generates document identifiers\ngiven an input query. Techniques for designing effective, high-quality document\nIDs remain largely unexplored. We introduce ACID, in which each document's ID\nis composed of abstractive keyphrases generated by a large language model,\nrather than an integer ID sequence as done in past work. We compare our method\nwith the current state-of-the-art technique for ID generation, which produces\nIDs through hierarchical clustering of document embeddings. We also examine\nsimpler methods to generate natural-language document IDs, including the naive\napproach of using the first k words of each document as its ID or words with\nhigh BM25 scores in that document. We show that using ACID improves top-10 and\ntop-20 accuracy by 15.6% and 14.4% (relative) respectively versus the\nstate-of-the-art baseline on the MSMARCO 100k retrieval task, and 4.4% and 4.0%\nrespectively on the Natural Questions 100k retrieval task. Our results\ndemonstrate the effectiveness of human-readable, natural-language IDs in\ngenerative retrieval with LMs. The code for reproducing our results and the\nkeyword-augmented datasets will be released on formal publication.\n","authors":["Haoxin Li","Phillip Keung","Daniel Cheng","Jungo Kasai","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2311.08593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08592v1","updated":"2023-11-14T23:28:23Z","published":"2023-11-14T23:28:23Z","title":"AART: AI-Assisted Red-Teaming with Diverse Data Generation for New\n  LLM-powered Applications","summary":"  Adversarial testing of large language models (LLMs) is crucial for their safe\nand responsible deployment. We introduce a novel approach for automated\ngeneration of adversarial evaluation datasets to test the safety of LLM\ngenerations on new downstream applications. We call it AI-assisted Red-Teaming\n(AART) - an automated alternative to current manual red-teaming efforts. AART\noffers a data generation and augmentation pipeline of reusable and customizable\nrecipes that reduce human effort significantly and enable integration of\nadversarial testing earlier in new product development. AART generates\nevaluation datasets with high diversity of content characteristics critical for\neffective adversarial testing (e.g. sensitive and harmful concepts, specific to\na wide range of cultural and geographic regions and application scenarios). The\ndata generation is steered by AI-assisted recipes to define, scope and\nprioritize diversity within the application context. This feeds into a\nstructured LLM-generation process that scales up evaluation priorities.\nCompared to some state-of-the-art tools, AART shows promising results in terms\nof concept coverage and data quality.\n","authors":["Bhaktipriya Radharapu","Kevin Robinson","Lora Aroyo","Preethi Lahoti"],"pdf_url":"https://arxiv.org/pdf/2311.08592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08590v1","updated":"2023-11-14T23:20:51Z","published":"2023-11-14T23:20:51Z","title":"PEMA: Plug-in External Memory Adaptation for Language Models","summary":"  Pre-trained language models (PLMs) have demonstrated impressive performance\nacross various downstream NLP tasks. Nevertheless, the resource requirements of\npre-training large language models in terms of memory and training compute pose\nsignificant challenges. Furthermore, due to the substantial resources required,\nmany PLM weights are confidential. Consequently, users are compelled to share\ntheir data with model owners for fine-tuning on specific tasks. To overcome the\nlimitations, we introduce Plug-in External Memory Adaptation (PEMA), a\nParameter-Efficient Fine-Tuning (PEFT) approach designed for fine-tuning PLMs\nwithout the need for all weights. PEMA can be integrated into the context\nrepresentation of test data during inference to execute downstream tasks. It\nleverages an external memory to store context representations generated by a\nPLM, mapped with the desired target word. Our method entails training\nLoRA-based weight matrices within the final layer of the PLM for enhanced\nefficiency. The probability is then interpolated with the next-word\ndistribution from the PLM to perform downstream tasks. To improve the\ngeneration quality, we propose a novel interpolation strategy named Gradual\nUnrolling. To demonstrate the effectiveness of our proposed method, we conduct\nexperiments to demonstrate the efficacy of PEMA with a syntactic dataset and\nassess its performance on machine translation and style transfer tasks using\nreal datasets. PEMA outperforms other PEFT methods in terms of memory and\nlatency efficiency for training and inference. Furthermore, it outperforms\nother baselines in preserving the meaning of sentences while generating\nappropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08588v1","updated":"2023-11-14T23:18:52Z","published":"2023-11-14T23:18:52Z","title":"CodeScope: An Execution-based Multilingual Multitask Multidimensional\n  Benchmark for Evaluating LLMs on Code Understanding and Generation","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance on\ncoding related tasks, particularly on assisting humans in programming and\nfacilitating programming automation. However, existing benchmarks for\nevaluating the code understanding and generation capacities of LLMs suffer from\nsevere limitations. First, most benchmarks are deficient as they focus on a\nnarrow range of popular programming languages and specific tasks, whereas the\nreal-world software development scenarios show dire need to implement systems\nwith multilingual programming environments to satisfy diverse requirements.\nPractical programming practices also strongly expect multi-task settings for\ntesting coding capabilities of LLMs comprehensively and robustly. Second, most\nbenchmarks also fail to consider the actual executability and the consistency\nof execution results of the generated code. To bridge these gaps between\nexisting benchmarks and expectations from practical applications, we introduce\nCodeScope, an execution-based, multilingual, multi-task, multi-dimensional\nevaluation benchmark for comprehensively gauging LLM capabilities on coding\ntasks. CodeScope covers 43 programming languages and 8 coding tasks. It\nevaluates the coding performance of LLMs from three dimensions (perspectives):\ndifficulty, efficiency, and length. To facilitate execution-based evaluations\nof code generation, we develop MultiCodeEngine, an automated code execution\nengine that supports 14 programming languages. Finally, we systematically\nevaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the\nsuperior breadth and challenges of CodeScope for evaluating LLMs on code\nunderstanding and generation tasks compared to other benchmarks. The CodeScope\nbenchmark and datasets are publicly available at\nhttps://github.com/WeixiangYAN/CodeScope.\n","authors":["Weixiang Yan","Haitian Liu","Yunkun Wang","Yunzhe Li","Qian Chen","Wen Wang","Tingyu Lin","Weishan Zhao","Li Zhu","Shuiguang Deng","Hari Sundaram"],"pdf_url":"https://arxiv.org/pdf/2311.08588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08584v1","updated":"2023-11-14T23:13:27Z","published":"2023-11-14T23:13:27Z","title":"Asking More Informative Questions for Grounded Retrieval","summary":"  When a model is trying to gather information in an interactive setting, it\nbenefits from asking informative questions. However, in the case of a grounded\nmulti-turn image identification task, previous studies have been constrained to\npolar yes/no questions, limiting how much information the model can gain in a\nsingle turn. We present an approach that formulates more informative,\nopen-ended questions. In doing so, we discover that off-the-shelf visual\nquestion answering (VQA) models often make presupposition errors, which\nstandard information gain question selection methods fail to account for. To\naddress this issue, we propose a method that can incorporate presupposition\nhandling into both question selection and belief updates. Specifically, we use\na two-stage process, where the model first filters out images which are\nirrelevant to a given question, then updates its beliefs about which image the\nuser intends. Through self-play and human evaluations, we show that our method\nis successful in asking informative open-ended questions, increasing accuracy\nover the past state-of-the-art by 14%, while resulting in 48% more efficient\ngames in human evaluations.\n","authors":["Sedrick Keh","Justin T. Chiu","Daniel Fried"],"pdf_url":"https://arxiv.org/pdf/2311.08584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08579v1","updated":"2023-11-14T22:47:23Z","published":"2023-11-14T22:47:23Z","title":"Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational\n  AutoEncoders","summary":"  The injection of syntactic information in Variational AutoEncoders (VAEs) has\nbeen shown to result in an overall improvement of performances and\ngeneralisation. An effective strategy to achieve such a goal is to separate the\nencoding of distributional semantic features and syntactic structures into\nheterogeneous latent spaces via multi-task learning or dual encoder\narchitectures. However, existing works employing such techniques are limited to\nLSTM-based VAEs. In this paper, we investigate latent space separation methods\nfor structural syntactic injection in Transformer-based VAE architectures\n(i.e., Optimus). Specifically, we explore how syntactic structures can be\nleveraged in the encoding stage through the integration of graph-based and\nsequential models, and how multiple, specialised latent representations can be\ninjected into the decoder's attention mechanism via low-rank operators. Our\nempirical evaluation, carried out on natural language sentences and\nmathematical expressions, reveals that the proposed end-to-end VAE architecture\ncan result in a better overall organisation of the latent space, alleviating\nthe information loss occurring in standard VAE setups, resulting in enhanced\nperformances on language modelling and downstream generation tasks.\n","authors":["Yingji Zhang","Marco Valentino","Danilo S. Carvalho","Ian Pratt-Hartmann","André Freitas"],"pdf_url":"https://arxiv.org/pdf/2311.08579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08576v1","updated":"2023-11-14T22:45:44Z","published":"2023-11-14T22:45:44Z","title":"Towards Evaluating AI Systems for Moral Status Using Self-Reports","summary":"  As AI systems become more advanced and widely deployed, there will likely be\nincreasing debate over whether AI systems could have conscious experiences,\ndesires, or other states of potential moral significance. It is important to\ninform these discussions with empirical evidence to the extent possible. We\nargue that under the right circumstances, self-reports, or an AI system's\nstatements about its own internal states, could provide an avenue for\ninvestigating whether AI systems have states of moral significance.\nSelf-reports are the main way such states are assessed in humans (\"Are you in\npain?\"), but self-reports from current systems like large language models are\nspurious for many reasons (e.g. often just reflecting what humans would say).\nTo make self-reports more appropriate for this purpose, we propose to train\nmodels to answer many kinds of questions about themselves with known answers,\nwhile avoiding or limiting training incentives that bias self-reports. The hope\nof this approach is that models will develop introspection-like capabilities,\nand that these capabilities will generalize to questions about states of moral\nsignificance. We then propose methods for assessing the extent to which these\ntechniques have succeeded: evaluating self-report consistency across contexts\nand between similar models, measuring the confidence and resilience of models'\nself-reports, and using interpretability to corroborate self-reports. We also\ndiscuss challenges for our approach, from philosophical difficulties in\ninterpreting self-reports to technical reasons why our proposal might fail. We\nhope our discussion inspires philosophers and AI researchers to criticize and\nimprove our proposed methodology, as well as to run experiments to test whether\nself-reports can be made reliable enough to provide information about states of\nmoral significance.\n","authors":["Ethan Perez","Robert Long"],"pdf_url":"https://arxiv.org/pdf/2311.08576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14964v2","updated":"2023-11-14T22:41:40Z","published":"2023-05-24T09:57:12Z","title":"Detecting Multidimensional Political Incivility on Social Media","summary":"  The rise of social media has been argued to intensify uncivil and hostile\nonline political discourse. Yet, to date, there is a lack of clarity on what\nincivility means in the political sphere. In this work, we utilize a\nmultidimensional perspective of political incivility, developed in the fields\nof political science and communication, that differentiates between\nimpoliteness and political intolerance. We present state-of-the-art incivility\ndetection results using a large dataset of 13K political tweets, collected and\nannotated per this distinction. Applying political incivility detection at\nlarge-scale, we observe that political incivility demonstrates a highly skewed\ndistribution over users, and examine social factors that correlate with\nincivility at subpopulation and user-level. Finally, we propose an approach for\nmodeling social context information about the tweet author alongside the tweet\ncontent, showing that this leads to improved performance on the task of\npolitical incivility detection. We believe that this latter result holds\npromise for socially-informed text processing in general.\n","authors":["Sagi Pendzel","Nir Lotan","Alon Zoizner","Einat Minkov"],"pdf_url":"https://arxiv.org/pdf/2305.14964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08572v1","updated":"2023-11-14T22:32:39Z","published":"2023-11-14T22:32:39Z","title":"Parameter-Efficient Multilingual Summarisation: An Empirical Study","summary":"  With the increasing prevalence of Large Language Models, traditional full\nfine-tuning approaches face growing challenges, especially in memory-intensive\ntasks. This paper investigates the potential of Parameter-Efficient\nFine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and\nunder-explored multilingual summarisation tasks. We conduct an extensive study\nacross different data availability scenarios, including full-data, low-data,\nand cross-lingual transfer, leveraging models of different sizes. Our findings\nreveal that LoRA lags behind full fine-tuning when trained with full data,\nhowever, it excels in low-data scenarios and cross-lingual transfer.\nInterestingly, as models scale up, the performance gap between LoRA and full\nfine-tuning diminishes. Additionally, we investigate effective strategies for\nfew-shot cross-lingual transfer, finding that continued LoRA tuning achieves\nthe best performance compared to both full fine-tuning and dynamic composition\nof language-specific LoRA modules.\n","authors":["Chenxi Whitehouse","Fantine Huot","Jasmijn Bastings","Mostafa Dehghani","Chu-Cheng Lin","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2311.08572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14088v3","updated":"2023-11-14T21:59:56Z","published":"2023-10-21T18:59:41Z","title":"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark\n  for Language Model Evaluation","summary":"  Curated datasets for healthcare are often limited due to the need of human\nannotations from experts. In this paper, we present MedEval, a multi-level,\nmulti-task, and multi-domain medical benchmark to facilitate the development of\nlanguage models for healthcare. MedEval is comprehensive and consists of data\nfrom several healthcare systems and spans 35 human body regions from 8\nexamination modalities. With 22,779 collected sentences and 21,228 reports, we\nprovide expert annotations at multiple levels, offering a granular potential\nusage of the data and supporting a wide range of tasks. Moreover, we\nsystematically evaluated 10 generic and domain-specific language models under\nzero-shot and finetuning settings, from domain-adapted baselines in healthcare\nto general-purposed state-of-the-art large language models (e.g., ChatGPT). Our\nevaluations reveal varying effectiveness of the two categories of language\nmodels across different tasks, from which we notice the importance of\ninstruction tuning for few-shot usage of large language models. Our\ninvestigation paves the way toward benchmarking language models for healthcare\nand provides valuable insights into the strengths and limitations of adopting\nlarge language models in medical domains, informing their practical\napplications and future advancements.\n","authors":["Zexue He","Yu Wang","An Yan","Yao Liu","Eric Y. Chang","Amilcare Gentili","Julian McAuley","Chun-Nan Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.14088v3.pdf","comment":"Accepted to EMNLP 2023. Camera-ready version: updated IRB, added more\n  evaluation results on LLMs such as GPT4, LLaMa2, and LLaMa2-chat"},{"id":"http://arxiv.org/abs/2306.06190v2","updated":"2023-11-14T21:51:21Z","published":"2023-06-09T18:42:19Z","title":"$FastDoc$: Domain-Specific Fast Pre-training Technique using\n  Document-Level Metadata and Taxonomy","summary":"  As the demand for sophisticated Natural Language Processing (NLP) models\ncontinues to grow, so does the need for efficient pre-training techniques.\nCurrent NLP models undergo resource-intensive pre-training. In response, we\nintroduce $FastDoc$ (Fast Pre-training Technique using Document-Level Metadata\nand Taxonomy), a novel approach designed to significantly reduce computational\ndemands. $FastDoc$ leverages document metadata and domain-specific taxonomy as\nsupervision signals. It involves continual pre-training of an open-domain\ntransformer encoder using sentence-level embeddings, followed by fine-tuning\nusing token-level embeddings. We evaluate $FastDoc$ on six tasks across nine\ndatasets spanning three distinct domains. Remarkably, $FastDoc$ achieves\nremarkable compute reductions of approximately 1,000x, 4,500x, 500x compared to\ncompetitive approaches in Customer Support, Scientific, and Legal domains,\nrespectively. Importantly, these efficiency gains do not compromise performance\nrelative to competitive baselines. Furthermore, reduced pre-training data\nmitigates catastrophic forgetting, ensuring consistent performance in\nopen-domain scenarios. $FastDoc$ offers a promising solution for\nresource-efficient pre-training, with potential applications spanning various\ndomains.\n","authors":["Abhilash Nandy","Manav Nitin Kapadnis","Sohan Patnaik","Yash Parag Butala","Pawan Goyal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2306.06190v2.pdf","comment":"38 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.08562v1","updated":"2023-11-14T21:46:27Z","published":"2023-11-14T21:46:27Z","title":"MAgIC: Benchmarking Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration","summary":"  Large Language Models (LLMs) have marked a significant advancement in the\nfield of natural language processing, demonstrating exceptional capabilities in\nreasoning, tool usage, and memory. As their applications extend into\nmulti-agent environments, a need has arisen for a comprehensive evaluation\nframework that captures their abilities in reasoning, planning, collaboration,\nand more. This work introduces a novel benchmarking framework specifically\ntailored to assess LLMs within multi-agent settings, providing quantitative\nmetrics to evaluate their judgment, reasoning, deception, self-awareness,\ncollaboration, coordination, and rationality. We utilize games such as\nChameleon and Undercover, alongside game theory scenarios like Cost Sharing,\nMulti-player Prisoner's Dilemma, and Public Good, to create diverse testing\nenvironments. Our framework is fortified with the Probabilistic Graphical\nModeling (PGM) method, enhancing the LLMs' capabilities in navigating complex\nsocial and cognitive dimensions. The benchmark evaluates seven multi-agent\nsystems powered by different LLMs, quantitatively highlighting a significant\ncapability gap over threefold between the strongest, GPT-4, and the weakest,\nLlama-2-70B. It also confirms that our PGM enhancement boosts the inherent\nabilities of all selected models by 50% on average. Our codes are released here\nhttps://github.com/cathyxl/MAgIC.\n","authors":["Lin Xu","Zhiyuan Hu","Daquan Zhou","Hongyu Ren","Zhen Dong","Kurt Keutzer","See Kiong Ng","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2311.08562v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2311.08552v1","updated":"2023-11-14T21:28:10Z","published":"2023-11-14T21:28:10Z","title":"UT5: Pretraining Non autoregressive T5 with unrolled denoising","summary":"  Recent advances in Transformer-based Large Language Models have made great\nstrides in natural language generation. However, to decode K tokens, an\nautoregressive model needs K sequential forward passes, which may be a\nperformance bottleneck for large language models. Many non-autoregressive (NAR)\nresearch are aiming to address this sequentiality bottleneck, albeit many have\nfocused on a dedicated architecture in supervised benchmarks. In this work, we\nstudied unsupervised pretraining for non auto-regressive T5 models via unrolled\ndenoising and shown its SoTA results in downstream generation tasks such as\nSQuAD question generation and XSum.\n","authors":["Mahmoud G. Salem","Jiayu Ye","Chu-Cheng Lin","Frederick Liu"],"pdf_url":"https://arxiv.org/pdf/2311.08552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19011v3","updated":"2023-11-14T21:22:25Z","published":"2023-05-30T13:07:33Z","title":"MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models","summary":"  SUPERB was proposed to evaluate the generalizability of self-supervised\nlearning (SSL) speech models across various tasks. However, it incurs high\ncomputational costs due to the large datasets and diverse tasks. In this paper,\nwe introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL\nspeech models with comparable results to SUPERB but lower computational costs\nsignificantly. We carefully select representative tasks, sample datasets, and\nextract model representations offline. Our approach achieves a Spearman's rank\ncorrelation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge,\nrespectively. Additionally, we reduce the computational cost by 97% in terms of\nMultiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech\nmodels in few-shot scenarios and observe significant variations in their\nperformance. To our knowledge, this is the first study to examine both the\ncomputational cost of the model itself and the cost of evaluating it on a\nbenchmark.\n","authors":["Yu-Hsiang Wang","Huang-Yu Chen","Kai-Wei Chang","Winston Hsu","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2305.19011v3.pdf","comment":"Accepted to IEEE ASRU 2023"},{"id":"http://arxiv.org/abs/2311.08545v1","updated":"2023-11-14T21:19:14Z","published":"2023-11-14T21:19:14Z","title":"Efficient Continual Pre-training for Building Domain Specific Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable open-domain\ncapabilities. Traditionally, LLMs tailored for a domain are trained from\nscratch to excel at handling domain-specific tasks. In this work, we explore an\nalternative strategy of continual pre-training as a means to develop\ndomain-specific LLMs. We introduce FinPythia-6.9B, developed through\ndomain-adaptive continual pre-training on the financial domain. Continual\npre-trained FinPythia showcases consistent improvements on financial tasks over\nthe original foundational model. We further explore simple but effective data\nselection strategies for continual pre-training. Our data selection strategies\noutperforms vanilla continual pre-training's performance with just 10% of\ncorpus size and cost, without any degradation on open-domain standard tasks.\nOur work proposes an alternative solution to building domain-specific LLMs from\nscratch in a cost-effective manner.\n","authors":["Yong Xie","Karan Aggarwal","Aitzaz Ahmad"],"pdf_url":"https://arxiv.org/pdf/2311.08545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02971v3","updated":"2023-11-14T21:15:01Z","published":"2023-10-04T17:07:32Z","title":"Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech\n  Model","summary":"  Prompting and adapter tuning have emerged as efficient alternatives to\nfine-tuning (FT) methods. However, existing studies on speech prompting focused\non classification tasks and failed on more complex sequence generation tasks.\nBesides, adapter tuning is primarily applied with a focus on encoder-only\nself-supervised models. Our experiments show that prompting on Wav2Seq, a\nself-supervised encoder-decoder model, surpasses previous works in sequence\ngeneration tasks. It achieves a remarkable 53% relative improvement in word\nerror rate for ASR and a 27% in F1 score for slot filling. Additionally,\nprompting competes with the FT method in the low-resource scenario. Moreover,\nwe show the transferability of prompting and adapter tuning on Wav2Seq in\ncross-lingual ASR. When limited trainable parameters are involved, prompting\nand adapter tuning consistently outperform conventional FT across 7 languages.\nNotably, in the low-resource scenario, prompting consistently outperforms\nadapter tuning.\n","authors":["Kai-Wei Chang","Ming-Hsin Chen","Yun-Ping Lin","Jing Neng Hsu","Paul Kuo-Ming Huang","Chien-yu Huang","Shang-Wen Li","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.02971v3.pdf","comment":"Accepted to IEEE ASRU 2023"},{"id":"http://arxiv.org/abs/2311.08538v1","updated":"2023-11-14T21:04:03Z","published":"2023-11-14T21:04:03Z","title":"Extending Multilingual Machine Translation through Imitation Learning","summary":"  Despite the growing variety of languages supported by existing multilingual\nneural machine translation (MNMT) models, most of the world's languages are\nstill being left behind. We aim to extend large-scale MNMT models to a new\nlanguage, allowing for translation between the newly added and all of the\nalready supported languages in a challenging scenario: using only a parallel\ncorpus between the new language and English. Previous approaches, such as\ncontinued training on parallel data including the new language, suffer from\ncatastrophic forgetting (i.e., performance on other languages is reduced). Our\nnovel approach Imit-MNMT treats the task as an imitation learning process,\nwhich mimicks the behavior of an expert, a technique widely used in the\ncomputer vision area, but not well explored in NLP. More specifically, we\nconstruct a pseudo multi-parallel corpus of the new and the original languages\nby pivoting through English, and imitate the output distribution of the\noriginal MNMT model. Extensive experiments show that our approach significantly\nimproves the translation performance between the new and the original\nlanguages, without severe catastrophic forgetting. We also demonstrate that our\napproach is capable of solving copy and off-target problems, which are two\ncommon issues existence in current large-scale MNMT models.\n","authors":["Wen Lai","Viktor Hangya","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2311.08538v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2311.08403v1","updated":"2023-11-14T18:59:59Z","published":"2023-11-14T18:59:59Z","title":"Instant3D: Instant Text-to-3D Generation","summary":"  Text-to-3D generation, which aims to synthesize vivid 3D objects from text\nprompts, has attracted much attention from the computer vision community. While\nseveral existing works have achieved impressive results for this task, they\nmainly rely on a time-consuming optimization paradigm. Specifically, these\nmethods optimize a neural field from scratch for each text prompt, taking\napproximately one hour or more to generate one object. This heavy and\nrepetitive training cost impedes their practical deployment. In this paper, we\npropose a novel framework for fast text-to-3D generation, dubbed Instant3D.\nOnce trained, Instant3D is able to create a 3D object for an unseen text prompt\nin less than one second with a single run of a feedforward network. We achieve\nthis remarkable speed by devising a new network that directly constructs a 3D\ntriplane from a text prompt. The core innovation of our Instant3D lies in our\nexploration of strategies to effectively inject text conditions into the\nnetwork. Furthermore, we propose a simple yet effective activation function,\nthe scaled-sigmoid, to replace the original sigmoid function, which speeds up\nthe training convergence by more than ten times. Finally, to address the Janus\n(multi-head) problem in 3D generation, we propose an adaptive Perp-Neg\nalgorithm that can dynamically adjust its concept negation scales according to\nthe severity of the Janus problem during training, effectively reducing the\nmulti-head effect. Extensive experiments on a wide variety of benchmark\ndatasets demonstrate that the proposed algorithm performs favorably against the\nstate-of-the-art methods both qualitatively and quantitatively, while achieving\nsignificantly better efficiency. The project page is at\nhttps://ming1993li.github.io/Instant3DProj.\n","authors":["Ming Li","Pan Zhou","Jia-Wei Liu","Jussi Keppo","Min Lin","Shuicheng Yan","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2311.08403v1.pdf","comment":"Project page: https://ming1993li.github.io/Instant3DProj"},{"id":"http://arxiv.org/abs/2311.08400v1","updated":"2023-11-14T18:59:01Z","published":"2023-11-14T18:59:01Z","title":"Towards Open-Ended Visual Recognition with Large Language Model","summary":"  Localizing and recognizing objects in the open-ended physical world poses a\nlong-standing challenge within the domain of machine perception. Recent methods\nhave endeavored to address the issue by employing a class-agnostic mask (or\nbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)\nusing pre-extracted text embeddings. However, it is worth noting that these\nopen-vocabulary recognition models still exhibit limitations in practical\napplications. On one hand, they rely on the provision of class names during\ntesting, where the recognition performance heavily depends on this predefined\nset of semantic classes by users. On the other hand, when training with\nmultiple datasets, human intervention is required to alleviate the label\ndefinition conflict between them. In this paper, we introduce the OmniScient\nModel (OSM), a novel Large Language Model (LLM) based mask classifier, as a\nstraightforward and effective solution to the aforementioned challenges.\nSpecifically, OSM predicts class labels in a generative manner, thus removing\nthe supply of class names during both training and testing. It also enables\ncross-dataset training without any human interference, exhibiting robust\ngeneralization capabilities due to the world knowledge acquired from the LLM.\nBy combining OSM with an off-the-shelf mask proposal model, we present\npromising results on various benchmarks, and demonstrate its effectiveness in\nhandling novel concepts. Code/model are available at\nhttps://github.com/bytedance/OmniScient-Model.\n","authors":["Qihang Yu","Xiaohui Shen","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08393v1","updated":"2023-11-14T18:53:28Z","published":"2023-11-14T18:53:28Z","title":"MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable\n  Trajectory Generation","summary":"  The learn-from-observation (LfO) paradigm is a human-inspired mode for a\nrobot to learn to perform a task simply by watching it being performed. LfO can\nfacilitate robot integration on factory floors by minimizing disruption and\nreducing tedious programming. A key component of the LfO pipeline is a\ntransformation of the depth camera frames to the corresponding task state and\naction pairs, which are then relayed to learning techniques such as imitation\nor inverse reinforcement learning for understanding the task parameters. While\nseveral existing computer vision models analyze videos for activity\nrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,\nSA-Net and many other models analyze frame data captured from a single\nviewpoint. Their analysis is therefore highly sensitive to occlusions of the\nobserved task, which are frequent in deployments. An obvious way of reducing\nocclusions is to simultaneously observe the task from multiple viewpoints and\nsynchronously fuse the multiple streams in the model. Toward this, we present\nmulti-view SA-Net, which generalizes the SA-Net model to allow the perception\nof multiple viewpoints of the task activity, integrate them, and better\nrecognize the state and action in each frame. Performance evaluations on two\ndistinct domains establish that MVSA-Net recognizes the state-action pairs\nunder occlusion more accurately compared to single-view MVSA-Net and other\nbaselines. Our ablation studies further evaluate its performance under\ndifferent ambient conditions and establish the contribution of the architecture\ncomponents. As such, MVSA-Net offers a significantly more robust and deployable\nstate-action trajectory generation compared to previous methods.\n","authors":["Ehsan Asali","Prashant Doshi","Jin Sun"],"pdf_url":"https://arxiv.org/pdf/2311.08393v1.pdf","comment":"Conference on Robot Learning 2023 (CoRL2023)"},{"id":"http://arxiv.org/abs/2311.08371v1","updated":"2023-11-14T18:34:18Z","published":"2023-11-14T18:34:18Z","title":"USLR: an open-source tool for unbiased and smooth longitudinal\n  registration of brain MR","summary":"  We present USLR, a computational framework for longitudinal registration of\nbrain MRI scans to estimate nonlinear image trajectories that are smooth across\ntime, unbiased to any timepoint, and robust to imaging artefacts. It operates\non the Lie algebra parameterisation of spatial transforms (which is compatible\nwith rigid transforms and stationary velocity fields for nonlinear deformation)\nand takes advantage of log-domain properties to solve the problem using\nBayesian inference. USRL estimates rigid and nonlinear registrations that: (i)\nbring all timepoints to an unbiased subject-specific space; and (i) compute a\nsmooth trajectory across the imaging time-series. We capitalise on\nlearning-based registration algorithms and closed-form expressions for fast\ninference. A use-case Alzheimer's disease study is used to showcase the\nbenefits of the pipeline in multiple fronts, such as time-consistent image\nsegmentation to reduce intra-subject variability, subject-specific prediction\nor population analysis using tensor-based morphometry. We demonstrate that such\napproach improves upon cross-sectional methods in identifying group\ndifferences, which can be helpful in detecting more subtle atrophy levels or in\nreducing sample sizes in clinical trials. The code is publicly available in\nhttps://github.com/acasamitjana/uslr\n","authors":["Adrià Casamitjana","Roser Sala-Llonch","Karim Lekadir","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2311.08371v1.pdf","comment":"Submitted to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2311.08359v1","updated":"2023-11-14T18:01:15Z","published":"2023-11-14T18:01:15Z","title":"Rotation-Agnostic Image Representation Learning for Digital Pathology","summary":"  This paper addresses complex challenges in histopathological image analysis\nthrough three key contributions. Firstly, it introduces a fast patch selection\nmethod, FPS, for whole-slide image (WSI) analysis, significantly reducing\ncomputational cost while maintaining accuracy. Secondly, it presents PathDino,\na lightweight histopathology feature extractor with a minimal configuration of\nfive Transformer blocks and only 9 million parameters, markedly fewer than\nalternatives. Thirdly, it introduces a rotation-agnostic representation\nlearning paradigm using self-supervised learning, effectively mitigating\noverfitting. We also show that our compact model outperforms existing\nstate-of-the-art histopathology-specific vision transformers on 12 diverse\ndatasets, including both internal datasets spanning four sites (breast, liver,\nskin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,\nDigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training\ndataset of 6 million histopathology patches from The Cancer Genome Atlas\n(TCGA), our approach demonstrates an average 8.5% improvement in patch-level\nmajority vote performance. These contributions provide a robust framework for\nenhancing image analysis in digital pathology, rigorously validated through\nextensive evaluation. Project Page: https://rhazeslab.github.io/PathDino-Page/\n","authors":["Saghir Alfasly","Abubakr Shafique","Peyman Nejat","Jibran Khan","Areej Alsaafin","Ghazal Alabtah","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2311.08359v1.pdf","comment":"23 pages, 10 figures, 18 tables. Histopathological Image Analysis"},{"id":"http://arxiv.org/abs/2301.09702v4","updated":"2023-11-14T17:44:53Z","published":"2023-01-23T20:11:24Z","title":"Illumination Variation Correction Using Image Synthesis For Unsupervised\n  Domain Adaptive Person Re-Identification","summary":"  Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to\nlearn identity information from labeled images in source domains and apply it\nto unlabeled images in a target domain. One major issue with many unsupervised\nre-identification methods is that they do not perform well relative to large\ndomain variations such as illumination, viewpoint, and occlusions. In this\npaper, we propose a Synthesis Model Bank (SMB) to deal with illumination\nvariation in unsupervised person re-ID. The proposed SMB consists of several\nconvolutional neural networks (CNN) for feature extraction and Mahalanobis\nmatrices for distance metrics. They are trained using synthetic data with\ndifferent illumination conditions such that their synergistic effect makes the\nSMB robust against illumination variation. To better quantify the illumination\nintensity and improve the quality of synthetic images, we introduce a new 3D\nvirtual-human dataset for GAN-based image synthesis. From our experiments, the\nproposed SMB outperforms other synthesis methods on several re-ID benchmarks.\n","authors":["Jiaqi Guo","Amy R. Reibman","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2301.09702v4.pdf","comment":"10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2311.08314v1","updated":"2023-11-14T16:58:18Z","published":"2023-11-14T16:58:18Z","title":"Convolutional Neural Networks Exploiting Attributes of Biological\n  Neurons","summary":"  In this era of artificial intelligence, deep neural networks like\nConvolutional Neural Networks (CNNs) have emerged as front-runners, often\nsurpassing human capabilities. These deep networks are often perceived as the\npanacea for all challenges. Unfortunately, a common downside of these networks\nis their ''black-box'' character, which does not necessarily mirror the\noperation of biological neural systems. Some even have millions/billions of\nlearnable (tunable) parameters, and their training demands extensive data and\ntime.\n  Here, we integrate the principles of biological neurons in certain layer(s)\nof CNNs. Specifically, we explore the use of neuro-science-inspired\ncomputational models of the Lateral Geniculate Nucleus (LGN) and simple cells\nof the primary visual cortex. By leveraging such models, we aim to extract\nimage features to use as input to CNNs, hoping to enhance training efficiency\nand achieve better accuracy. We aspire to enable shallow networks with a\nPush-Pull Combination of Receptive Fields (PP-CORF) model of simple cells as\nthe foundation layer of CNNs to enhance their learning process and performance.\nTo achieve this, we propose a two-tower CNN, one shallow tower and the other as\nResNet 18. Rather than extracting the features blindly, it seeks to mimic how\nthe brain perceives and extracts features. The proposed system exhibits a\nnoticeable improvement in the performance (on an average of $5\\%-10\\%$) on\nCIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. We also\ncheck the efficiency of only the Push-Pull tower of the network.\n","authors":["Neeraj Kumar Singh","Nikhil R. Pal"],"pdf_url":"https://arxiv.org/pdf/2311.08314v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08308v1","updated":"2023-11-14T16:53:45Z","published":"2023-11-14T16:53:45Z","title":"The Heat is On: Thermal Facial Landmark Tracking","summary":"  Facial landmark tracking for thermal images requires tracking certain\nimportant regions of subjects' faces, using images from thermal images, which\nomit lighting and shading, but show the temperatures of their subjects. The\nfluctuations of heat in particular places reflect physiological changes like\nbloodflow and perspiration, which can be used to remotely gauge things like\nanxiety and excitement. Past work in this domain has been limited to only a\nvery limited set of architectures and techniques. This work goes further by\ntrying a comprehensive suit of various models with different components, such\nas residual connections, channel and feature-wise attention, as well as the\npractice of ensembling components of the network to work in parallel. The best\nmodel integrated convolutional and residual layers followed by a channel-wise\nself-attention layer, requiring less than 100K parameters.\n","authors":["James Baker"],"pdf_url":"https://arxiv.org/pdf/2311.08308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06210v3","updated":"2023-11-14T16:46:00Z","published":"2023-05-26T13:06:38Z","title":"Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion","summary":"  Recent breakthroughs in generative modeling have sparked interest in\npractical single-model attribution. Such methods predict whether a sample was\ngenerated by a specific generator or not, for instance, to prove intellectual\nproperty theft. However, previous works are either limited to the closed-world\nsetting or require undesirable changes to the generative model. We address\nthese shortcomings by, first, viewing single-model attribution through the lens\nof anomaly detection. Arising from this change of perspective, we propose\nFLIPAD, a new approach for single-model attribution in the open-world setting\nbased on final-layer inversion and anomaly detection. We show that the utilized\nfinal-layer inversion can be reduced to a convex lasso optimization problem,\nmaking our approach theoretically sound and computationally efficient. The\ntheoretical findings are accompanied by an experimental study demonstrating the\neffectiveness of our approach and its flexibility to various domains.\n","authors":["Mike Laszkiewicz","Jonas Ricker","Johannes Lederer","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2306.06210v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08284v1","updated":"2023-11-14T16:27:33Z","published":"2023-11-14T16:27:33Z","title":"Level Set KSVD","summary":"  We present a new algorithm for image segmentation - Level-set KSVD. Level-set\nKSVD merges the methods of sparse dictionary learning for feature extraction\nand variational level-set method for image segmentation. Specifically, we use a\ngeneralization of the Chan-Vese functional with features learned by KSVD. The\nmotivation for this model is agriculture based. Aerial images are taken in\norder to detect the spread of fungi in various crops. Our model is tested on\nsuch images of cotton fields. The results are compared to other methods.\n","authors":["Omer Sapir","Iftach Klapp","Nir Sochen"],"pdf_url":"https://arxiv.org/pdf/2311.08284v1.pdf","comment":"25 pages, 14 figures. Submitted to IJCV"},{"id":"http://arxiv.org/abs/2311.08278v1","updated":"2023-11-14T16:19:29Z","published":"2023-11-14T16:19:29Z","title":"ARTEMIS: Using GANs with Multiple Discriminators to Generate Art","summary":"  We propose a novel method for generating abstract art. First an autoencoder\nis trained to encode and decode the style representations of images, which are\nextracted from source images with a pretrained VGG network. Then, the decoder\ncomponent of the autoencoder is extracted and used as a generator in a GAN. The\ngenerator works with an ensemble of discriminators. Each discriminator takes\ndifferent style representations of the same images, and the generator is\ntrained to create images that create convincing style representations in order\nto deceive all of the generators. The generator is also trained to maximize a\ndiversity term. The resulting images had a surreal, geometric quality. We call\nour approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including\nSelf-Attention), as it uses the self-attention layers and an encoder-decoder\narchitecture.\n","authors":["James Baker"],"pdf_url":"https://arxiv.org/pdf/2311.08278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16694v3","updated":"2023-11-14T16:06:56Z","published":"2023-07-31T14:09:03Z","title":"Investigating and Improving Latent Density Segmentation Models for\n  Aleatoric Uncertainty Quantification in Medical Imaging","summary":"  Data uncertainties, such as sensor noise or occlusions, can introduce\nirreducible ambiguities in images, which result in varying, yet plausible,\nsemantic hypotheses. In Machine Learning, this ambiguity is commonly referred\nto as aleatoric uncertainty. Latent density models can be utilized to address\nthis problem in image segmentation. The most popular approach is the\nProbabilistic U-Net (PU-Net), which uses latent Normal densities to optimize\nthe conditional data log-likelihood Evidence Lower Bound. In this work, we\ndemonstrate that the PU- Net latent space is severely inhomogenous. As a\nresult, the effectiveness of gradient descent is inhibited and the model\nbecomes extremely sensitive to the localization of the latent space samples,\nresulting in defective predictions. To address this, we present the Sinkhorn\nPU-Net (SPU-Net), which uses the Sinkhorn Divergence to promote homogeneity\nacross all latent dimensions, effectively improving gradient-descent updates\nand model robustness. Our results show that by applying this on public datasets\nof various clinical segmentation problems, the SPU-Net receives up to 11%\nperformance gains compared against preceding latent variable models for\nprobabilistic segmentation on the Hungarian-Matched metric. The results\nindicate that by encouraging a homogeneous latent space, one can significantly\nimprove latent density modeling for medical image segmentation.\n","authors":["M. M. Amaan Valiuddin","Christiaan G. A. Viviers","Ruud J. G. van Sloun","Peter H. N. de With","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2307.16694v3.pdf","comment":"12 pages incl. references, 11 figures. EDIT: updated figure 6 due to\n  rendering error"},{"id":"http://arxiv.org/abs/2206.03359v2","updated":"2023-11-14T16:06:45Z","published":"2022-06-07T14:53:35Z","title":"An efficient semi-supervised quality control system trained using\n  physics-based MRI-artefact generators and adversarial training","summary":"  Large medical imaging data sets are becoming increasingly available, but\nensuring sample quality without significant artefacts is challenging. Existing\nmethods for identifying imperfections in medical imaging rely on data-intensive\napproaches, compounded by a scarcity of artefact-rich scans for training\nmachine learning models in clinical research. To tackle this problem, we\npropose a framework with four main components: 1) artefact generators inspired\nby magnetic resonance physics to corrupt brain MRI scans and augment a training\ndataset, 2) abstract and engineered features to represent images compactly, 3)\na feature selection process depending on the artefact class to improve\nclassification, and 4) SVM classifiers to identify artefacts. Our contributions\nare threefold: first, physics-based artefact generators produce synthetic brain\nMRI scans with controlled artefacts for data augmentation. This will avoid the\nlabour-intensive collection and labelling process of scans with rare artefacts.\nSecond, we propose a pool of abstract and engineered image features to identify\n9 different artefacts for structural MRI. Finally, we use an artefact-based\nfeature selection block that, for each class of artefacts, finds the set of\nfeatures providing the best classification performance. We performed validation\nexperiments on a large data set of scans with artificially-generated artefacts,\nand in a multiple sclerosis clinical trial where real artefacts were identified\nby experts, showing that the proposed pipeline outperforms traditional methods.\nIn particular, our data augmentation increases performance by up to 12.5\npercentage points on accuracy, precision, and recall. The computational\nefficiency of our pipeline enables potential real-time deployment, promising\nhigh-throughput clinical applications through automated image-processing\npipelines driven by quality control systems.\n","authors":["Daniele Ravi","Frederik Barkhof","Daniel C. Alexander","Lemuel Puglisi","Geoffrey JM Parker","Arman Eshaghi"],"pdf_url":"https://arxiv.org/pdf/2206.03359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08269v1","updated":"2023-11-14T16:02:18Z","published":"2023-11-14T16:02:18Z","title":"Defining the boundaries: challenges and advances in identifying cells in\n  microscopy images","summary":"  Segmentation, or the outlining of objects within images, is a critical step\nin the measurement and analysis of cells within microscopy images. While\nimprovements continue to be made in tools that rely on classical methods for\nsegmentation, deep learning-based tools increasingly dominate advances in the\ntechnology. Specialist models such as Cellpose continue to improve in accuracy\nand user-friendliness, and segmentation challenges such as the Multi-Modality\nCell Segmentation Challenge continue to push innovation in accuracy across\nwidely-varying test data as well as efficiency and usability. Increased\nattention on documentation, sharing, and evaluation standards are leading to\nincreased user-friendliness and acceleration towards the goal of a truly\nuniversal method.\n","authors":["Nodar Gogoberidze","Beth A. Cimini"],"pdf_url":"https://arxiv.org/pdf/2311.08269v1.pdf","comment":"11 pages, 1 figure, submitted to \"Current Opinion in Biotechnology\""},{"id":"http://arxiv.org/abs/2311.08265v1","updated":"2023-11-14T16:00:29Z","published":"2023-11-14T16:00:29Z","title":"On The Relationship Between Universal Adversarial Attacks And Sparse\n  Representations","summary":"  The prominent success of neural networks, mainly in computer vision tasks, is\nincreasingly shadowed by their sensitivity to small, barely perceivable\nadversarial perturbations in image input.\n  In this work, we aim at explaining this vulnerability through the framework\nof sparsity.\n  We show the connection between adversarial attacks and sparse\nrepresentations, with a focus on explaining the universality and\ntransferability of adversarial examples in neural networks.\n  To this end, we show that sparse coding algorithms, and the neural\nnetwork-based learned iterative shrinkage thresholding algorithm (LISTA) among\nthem, suffer from this sensitivity, and that common attacks on neural networks\ncan be expressed as attacks on the sparse representation of the input image.\nThe phenomenon that we observe holds true also when the network is agnostic to\nthe sparse representation and dictionary, and thus can provide a possible\nexplanation for the universality and transferability of adversarial attacks.\n  The code is available at\nhttps://github.com/danawr/adversarial_attacks_and_sparse_representations.\n","authors":["Dana Weitzner","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2311.08265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08245v1","updated":"2023-11-14T15:30:17Z","published":"2023-11-14T15:30:17Z","title":"TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity\n  Recognition","summary":"  Recent achievements in language models have showcased their extraordinary\ncapabilities in bridging visual information with semantic language\nunderstanding. This leads us to a novel question: can language models connect\ntextual semantics with IoT sensory signals to perform recognition tasks, e.g.,\nHuman Activity Recognition (HAR)? If so, an intelligent HAR system with\nhuman-like cognition can be built, capable of adapting to new environments and\nunseen categories. This paper explores its feasibility with an innovative\napproach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointly\naligns textual embeddings with IoT sensor signals, including camera video,\nLiDAR, and mmWave. Through the IoT-language contrastive learning, we derive a\nunified semantic feature space that aligns multi-modal features with language\nembeddings, so that the IoT data corresponds to specific words that describe\nthe IoT data. To enhance the connection between textual categories and their\nIoT data, we propose supplementary descriptions and learnable prompts that\nbring more semantic information into the joint feature space. TENT can not only\nrecognize actions that have been seen but also ``guess'' the unseen action by\nthe closest textual words from the feature space. We demonstrate TENT achieves\nstate-of-the-art performance on zero-shot HAR tasks using different modalities,\nimproving the best vision-language models by over 12%.\n","authors":["Yunjiao Zhou","Jianfei Yang","Han Zou","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2311.08245v1.pdf","comment":"Preprint manuscript in submission"},{"id":"http://arxiv.org/abs/2311.08239v1","updated":"2023-11-14T15:20:42Z","published":"2023-11-14T15:20:42Z","title":"Learning Physics-Inspired Regularization for Medical Image Registration\n  with Hypernetworks","summary":"  Medical image registration aims at identifying the spatial deformation\nbetween images of the same anatomical region and is fundamental to image-based\ndiagnostics and therapy. To date, the majority of the deep learning-based\nregistration methods employ regularizers that enforce global spatial\nsmoothness, e.g., the diffusion regularizer. However, such regularizers are not\ntailored to the data and might not be capable of reflecting the complex\nunderlying deformation. In contrast, physics-inspired regularizers promote\nphysically plausible deformations. One such regularizer is the linear elastic\nregularizer which models the deformation of elastic material. These\nregularizers are driven by parameters that define the material's physical\nproperties. For biological tissue, a wide range of estimations of such\nparameters can be found in the literature and it remains an open challenge to\nidentify suitable parameter values for successful registration. To overcome\nthis problem and to incorporate physical properties into learning-based\nregistration, we propose to use a hypernetwork that learns the effect of the\nphysical parameters of a physics-inspired regularizer on the resulting spatial\ndeformation field. In particular, we adapt the HyperMorph framework to learn\nthe effect of the two elasticity parameters of the linear elastic regularizer.\nOur approach enables the efficient discovery of suitable, data-specific\nphysical parameters at test time.\n","authors":["Anna Reithmeir","Julia A. Schnabel","Veronika A. Zimmer"],"pdf_url":"https://arxiv.org/pdf/2311.08239v1.pdf","comment":"Abstract accepted at SPIE Medical Imaging 2024. Manuscript will be\n  published in Proceedings of the SPIE Digital Library"},{"id":"http://arxiv.org/abs/2311.08236v1","updated":"2023-11-14T15:18:54Z","published":"2023-11-14T15:18:54Z","title":"MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image\n  Diagnosis","summary":"  The common practice in developing computer-aided diagnosis (CAD) models based\non transformer architectures usually involves fine-tuning from ImageNet\npre-trained weights. However, with recent advances in large-scale pre-training\nand the practice of scaling laws, Vision Transformers (ViT) have become much\nlarger and less accessible to medical imaging communities. Additionally, in\nreal-world scenarios, the deployments of multiple CAD models can be troublesome\ndue to problems such as limited storage space and time-consuming model\nswitching. To address these challenges, we propose a new method MeLo (Medical\nimage Low-rank adaptation), which enables the development of a single CAD model\nfor multiple clinical tasks in a lightweight manner. It adopts low-rank\nadaptation instead of resource-demanding fine-tuning. By fixing the weight of\nViT models and only adding small low-rank plug-ins, we achieve competitive\nresults on various diagnosis tasks across different imaging modalities using\nonly a few trainable parameters. Specifically, our proposed method achieves\ncomparable performance to fully fine-tuned ViT models on four distinct medical\nimaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds\nonly about 0.5MB of storage space and allows for extremely fast model switching\nin deployment and inference. Our source code and pre-trained weights are\navailable on our website (https://absterzhu.github.io/melo.github.io/).\n","authors":["Yitao Zhu","Zhenrong Shen","Zihao Zhao","Sheng Wang","Xin Wang","Xiangyu Zhao","Dinggang Shen","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08236v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.08225v1","updated":"2023-11-14T15:05:59Z","published":"2023-11-14T15:05:59Z","title":"Uni-COAL: A Unified Framework for Cross-Modality Synthesis and\n  Super-Resolution of MR Images","summary":"  Cross-modality synthesis (CMS), super-resolution (SR), and their combination\n(CMSR) have been extensively studied for magnetic resonance imaging (MRI).\nTheir primary goals are to enhance the imaging quality by synthesizing the\ndesired modality and reducing the slice thickness. Despite the promising\nsynthetic results, these techniques are often tailored to specific tasks,\nthereby limiting their adaptability to complex clinical scenarios. Therefore,\nit is crucial to build a unified network that can handle various image\nsynthesis tasks with arbitrary requirements of modality and resolution\nsettings, so that the resources for training and deploying the models can be\ngreatly reduced. However, none of the previous works is capable of performing\nCMS, SR, and CMSR using a unified network. Moreover, these MRI reconstruction\nmethods often treat alias frequencies improperly, resulting in suboptimal\ndetail restoration. In this paper, we propose a Unified Co-Modulated Alias-free\nframework (Uni-COAL) to accomplish the aforementioned tasks with a single\nnetwork. The co-modulation design of the image-conditioned and stochastic\nattribute representations ensures the consistency between CMS and SR, while\nsimultaneously accommodating arbitrary combinations of input/output modalities\nand thickness. The generator of Uni-COAL is also designed to be alias-free\nbased on the Shannon-Nyquist signal processing framework, ensuring effective\nsuppression of alias frequencies. Additionally, we leverage the semantic prior\nof Segment Anything Model (SAM) to guide Uni-COAL, ensuring a more authentic\npreservation of anatomical structures during synthesis. Experiments on three\ndatasets demonstrate that Uni-COAL outperforms the alternatives in CMS, SR, and\nCMSR tasks for MR images, which highlights its generalizability to wide-range\napplications.\n","authors":["Zhiyun Song","Zengxin Qi","Xin Wang","Xiangyu Zhao","Zhenrong Shen","Sheng Wang","Manman Fei","Zhe Wang","Di Zang","Dongdong Chen","Linlin Yao","Qian Wang","Xuehai Wu","Lichi Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08223v1","updated":"2023-11-14T15:01:58Z","published":"2023-11-14T15:01:58Z","title":"Improving Image Captioning via Predicting Structured Concepts","summary":"  Having the difficulty of solving the semantic gap between images and texts\nfor the image captioning task, conventional studies in this area paid some\nattention to treating semantic concepts as a bridge between the two modalities\nand improved captioning performance accordingly. Although promising results on\nconcept prediction were obtained, the aforementioned studies normally ignore\nthe relationship among concepts, which relies on not only objects in the image,\nbut also word dependencies in the text, so that offers a considerable potential\nfor improving the process of generating good descriptions. In this paper, we\npropose a structured concept predictor (SCP) to predict concepts and their\nstructures, then we integrate them into captioning, so as to enhance the\ncontribution of visual signals in this task via concepts and further use their\nrelations to distinguish cross-modal semantics for better description\ngeneration. Particularly, we design weighted graph convolutional networks\n(W-GCN) to depict concept relations driven by word dependencies, and then\nlearns differentiated contributions from these concepts for following decoding\nprocess. Therefore, our approach captures potential relations among concepts\nand discriminatively learns different concepts, so that effectively facilitates\nimage captioning with inherited information across modalities. Extensive\nexperiments and their results demonstrate the effectiveness of our approach as\nwell as each proposed module in this work.\n","authors":["Ting Wang","Weidong Chen","Yuanhe Tian","Yan Song","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2311.08223v1.pdf","comment":"13 pages, 4 figures. Published at EMNLP 2023 (Main Conference, Oral)"},{"id":"http://arxiv.org/abs/2311.08217v1","updated":"2023-11-14T14:55:42Z","published":"2023-11-14T14:55:42Z","title":"Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot\n  Image Generation","summary":"  Few-shot image generation aims to train generative models using a small\nnumber of training images. When there are few images available for training\n(e.g. 10 images), Learning From Scratch (LFS) methods often generate images\nthat closely resemble the training data while Transfer Learning (TL) methods\ntry to improve performance by leveraging prior knowledge from GANs pre-trained\non large-scale datasets. However, current TL methods may not allow for\nsufficient control over the degree of knowledge preservation from the source\nmodel, making them unsuitable for setups where the source and target domains\nare not closely related. To address this, we propose a novel pipeline called\nPeer is your Pillar (PIP), which combines a target few-shot dataset with a peer\ndataset to create a data-unbalanced conditional generation. Our approach\nincludes a class embedding method that separates the class space from the\nlatent space, and we use a direction loss based on pre-trained CLIP to improve\nimage diversity. Experiments on various few-shot datasets demonstrate the\nadvancement of the proposed PIP, especially reduces the training requirements\nof few-shot image generation.\n","authors":["Ziqiang Li","Chaoyue Wang","Xue Rui","Chao Xue","Jiaxu Leng","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2311.08217v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2311.08213v1","updated":"2023-11-14T14:49:46Z","published":"2023-11-14T14:49:46Z","title":"Unlock the Power: Competitive Distillation for Multi-Modal Large\n  Language Models","summary":"  Recently, multi-modal content generation has attracted lots of attention from\nresearchers by investigating the utilization of visual instruction tuning based\non large language models (LLMs). To enhance the performance and generalization\nability of such LLMs, the practice of distilling knowledge from pretrained\nmulti-modal models (a.k.a. teachers) to more compact multi-modal LLMs\n(students) has gained considerable interest. However, the prevailing paradigm\nof instructiontuning in multi-modal LLMs knowledge distillation is\nresource-intensive and unidirectional, neglecting the potential for mutual\nfeedback between the student and teacher models. Thus, we propose an innovative\nCompetitive Multi-modal Distillation framework (CoMD), which captures\nbidirectional feedback between teacher and student models and continually\nupdates the multi-modal capabilities that the student model has learned. It\ncomprises two stages: multi-modal pre-training and multi-modal competitive\ndistillation. The first stage pre-trains the student model on a large number of\nfiltered multi-modal datasets. The second stage facilitates a bidirectional\nknowledge transfer between the student and teacher models. Our experimental\nanalysis of diverse datasets shows that our knowledge transfer method\nconsistently improves the capabilities of the student model. Finally, the\n7B-sized student model after four distillations surpassed the current\nstate-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also\noutperforms other strong baselines in the zero-shot setting.\n","authors":["Xinwei Li","Li Lin","Shuai Wang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2311.08213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02251v2","updated":"2023-11-14T14:46:05Z","published":"2023-10-03T17:53:51Z","title":"Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving","summary":"  Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye\nview (BEV) maps in autonomous driving contexts. While existing perception\nsystems for autonomous driving scenarios have largely focused on a pre-defined\n(closed) set of object categories and driving scenarios, Talk2BEV blends recent\nadvances in general-purpose language and vision models with BEV-structured map\nrepresentations, eliminating the need for task-specific models. This enables a\nsingle system to cater to a variety of autonomous driving tasks encompassing\nvisual and spatial reasoning, predicting the intents of traffic actors, and\ndecision-making based on visual cues. We extensively evaluate Talk2BEV on a\nlarge number of scene understanding tasks that rely on both the ability to\ninterpret free-form natural language queries, and in grounding these queries to\nthe visual context embedded into the language-enhanced BEV map. To enable\nfurther research in LVLMs for autonomous driving scenarios, we develop and\nrelease Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV\nscenarios, with more than 20,000 questions and ground-truth responses from the\nNuScenes dataset.\n","authors":["Tushar Choudhary","Vikrant Dewangan","Shivam Chandhok","Shubham Priyadarshan","Anushka Jain","Arun K. Singh","Siddharth Srivastava","Krishna Murthy Jatavallabhula","K. Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.02251v2.pdf","comment":"Project page at https://llmbev.github.io/talk2bev/"},{"id":"http://arxiv.org/abs/2311.08199v1","updated":"2023-11-14T14:33:39Z","published":"2023-11-14T14:33:39Z","title":"Diffusion-based generation of Histopathological Whole Slide Images at a\n  Gigapixel scale","summary":"  We present a novel diffusion-based approach to generate synthetic\nhistopathological Whole Slide Images (WSIs) at an unprecedented gigapixel\nscale. Synthetic WSIs have many potential applications: They can augment\ntraining datasets to enhance the performance of many computational pathology\napplications. They allow the creation of synthesized copies of datasets that\ncan be shared without violating privacy regulations. Or they can facilitate\nlearning representations of WSIs without requiring data annotations. Despite\nthis variety of applications, no existing deep-learning-based method generates\nWSIs at their typically high resolutions. Mainly due to the high computational\ncomplexity. Therefore, we propose a novel coarse-to-fine sampling scheme to\ntackle image generation of high-resolution WSIs. In this scheme, we increase\nthe resolution of an initial low-resolution image to a high-resolution WSI.\nParticularly, a diffusion model sequentially adds fine details to images and\nincreases their resolution. In our experiments, we train our method with WSIs\nfrom the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also\nperformed a user study with pathologists. The study results suggest that our\ngenerated WSIs resemble the structure of real WSIs.\n","authors":["Robert Harb","Thomas Pock","Heimo Müller"],"pdf_url":"https://arxiv.org/pdf/2311.08199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08190v1","updated":"2023-11-14T14:23:09Z","published":"2023-11-14T14:23:09Z","title":"SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage\n  Segmentation","summary":"  Segment Anything Model (SAM), a vision foundation model trained on\nlarge-scale annotations, has recently continued raising awareness within\nmedical image segmentation. Despite the impressive capabilities of SAM on\nnatural scenes, it struggles with performance decline when confronted with\nmedical images, especially those involving blurry boundaries and highly\nirregular regions of low contrast. In this paper, a SAM-based\nparameter-efficient fine-tuning method, called SAMIHS, is proposed for\nintracranial hemorrhage segmentation, which is a crucial and challenging step\nin stroke diagnosis and surgical planning. Distinguished from previous SAM and\nSAM-based methods, SAMIHS incorporates parameter-refactoring adapters into\nSAM's image encoder and considers the efficient and flexible utilization of\nadapters' parameters. Additionally, we employ a combo loss that combines binary\ncross-entropy loss and boundary-sensitive loss to enhance SAMIHS's ability to\nrecognize the boundary regions. Our experimental results on two public datasets\ndemonstrate the effectiveness of our proposed method. Code is available at\nhttps://github.com/mileswyn/SAMIHS .\n","authors":["Yinuo Wang","Kai Chen","Weimin Yuan","Cai Meng","XiangZhi Bai"],"pdf_url":"https://arxiv.org/pdf/2311.08190v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.08176v1","updated":"2023-11-14T14:04:35Z","published":"2023-11-14T14:04:35Z","title":"A deformation-based morphometry framework for disentangling Alzheimer's\n  disease from normal aging using learned normal aging templates","summary":"  Alzheimer's Disease and normal aging are both characterized by brain atrophy.\nThe question of whether AD-related brain atrophy represents accelerated aging\nor a neurodegeneration process distinct from that in normal aging remains\nunresolved. Moreover, precisely disentangling AD-related brain atrophy from\nnormal aging in a clinical context is complex. In this study, we propose a\ndeformation-based morphometry framework to estimate normal aging and\nAD-specific atrophy patterns of subjects from morphological MRI scans. We first\nleverage deep-learning-based methods to create age-dependent templates of\ncognitively normal (CN) subjects. These templates model the normal aging\natrophy patterns in a CN population. Then, we use the learned diffeomorphic\nregistration to estimate the one-year normal aging pattern at the voxel level.\nWe register the testing image to the 60-year-old CN template in the second\nstep. Finally, normal aging and AD-specific scores are estimated by measuring\nthe alignment of this registration with the one-year normal aging pattern. The\nmethodology was developed and evaluated on the OASIS3 dataset with 1,014\nT1-weighted MRI scans. Of these, 326 scans were from CN subjects, and 688 scans\nwere from individuals clinically diagnosed with AD at different stages of\nclinical severity defined by clinical dementia rating (CDR) scores. The results\nshow that ventricles predominantly follow an accelerated normal aging pattern\nin subjects with AD. In turn, hippocampi and amygdala regions were affected by\nboth normal aging and AD-specific factors. Interestingly, hippocampi and\namygdala regions showed more of an accelerated normal aging pattern for\nsubjects during the early clinical stages of the disease, while the AD-specific\nscore increases in later clinical stages. Our code is freely available at\nhttps://github.com/Fjr9516/DBM_with_DL.\n","authors":["Jingru Fu","Daniel Ferreira","Örjan Smedby","Rodrigo Moreno"],"pdf_url":"https://arxiv.org/pdf/2311.08176v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.08172v1","updated":"2023-11-14T14:02:32Z","published":"2023-11-14T14:02:32Z","title":"Vision-Language Instruction Tuning: A Review and Analysis","summary":"  Instruction tuning is an essential supervised training phase for Large\nLanguage Models (LLMs), with the goal of enhancing LLMs' capacity to generalize\ninstruction execution and adapt to user preferences. With the growing\nincorporation of multi-modal data into LLMs, there is an increasing interest in\nthe performance of vision-language instruction tuning which presents more\ncomplex features in comparison to pure text instructions. In this paper, we\nsystematically review the latest vision-language instruction tuning settings\nand datasets in multi-modal LLMs and summarize the characteristics that\nhigh-quality vision-language tuning data should have. We consider these\ncharacteristics as the foundational principles for constructing vision-language\ninstruction data and propose a complete construction pipeline consisting of\ndata collection, instruction generation, and quality control modules that\nincorporate meticulously designed instruction property evaluation indicators.\nWe perform vision-language instruction tuning on three widely used multi-modal\nLLMs based on the instruction data we constructed and conduct extensive\nexperiments on the corresponding metrics to demonstrate the rationality of the\nconstruction principles proposed in this paper. The code and dataset related to\nthis paper have been open-sourced at\n\\url{https://github.com/palchenli/VL-Instruction-Tuning}.\n","authors":["Chen Li","Yixiao Ge","Dian Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.08172v1.pdf","comment":"36 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08159v1","updated":"2023-11-14T13:39:01Z","published":"2023-11-14T13:39:01Z","title":"DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an\n  Optimizable Feature Grid","summary":"  We propose DynamicSurf, a model-free neural implicit surface reconstruction\nmethod for high-fidelity 3D modelling of non-rigid surfaces from monocular\nRGB-D video. To cope with the lack of multi-view cues in monocular sequences of\ndeforming surfaces, one of the most challenging settings for 3D reconstruction,\nDynamicSurf exploits depth, surface normals, and RGB losses to improve\nreconstruction fidelity and optimisation time. DynamicSurf learns a neural\ndeformation field that maps a canonical representation of the surface geometry\nto the current frame. We depart from current neural non-rigid surface\nreconstruction models by designing the canonical representation as a learned\nfeature grid which leads to faster and more accurate surface reconstruction\nthan competing approaches that use a single MLP. We demonstrate DynamicSurf on\npublic datasets and show that it can optimize sequences of varying frames with\n$6\\times$ speedup over pure MLP-based approaches while achieving comparable\nresults to the state-of-the-art methods. Project is available at\nhttps://mirgahney.github.io//DynamicSurf.io/.\n","authors":["Mirgahney Mohamed","Lourdes Agapito"],"pdf_url":"https://arxiv.org/pdf/2311.08159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08151v1","updated":"2023-11-14T13:27:03Z","published":"2023-11-14T13:27:03Z","title":"Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video\n  Parsing","summary":"  Existing works on weakly-supervised audio-visual video parsing adopt hybrid\nattention network (HAN) as the multi-modal embedding to capture the cross-modal\ncontext. It embeds the audio and visual modalities with a shared network, where\nthe cross-attention is performed at the input. However, such an early fusion\nmethod highly entangles the two non-fully correlated modalities and leads to\nsub-optimal performance in detecting single-modality events. To deal with this\nproblem, we propose the messenger-guided mid-fusion transformer to reduce the\nuncorrelated cross-modal context in the fusion. The messengers condense the\nfull cross-modal context into a compact representation to only preserve useful\ncross-modal information. Furthermore, due to the fact that microphones capture\naudio events from all directions, while cameras only record visual events\nwithin a restricted field of view, there is a more frequent occurrence of\nunaligned cross-modal context from audio for visual event predictions. We thus\npropose cross-audio prediction consistency to suppress the impact of irrelevant\naudio information on visual event prediction. Experiments consistently\nillustrate the superior performance of our framework compared to existing\nstate-of-the-art methods.\n","authors":["Yating Xu","Conghui Hu","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2311.08151v1.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2311.08148v1","updated":"2023-11-14T13:25:41Z","published":"2023-11-14T13:25:41Z","title":"Cattle Identification Using Muzzle Images and Deep Learning Techniques","summary":"  Traditional animal identification methods such as ear-tagging, ear notching,\nand branding have been effective but pose risks to the animal and have\nscalability issues. Electrical methods offer better tracking and monitoring but\nrequire specialized equipment and are susceptible to attacks. Biometric\nidentification using time-immutable dermatoglyphic features such as muzzle\nprints and iris patterns is a promising solution. This project explores cattle\nidentification using 4923 muzzle images collected from 268 beef cattle. Two\ndeep learning classification models are implemented - wide ResNet50 and\nVGG16\\_BN and image compression is done to lower the image quality and adapt\nthe models to work for the African context. From the experiments run, a maximum\naccuracy of 99.5\\% is achieved while using the wide ResNet50 model with a\ncompression retaining 25\\% of the original image. From the study, it is noted\nthat the time required by the models to train and converge as well as\nrecognition time are dependent on the machine used to run the model.\n","authors":["G. N. Kimani","P. Oluwadara","P. Fashingabo","M. Busogi","E. Luhanga","K. Sowon","L. Chacha"],"pdf_url":"https://arxiv.org/pdf/2311.08148v1.pdf","comment":"8 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2201.10859v2","updated":"2023-11-14T13:23:25Z","published":"2022-01-26T10:40:55Z","title":"Visualizing the Diversity of Representations Learned by Bayesian Neural\n  Networks","summary":"  Explainable Artificial Intelligence (XAI) aims to make learning machines less\nopaque, and offers researchers and practitioners various tools to reveal the\ndecision-making strategies of neural networks. In this work, we investigate how\nXAI methods can be used for exploring and visualizing the diversity of feature\nrepresentations learned by Bayesian Neural Networks (BNNs). Our goal is to\nprovide a global understanding of BNNs by making their decision-making\nstrategies a) visible and tangible through feature visualizations and b)\nquantitatively measurable with a distance measure learned by contrastive\nlearning. Our work provides new insights into the \\emph{posterior} distribution\nin terms of human-understandable feature information with regard to the\nunderlying decision making strategies. The main findings of our work are the\nfollowing: 1) global XAI methods can be applied to explain the diversity of\ndecision-making strategies of BNN instances, 2) Monte Carlo dropout with\ncommonly used Dropout rates exhibit increased diversity in feature\nrepresentations compared to the multimodal posterior approximation of\nMultiSWAG, 3) the diversity of learned feature representations highly\ncorrelates with the uncertainty estimate for the output and 4) the inter-mode\ndiversity of the multimodal posterior decreases as the network width increases,\nwhile the intra mode diversity increases. These findings are consistent with\nthe recent Deep Neural Networks theory, providing additional intuitions about\nwhat the theory implies in terms of humanly understandable concepts.\n","authors":["Dennis Grinwald","Kirill Bykov","Shinichi Nakajima","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2201.10859v2.pdf","comment":"16 pages, 18 figures"},{"id":"http://arxiv.org/abs/2311.08141v1","updated":"2023-11-14T13:12:47Z","published":"2023-11-14T13:12:47Z","title":"GMTR: Graph Matching Transformers","summary":"  Vision transformers (ViTs) have recently been used for visual matching beyond\nobject detection and segmentation. However, the original grid dividing strategy\nof ViTs neglects the spatial information of the keypoints, limiting the\nsensitivity to local information. Therefore, we propose \\textbf{QueryTrans}\n(Query Transformer), which adopts a cross-attention module and keypoints-based\ncenter crop strategy for better spatial information extraction. We further\nintegrate the graph attention module and devise a transformer-based graph\nmatching approach \\textbf{GMTR} (Graph Matching TRansformers) whereby the\ncombinatorial nature of GM is addressed by a graph transformer neural GM\nsolver. On standard GM benchmarks, GMTR shows competitive performance against\nthe SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves\n$\\mathbf{83.6\\%}$ accuracy, $\\mathbf{0.9\\%}$ higher than the SOTA framework. On\nSpair-71k, GMTR shows great potential and outperforms most of the previous\nworks. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from\n$80.1\\%$ to $\\mathbf{83.3\\%}$, and BBGM from $79.0\\%$ to $\\mathbf{84.5\\%}$. On\nSpair-71k, QueryTrans improves NGMv2 from $80.6\\%$ to $\\mathbf{82.5\\%}$, and\nBBGM from $82.1\\%$ to $\\mathbf{83.9\\%}$. Source code will be made publicly\navailable.\n","authors":["Jinpei Guo","Shaofeng Zhang","Runzhong Wang","Chang Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.08141v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.08129v1","updated":"2023-11-14T12:48:17Z","published":"2023-11-14T12:48:17Z","title":"Learning based Deep Disentangling Light Field Reconstruction and\n  Disparity Estimation Application","summary":"  Light field cameras have a wide range of uses due to their ability to\nsimultaneously record light intensity and direction. The angular resolution of\nlight fields is important for downstream tasks such as depth estimation, yet is\noften difficult to improve due to hardware limitations. Conventional methods\ntend to perform poorly against the challenge of large disparity in sparse light\nfields, while general CNNs have difficulty extracting spatial and angular\nfeatures coupled together in 4D light fields. The light field disentangling\nmechanism transforms the 4D light field into 2D image format, which is more\nfavorable for CNN for feature extraction. In this paper, we propose a Deep\nDisentangling Mechanism, which inherits the principle of the light field\ndisentangling mechanism and further develops the design of the feature\nextractor and adds advanced network structure. We design a light-field\nreconstruction network (i.e., DDASR) on the basis of the Deep Disentangling\nMechanism, and achieve SOTA performance in the experiments. In addition, we\ndesign a Block Traversal Angular Super-Resolution Strategy for the practical\napplication of depth estimation enhancement where the input views is often\nhigher than 2x2 in the experiments resulting in a high memory usage, which can\nreduce the memory usage while having a better reconstruction performance.\n","authors":["Langqing Shi","Ping Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19522v2","updated":"2023-11-14T12:21:41Z","published":"2023-10-30T13:21:56Z","title":"Are Natural Domain Foundation Models Useful for Medical Image\n  Classification?","summary":"  The deep learning field is converging towards the use of general foundation\nmodels that can be easily adapted for diverse tasks. While this paradigm shift\nhas become common practice within the field of natural language processing,\nprogress has been slower in computer vision. In this paper we attempt to\naddress this issue by investigating the transferability of various\nstate-of-the-art foundation models to medical image classification tasks.\nSpecifically, we evaluate the performance of five foundation models, namely\nSAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical\nimaging datasets. We explore different training settings to fully harness the\npotential of these models. Our study shows mixed results. DINOv2 consistently\noutperforms the standard practice of ImageNet pretraining. However, other\nfoundation models failed to consistently beat this established baseline\nindicating limitations in their transferability to medical image classification\ntasks.\n","authors":["Joana Palés Huix","Adithya Raju Ganeshan","Johan Fredin Haslum","Magnus Söderberg","Christos Matsoukas","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2310.19522v2.pdf","comment":"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV\n  2024)"},{"id":"http://arxiv.org/abs/2308.10557v2","updated":"2023-11-14T12:20:02Z","published":"2023-08-21T08:17:42Z","title":"Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition","summary":"  Hand action recognition is essential. Communication, human-robot\ninteractions, and gesture control are dependent on it. Skeleton-based action\nrecognition traditionally includes hands, which belong to the classes which\nremain challenging to correctly recognize to date. We propose a method\nspecifically designed for hand action recognition which uses relative angular\nembeddings and local Spherical Harmonics to create novel hand representations.\nThe use of Spherical Harmonics creates rotation-invariant representations which\nmake hand action recognition even more robust against inter-subject differences\nand viewpoint changes. We conduct extensive experiments on the hand joints in\nthe First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose\nAnnotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit of\nusing Local Spherical Harmonics Representations. Our code is available at\nhttps://github.com/KathPra/LSHR_LSHT.\n","authors":["Katharina Prasse","Steffen Jung","Yuxuan Zhou","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2308.10557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08110v1","updated":"2023-11-14T12:14:54Z","published":"2023-11-14T12:14:54Z","title":"Improving hateful memes detection via learning hatefulness-aware\n  embedding space through retrieval-guided contrastive learning","summary":"  Hateful memes have emerged as a significant concern on the Internet. These\nmemes, which are a combination of image and text, often convey messages vastly\ndifferent from their individual meanings. Thus, detecting hateful memes\nrequires the system to jointly understand the visual and textual modalities.\nHowever, our investigation reveals that the embedding space of existing\nCLIP-based systems lacks sensitivity to subtle differences in memes that are\nvital for correct hatefulness classification. To address this issue, we propose\nconstructing a hatefulness-aware embedding space through retrieval-guided\ncontrastive training. Specifically, we add an auxiliary loss that utilizes hard\nnegative and pseudo-gold samples to train the embedding space. Our approach\nachieves state-of-the-art performance on the HatefulMemes dataset with an AUROC\nof 86.7. Notably, our approach outperforms much larger fine-tuned Large\nMultimodal Models like Flamingo and LLaVA. Finally, we demonstrate a\nretrieval-based hateful memes detection system, which is capable of making\nhatefulness classification based on data unseen in training from a database.\nThis allows developers to update the hateful memes detection system by simply\nadding new data without retraining, a desirable feature for real services in\nthe constantly-evolving landscape of hateful memes on the Internet.\n","authors":["Jingbiao Mei","Jinghong Chen","Weizhe Lin","Bill Byrne","Marcus Tomalin"],"pdf_url":"https://arxiv.org/pdf/2311.08110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12334v2","updated":"2023-11-14T12:04:24Z","published":"2023-10-18T21:20:44Z","title":"Improving Representation Learning for Histopathologic Images with\n  Cluster Constraints","summary":"  Recent advances in whole-slide image (WSI) scanners and computational\ncapabilities have significantly propelled the application of artificial\nintelligence in histopathology slide analysis. While these strides are\npromising, current supervised learning approaches for WSI analysis come with\nthe challenge of exhaustively labeling high-resolution slides - a process that\nis both labor-intensive and time-consuming. In contrast, self-supervised\nlearning (SSL) pretraining strategies are emerging as a viable alternative,\ngiven that they don't rely on explicit data annotations. These SSL strategies\nare quickly bridging the performance disparity with their supervised\ncounterparts. In this context, we introduce an SSL framework. This framework\naims for transferable representation learning and semantically meaningful\nclustering by synergizing invariance loss and clustering loss in WSI analysis.\nNotably, our approach outperforms common SSL methods in downstream\nclassification and clustering tasks, as evidenced by tests on the Camelyon16\nand a pancreatic cancer dataset.\n","authors":["Weiyi Wu","Chongyang Gao","Joseph DiPalma","Soroush Vosoughi","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2310.12334v2.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2311.08100v1","updated":"2023-11-14T11:53:24Z","published":"2023-11-14T11:53:24Z","title":"DeepEMplanner: An EM Motion Planner with Iterative Interactions","summary":"  Motion planning is a computational problem that finds a sequence of valid\ntrajectories, often based on surrounding agents' forecasting, environmental\nunderstanding, and historical and future contexts. It can also be viewed as a\ngame in which agents continuously plan their next move according to other\nagents' intentions and the encountering environment, further achieving their\nultimate goals through incremental actions. To model the dynamic planning and\ninteraction process, we propose a novel framework, DeepEMplanner, which takes\nthe stepwise interaction into account for fine-grained behavior learning. The\nego vehicle maximizes each step motion to reach its eventual driving outcome\nbased on the stepwise expectation from agents and its upcoming road conditions.\nOn the other hand, the agents also follow the same philosophy to maximize their\nstepwise behavior under the encountering environment and the expectations from\nego and other agents. Our DeepEMplanner models the interactions among ego,\nagents, and the dynamic environment in an autoregressive manner by interleaving\nthe Expectation and Maximization processes. Further, we design ego-to-agents,\nego-to-map, and ego-to-BEV interaction mechanisms with hierarchical dynamic key\nobjects attention to better model the interactions. Experiments on the nuScenes\nbenchmark show that our approach achieves state-of-the-art results.\n","authors":["Zhili Chen","Maosheng Ye","Shuangjie Xu","Tongyi Cao","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08094v1","updated":"2023-11-14T11:38:38Z","published":"2023-11-14T11:38:38Z","title":"Act-VIT: A Representationally Robust Attention Architecture for Skeleton\n  Based Action Recognition Using Vision Transformer","summary":"  Skeleton-based action recognition receives the attention of many researchers\nas it is robust to viewpoint and illumination changes, and its processing is\nmuch more efficient than video frames. With the emergence of deep learning\nmodels, it has become very popular to represent the skeleton data in\npseudo-image form and apply Convolutional Neural Networks for action\nrecognition. Thereafter, studies concentrated on finding effective methods for\nforming pseudo-images. Recently, attention networks, more specifically\ntransformers have provided promising results in various vision problems. In\nthis study, the effectiveness of vision transformers for skeleton-based action\nrecognition is examined and its robustness on the pseudo-image representation\nscheme is investigated. To this end, a three-level architecture, Act-VIT is\nproposed, which forms a set of pseudo images apply a classifier on each of the\nrepresentation and combine their results to find the final action class. The\nclassifiers of Act-VIT are first realized by CNNs and then by VITs and their\nperformances are compared. Experimental studies reveal that the vision\ntransformer is less sensitive to the initial pseudo-image representation\ncompared to CNN. Nevertheless, even with the vision transformer, the\nrecognition performance can be further improved by consensus of classifiers.\n","authors":["Ozge Oztimur Karadag"],"pdf_url":"https://arxiv.org/pdf/2311.08094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12570v2","updated":"2023-11-14T11:32:53Z","published":"2023-10-19T08:25:03Z","title":"DA-TransUNet: Integrating Spatial and Channel Dual Attention with\n  Transformer U-Net for Medical Image Segmentation","summary":"  Accurate medical image segmentation is critical for disease quantification\nand treatment evaluation. While traditional Unet architectures and their\ntransformer-integrated variants excel in automated segmentation tasks. However,\nthey lack the ability to harness the intrinsic position and channel features of\nimage. Existing models also struggle with parameter efficiency and\ncomputational complexity, often due to the extensive use of Transformers. To\naddress these issues, this study proposes a novel deep medical image\nsegmentation framework, called DA-TransUNet, aiming to integrate the\nTransformer and dual attention block(DA-Block) into the traditional U-shaped\narchitecture. Unlike earlier transformer-based U-net models, DA-TransUNet\nutilizes Transformers and DA-Block to integrate not only global and local\nfeatures, but also image-specific positional and channel features, improving\nthe performance of medical image segmentation. By incorporating a DA-Block at\nthe embedding layer and within each skip connection layer, we substantially\nenhance feature extraction capabilities and improve the efficiency of the\nencoder-decoder structure. DA-TransUNet demonstrates superior performance in\nmedical image segmentation tasks, consistently outperforming state-of-the-art\ntechniques across multiple datasets. In summary, DA-TransUNet offers a\nsignificant advancement in medical image segmentation, providing an effective\nand powerful alternative to existing techniques. Our architecture stands out\nfor its ability to improve segmentation accuracy, thereby advancing the field\nof automated medical image diagnostics. The codes and parameters of our model\nwill be publicly available at https://github.com/SUN-1024/DA-TransUnet.\n","authors":["Guanqun Sun","Yizhi Pan","Weikun Kong","Zichang Xu","Jianhua Ma","Teeradaj Racharak","Le-Minh Nguyen","Junyi Xin"],"pdf_url":"https://arxiv.org/pdf/2310.12570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08253v3","updated":"2023-11-14T11:16:59Z","published":"2022-11-15T15:59:43Z","title":"HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization","summary":"  Due to domain shifts, machine learning systems typically struggle to\ngeneralize well to new domains that differ from those of training data, which\nis what domain generalization (DG) aims to address. Although a variety of DG\nmethods have been proposed, most of them fall short in interpretability and\nrequire domain labels, which are not available in many real-world scenarios.\nThis paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture\nof Experts (MoE), which does not rely on domain labels and is more\ninterpretable. MoE proves effective in identifying heterogeneous patterns in\ndata. For the DG problem, heterogeneity arises exactly from domain shifts. HMOE\nemploys hypernetworks taking vectors as input to generate the weights of\nexperts, which promotes knowledge sharing among experts and enables the\nexploration of their similarities in a low-dimensional vector space. We\nbenchmark HMOE against other DG methods under a fair evaluation framework --\nDomainBed. Our extensive experiments show that HMOE can effectively separate\nmixed-domain data into distinct clusters that are surprisingly more consistent\nwith human intuition than original domain labels. Using self-learned domain\ninformation, HMOE achieves state-of-the-art results on most datasets and\nsignificantly surpasses other DG methods in average accuracy across all\ndatasets.\n","authors":["Jingang Qu","Thibault Faney","Ze Wang","Patrick Gallinari","Soleiman Yousef","Jean-Charles de Hemptinne"],"pdf_url":"https://arxiv.org/pdf/2211.08253v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08083v1","updated":"2023-11-14T11:10:46Z","published":"2023-11-14T11:10:46Z","title":"Solving ARC visual analogies with neural embeddings and vector\n  arithmetic: A generalized method","summary":"  Analogical reasoning derives information from known relations and generalizes\nthis information to similar yet unfamiliar situations. One of the first\ngeneralized ways in which deep learning models were able to solve verbal\nanalogies was through vector arithmetic of word embeddings, essentially\nrelating words that were mapped to a vector space (e.g., king - man + woman =\n__?). In comparison, most attempts to solve visual analogies are still\npredominantly task-specific and less generalizable. This project focuses on\nvisual analogical reasoning and applies the initial generalized mechanism used\nto solve verbal analogies to the visual realm. Taking the Abstraction and\nReasoning Corpus (ARC) as an example to investigate visual analogy solving, we\nuse a variational autoencoder (VAE) to transform ARC items into low-dimensional\nlatent vectors, analogous to the word embeddings used in the verbal approaches.\nThrough simple vector arithmetic, underlying rules of ARC items are discovered\nand used to solve them. Results indicate that the approach works well on simple\nitems with fewer dimensions (i.e., few colors used, uniform shapes), similar\ninput-to-output examples, and high reconstruction accuracy on the VAE.\nPredictions on more complex items showed stronger deviations from expected\noutputs, although, predictions still often approximated parts of the item's\nrule set. Error patterns indicated that the model works as intended. On the\nofficial ARC paradigm, the model achieved a score of 2% (cf. current world\nrecord is 21%) and on ConceptARC it scored 8.8%. Although the methodology\nproposed involves basic dimensionality reduction techniques and standard vector\narithmetic, this approach demonstrates promising outcomes on ARC and can easily\nbe generalized to other abstract visual reasoning tasks.\n","authors":["Luca H. Thoms","Karel A. Veldkamp","Hannes Rosenbusch","Claire E. Stevenson"],"pdf_url":"https://arxiv.org/pdf/2311.08083v1.pdf","comment":"Data and code can be found on\n  https://github.com/foger3/ARC_DeepLearning"},{"id":"http://arxiv.org/abs/2311.08080v1","updated":"2023-11-14T11:08:34Z","published":"2023-11-14T11:08:34Z","title":"Identifying Light-curve Signals with a Deep Learning Based Object\n  Detection Algorithm. II. A General Light Curve Classification Framework","summary":"  Vast amounts of astronomical photometric data are generated from various\nprojects, requiring significant efforts to identify variable stars and other\nobject classes. In light of this, a general, widely applicable classification\nframework would simplify the task of designing custom classifiers. We present a\nnovel deep learning framework for classifying light curves using a weakly\nsupervised object detection model. Our framework identifies the optimal windows\nfor both light curves and power spectra automatically, and zooms in on their\ncorresponding data. This allows for automatic feature extraction from both time\nand frequency domains, enabling our model to handle data across different\nscales and sampling intervals. We train our model on datasets obtained from\nboth space-based and ground-based multi-band observations of variable stars and\ntransients. We achieve an accuracy of 87% for combined variables and transient\nevents, which is comparable to the performance of previous feature-based\nmodels. Our trained model can be utilized directly to other missions, such as\nASAS-SN, without requiring any retraining or fine-tuning. To address known\nissues with miscalibrated predictive probabilities, we apply conformal\nprediction to generate robust predictive sets that guarantee true label\ncoverage with a given probability. Additionally, we incorporate various anomaly\ndetection algorithms to empower our model with the ability to identify\nout-of-distribution objects. Our framework is implemented in the Deep-LC\ntoolkit, which is an open-source Python package hosted on Github and PyPI.\n","authors":["Kaiming Cui","D. J. Armstrong","Fabo Feng"],"pdf_url":"https://arxiv.org/pdf/2311.08080v1.pdf","comment":"26 pages, 19 figures, 6 tables. Submitted to AAS Journal. Code is\n  available on https://github.com/ckm3/Deep-LC"},{"id":"http://arxiv.org/abs/2311.08077v1","updated":"2023-11-14T11:05:08Z","published":"2023-11-14T11:05:08Z","title":"Zero-Shot Segmentation of Eye Features Using the Segment Anything Model\n  (SAM)","summary":"  The advent of foundation models signals a new era in artificial intelligence.\nThe Segment Anything Model (SAM) is the first foundation model for image\nsegmentation. In this study, we evaluate SAM's ability to segment features from\neye images recorded in virtual reality setups. The increasing requirement for\nannotated eye-image datasets presents a significant opportunity for SAM to\nredefine the landscape of data annotation in gaze estimation. Our investigation\ncenters on SAM's zero-shot learning abilities and the effectiveness of prompts\nlike bounding boxes or point clicks. Our results are consistent with studies in\nother domains, demonstrating that SAM's segmentation effectiveness can be\non-par with specialized models depending on the feature, with prompts improving\nits performance, evidenced by an IoU of 93.34% for pupil segmentation in one\ndataset. Foundation models like SAM could revolutionize gaze estimation by\nenabling quick and easy image segmentation, reducing reliance on specialized\nmodels and extensive manual annotation.\n","authors":["Virmarie Maquiling","Sean Anthony Byrne","Diederick C. Niehorster","Marcus Nyström","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2311.08077v1.pdf","comment":"14 pages, 8 figures, 1 table, submitted to ETRA 2024: ACM Symposium\n  on Eye Tracking Research & Applications"},{"id":"http://arxiv.org/abs/2311.08075v1","updated":"2023-11-14T10:59:45Z","published":"2023-11-14T10:59:45Z","title":"GlanceSeg: Real-time microaneurysm lesion segmentation with\n  gaze-map-guided foundation model for early detection of diabetic retinopathy","summary":"  Early-stage diabetic retinopathy (DR) presents challenges in clinical\ndiagnosis due to inconspicuous and minute microangioma lesions, resulting in\nlimited research in this area. Additionally, the potential of emerging\nfoundation models, such as the segment anything model (SAM), in medical\nscenarios remains rarely explored. In this work, we propose a\nhuman-in-the-loop, label-free early DR diagnosis framework called GlanceSeg,\nbased on SAM. GlanceSeg enables real-time segmentation of microangioma lesions\nas ophthalmologists review fundus images. Our human-in-the-loop framework\nintegrates the ophthalmologist's gaze map, allowing for rough localization of\nminute lesions in fundus images. Subsequently, a saliency map is generated\nbased on the located region of interest, which provides prompt points to assist\nthe foundation model in efficiently segmenting microangioma lesions. Finally, a\ndomain knowledge filter refines the segmentation of minute lesions. We\nconducted experiments on two newly-built public datasets, i.e., IDRiD and\nRetinal-Lesions, and validated the feasibility and superiority of GlanceSeg\nthrough visualized illustrations and quantitative measures. Additionally, we\ndemonstrated that GlanceSeg improves annotation efficiency for clinicians and\nenhances segmentation performance through fine-tuning using annotations. This\nstudy highlights the potential of GlanceSeg-based annotations for self-model\noptimization, leading to enduring performance advancements through continual\nlearning.\n","authors":["Hongyang Jiang","Mengdi Gao","Zirong Liu","Chen Tang","Xiaoqing Zhang","Shuai Jiang","Wu Yuan","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.08075v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2309.00305v2","updated":"2023-11-14T10:44:15Z","published":"2023-09-01T07:29:44Z","title":"Efficient Surrogate Models for Materials Science Simulations: Machine\n  Learning-based Prediction of Microstructure Properties","summary":"  Determining, understanding, and predicting the so-called structure-property\nrelation is an important task in many scientific disciplines, such as\nchemistry, biology, meteorology, physics, engineering, and materials science.\nStructure refers to the spatial distribution of, e.g., substances, material, or\nmatter in general, while property is a resulting characteristic that usually\ndepends in a non-trivial way on spatial details of the structure.\nTraditionally, forward simulations models have been used for such tasks.\nRecently, several machine learning algorithms have been applied in these\nscientific fields to enhance and accelerate simulation models or as surrogate\nmodels. In this work, we develop and investigate the applications of six\nmachine learning techniques based on two different datasets from the domain of\nmaterials science: data from a two-dimensional Ising model for predicting the\nformation of magnetic domains and data representing the evolution of dual-phase\nmicrostructures from the Cahn-Hilliard model. We analyze the accuracy and\nrobustness of all models and elucidate the reasons for the differences in their\nperformances. The impact of including domain knowledge through tailored\nfeatures is studied, and general recommendations based on the availability and\nquality of training data are derived from this.\n","authors":["Binh Duong Nguyen","Pavlo Potapenko","Aytekin Dermici","Kishan Govind","Sébastien Bompas","Stefan Sandfeld"],"pdf_url":"https://arxiv.org/pdf/2309.00305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08059v1","updated":"2023-11-14T10:32:17Z","published":"2023-11-14T10:32:17Z","title":"FS-Net: Full Scale Network and Adaptive Threshold for Improving\n  Extraction of Micro-Retinal Vessel Structures","summary":"  Retinal vascular segmentation, is a widely researched subject in biomedical\nimage processing, aims to relieve ophthalmologists' workload when treating and\ndetecting retinal disorders. However, segmenting retinal vessels has its own\nset of challenges, with prior techniques failing to generate adequate results\nwhen segmenting branches and microvascular structures. The neural network\napproaches used recently are characterized by the inability to keep local and\nglobal properties together and the failure to capture tiny end vessels make it\nchallenging to attain the desired result. To reduce this retinal vessel\nsegmentation problem, we propose a full-scale micro-vessel extraction mechanism\nbased on an encoder-decoder neural network architecture, sigmoid smoothing, and\nan adaptive threshold method. The network consists of of residual, encoder\nbooster, bottleneck enhancement, squeeze, and excitation building blocks. All\nof these blocks together help to improve the feature extraction and prediction\nof the segmentation map. The proposed solution has been evaluated using the\nDRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when\ncompared with previous studies. The AUC and accuracy on the DRIVE dataset are\n0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are\n0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916\nand 0.9750, respectively. The performance achieved is one step ahead of what\nhas been done in previous studies, and this results in a higher chance of\nhaving this solution in real-life diagnostic centers that seek ophthalmologists\nattention.\n","authors":["Melaku N. Getahun","Oleg Y. Rogov","Dmitry V. Dylov","Andrey Somov","Ahmed Bouridane","Rifat Hamoudi"],"pdf_url":"https://arxiv.org/pdf/2311.08059v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.08046v1","updated":"2023-11-14T10:11:36Z","published":"2023-11-14T10:11:36Z","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models\n  with Image and Video Understanding","summary":"  Large language models have demonstrated impressive universal capabilities\nacross a wide range of open-ended tasks and have extended their utility to\nencompass multimodal conversations. However, existing methods encounter\nchallenges in effectively handling both image and video understanding,\nparticularly with limited visual tokens. In this work, we introduce Chat-UniVi,\na unified vision-language model capable of comprehending and engaging in\nconversations involving images and videos through a unified visual\nrepresentation. Specifically, we employ a set of dynamic visual tokens to\nuniformly represent images and videos. This representation framework empowers\nthe model to efficiently utilize a limited number of visual tokens to\nsimultaneously capture the spatial details necessary for images and the\ncomprehensive temporal relationship required for videos. Moreover, we leverage\na multi-scale representation, enabling the model to perceive both high-level\nsemantic concepts and low-level visual details. Notably, Chat-UniVi is trained\non a mixed dataset containing both images and videos, allowing direct\napplication to tasks involving both mediums without requiring any\nmodifications. Extensive experimental results demonstrate that Chat-UniVi, as a\nunified model, consistently outperforms even existing methods exclusively\ndesigned for either images or videos.\n","authors":["Peng Jin","Ryuichi Takanobu","Caiwan Zhang","Xiaochun Cao","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2311.08046v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2311.08043v1","updated":"2023-11-14T10:07:52Z","published":"2023-11-14T10:07:52Z","title":"Contrastive Learning for Multi-Object Tracking with Transformers","summary":"  The DEtection TRansformer (DETR) opened new possibilities for object\ndetection by modeling it as a translation task: converting image features into\nobject-level representations. Previous works typically add expensive modules to\nDETR to perform Multi-Object Tracking (MOT), resulting in more complicated\narchitectures. We instead show how DETR can be turned into a MOT model by\nemploying an instance-level contrastive loss, a revised sampling strategy and a\nlightweight assignment method. Our training scheme learns object appearances\nwhile preserving detection capabilities and with little overhead. Its\nperformance surpasses the previous state-of-the-art by +2.6 mMOTA on the\nchallenging BDD100K dataset and is comparable to existing transformer-based\nmethods on the MOT17 dataset.\n","authors":["Pierre-François De Plaen","Nicola Marinello","Marc Proesmans","Tinne Tuytelaars","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2311.08043v1.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2305.02143v2","updated":"2023-11-14T10:02:00Z","published":"2023-05-03T14:22:48Z","title":"GANonymization: A GAN-based Face Anonymization Framework for Preserving\n  Emotional Expressions","summary":"  In recent years, the increasing availability of personal data has raised\nconcerns regarding privacy and security. One of the critical processes to\naddress these concerns is data anonymization, which aims to protect individual\nprivacy and prevent the release of sensitive information. This research focuses\non the importance of face anonymization. Therefore, we introduce\nGANonymization, a novel face anonymization framework with facial\nexpression-preserving abilities. Our approach is based on a high-level\nrepresentation of a face, which is synthesized into an anonymized version based\non a generative adversarial network (GAN). The effectiveness of the approach\nwas assessed by evaluating its performance in removing identifiable facial\nattributes to increase the anonymity of the given individual face.\nAdditionally, the performance of preserving facial expressions was evaluated on\nseveral affect recognition datasets and outperformed the state-of-the-art\nmethods in most categories. Finally, our approach was analyzed for its ability\nto remove various facial traits, such as jewelry, hair color, and multiple\nothers. Here, it demonstrated reliable performance in removing these\nattributes. Our results suggest that GANonymization is a promising approach for\nanonymizing faces while preserving facial expressions.\n","authors":["Fabio Hellmann","Silvan Mertes","Mohamed Benouis","Alexander Hustinx","Tzung-Chien Hsieh","Cristina Conati","Peter Krawitz","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2305.02143v2.pdf","comment":"26 pages, 11 figures, 6 tables, ACM Transactions on Multimedia\n  Computing, Communications, and Applications"},{"id":"http://arxiv.org/abs/2311.08032v1","updated":"2023-11-14T09:51:00Z","published":"2023-11-14T09:51:00Z","title":"ELF: An End-to-end Local and Global Multimodal Fusion Framework for\n  Glaucoma Grading","summary":"  Glaucoma is a chronic neurodegenerative condition that can lead to blindness.\nEarly detection and curing are very important in stopping the disease from\ngetting worse for glaucoma patients. The 2D fundus images and optical coherence\ntomography(OCT) are useful for ophthalmologists in diagnosing glaucoma. There\nare many methods based on the fundus images or 3D OCT volumes; however, the\nmining for multi-modality, including both fundus images and data, is less\nstudied. In this work, we propose an end-to-end local and global multi-modal\nfusion framework for glaucoma grading, named ELF for short. ELF can fully\nutilize the complementary information between fundus and OCT. In addition,\nunlike previous methods that concatenate the multi-modal features together,\nwhich lack exploring the mutual information between different modalities, ELF\ncan take advantage of local-wise and global-wise mutual information. The\nextensive experiment conducted on the multi-modal glaucoma grading GAMMA\ndataset can prove the effiectness of ELF when compared with other\nstate-of-the-art methods.\n","authors":["Wenyun Li","Chi-Man Pun"],"pdf_url":"https://arxiv.org/pdf/2311.08032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02877v4","updated":"2023-11-14T09:49:38Z","published":"2023-11-06T05:14:24Z","title":"Inner-IoU: More Effective Intersection over Union Loss with Auxiliary\n  Bounding Box","summary":"  With the rapid development of detectors, Bounding Box Regression (BBR) loss\nfunction has constantly updated and optimized. However, the existing IoU-based\nBBR still focus on accelerating convergence by adding new loss terms, ignoring\nthe limitations of IoU loss term itself. Although theoretically IoU loss can\neffectively describe the state of bounding box regression,in practical\napplications, it cannot adjust itself according to different detectors and\ndetection tasks, and does not have strong generalization. Based on the above,\nwe first analyzed the BBR model and concluded that distinguishing different\nregression samples and using different scales of auxiliary bounding boxes to\ncalculate losses can effectively accelerate the bounding box regression\nprocess. For high IoU samples, using smaller auxiliary bounding boxes to\ncalculate losses can accelerate convergence, while larger auxiliary bounding\nboxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which\ncalculates IoU loss through auxiliary bounding boxes. For different datasets\nand detectors, we introduce a scaling factor ratio to control the scale size of\nthe auxiliary bounding boxes for calculating losses. Finally, integrate\nInner-IoU into the existing IoU-based loss functions for simulation and\ncomparative experiments. The experiment result demonstrate a further\nenhancement in detection performance with the utilization of the method\nproposed in this paper, verifying the effectiveness and generalization ability\nof Inner-IoU loss. Code is available at\nhttps://github.com/malagoutou/Inner-IoU.\n","authors":["Hao Zhang","Cong Xu","Shuaijie Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.02877v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04156v3","updated":"2023-11-14T09:43:45Z","published":"2023-08-08T09:37:18Z","title":"Towards Top-Down Stereo Image Quality Assessment via Stereo Attention","summary":"  Stereo image quality assessment (SIQA) plays a crucial role in evaluating and\nimproving the visual experience of 3D content. Existing visual properties-based\nmethods for SIQA have achieved promising performance. However, these approaches\nignore the top-down philosophy, leading to a lack of a comprehensive grasp of\nthe human visual system (HVS) and SIQA. This paper presents a novel Stereo\nAttenTion Network (SATNet), which employs a top-down perspective to guide the\nquality assessment process. Specifically, our generalized Stereo AttenTion\n(SAT) structure adapts components and input/output for stereo scenarios. It\nleverages the fusion-generated attention map as a higher-level binocular\nmodulator to influence two lower-level monocular features, allowing progressive\nrecalibration of both throughout the pipeline. Additionally, we introduce an\nEnergy Coefficient (EC) to flexibly tune the magnitude of binocular response,\naccounting for the fact that binocular responses in the primate primary visual\ncortex are less than the sum of monocular responses. To extract the most\ndiscriminative quality information from the summation and subtraction of the\ntwo branches of monocular features, we utilize a dual-pooling strategy that\napplies min-pooling and max-pooling operations to the respective branches.\nExperimental results highlight the superiority of our top-down method in\nadvancing the state-of-the-art in the SIQA field. The code is available at\nhttps://github.com/Fanning-Zhang/SATNet.\n","authors":["Huilin Zhang","Sumei Li","Haoxiang Chang","Peiming Lin"],"pdf_url":"https://arxiv.org/pdf/2308.04156v3.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.08024v1","updated":"2023-11-14T09:33:33Z","published":"2023-11-14T09:33:33Z","title":"MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with\n  Semi Supervised Learning for Low Dose CT","summary":"  Image quality assessment (IQA) plays a critical role in optimizing radiation\ndose and developing novel medical imaging techniques in computed tomography\n(CT). Traditional IQA methods relying on hand-crafted features have limitations\nin summarizing the subjective perceptual experience of image quality. Recent\ndeep learning-based approaches have demonstrated strong modeling capabilities\nand potential for medical IQA, but challenges remain regarding model\ngeneralization and perceptual accuracy. In this work, we propose a multi-scale\ndistributions regression approach to predict quality scores by constraining the\noutput distribution, thereby improving model generalization. Furthermore, we\ndesign a dual-branch alignment network to enhance feature extraction\ncapabilities. Additionally, semi-supervised learning is introduced by utilizing\npseudo-labels for unlabeled data to guide model training. Extensive qualitative\nexperiments demonstrate the effectiveness of our proposed method for advancing\nthe state-of-the-art in deep learning-based medical IQA. Code is available at:\nhttps://github.com/zunzhumu/MD-IQA.\n","authors":["Tao Song","Ruizhi Hou","Lisong Dai","Lei Xiang"],"pdf_url":"https://arxiv.org/pdf/2311.08024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08013v1","updated":"2023-11-14T09:17:15Z","published":"2023-11-14T09:17:15Z","title":"CP-SLAM: Collaborative Neural Point-based SLAM System","summary":"  This paper presents a collaborative implicit neural simultaneous localization\nand mapping (SLAM) system with RGB-D image sequences, which consists of\ncomplete front-end and back-end modules including odometry, loop detection,\nsub-map fusion, and global refinement. In order to enable all these modules in\na unified framework, we propose a novel neural point based 3D scene\nrepresentation in which each point maintains a learnable neural feature for\nscene encoding and is associated with a certain keyframe. Moreover, a\ndistributed-to-centralized learning strategy is proposed for the collaborative\nimplicit SLAM to improve consistency and cooperation. A novel global\noptimization framework is also proposed to improve the system accuracy like\ntraditional bundle adjustment. Experiments on various datasets demonstrate the\nsuperiority of the proposed method in both camera tracking and mapping.\n","authors":["Jiarui Hu","Mao Mao","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2311.08013v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2307.12540v2","updated":"2023-11-14T09:10:14Z","published":"2023-07-24T06:04:12Z","title":"UniFormaly: Towards Task-Agnostic Unified Framework for Visual Anomaly\n  Detection","summary":"  Visual anomaly detection aims to learn normality from normal images, but\nexisting approaches are fragmented across various tasks: defect detection,\nsemantic anomaly detection, multi-class anomaly detection, and anomaly\nclustering. This one-task-one-model approach is resource-intensive and incurs\nhigh maintenance costs as the number of tasks increases. We present UniFormaly,\na universal and powerful anomaly detection framework. We emphasize the\nnecessity of our off-the-shelf approach by pointing out a suboptimal issue in\nonline encoder-based methods. We introduce Back Patch Masking (BPM) and top\nk-ratio feature matching to achieve unified anomaly detection. BPM eliminates\nirrelevant background regions using a self-attention map from self-supervised\nViTs. This operates in a task-agnostic manner and alleviates memory storage\nconsumption, scaling to tasks with large-scale datasets. Top k-ratio feature\nmatching unifies anomaly levels and tasks by casting anomaly scoring into\nmultiple instance learning. Finally, UniFormaly achieves outstanding results on\nvarious tasks and datasets. Codes are available at\nhttps://github.com/YoojLee/Uniformaly.\n","authors":["Yujin Lee","Harin Lim","Seoyoon Jang","Hyunsoo Yoon"],"pdf_url":"https://arxiv.org/pdf/2307.12540v2.pdf","comment":"23 pages, 13 figures. Codes are available at\n  https://github.com/YoojLee/Uniformaly"},{"id":"http://arxiv.org/abs/2311.08007v1","updated":"2023-11-14T09:08:30Z","published":"2023-11-14T09:08:30Z","title":"Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame\n  Interpolation","summary":"  Existing video frame interpolation (VFI) methods blindly predict where each\nobject is at a specific timestep t (\"time indexing\"), which struggles to\npredict precise object movements. Given two images of a baseball, there are\ninfinitely many possible trajectories: accelerating or decelerating, straight\nor curved. This often results in blurry frames as the method averages out these\npossibilities. Instead of forcing the network to learn this complicated\ntime-to-location mapping implicitly together with predicting the frames, we\nprovide the network with an explicit hint on how far the object has traveled\nbetween start and end frames, a novel approach termed \"distance indexing\". This\nmethod offers a clearer learning goal for models, reducing the uncertainty tied\nto object speeds. We further observed that, even with this extra guidance,\nobjects can still be blurry especially when they are equally far from both\ninput frames (i.e., halfway in-between), due to the directional ambiguity in\nlong-range motion. To solve this, we propose an iterative reference-based\nestimation strategy that breaks down a long-range prediction into several\nshort-range steps. When integrating our plug-and-play strategies into\nstate-of-the-art learning-based models, they exhibit markedly sharper outputs\nand superior perceptual quality in arbitrary time interpolations, using a\nuniform distance indexing map in the same format as time indexing.\nAdditionally, distance indexing can be specified pixel-wise, which enables\ntemporal manipulation of each object independently, offering a novel tool for\nvideo editing tasks like re-timing.\n","authors":["Zhihang Zhong","Gurunandan Krishnan","Xiao Sun","Yu Qiao","Sizhuo Ma","Jian Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08007v1.pdf","comment":"Project page: https://zzh-tech.github.io/InterpAny-Clearer/ ; Code:\n  https://github.com/zzh-tech/InterpAny-Clearer"},{"id":"http://arxiv.org/abs/2311.07993v1","updated":"2023-11-14T08:47:38Z","published":"2023-11-14T08:47:38Z","title":"Explicit Change Relation Learning for Change Detection in VHR Remote\n  Sensing Images","summary":"  Change detection has always been a concerned task in the interpretation of\nremote sensing images. It is essentially a unique binary classification task\nwith two inputs, and there is a change relationship between these two inputs.\nAt present, the mining of change relationship features is usually implicit in\nthe network architectures that contain single-branch or two-branch encoders.\nHowever, due to the lack of artificial prior design for change relationship\nfeatures, these networks cannot learn enough change semantic information and\nlose more accurate change detection performance. So we propose a network\narchitecture NAME for the explicit mining of change relation features. In our\nopinion, the change features of change detection should be divided into\npre-changed image features, post-changed image features and change relation\nfeatures. In order to fully mine these three kinds of change features, we\npropose the triple branch network combining the transformer and convolutional\nneural network (CNN) to extract and fuse these change features from two\nperspectives of global information and local information, respectively. In\naddition, we design the continuous change relation (CCR) branch to further\nobtain the continuous and detail change relation features to improve the change\ndiscrimination capability of the model. The experimental results show that our\nnetwork performs better, in terms of F1, IoU, and OA, than those of the\nexisting advanced networks for change detection on four public very\nhigh-resolution (VHR) remote sensing datasets. Our source code is available at\nhttps://github.com/DalongZ/NAME.\n","authors":["Dalong Zheng","Zebin Wu","Jia Liu","Chih-Cheng Hung","Zhihui Wei"],"pdf_url":"https://arxiv.org/pdf/2311.07993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13277v2","updated":"2023-11-14T08:30:10Z","published":"2023-05-22T17:37:10Z","title":"U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical\n  Satellite Time Series","summary":"  Satellite image time series in the optical and infrared spectrum suffer from\nfrequent data gaps due to cloud cover, cloud shadows, and temporary sensor\noutages. It has been a long-standing problem of remote sensing research how to\nbest reconstruct the missing pixel values and obtain complete, cloud-free image\nsequences. We approach that problem from the perspective of representation\nlearning and develop U-TILISE, an efficient neural model that is able to\nimplicitly capture spatio-temporal patterns of the spectral intensities, and\nthat can therefore be trained to map a cloud-masked input sequence to a\ncloud-free output sequence. The model consists of a convolutional spatial\nencoder that maps each individual frame of the input sequence to a latent\nencoding; an attention-based temporal encoder that captures dependencies\nbetween those per-frame encodings and lets them exchange information along the\ntime dimension; and a convolutional spatial decoder that decodes the latent\nembeddings back into multi-spectral images. We experimentally evaluate the\nproposed model on EarthNet2021, a dataset of Sentinel-2 time series acquired\nall over Europe, and demonstrate its superior ability to reconstruct the\nmissing pixels. Compared to a standard interpolation baseline, it increases the\nPSNR by 1.8 dB at previously seen locations and by 1.3 dB at unseen locations.\n","authors":["Corinne Stucker","Vivien Sainte Fare Garnot","Konrad Schindler"],"pdf_url":"https://arxiv.org/pdf/2305.13277v2.pdf","comment":"Accepted for publication in the IEEE Transactions on Geoscience and\n  Remote Sensing"},{"id":"http://arxiv.org/abs/2306.09126v2","updated":"2023-11-14T08:29:23Z","published":"2023-06-15T13:37:14Z","title":"STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes\n  with Spatiotemporal Annotations of Sound Events","summary":"  While direction of arrival (DOA) of sound events is generally estimated from\nmultichannel audio data recorded in a microphone array, sound events usually\nderive from visually perceptible source objects, e.g., sounds of footsteps come\nfrom the feet of a walker. This paper proposes an audio-visual sound event\nlocalization and detection (SELD) task, which uses multichannel audio and video\ninformation to estimate the temporal activation and DOA of target sound events.\nAudio-visual SELD systems can detect and localize sound events using signals\nfrom a microphone array and audio-visual correspondence. We also introduce an\naudio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23),\nwhich consists of multichannel audio data recorded with a microphone array,\nvideo data, and spatiotemporal annotation of sound events. Sound scenes in\nSTARSS23 are recorded with instructions, which guide recording participants to\nensure adequate activity and occurrences of sound events. STARSS23 also serves\nhuman-annotated temporal activation labels and human-confirmed DOA labels,\nwhich are based on tracking results of a motion capture system. Our benchmark\nresults demonstrate the benefits of using visual object positions in\naudio-visual SELD tasks. The data is available at\nhttps://zenodo.org/record/7880637.\n","authors":["Kazuki Shimada","Archontis Politis","Parthasaarathy Sudarsanam","Daniel Krause","Kengo Uchida","Sharath Adavanne","Aapo Hakala","Yuichiro Koyama","Naoya Takahashi","Shusuke Takahashi","Tuomas Virtanen","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2306.09126v2.pdf","comment":"27 pages, 9 figures, accepted for publication in NeurIPS 2023 Track\n  on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2311.07981v1","updated":"2023-11-14T08:21:36Z","published":"2023-11-14T08:21:36Z","title":"Benchmarking Individual Tree Mapping with Sub-meter Imagery","summary":"  There is a rising interest in mapping trees using satellite or aerial\nimagery, but there is no standardized evaluation protocol for comparing and\nenhancing methods. In dense canopy areas, the high variability of tree sizes\nand their spatial proximity makes it arduous to define the quality of the\npredictions. Concurrently, object-centric approaches such as bounding box\ndetection usuallyperform poorly on small and dense objects. It thus remains\nunclear what is the ideal framework for individual tree mapping, in regards to\ndetection and segmentation approaches, convolutional neural networks and\ntransformers. In this paper, we introduce an evaluation framework suited for\nindividual tree mapping in any physical environment, with annotation costs and\napplicative goals in mind. We review and compare different approaches and deep\narchitectures, and introduce a new method that we experimentally prove to be a\ngood compromise between segmentation and detection.\n","authors":["Dimitri Gominski","Ankit Kariryaa","Martin Brandt","Christian Igel","Sizhuo Li","Maurice Mugabowindekwe","Rasmus Fensholt"],"pdf_url":"https://arxiv.org/pdf/2311.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.05071v5","updated":"2023-11-14T07:59:45Z","published":"2023-04-11T09:08:09Z","title":"Fracture Detection in Pediatric Wrist Trauma X-ray Images Using YOLOv8\n  Algorithm","summary":"  Hospital emergency departments frequently receive lots of bone fracture\ncases, with pediatric wrist trauma fracture accounting for the majority of\nthem. Before pediatric surgeons perform surgery, they need to ask patients how\nthe fracture occurred and analyze the fracture situation by interpreting X-ray\nimages. The interpretation of X-ray images often requires a combination of\ntechniques from radiologists and surgeons, which requires time-consuming\nspecialized training. With the rise of deep learning in the field of computer\nvision, network models applying for fracture detection has become an important\nresearch topic. In this paper, we use data augmentation to improve the model\nperformance of YOLOv8 algorithm (the latest version of You Only Look Once) on a\npediatric wrist trauma X-ray dataset (GRAZPEDWRI-DX), which is a public\ndataset. The experimental results show that our model has reached the\nstate-of-the-art (SOTA) mean average precision (mAP 50). Specifically, mAP 50\nof our model is 0.638, which is significantly higher than the 0.634 and 0.636\nof the improved YOLOv7 and original YOLOv8 models. To enable surgeons to use\nour model for fracture detection on pediatric wrist trauma X-ray images, we\nhave designed the application \"Fracture Detection Using YOLOv8 App\" to assist\nsurgeons in diagnosing fractures, reducing the probability of error analysis,\nand providing more useful information for surgery.\n","authors":["Rui-Yang Ju","Weiming Cai"],"pdf_url":"https://arxiv.org/pdf/2304.05071v5.pdf","comment":"Scientific Reports"},{"id":"http://arxiv.org/abs/2311.07967v1","updated":"2023-11-14T07:46:03Z","published":"2023-11-14T07:46:03Z","title":"Comparison of two data fusion approaches for land use classification","summary":"  Accurate land use maps, describing the territory from an anthropic\nutilisation point of view, are useful tools for land management and planning.\nTo produce them, the use of optical images alone remains limited. It is\ntherefore necessary to make use of several heterogeneous sources, each carrying\ncomplementary or contradictory information due to their imperfections or their\ndifferent specifications. This study compares two different approaches i.e. a\npre-classification and a post-classification fusion approach for combining\nseveral sources of spatial data in the context of land use classification. The\napproaches are applied on authoritative land use data located in the Gers\ndepartment in the southwest of France. Pre-classification fusion, while not\nexplicitly modeling imperfections, has the best final results, reaching an\noverall accuracy of 97% and a macro-mean F1 score of 88%.\n","authors":["Martin Cubaud","Arnaud Le Bris","Laurence Jolivet","Ana-Maria Olteanu-Raimond"],"pdf_url":"https://arxiv.org/pdf/2311.07967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09345v2","updated":"2023-11-14T07:39:32Z","published":"2023-08-18T07:07:15Z","title":"Denoising diffusion-based MRI to CT image translation enables automated\n  spinal segmentation","summary":"  Background: Automated segmentation of spinal MR images plays a vital role\nboth scientifically and clinically. However, accurately delineating posterior\nspine structures presents challenges.\n  Methods: This retrospective study, approved by the ethical committee,\ninvolved translating T1w and T2w MR image series into CT images in a total of\nn=263 pairs of CT/MR series. Landmark-based registration was performed to align\nimage pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit\nmodels (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired\ntranslation, SynDiff) image-to-image translation using \"peak signal to noise\nratio\" (PSNR) as quality measure. A publicly available segmentation network\nsegmented the synthesized CT datasets, and Dice scores were evaluated on\nin-house test sets and the \"MRSpineSeg Challenge\" volumes. The 2D findings were\nextended to 3D Pix2Pix and DDIM.\n  Results: 2D paired methods and SynDiff exhibited similar translation\nperformance and Dice scores on paired data. DDIM image mode achieved the\nhighest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated\nsimilar Dice scores (0.77). For craniocaudal axis rotations, at least two\nlandmarks per vertebra were required for registration. The 3D translation\noutperformed the 2D approach, resulting in improved Dice scores (0.80) and\nanatomically accurate segmentations in a higher resolution than the original MR\nimage.\n  Conclusion: Two landmarks per vertebra registration enabled paired\nimage-to-image translation from MR to CT and outperformed all unpaired\napproaches. The 3D techniques provided anatomically correct segmentations,\navoiding underprediction of small structures like the spinous process.\n","authors":["Robert Graf","Joachim Schmitt","Sarah Schlaeger","Hendrik Kristian Möller","Vasiliki Sideri-Lampretsa","Anjany Sekuboyina","Sandro Manuel Krieg","Benedikt Wiestler","Bjoern Menze","Daniel Rueckert","Jan Stefan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2308.09345v2.pdf","comment":"35 pages, 7 figures, Code and a model weights available\n  https://doi.org/10.5281/zenodo.8221159 and\n  https://doi.org/10.5281/zenodo.8198697"},{"id":"http://arxiv.org/abs/2311.07247v2","updated":"2023-11-14T07:36:39Z","published":"2023-11-13T11:29:38Z","title":"Simultaneous Clutter Detection and Semantic Segmentation of Moving\n  Objects for Automotive Radar Data","summary":"  The unique properties of radar sensors, such as their robustness to adverse\nweather conditions, make them an important part of the environment perception\nsystem of autonomous vehicles. One of the first steps during the processing of\nradar point clouds is often the detection of clutter, i.e. erroneous points\nthat do not correspond to real objects. Another common objective is the\nsemantic segmentation of moving road users. These two problems are handled\nstrictly separate from each other in literature. The employed neural networks\nare always focused entirely on only one of the tasks. In contrast to this, we\nexamine ways to solve both tasks at the same time with a single jointly used\nmodel. In addition to a new augmented multi-head architecture, we also devise a\nmethod to represent a network's predictions for the two tasks with only one\noutput value. This novel approach allows us to solve the tasks simultaneously\nwith the same inference time as a conventional task-specific model. In an\nextensive evaluation, we show that our setup is highly effective and\noutperforms every existing network for semantic segmentation on the RadarScenes\ndataset.\n","authors":["Johannes Kopp","Dominik Kellner","Aldi Piroli","Vinzenz Dallabetta","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2311.07247v2.pdf","comment":"Published at IEEE International Conference on Intelligent\n  Transportation Systems (ITSC), Bilbao, ESP, 2023"},{"id":"http://arxiv.org/abs/2210.03461v4","updated":"2023-11-14T07:25:48Z","published":"2022-10-07T11:16:36Z","title":"FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using\n  Style Representations","summary":"  In recent years, language-driven artistic style transfer has emerged as a new\ntype of style transfer technique, eliminating the need for a reference style\nimage by using natural language descriptions of the style. The first model to\nachieve this, called CLIPstyler, has demonstrated impressive stylisation\nresults. However, its lengthy optimisation procedure at runtime for each query\nlimits its suitability for many practical applications. In this work, we\npresent FastCLIPstyler, a generalised text-based image style transfer model\ncapable of stylising images in a single forward pass for arbitrary text inputs.\nFurthermore, we introduce EdgeCLIPstyler, a lightweight model designed for\ncompatibility with resource-constrained devices. Through quantitative and\nqualitative comparisons with state-of-the-art approaches, we demonstrate that\nour models achieve superior stylisation quality based on measurable metrics\nwhile offering significantly improved runtime efficiency, particularly on edge\ndevices.\n","authors":["Ananda Padhmanabhan Suresh","Sanjana Jain","Pavit Noinongyao","Ankush Ganguly","Ukrit Watchareeruetai","Aubin Samacoits"],"pdf_url":"https://arxiv.org/pdf/2210.03461v4.pdf","comment":"Accepted at the 2024 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2024)"},{"id":"http://arxiv.org/abs/2311.07956v1","updated":"2023-11-14T07:20:46Z","published":"2023-11-14T07:20:46Z","title":"Robust Learning Based Condition Diagnosis Method for Distribution\n  Network Switchgear","summary":"  This paper introduces a robust, learning-based method for diagnosing the\nstate of distribution network switchgear, which is crucial for maintaining the\npower quality for end users. Traditional diagnostic models often rely heavily\non expert knowledge and lack robustness. To address this, our method\nincorporates an expanded feature vector that includes environmental data,\ntemperature readings, switch position, motor operation, insulation conditions,\nand local discharge information. We tackle the issue of high dimensionality\nthrough feature mapping. The method introduces a decision radius to categorize\nunlabeled samples and updates the model parameters using a combination of\nsupervised and unsupervised loss, along with a consistency regularization\nfunction. This approach ensures robust learning even with a limited number of\nlabeled samples. Comparative analysis demonstrates that this method\nsignificantly outperforms existing models in both accuracy and robustness.\n","authors":["Wenxi Zhang","Zhe Li","Weixi Li","Weisi Ma","Xinyi Chen","Sizhe Li"],"pdf_url":"https://arxiv.org/pdf/2311.07956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07955v1","updated":"2023-11-14T07:20:38Z","published":"2023-11-14T07:20:38Z","title":"Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle\n  Imagery: Review and Experimental Comparisons","summary":"  With the advancement of maritime unmanned aerial vehicles (UAVs) and deep\nlearning technologies, the application of UAV-based object detection has become\nincreasingly significant in the fields of maritime industry and ocean\nengineering. Endowed with intelligent sensing capabilities, the maritime UAVs\nenable effective and efficient maritime surveillance. To further promote the\ndevelopment of maritime UAV-based object detection, this paper provides a\ncomprehensive review of challenges, relative methods, and UAV aerial datasets.\nSpecifically, in this work, we first briefly summarize four challenges for\nobject detection on maritime UAVs, i.e., object feature diversity, device\nlimitation, maritime environment variability, and dataset scarcity. We then\nfocus on computational methods to improve maritime UAV-based object detection\nperformance in terms of scale-aware, small object detection, view-aware,\nrotated object detection, lightweight methods, and others. Next, we review the\nUAV aerial image/video datasets and propose a maritime UAV aerial dataset named\nMS2ship for ship detection. Furthermore, we conduct a series of experiments to\npresent the performance evaluation and robustness analysis of object detection\nmethods on maritime datasets. Eventually, we give the discussion and outlook on\nfuture works for maritime UAV-based object detection. The MS2ship dataset is\navailable at\n\\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.\n","authors":["Chenjie Zhao","Ryan Wen Liu","Jingxiang Qu","Ruobin Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07955v1.pdf","comment":"31 pages, 16 figures"},{"id":"http://arxiv.org/abs/2311.07928v1","updated":"2023-11-14T06:13:52Z","published":"2023-11-14T06:13:52Z","title":"Towards Improving Robustness Against Common Corruptions in Object\n  Detectors Using Adversarial Contrastive Learning","summary":"  Neural networks have revolutionized various domains, exhibiting remarkable\naccuracy in tasks like natural language processing and computer vision.\nHowever, their vulnerability to slight alterations in input samples poses\nchallenges, particularly in safety-critical applications like autonomous\ndriving. Current approaches, such as introducing distortions during training,\nfall short in addressing unforeseen corruptions. This paper proposes an\ninnovative adversarial contrastive learning framework to enhance neural network\nrobustness simultaneously against adversarial attacks and common corruptions.\nBy generating instance-wise adversarial examples and optimizing contrastive\nloss, our method fosters representations that resist adversarial perturbations\nand remain robust in real-world scenarios. Subsequent contrastive learning then\nstrengthens the similarity between clean samples and their adversarial\ncounterparts, fostering representations resistant to both adversarial attacks\nand common distortions. By focusing on improving performance under adversarial\nand real-world conditions, our approach aims to bolster the robustness of\nneural networks in safety-critical applications, such as autonomous vehicles\nnavigating unpredictable weather conditions. We anticipate that this framework\nwill contribute to advancing the reliability of neural networks in challenging\nenvironments, facilitating their widespread adoption in mission-critical\nscenarios.\n","authors":["Shashank Kotyan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2311.07928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07362v2","updated":"2023-11-14T06:04:31Z","published":"2023-11-13T14:26:24Z","title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback\n  Guided Revision","summary":"  Large multimodal models (LMMs) suffer from multimodal hallucination, where\nthey provide incorrect responses misaligned with the given visual information.\nRecent works have conjectured that one of the reasons behind multimodal\nhallucination might be due to the vision encoder failing to ground on the image\nproperly. To mitigate this issue, we propose a novel approach that leverages\nself-feedback as visual cues. Building on this approach, we introduce Volcano,\na multimodal self-feedback guided revision model. Volcano generates natural\nlanguage feedback to its initial response based on the provided visual\ninformation and utilizes this feedback to self-revise its initial response.\nVolcano effectively reduces multimodal hallucination and achieves\nstate-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general\nmultimodal abilities and outperforms previous models on MM-Vet and MMBench.\nThrough a qualitative analysis, we show that Volcano's feedback is properly\ngrounded on the image than the initial response. This indicates that Volcano\ncan provide itself with richer visual information, helping alleviate multimodal\nhallucination. We publicly release Volcano models of 7B and 13B sizes along\nwith the data and code at https://github.com/kaistAI/Volcano.\n","authors":["Seongyun Lee","Sue Hyun Park","Yongrae Jo","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.07362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14277v3","updated":"2023-11-14T06:03:35Z","published":"2023-07-26T16:14:21Z","title":"G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and\n  Game Theory","summary":"  The recent video grounding works attempt to introduce vanilla contrastive\nlearning into video grounding. However, we claim that this naive solution is\nsuboptimal. Contrastive learning requires two key properties: (1)\n\\emph{alignment} of features of similar samples, and (2) \\emph{uniformity} of\nthe induced distribution of the normalized features on the hypersphere. Due to\ntwo annoying issues in video grounding: (1) the co-existence of some visual\nentities in both ground truth and other moments, \\ie semantic overlapping; (2)\nonly a few moments in the video are annotated, \\ie sparse annotation dilemma,\nvanilla contrastive learning is unable to model the correlations between\ntemporally distant moments and learned inconsistent video representations. Both\ncharacteristics lead to vanilla contrastive learning being unsuitable for video\ngrounding. In this paper, we introduce Geodesic and Game Localization (G2L), a\nsemantically aligned and uniform video grounding framework via geodesic and\ngame theory. We quantify the correlations among moments leveraging the geodesic\ndistance that guides the model to learn the correct cross-modal\nrepresentations. Furthermore, from the novel perspective of game theory, we\npropose semantic Shapley interaction based on geodesic distance sampling to\nlearn fine-grained semantic alignment in similar moments. Experiments on three\nbenchmarks demonstrate the effectiveness of our method.\n","authors":["Hongxiang Li","Meng Cao","Xuxin Cheng","Yaowei Li","Zhihong Zhu","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2307.14277v3.pdf","comment":"ICCV2023 oral"},{"id":"http://arxiv.org/abs/2311.07912v1","updated":"2023-11-14T05:14:41Z","published":"2023-11-14T05:14:41Z","title":"Detection of Small Targets in Sea Clutter Based on RepVGG and Continuous\n  Wavelet Transform","summary":"  Constructing a high-performance target detector under the background of sea\nclutter is always necessary and important. In this work, we propose a\nRepVGGA0-CWT detector, where RepVGG is a residual network that gains a high\ndetection accuracy. Different from traditional residual networks, RepVGG keeps\nan acceptable calculation speed. Giving consideration to both accuracy and\nspeed, the RepVGGA0 is selected among all the variants of RepVGG. Also,\ncontinuous wavelet transform (CWT) is employed to extract the radar echoes'\ntime-frequency feature effectively. In the tests, other networks (ResNet50,\nResNet18 and AlexNet) and feature extraction methods (short-time Fourier\ntransform (STFT), CWT) are combined to build detectors for comparison. The\nresult of different datasets shows that the RepVGGA0-CWT detector performs\nbetter than those detectors in terms of low controllable false alarm rate, high\ntraining speed, high inference speed and low memory usage. This RepVGGA0-CWT\ndetector is hardware-friendly and can be applied in real-time scenes for its\nhigh inference speed in detection.\n","authors":["Jingchen Ni","Haoru Li","Lilin Xu","Jing Liang"],"pdf_url":"https://arxiv.org/pdf/2311.07912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13591v5","updated":"2023-11-14T03:53:55Z","published":"2023-01-31T12:43:54Z","title":"Zero3D: Semantic-Driven Multi-Category 3D Shape Generation","summary":"  Semantic-driven 3D shape generation aims to generate 3D objects conditioned\non text. Previous works face problems with single-category generation,\nlow-frequency 3D details, and requiring a large number of paired datasets for\ntraining. To tackle these challenges, we propose a multi-category conditional\ndiffusion model. Specifically, 1) to alleviate the problem of lack of\nlarge-scale paired data, we bridge the text, 2D image and 3D shape based on the\npre-trained CLIP model, and 2) to obtain the multi-category 3D shape feature,\nwe apply the conditional flow model to generate 3D shape vector conditioned on\nCLIP embedding. 3) to generate multi-category 3D shape, we employ the\nhidden-layer diffusion model conditioned on the multi-category shape vector,\nwhich greatly reduces the training time and memory consumption.\n","authors":["Bo Han","Yitong Fu","Yixuan Shen"],"pdf_url":"https://arxiv.org/pdf/2301.13591v5.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2304.03174v3","updated":"2023-11-14T03:53:24Z","published":"2023-04-06T15:54:18Z","title":"SketchFFusion: Sketch-guided image editing with diffusion model","summary":"  Sketch-guided image editing aims to achieve local fine-tuning of the image\nbased on the sketch information provided by the user, while maintaining the\noriginal status of the unedited areas. Due to the high cost of acquiring human\nsketches, previous works mostly relied on edge maps as a substitute for\nsketches, but sketches possess more rich structural information. In this paper,\nwe propose a sketch generation scheme that can preserve the main contours of an\nimage and closely adhere to the actual sketch style drawn by the user.\nSimultaneously, current image editing methods often face challenges such as\nimage distortion, training cost, and loss of fine details in the sketch. To\naddress these limitations, We propose a conditional diffusion model\n(SketchFFusion) based on the sketch structure vector. We evaluate the\ngenerative performance of our model and demonstrate that it outperforms\nexisting methods.\n","authors":["Weihang Mao","Bo Han","Zihao Wang"],"pdf_url":"https://arxiv.org/pdf/2304.03174v3.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2311.07885v1","updated":"2023-11-14T03:40:25Z","published":"2023-11-14T03:40:25Z","title":"One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View\n  Generation and 3D Diffusion","summary":"  Recent advancements in open-world 3D object generation have been remarkable,\nwith image-to-3D methods offering superior fine-grained control over their\ntext-to-3D counterparts. However, most existing models fall short in\nsimultaneously providing rapid generation speeds and high fidelity to input\nimages - two features essential for practical applications. In this paper, we\npresent One-2-3-45++, an innovative method that transforms a single image into\na detailed 3D textured mesh in approximately one minute. Our approach aims to\nfully harness the extensive knowledge embedded in 2D diffusion models and\npriors from valuable yet limited 3D data. This is achieved by initially\nfinetuning a 2D diffusion model for consistent multi-view image generation,\nfollowed by elevating these images to 3D with the aid of multi-view conditioned\n3D native diffusion models. Extensive experimental evaluations demonstrate that\nour method can produce high-quality, diverse 3D assets that closely mirror the\noriginal input image. Our project webpage:\nhttps://sudo-ai-3d.github.io/One2345plus_page.\n","authors":["Minghua Liu","Ruoxi Shi","Linghao Chen","Zhuoyang Zhang","Chao Xu","Xinyue Wei","Hansheng Chen","Chong Zeng","Jiayuan Gu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2311.07885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07880v1","updated":"2023-11-14T03:19:55Z","published":"2023-11-14T03:19:55Z","title":"VegaEdge: Edge AI Confluence Anomaly Detection for Real-Time Highway\n  IoT-Applications","summary":"  Vehicle anomaly detection plays a vital role in highway safety applications\nsuch as accident prevention, rapid response, traffic flow optimization, and\nwork zone safety. With the surge of the Internet of Things (IoT) in recent\nyears, there has arisen a pressing demand for Artificial Intelligence (AI)\nbased anomaly detection methods designed to meet the requirements of IoT\ndevices. Catering to this futuristic vision, we introduce a lightweight\napproach to vehicle anomaly detection by utilizing the power of trajectory\nprediction. Our proposed design identifies vehicles deviating from expected\npaths, indicating highway risks from different camera-viewing angles from\nreal-world highway datasets. On top of that, we present VegaEdge - a\nsophisticated AI confluence designed for real-time security and surveillance\napplications in modern highway settings through edge-centric IoT-embedded\nplatforms equipped with our anomaly detection approach. Extensive testing\nacross multiple platforms and traffic scenarios showcases the versatility and\neffectiveness of VegaEdge. This work also presents the Carolinas Anomaly\nDataset (CAD), to bridge the existing gap in datasets tailored for highway\nanomalies. In real-world scenarios, our anomaly detection approach achieves an\nAUC-ROC of 0.94, and our proposed VegaEdge design, on an embedded IoT platform,\nprocesses 738 trajectories per second in a typical highway setting. The dataset\nis available at\nhttps://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set .\n","authors":["Vinit Katariya","Fatema-E- Jannat","Armin Danesh Pazho","Ghazal Alinezhad Noghre","Hamed Tabkhi"],"pdf_url":"https://arxiv.org/pdf/2311.07880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07877v1","updated":"2023-11-14T03:13:47Z","published":"2023-11-14T03:13:47Z","title":"Test-Time Training for Semantic Segmentation with Output Contrastive\n  Loss","summary":"  Although deep learning-based segmentation models have achieved impressive\nperformance on public benchmarks, generalizing well to unseen environments\nremains a major challenge. To improve the model's generalization ability to the\nnew domain during evaluation, the test-time training (TTT) is a challenging\nparadigm that adapts the source-pretrained model in an online fashion. Early\nefforts on TTT mainly focus on the image classification task. Directly\nextending these methods to semantic segmentation easily experiences unstable\nadaption due to segmentation's inherent characteristics, such as extreme class\nimbalance and complex decision spaces. To stabilize the adaptation process, we\nintroduce contrastive loss (CL), known for its capability to learn robust and\ngeneralized representations. Nevertheless, the traditional CL operates in the\nrepresentation space and cannot directly enhance predictions. In this paper, we\nresolve this limitation by adapting the CL to the output space, employing a\nhigh temperature, and simplifying the formulation, resulting in a\nstraightforward yet effective loss function called Output Contrastive Loss\n(OCL). Our comprehensive experiments validate the efficacy of our approach\nacross diverse evaluation scenarios. Notably, our method excels even when\napplied to models initially pre-trained using domain adaptation methods on test\ndomain data, showcasing its resilience and adaptability.\\footnote{Code and more\ninformation could be found at~ \\url{https://github.com/dazhangyu123/OCL}}\n","authors":["Yunlong Zhang","Yuxuan Sun","Sunyi Zheng","Zhongyi Shui","Chenglu Zhu","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.07877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11755v3","updated":"2023-11-14T03:07:45Z","published":"2023-10-18T07:30:08Z","title":"RGM: A Robust Generalist Matching Model","summary":"  Finding corresponding pixels within a pair of images is a fundamental\ncomputer vision task with various applications. Due to the specific\nrequirements of different tasks like optical flow estimation and local feature\nmatching, previous works are primarily categorized into dense matching and\nsparse feature matching focusing on specialized architectures along with\ntask-specific datasets, which may somewhat hinder the generalization\nperformance of specialized models. In this paper, we propose a deep model for\nsparse and dense matching, termed RGM (Robust Generalist Matching). In\nparticular, we elaborately design a cascaded GRU module for refinement by\nexploring the geometric similarity iteratively at multiple scales following an\nadditional uncertainty estimation module for sparsification. To narrow the gap\nbetween synthetic training samples and real-world scenarios, we build a new,\nlarge-scale dataset with sparse correspondence ground truth by generating\noptical flow supervision with greater intervals. As such, we are able to mix up\nvarious dense and sparse matching datasets, significantly improving the\ntraining diversity. The generalization capacity of our proposed RGM is greatly\nimproved by learning the matching and uncertainty estimation in a two-stage\nmanner on the large, mixed data. Superior performance is achieved for zero-shot\nmatching and downstream geometry estimation across multiple datasets,\noutperforming the previous methods by a large margin.\n","authors":["Songyan Zhang","Xinyu Sun","Hao Chen","Bo Li","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2310.11755v3.pdf","comment":"17 pages. Fixed typo in the first two equations. Code is available\n  at: https://github.com/aim-uofa/RGM"},{"id":"http://arxiv.org/abs/2311.07871v1","updated":"2023-11-14T03:03:21Z","published":"2023-11-14T03:03:21Z","title":"Dual-channel Prototype Network for few-shot Classification of\n  Pathological Images","summary":"  In pathology, the rarity of certain diseases and the complexity in annotating\npathological images significantly hinder the creation of extensive,\nhigh-quality datasets. This limitation impedes the progress of deep\nlearning-assisted diagnostic systems in pathology. Consequently, it becomes\nimperative to devise a technology that can discern new disease categories from\na minimal number of annotated examples. Such a technology would substantially\nadvance deep learning models for rare diseases. Addressing this need, we\nintroduce the Dual-channel Prototype Network (DCPN), rooted in the few-shot\nlearning paradigm, to tackle the challenge of classifying pathological images\nwith limited samples. DCPN augments the Pyramid Vision Transformer (PVT)\nframework for few-shot classification via self-supervised learning and\nintegrates it with convolutional neural networks. This combination forms a\ndual-channel architecture that extracts multi-scale, highly precise\npathological features. The approach enhances the versatility of prototype\nrepresentations and elevates the efficacy of prototype networks in few-shot\npathological image classification tasks. We evaluated DCPN using three publicly\navailable pathological datasets, configuring small-sample classification tasks\nthat mirror varying degrees of clinical scenario domain shifts. Our\nexperimental findings robustly affirm DCPN's superiority in few-shot\npathological image classification, particularly in tasks within the same\ndomain, where it achieves the benchmarks of supervised learning.\n","authors":["Hao Quan","Xinjia Li","Dayu Hu","Tianhang Nan","Xiaoyu Cui"],"pdf_url":"https://arxiv.org/pdf/2311.07871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14191v3","updated":"2023-11-14T02:43:33Z","published":"2022-07-28T15:57:46Z","title":"Learning with Limited Annotations: A Survey on Deep Semi-Supervised\n  Learning for Medical Image Segmentation","summary":"  Medical image segmentation is a fundamental and critical step in many\nimage-guided clinical approaches. Recent success of deep learning-based\nsegmentation methods usually relies on a large amount of labeled data, which is\nparticularly difficult and costly to obtain especially in the medical imaging\ndomain where only experts can provide reliable and accurate annotations.\nSemi-supervised learning has emerged as an appealing strategy and been widely\napplied to medical image segmentation tasks to train deep models with limited\nannotations. In this paper, we present a comprehensive review of recently\nproposed semi-supervised learning methods for medical image segmentation and\nsummarized both the technical novelties and empirical results. Furthermore, we\nanalyze and discuss the limitations and several unsolved problems of existing\napproaches. We hope this review could inspire the research community to explore\nsolutions for this challenge and further promote the developments in medical\nimage segmentation field.\n","authors":["Rushi Jiao","Yichi Zhang","Le Ding","Rong Cai","Jicong Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.14191v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08355v5","updated":"2023-11-14T02:43:04Z","published":"2022-09-17T15:47:01Z","title":"Differentiable Topology-Preserved Distance Transform for Pulmonary\n  Airway Segmentation","summary":"  Detailed pulmonary airway segmentation is a clinically important task for\nendobronchial intervention and treatment of peripheral located lung cancer\nlesions. Convolutional Neural Networks (CNNs) are promising tools for medical\nimage analysis but have been performing poorly for cases when existing a\nsignificant imbalanced feature distribution, which is true for the airway data\nas the trachea and principal bronchi dominate most of the voxels whereas the\nlobar bronchi and distal segmental bronchi occupy a small proportion. In this\npaper, we propose a Differentiable Topology-Preserved Distance Transform\n(DTPDT) framework to improve the performance of airway segmentation. A\nTopology-Preserved Surrogate (TPS) learning strategy is first proposed to\nbalance the training progress within-class distribution. Furthermore, a\nConvolutional Distance Transform (CDT) is designed to identify the breakage\nphenomenon with superior sensitivity and minimize the variation of the distance\nmap between the predictionand ground-truth. The proposed method is validated\nwith the publically available reference airway segmentation datasets. The\ndetected rate of branch and length on public EXACT'09 and BAS datasets are\n82.1%/79.6% and 96.5%/91.5% respectively, demonstrating the reliability and\nefficiency of the method in terms of improving the topology completeness of the\nsegmentation performance while maintaining the overall topology accuracy.\n","authors":["Minghui Zhang","Guang-Zhong Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2209.08355v5.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.07864v1","updated":"2023-11-14T02:33:54Z","published":"2023-11-14T02:33:54Z","title":"Probing clustering in neural network representations","summary":"  Neural network representations contain structure beyond what was present in\nthe training labels. For instance, representations of images that are visually\nor semantically similar tend to lie closer to each other than to dissimilar\nimages, regardless of their labels. Clustering these representations can thus\nprovide insights into dataset properties as well as the network internals. In\nthis work, we study how the many design choices involved in neural network\ntraining affect the clusters formed in the hidden representations. To do so, we\nestablish an evaluation setup based on the BREEDS hierarchy, for the task of\nsubclass clustering after training models with only superclass information. We\nisolate the training dataset and architecture as important factors affecting\nclusterability. Datasets with labeled classes consisting of unrelated\nsubclasses yield much better clusterability than those following a natural\nhierarchy. When using pretrained models to cluster representations on\ndownstream datasets, models pretrained on subclass labels provide better\nclusterability than models pretrained on superclass labels, but only when there\nis a high degree of domain overlap between the pretraining and downstream data.\nArchitecturally, we find that normalization strategies affect which layers\nyield the best clustering performance, and, surprisingly, Vision Transformers\nattain lower subclass clusterability than ResNets.\n","authors":["Thao Nguyen","Simon Kornblith"],"pdf_url":"https://arxiv.org/pdf/2311.07864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02867v3","updated":"2023-11-14T02:32:41Z","published":"2023-03-06T03:36:06Z","title":"Boundary-semantic collaborative guidance network with dual-stream\n  feedback mechanism for salient object detection in optical remote sensing\n  imagery","summary":"  With the increasing application of deep learning in various domains, salient\nobject detection in optical remote sensing images (ORSI-SOD) has attracted\nsignificant attention. However, most existing ORSI-SOD methods predominantly\nrely on local information from low-level features to infer salient boundary\ncues and supervise them using boundary ground truth, but fail to sufficiently\noptimize and protect the local information, and almost all approaches ignore\nthe potential advantages offered by the last layer of the decoder to maintain\nthe integrity of saliency maps. To address these issues, we propose a novel\nmethod named boundary-semantic collaborative guidance network (BSCGNet) with\ndual-stream feedback mechanism. First, we propose a boundary protection\ncalibration (BPC) module, which effectively reduces the loss of edge position\ninformation during forward propagation and suppresses noise in low-level\nfeatures without relying on boundary ground truth. Second, based on the BPC\nmodule, a dual feature feedback complementary (DFFC) module is proposed, which\naggregates boundary-semantic dual features and provides effective feedback to\ncoordinate features across different layers, thereby enhancing cross-scale\nknowledge communication. Finally, to obtain more complete saliency maps, we\nconsider the uniqueness of the last layer of the decoder for the first time and\npropose the adaptive feedback refinement (AFR) module, which further refines\nfeature representation and eliminates differences between features through a\nunique feedback mechanism. Extensive experiments on three benchmark datasets\ndemonstrate that BSCGNet exhibits distinct advantages in challenging scenarios\nand outperforms the 17 state-of-the-art (SOTA) approaches proposed in recent\nyears. Codes and results have been released on GitHub:\nhttps://github.com/YUHsss/BSCGNet.\n","authors":["Dejun Feng","Hongyu Chen","Suning Liu","Ziyang Liao","Xingyu Shen","Yakun Xie","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.02867v3.pdf","comment":"Accepted by TGRS"},{"id":"http://arxiv.org/abs/2311.05410v2","updated":"2023-11-14T02:26:09Z","published":"2023-11-09T14:45:22Z","title":"Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated\n  Convolution for Oriented Object Detection","summary":"  In oriented object detection, current representations of oriented bounding\nboxes (OBBs) often suffer from boundary discontinuity problem. Methods of\ndesigning continuous regression losses do not essentially solve this problem.\nAlthough Gaussian bounding box (GBB) representation avoids this problem,\ndirectly regressing GBB is susceptible to numerical instability. We propose\nlinear GBB (LGBB), a novel OBB representation. By linearly transforming the\nelements of GBB, LGBB avoids the boundary discontinuity problem and has high\nnumerical stability. In addition, existing convolution-based rotation-sensitive\nfeature extraction methods only have local receptive fields, resulting in slow\nfeature aggregation. We propose ring-shaped rotated convolution (RRC), which\nadaptively rotates feature maps to arbitrary orientations to extract\nrotation-sensitive features under a ring-shaped receptive field, rapidly\naggregating features and contextual information. Experimental results\ndemonstrate that LGBB and RRC achieve state-of-the-art performance.\nFurthermore, integrating LGBB and RRC into various models effectively improves\ndetection accuracy.\n","authors":["Zhen Zhou","Yunkai Ma","Junfeng Fan","Zhaoyang Liu","Fengshui Jing","Min Tan"],"pdf_url":"https://arxiv.org/pdf/2311.05410v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13475v2","updated":"2023-11-14T02:20:44Z","published":"2023-09-23T20:33:38Z","title":"Detecting and Mitigating System-Level Anomalies of Vision-Based\n  Controllers","summary":"  Autonomous systems, such as self-driving cars and drones, have made\nsignificant strides in recent years by leveraging visual inputs and machine\nlearning for decision-making and control. Despite their impressive performance,\nthese vision-based controllers can make erroneous predictions when faced with\nnovel or out-of-distribution inputs. Such errors can cascade to catastrophic\nsystem failures and compromise system safety. In this work, we introduce a\nrun-time anomaly monitor to detect and mitigate such closed-loop, system-level\nfailures. Specifically, we leverage a reachability-based framework to\nstress-test the vision-based controller offline and mine its system-level\nfailures. This data is then used to train a classifier that is leveraged online\nto flag inputs that might cause system breakdowns. The anomaly detector\nhighlights issues that transcend individual modules and pertain to the safety\nof the overall system. We also design a fallback controller that robustly\nhandles these detected anomalies to preserve system safety. We validate the\nproposed approach on an autonomous aircraft taxiing system that uses a\nvision-based controller for taxiing. Our results show the efficacy of the\nproposed approach in identifying and handling system-level anomalies,\noutperforming methods such as prediction error-based detection, and ensembling,\nthereby enhancing the overall safety and robustness of autonomous systems.\n","authors":["Aryaman Gupta","Kaustav Chakraborty","Somil Bansal"],"pdf_url":"https://arxiv.org/pdf/2309.13475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17105v3","updated":"2023-11-14T02:05:48Z","published":"2023-09-29T10:06:28Z","title":"Continual Action Assessment via Task-Consistent Score-Discriminative\n  Feature Distribution Modeling","summary":"  Action Quality Assessment (AQA) is a task that tries to answer how well an\naction is carried out. While remarkable progress has been achieved, existing\nworks on AQA assume that all the training data are visible for training in one\ntime, but do not enable continual learning on assessing new technical actions.\nIn this work, we address such a Continual Learning problem in AQA\n(Continual-AQA), which urges a unified model to learn AQA tasks sequentially\nwithout forgetting. Our idea for modeling Continual-AQA is to sequentially\nlearn a task-consistent score-discriminative feature distribution, in which the\nlatent features express a strong correlation with the score labels regardless\nof the task or action types. From this perspective, we aim to mitigate the\nforgetting in Continual-AQA from two aspects. Firstly, to fuse the features of\nnew and previous data into a score-discriminative distribution, a novel\nFeature-Score Correlation-Aware Rehearsal is proposed to store and reuse data\nfrom previous tasks with limited memory size. Secondly, an Action\nGeneral-Specific Graph is developed to learn and decouple the action-general\nand action-specific knowledge so that the task-consistent score-discriminative\nfeatures can be better extracted across various tasks. Extensive experiments\nare conducted to evaluate the contributions of proposed components. The\ncomparisons with the existing continual learning methods additionally verify\nthe effectiveness and versatility of our approach.\n","authors":["Yuan-Ming Li","Ling-An Zeng","Jing-Ke Meng","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2309.17105v3.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2304.02192v2","updated":"2023-11-14T02:01:38Z","published":"2023-04-05T02:13:42Z","title":"A Diffusion-based Method for Multi-turn Compositional Image Generation","summary":"  Multi-turn compositional image generation (M-CIG) is a challenging task that\naims to iteratively manipulate a reference image given a modification text.\nWhile most of the existing methods for M-CIG are based on generative\nadversarial networks (GANs), recent advances in image generation have\ndemonstrated the superiority of diffusion models over GANs. In this paper, we\npropose a diffusion-based method for M-CIG named conditional denoising\ndiffusion with image compositional matching (CDD-ICM). We leverage CLIP as the\nbackbone of image and text encoders, and incorporate a gated fusion mechanism,\noriginally proposed for question answering, to compositionally fuse the\nreference image and the modification text at each turn of M-CIG. We introduce a\nconditioning scheme to generate the target image based on the fusion results.\nTo prioritize the semantic quality of the generated target image, we learn an\nauxiliary image compositional match (ICM) objective, along with the conditional\ndenoising diffusion (CDD) objective in a multi-task learning framework.\nAdditionally, we also perform ICM guidance and classifier-free guidance to\nimprove performance. Experimental results show that CDD-ICM achieves\nstate-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and\ni-CLEVR.\n","authors":["Chao Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02192v2.pdf","comment":"WACV 2024 3rd Workshop on Image/Video/Audio Quality in Computer\n  Vision and Generative AI"},{"id":"http://arxiv.org/abs/2311.07840v1","updated":"2023-11-14T01:40:08Z","published":"2023-11-14T01:40:08Z","title":"Enabling Decision-Support Systems through Automated Cell Tower Detection","summary":"  Cell phone coverage and high-speed service gaps persist in rural areas in\nsub-Saharan Africa, impacting public access to mobile-based financial,\neducational, and humanitarian services. Improving maps of telecommunications\ninfrastructure can help inform strategies to eliminate gaps in mobile coverage.\nDeep neural networks, paired with remote sensing images, can be used for object\ndetection of cell towers and eliminate the need for inefficient and burdensome\nmanual mapping to find objects over large geographic regions. In this study, we\ndemonstrate a partially automated workflow to train an object detection model\nto locate cell towers using OpenStreetMap (OSM) features and high-resolution\nMaxar imagery. For model fine-tuning and evaluation, we curated a diverse\ndataset of over 6,000 unique images of cell towers in 26 countries in eastern,\nsouthern, and central Africa using automatically generated annotations from OSM\npoints. Our model achieves an average precision at 50% Intersection over Union\n(IoU) (AP@50) of 81.2 with good performance across different geographies and\nout-of-sample testing. Accurate localization of cell towers can yield more\naccurate cell coverage maps, in turn enabling improved delivery of digital\nservices for decision-support applications.\n","authors":["Natasha Krell","Will Gleave","Daniel Nakada","Justin Downes","Amanda Willet","Matthew Baran"],"pdf_url":"https://arxiv.org/pdf/2311.07840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08585v1","updated":"2023-11-14T23:13:59Z","published":"2023-11-14T23:13:59Z","title":"Unsupervised segmentation of irradiation$\\unicode{x2010}$induced\n  order$\\unicode{x2010}$disorder phase transitions in electron microscopy","summary":"  We present a method for the unsupervised segmentation of electron microscopy\nimages, which are powerful descriptors of materials and chemical systems.\nImages are oversegmented into overlapping chips, and similarity graphs are\ngenerated from embeddings extracted from a domain$\\unicode{x2010}$pretrained\nconvolutional neural network (CNN). The Louvain method for community detection\nis then applied to perform segmentation. The graph representation provides an\nintuitive way of presenting the relationship between chips and communities. We\ndemonstrate our method to track irradiation$\\unicode{x2010}$induced amorphous\nfronts in thin films used for catalysis and electronics. This method has\npotential for \"on$\\unicode{x2010}$the$\\unicode{x2010}$fly\" segmentation to\nguide emerging automated electron microscopes.\n","authors":["Arman H Ter-Petrosyan","Jenna A Bilbrey","Christina M Doty","Bethany E Matthews","Le Wang","Yingge Du","Eric Lang","Khalid Hattar","Steven R Spurgeon"],"pdf_url":"https://arxiv.org/pdf/2311.08585v1.pdf","comment":"7 pages, 3 figures. Accepted to Machine Learning and the Physical\n  Sciences Workshop, NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.17842v2","updated":"2023-11-14T23:03:35Z","published":"2023-10-27T01:46:37Z","title":"What You See Is What You Detect: Towards better Object Densification in\n  3D detection","summary":"  Recent works have demonstrated the importance of object completion in 3D\nPerception from Lidar signal. Several methods have been proposed in which\nmodules were used to densify the point clouds produced by laser scanners,\nleading to better recall and more accurate results. Pursuing in that direction,\nwe present, in this work, a counter-intuitive perspective: the widely-used\nfull-shape completion approach actually leads to a higher error-upper bound\nespecially for far away objects and small objects like pedestrians. Based on\nthis observation, we introduce a visible part completion method that requires\nonly 11.3\\% of the prediction points that previous methods generate. To recover\nthe dense representation, we propose a mesh-deformation-based method to augment\nthe point set associated with visible foreground objects. Considering that our\napproach focuses only on the visible part of the foreground objects to achieve\naccurate 3D detection, we named our method What You See Is What You Detect\n(WYSIWYD). Our proposed method is thus a detector-independent model that\nconsists of 2 parts: an Intra-Frustum Segmentation Transformer (IFST) and a\nMesh Depth Completion Network(MDCNet) that predicts the foreground depth from\nmesh deformation. This way, our model does not require the time-consuming\nfull-depth completion task used by most pseudo-lidar-based methods. Our\nexperimental evaluation shows that our approach can provide up to 12.2\\%\nperformance improvements over most of the public baseline models on the KITTI\nand NuScenes dataset bringing the state-of-the-art to a new level. The codes\nwill be available at\n\\textcolor[RGB]{0,0,255}{\\url{{https://github.com/Orbis36/WYSIWYD}}\n","authors":["Tianran Liu","Zeping Zhang","Morteza Mousa Pasandi","Robert Laganiere"],"pdf_url":"https://arxiv.org/pdf/2310.17842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08581v1","updated":"2023-11-14T22:54:29Z","published":"2023-11-14T22:54:29Z","title":"Drivable 3D Gaussian Avatars","summary":"  We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable\nmodel for human bodies rendered with Gaussian splats. Current photorealistic\ndrivable avatars require either accurate 3D registrations during training,\ndense input images during testing, or both. The ones based on neural radiance\nfields also tend to be prohibitively slow for telepresence applications. This\nwork uses the recently presented 3D Gaussian Splatting (3DGS) technique to\nrender realistic humans at real-time framerates, using dense calibrated\nmulti-view videos as input. To deform those primitives, we depart from the\ncommonly used point deformation method of linear blend skinning (LBS) and use a\nclassic volumetric deformation method: cage deformations. Given their smaller\nsize, we drive these deformations with joint angles and keypoints, which are\nmore suitable for communication applications. Our experiments on nine subjects\nwith varied body shapes, clothes, and motions obtain higher-quality results\nthan state-of-the-art methods when using the same training and test data.\n","authors":["Wojciech Zielonka","Timur Bagautdinov","Shunsuke Saito","Michael Zollhöfer","Justus Thies","Javier Romero"],"pdf_url":"https://arxiv.org/pdf/2311.08581v1.pdf","comment":"Website: https://zielon.github.io/d3ga/"},{"id":"http://arxiv.org/abs/2311.08577v1","updated":"2023-11-14T22:46:01Z","published":"2023-11-14T22:46:01Z","title":"Finding AI-Generated Faces in the Wild","summary":"  AI-based image generation has continued to rapidly improve, producing\nincreasingly more realistic images with fewer obvious visual flaws.\nAI-generated images are being used to create fake online profiles which in turn\nare being used for spam, fraud, and disinformation campaigns. As the general\nproblem of detecting any type of manipulated or synthesized content is\nreceiving increasing attention, here we focus on a more narrow task of\ndistinguishing a real face from an AI-generated face. This is particularly\napplicable when tackling inauthentic online accounts with a fake user profile\nphoto. We show that by focusing on only faces, a more resilient and\ngeneral-purpose artifact can be detected that allows for the detection of\nAI-generated faces from a variety of GAN- and diffusion-based synthesis\nengines, and across image resolutions (as low as 128 x 128 pixels) and\nqualities.\n","authors":["Gonzalo J. Aniano Porcile","Jack Gindi","Shivansh Mundra","James R. Verbus","Hany Farid"],"pdf_url":"https://arxiv.org/pdf/2311.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04818v2","updated":"2023-11-14T21:59:36Z","published":"2023-11-08T16:42:14Z","title":"Cross-Silo Federated Learning Across Divergent Domains with Iterative\n  Parameter Alignment","summary":"  Learning from the collective knowledge of data dispersed across private\nsources can provide neural networks with enhanced generalization capabilities.\nFederated learning, a method for collaboratively training a machine learning\nmodel across remote clients, achieves this by combining client models via the\norchestration of a central server. However, current approaches face two\ncritical limitations: i) they struggle to converge when client domains are\nsufficiently different, and ii) current aggregation techniques produce an\nidentical global model for each client. In this work, we address these issues\nby reformulating the typical federated learning setup: rather than learning a\nsingle global model, we learn N models each optimized for a common objective.\nTo achieve this, we apply a weighted distance minimization to model parameters\nshared in a peer-to-peer topology. The resulting framework, Iterative Parameter\nAlignment, applies naturally to the cross-silo setting, and has the following\nproperties: (i) a unique solution for each participant, with the option to\nglobally converge each model in the federation, and (ii) an optional\nearly-stopping mechanism to elicit fairness among peers in collaborative\nlearning settings. These characteristics jointly provide a flexible new\nframework for iteratively learning from peer models trained on disparate\ndatasets. We find that the technique achieves competitive results on a variety\nof data partitions compared to state-of-the-art approaches. Further, we show\nthat the method is robust to divergent domains (i.e. disjoint classes across\npeers) where existing approaches struggle.\n","authors":["Matt Gorbett","Hossein Shirazi","Indrakshi Ray"],"pdf_url":"https://arxiv.org/pdf/2311.04818v2.pdf","comment":"Published at IEEE Big Data 2023"},{"id":"http://arxiv.org/abs/2311.08557v1","updated":"2023-11-14T21:39:15Z","published":"2023-11-14T21:39:15Z","title":"Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges","summary":"  Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study comprehensively reviews recent developments in low-light\npedestrian detection approaches. It systematically categorizes and analyses\nvarious algorithms from region-based to non-region-based and graph-based\nlearning methodologies by highlighting their methodologies, implementation\nissues, and challenges. It also outlines the key benchmark datasets that can be\nused for research and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations\n","authors":["Hrishikesh Vachhani","Thangarajah Akilan","Yash Devmurari","Nisharaff Shaik","Dhruvisha Patel"],"pdf_url":"https://arxiv.org/pdf/2311.08557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08548v1","updated":"2023-11-14T21:20:54Z","published":"2023-11-14T21:20:54Z","title":"Topology of Surface Electromyogram Signals: Hand Gesture Decoding on\n  Riemannian Manifolds","summary":"  Decoding gestures from the upper limb using noninvasive surface\nelectromyogram (sEMG) signals is of keen interest for the rehabilitation of\namputees, artificial supernumerary limb augmentation, gestural control of\ncomputers, and virtual/augmented realities. We show that sEMG signals recorded\nacross an array of sensor electrodes in multiple spatial locations around the\nforearm evince a rich geometric pattern of global motor unit (MU) activity that\ncan be leveraged to distinguish different hand gestures. We demonstrate a\nsimple technique to analyze spatial patterns of muscle MU activity within a\ntemporal window and show that distinct gestures can be classified in both\nsupervised and unsupervised manners. Specifically, we construct symmetric\npositive definite (SPD) covariance matrices to represent the spatial\ndistribution of MU activity in a time window of interest, calculated as\npairwise covariance of electrical signals measured across different electrodes.\nThis allows us to understand and manipulate multivariate sEMG timeseries on a\nmore natural subspace -the Riemannian manifold. Furthermore, it directly\naddresses signal variability across individuals and sessions, which remains a\nmajor challenge in the field. sEMG signals measured at a single electrode lack\ncontextual information such as how various anatomical and physiological factors\ninfluence the signals and how their combined effect alters the evident\ninteraction among neighboring muscles. As we show here, analyzing spatial\npatterns using covariance matrices on Riemannian manifolds allows us to\nrobustly model complex interactions across spatially distributed MUs and\nprovides a flexible and transparent framework to quantify differences in sEMG\nsignals across individuals. The proposed method is novel in the study of sEMG\nsignals and its performance exceeds the current benchmarks while maintaining\nexceptional computational efficiency.\n","authors":["Harshavardhana T. Gowda","Lee M. Miller"],"pdf_url":"https://arxiv.org/pdf/2311.08548v1.pdf","comment":"15 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2311.08539v1","updated":"2023-11-14T21:04:49Z","published":"2023-11-14T21:04:49Z","title":"Physical Adversarial Examples for Multi-Camera Systems","summary":"  Neural networks build the foundation of several intelligent systems, which,\nhowever, are known to be easily fooled by adversarial examples. Recent advances\nmade these attacks possible even in air-gapped scenarios, where the autonomous\nsystem observes its surroundings by, e.g., a camera. We extend these ideas in\nour research and evaluate the robustness of multi-camera setups against such\nphysical adversarial examples. This scenario becomes ever more important with\nthe rise in popularity of autonomous vehicles, which fuse the information of\nseveral cameras for their driving decision. While we find that multi-camera\nsetups provide some robustness towards past attack methods, we see that this\nadvantage reduces when optimizing on multiple perspectives at once. We propose\na novel attack method that we call Transcender-MC, where we incorporate online\n3D renderings and perspective projections in the training process. Moreover, we\nmotivate that certain data augmentation techniques can facilitate the\ngeneration of successful adversarial examples even further. Transcender-MC is\n11% more effective in successfully attacking multi-camera setups than\nstate-of-the-art methods. Our findings offer valuable insights regarding the\nresilience of object detection in a setup with multiple cameras and motivate\nthe need of developing adequate defense mechanisms against them.\n","authors":["Ana Răduţoiu","Jan-Philipp Schulze","Philip Sperl","Konstantin Böttinger"],"pdf_url":"https://arxiv.org/pdf/2311.08539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08530v1","updated":"2023-11-14T20:55:40Z","published":"2023-11-14T20:55:40Z","title":"SceneScore: Learning a Cost Function for Object Arrangement","summary":"  Arranging objects correctly is a key capability for robots which unlocks a\nwide range of useful tasks. A prerequisite for creating successful arrangements\nis the ability to evaluate the desirability of a given arrangement. Our method\n\"SceneScore\" learns a cost function for arrangements, such that desirable,\nhuman-like arrangements have a low cost. We learn the distribution of training\narrangements offline using an energy-based model, solely from example images\nwithout requiring environment interaction or human supervision. Our model is\nrepresented by a graph neural network which learns object-object relations,\nusing graphs constructed from images. Experiments demonstrate that the learned\ncost function can be used to predict poses for missing objects, generalise to\nnovel objects using semantic features, and can be composed with other cost\nfunctions to satisfy constraints at inference time.\n","authors":["Ivan Kapelyukh","Edward Johns"],"pdf_url":"https://arxiv.org/pdf/2311.08530v1.pdf","comment":"Presented at CoRL 2023 LEAP Workshop. Webpage:\n  https://sites.google.com/view/scenescore"},{"id":"http://arxiv.org/abs/2311.08525v1","updated":"2023-11-14T20:37:54Z","published":"2023-11-14T20:37:54Z","title":"Efficient Rotation Invariance in Deep Neural Networks through Artificial\n  Mental Rotation","summary":"  Humans and animals recognize objects irrespective of the beholder's point of\nview, which may drastically change their appearances. Artificial pattern\nrecognizers also strive to achieve this, e.g., through translational invariance\nin convolutional neural networks (CNNs). However, both CNNs and vision\ntransformers (ViTs) perform very poorly on rotated inputs. Here we present\nartificial mental rotation (AMR), a novel deep learning paradigm for dealing\nwith in-plane rotations inspired by the neuro-psychological concept of mental\nrotation. Our simple AMR implementation works with all common CNN and ViT\narchitectures. We test it on ImageNet, Stanford Cars, and Oxford Pet. With a\ntop-1 error (averaged across datasets and architectures) of $0.743$, AMR\noutperforms the current state of the art (rotational data augmentation, average\ntop-1 error of $0.626$) by $19\\%$. We also easily transfer a trained AMR module\nto a downstream task to improve the performance of a pre-trained semantic\nsegmentation model on rotated CoCo from $32.7$ to $55.2$ IoU.\n","authors":["Lukas Tuggener","Thilo Stadelmann","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2311.08525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08524v1","updated":"2023-11-14T20:36:34Z","published":"2023-11-14T20:36:34Z","title":"Cross-dataset domain adaptation for the classification COVID-19 using\n  chest computed tomography images","summary":"  Detecting COVID-19 patients using Computed Tomography (CT) images of the\nlungs is an active area of research. Datasets of CT images from COVID-19\npatients are becoming available. Deep learning (DL) solutions and in particular\nConvolutional Neural Networks (CNN) have achieved impressive results for the\nclassification of COVID-19 CT images, but only when the training and testing\ntake place within the same dataset. Work on the cross-dataset problem is still\nlimited and the achieved results are low. Our work tackles the cross-dataset\nproblem through a Domain Adaptation (DA) technique with deep learning. Our\nproposed solution, COVID19-DANet, is based on pre-trained CNN backbone for\nfeature extraction. For this task, we select the pre-trained Efficientnet-B3\nCNN because it has achieved impressive classification accuracy in previous\nwork. The backbone CNN is followed by a prototypical layer which is a concept\nborrowed from prototypical networks in few-shot learning (FSL). It computes a\ncosine distance between given samples and the class prototypes and then\nconverts them to class probabilities using the Softmax function. To train the\nCOVID19-DANet model, we propose a combined loss function that is composed of\nthe standard cross-entropy loss for class discrimination and another entropy\nloss computed over the unlabelled target set only. This so-called unlabelled\ntarget entropy loss is minimized and maximized in an alternative fashion, to\nreach the two objectives of class discrimination and domain invariance.\nCOVID19-DANet is tested under four cross-dataset scenarios using the\nSARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results\ncompared to recent work in the literature.\n","authors":["Ridha Ouni","Haikel Alhichri"],"pdf_url":"https://arxiv.org/pdf/2311.08524v1.pdf","comment":"31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2206.01251v2","updated":"2023-11-14T20:25:21Z","published":"2022-06-02T19:05:13Z","title":"Using Representation Expressiveness and Learnability to Evaluate\n  Self-Supervised Learning Methods","summary":"  We address the problem of evaluating the quality of self-supervised learning\n(SSL) models without access to supervised labels, while being agnostic to the\narchitecture, learning algorithm or data manipulation used during training. We\nargue that representations can be evaluated through the lens of expressiveness\nand learnability. We propose to use the Intrinsic Dimension (ID) to assess\nexpressiveness and introduce Cluster Learnability (CL) to assess learnability.\nCL is measured in terms of the performance of a KNN classifier trained to\npredict labels obtained by clustering the representations with K-means. We thus\ncombine CL and ID into a single predictor -- CLID. Through a large-scale\nempirical study with a diverse family of SSL algorithms, we find that CLID\nbetter correlates with in-distribution model performance than other competing\nrecent evaluation schemes. We also benchmark CLID on out-of-domain\ngeneralization, where CLID serves as a predictor of the transfer performance of\nSSL models on several visual classification tasks, yielding improvements with\nrespect to the competing baselines.\n","authors":["Yuchen Lu","Zhen Liu","Aristide Baratin","Romain Laroche","Aaron Courville","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2206.01251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08503v1","updated":"2023-11-14T19:53:09Z","published":"2023-11-14T19:53:09Z","title":"MADG: Margin-based Adversarial Learning for Domain Generalization","summary":"  Domain Generalization (DG) techniques have emerged as a popular approach to\naddress the challenges of domain shift in Deep Learning (DL), with the goal of\ngeneralizing well to the target domain unseen during the training. In recent\nyears, numerous methods have been proposed to address the DG setting, among\nwhich one popular approach is the adversarial learning-based methodology. The\nmain idea behind adversarial DG methods is to learn domain-invariant features\nby minimizing a discrepancy metric. However, most adversarial DG methods use\n0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast,\nthe margin loss-based discrepancy metric has the following advantages: more\ninformative, tighter, practical, and efficiently optimizable. To mitigate this\ngap, this work proposes a novel adversarial learning DG algorithm, MADG,\nmotivated by a margin loss-based discrepancy metric. The proposed MADG model\nlearns domain-invariant features across all source domains and uses adversarial\ntraining to generalize well to the unseen target domain. We also provide a\ntheoretical analysis of the proposed MADG model based on the unseen target\nerror bound. Specifically, we construct the link between the source and unseen\ndomains in the real-valued hypothesis space and derive the generalization bound\nusing margin loss and Rademacher complexity. We extensively experiment with the\nMADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome,\nDomainNet, and TerraIncognita. We evaluate the proposed algorithm on\nDomainBed's benchmark and observe consistent performance across all the\ndatasets.\n","authors":["Aveen Dayal","Vimal K. B.","Linga Reddy Cenkeramaddi","C. Krishna Mohan","Abhinav Kumar","Vineeth N Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2311.08503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08493v1","updated":"2023-11-14T19:41:19Z","published":"2023-11-14T19:41:19Z","title":"Performance of Machine Learning Classification in Mammography Images\n  using BI-RADS","summary":"  This research aims to investigate the classification accuracy of various\nstate-of-the-art image classification models across different categories of\nbreast ultrasound images, as defined by the Breast Imaging Reporting and Data\nSystem (BI-RADS). To achieve this, we have utilized a comprehensively assembled\ndataset of 2,945 mammographic images sourced from 1,540 patients. In order to\nconduct a thorough analysis, we employed six advanced classification\narchitectures, including VGG19 \\cite{simonyan2014very}, ResNet50\n\\cite{he2016deep}, GoogleNet \\cite{szegedy2015going}, ConvNext\n\\cite{liu2022convnet}, EfficientNet \\cite{tan2019efficientnet}, and Vision\nTransformers (ViT) \\cite{dosovitskiy2020image}, instead of traditional machine\nlearning models. We evaluate models in three different settings: full\nfine-tuning, linear evaluation and training from scratch. Our findings\ndemonstrate the effectiveness and capability of our Computer-Aided Diagnosis\n(CAD) system, with a remarkable accuracy of 76.39\\% and an F1 score of 67.94\\%\nin the full fine-tuning setting. Our findings indicate the potential for\nenhanced diagnostic accuracy in the field of breast imaging, providing a solid\nfoundation for future endeavors aiming to improve the precision and reliability\nof CAD systems in medical imaging.\n","authors":["Malitha Gunawardhana","Norbert Zolek"],"pdf_url":"https://arxiv.org/pdf/2311.08493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08488v1","updated":"2023-11-14T19:31:19Z","published":"2023-11-14T19:31:19Z","title":"MUDD: A New Re-Identification Dataset with Efficient Annotation for\n  Off-Road Racers in Extreme Conditions","summary":"  Re-identifying individuals in unconstrained environments remains an open\nchallenge in computer vision. We introduce the Muddy Racer re-IDentification\nDataset (MUDD), the first large-scale benchmark for matching identities of\nmotorcycle racers during off-road competitions. MUDD exhibits heavy mud\nocclusion, motion blurring, complex poses, and extreme lighting conditions\npreviously unseen in existing re-id datasets. We present an annotation\nmethodology incorporating auxiliary information that reduced labeling time by\nover 65%. We establish benchmark performance using state-of-the-art re-id\nmodels including OSNet and ResNet-50. Without fine-tuning, the best models\nachieve only 33% Rank-1 accuracy. Fine-tuning on MUDD boosts results to 79%\nRank-1, but significant room for improvement remains. We analyze the impact of\nreal-world factors including mud, pose, lighting, and more. Our work exposes\nopen problems in re-identifying individuals under extreme conditions. We hope\nMUDD serves as a diverse and challenging benchmark to spur progress in robust\nre-id, especially for computer vision applications in emerging sports\nanalytics. All code and data can be found at https://github.com/JacobTyo/MUDD.\n","authors":["Jacob Tyo","Motolani Olarinre","Youngseog Chung","Zachary C. Lipton"],"pdf_url":"https://arxiv.org/pdf/2311.08488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08479v1","updated":"2023-11-14T19:10:56Z","published":"2023-11-14T19:10:56Z","title":"Leveraging Foundation Models to Improve Lightweight Clients in Federated\n  Learning","summary":"  Federated Learning (FL) is a distributed training paradigm that enables\nclients scattered across the world to cooperatively learn a global model\nwithout divulging confidential data. However, FL faces a significant challenge\nin the form of heterogeneous data distributions among clients, which leads to a\nreduction in performance and robustness. A recent approach to mitigating the\nimpact of heterogeneous data distributions is through the use of foundation\nmodels, which offer better performance at the cost of larger computational\noverheads and slower inference speeds. We introduce foundation model\ndistillation to assist in the federated training of lightweight client models\nand increase their performance under heterogeneous data settings while keeping\ninference costs low. Our results show improvement in the global model\nperformance on a balanced testing set, which contains rarely observed samples,\neven under extreme non-IID client data distributions. We conduct a thorough\nevaluation of our framework with different foundation model backbones on\nCIFAR10, with varying degrees of heterogeneous data distributions ranging from\nclass-specific data partitions across clients to dirichlet data sampling,\nparameterized by values between 0.01 and 1.0.\n","authors":["Xidong Wu","Wan-Yi Lin","Devin Willmott","Filipe Condessa","Yufei Huang","Zhenzhen Li","Madan Ravi Ganesh"],"pdf_url":"https://arxiv.org/pdf/2311.08479v1.pdf","comment":"6 Pages + Appendices"},{"id":"http://arxiv.org/abs/2308.02487v2","updated":"2023-11-14T19:10:49Z","published":"2023-08-04T17:59:01Z","title":"Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen\n  Convolutional CLIP","summary":"  Open-vocabulary segmentation is a challenging task requiring segmenting and\nrecognizing objects from an open set of categories. One way to address this\nchallenge is to leverage multi-modal models, such as CLIP, to provide image and\ntext features in a shared embedding space, which bridges the gap between\nclosed-vocabulary and open-vocabulary recognition. Hence, existing methods\noften adopt a two-stage framework to tackle the problem, where the inputs first\ngo through a mask generator and then through the CLIP model along with the\npredicted masks. This process involves extracting features from images multiple\ntimes, which can be ineffective and inefficient. By contrast, we propose to\nbuild everything into a single-stage framework using a shared Frozen\nConvolutional CLIP backbone, which not only significantly simplifies the\ncurrent two-stage pipeline, but also remarkably yields a better accuracy-cost\ntrade-off. The proposed FC-CLIP, benefits from the following observations: the\nfrozen CLIP backbone maintains the ability of open-vocabulary classification\nand can also serve as a strong mask generator, and the convolutional CLIP\ngeneralizes well to a larger input resolution than the one used during\ncontrastive image-text pretraining. When training on COCO panoptic data only\nand testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1\nmIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2\nmIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoU\non ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,\nrespectively. Additionally, the training and testing time of FC-CLIP is 7.5x\nand 6.6x significantly faster than the same prior art, while using 5.9x fewer\nparameters. FC-CLIP also sets a new state-of-the-art performance across various\nopen-vocabulary semantic segmentation datasets. Code at\nhttps://github.com/bytedance/fc-clip\n","authors":["Qihang Yu","Ju He","Xueqing Deng","Xiaohui Shen","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2308.02487v2.pdf","comment":"NeurIPS 2023 camera ready. code and model available at\n  https://github.com/bytedance/fc-clip"},{"id":"http://arxiv.org/abs/2311.08439v1","updated":"2023-11-14T15:10:05Z","published":"2023-11-14T15:10:05Z","title":"A Unified Approach for Comprehensive Analysis of Various Spectral and\n  Tissue Doppler Echocardiography","summary":"  Doppler echocardiography offers critical insights into cardiac function and\nphases by quantifying blood flow velocities and evaluating myocardial motion.\nHowever, previous methods for automating Doppler analysis, ranging from initial\nsignal processing techniques to advanced deep learning approaches, have been\nconstrained by their reliance on electrocardiogram (ECG) data and their\ninability to process Doppler views collectively. We introduce a novel unified\nframework using a convolutional neural network for comprehensive analysis of\nspectral and tissue Doppler echocardiography images that combines automatic\nmeasurements and end-diastole (ED) detection into a singular method. The\nnetwork automatically recognizes key features across various Doppler views,\nwith novel Doppler shape embedding and anti-aliasing modules enhancing\ninterpretation and ensuring consistent analysis. Empirical results indicate a\nconsistent outperformance in performance metrics, including dice similarity\ncoefficients (DSC) and intersection over union (IoU). The proposed framework\ndemonstrates strong agreement with clinicians in Doppler automatic measurements\nand competitive performance in ED detection.\n","authors":["Jaeik Jeon","Jiyeon Kim","Yeonggul Jang","Yeonyee E. Yoon","Dawun Jeong","Youngtaek Hong","Seung-Ah Lee","Hyuk-Jae Chang"],"pdf_url":"https://arxiv.org/pdf/2311.08439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08438v1","updated":"2023-11-14T14:27:53Z","published":"2023-11-14T14:27:53Z","title":"LocaliseBot: Multi-view 3D object localisation with differentiable\n  rendering for robot grasping","summary":"  Robot grasp typically follows five stages: object detection, object\nlocalisation, object pose estimation, grasp pose estimation, and grasp\nplanning. We focus on object pose estimation. Our approach relies on three\npieces of information: multiple views of the object, the camera's extrinsic\nparameters at those viewpoints, and 3D CAD models of objects. The first step\ninvolves a standard deep learning backbone (FCN ResNet) to estimate the object\nlabel, semantic segmentation, and a coarse estimate of the object pose with\nrespect to the camera. Our novelty is using a refinement module that starts\nfrom the coarse pose estimate and refines it by optimisation through\ndifferentiable rendering. This is a purely vision-based approach that avoids\nthe need for other information such as point cloud or depth images. We evaluate\nour object pose estimation approach on the ShapeNet dataset and show\nimprovements over the state of the art. We also show that the estimated object\npose results in 99.65% grasp accuracy with the ground truth grasp candidates on\nthe Object Clutter Indoor Dataset (OCID) Grasp dataset, as computed using\nstandard practice.\n","authors":["Sujal Vijayaraghavan","Redwan Alqasemi","Rajiv Dubey","Sudeep Sarkar"],"pdf_url":"https://arxiv.org/pdf/2311.08438v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.08402v1","updated":"2023-11-14T18:59:24Z","published":"2023-11-14T18:59:24Z","title":"Retrieve and Copy: Scaling ASR Personalization to Large Catalogs","summary":"  Personalization of automatic speech recognition (ASR) models is a widely\nstudied topic because of its many practical applications. Most recently,\nattention-based contextual biasing techniques are used to improve the\nrecognition of rare words and domain specific entities. However, due to\nperformance constraints, the biasing is often limited to a few thousand\nentities, restricting real-world usability. To address this, we first propose a\n\"Retrieve and Copy\" mechanism to improve latency while retaining the accuracy\neven when scaled to a large catalog. We also propose a training strategy to\novercome the degradation in recall at such scale due to an increased number of\nconfusing entities. Overall, our approach achieves up to 6% more Word Error\nRate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a\nstrong baseline. Our method also allows for large catalog sizes of up to 20K\nwithout significantly affecting WER and F1-scores, while achieving at least 20%\ninference speedup per acoustic frame.\n","authors":["Sai Muralidhar Jayanthi","Devang Kulshreshtha","Saket Dingliwal","Srikanth Ronanki","Sravan Bodapati"],"pdf_url":"https://arxiv.org/pdf/2311.08402v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.08350v1","updated":"2023-11-14T17:48:27Z","published":"2023-11-14T17:48:27Z","title":"ChoralSynth: Synthetic Dataset of Choral Singing","summary":"  Choral singing, a widely practiced form of ensemble singing, lacks\ncomprehensive datasets in the realm of Music Information Retrieval (MIR)\nresearch, due to challenges arising from the requirement to curate multitrack\nrecordings. To address this, we devised a novel methodology, leveraging\nstate-of-the-art synthesizers to create and curate quality renditions. The\nscores were sourced from Choral Public Domain Library(CPDL). This work is done\nin collaboration with a diverse team of musicians, software engineers and\nresearchers. The resulting dataset, complete with its associated metadata, and\nmethodology is released as part of this work, opening up new avenues for\nexploration and advancement in the field of singing voice research.\n","authors":["Jyoti Narang","Viviana De La Vega","Xavier Lizarraga","Oscar Mayor","Hector Parra","Jordi Janer","Xavier Serra"],"pdf_url":"https://arxiv.org/pdf/2311.08350v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2311.08302v1","updated":"2023-11-14T16:46:10Z","published":"2023-11-14T16:46:10Z","title":"Inverse Learning with Extremely Sparse Feedback for Recommendation","summary":"  Modern personalized recommendation services often rely on user feedback,\neither explicit or implicit, to improve the quality of services. Explicit\nfeedback refers to behaviors like ratings, while implicit feedback refers to\nbehaviors like user clicks. However, in the scenario of full-screen video\nviewing experiences like Tiktok and Reels, the click action is absent,\nresulting in unclear feedback from users, hence introducing noises in modeling\ntraining. Existing approaches on de-noising recommendation mainly focus on\npositive instances while ignoring the noise in a large amount of sampled\nnegative feedback. In this paper, we propose a meta-learning method to annotate\nthe unlabeled data from loss and gradient perspectives, which considers the\nnoises in both positive and negative instances. Specifically, we first propose\nan Inverse Dual Loss (IDL) to boost the true label learning and prevent the\nfalse label learning. Then we further propose an Inverse Gradient (IG) method\nto explore the correct updating gradient and adjust the updating based on\nmeta-learning. Finally, we conduct extensive experiments on both benchmark and\nindustrial datasets where our proposed method can significantly improve AUC by\n9.25% against state-of-the-art methods. Further analysis verifies the proposed\ninverse learning framework is model-agnostic and can improve a variety of\nrecommendation backbones. The source code, along with the best hyper-parameter\nsettings, is available at this link:\nhttps://github.com/Guanyu-Lin/InverseLearning.\n","authors":["Guanyu Lin","Chen Gao","Yu Zheng","Yinfeng Li","Jianxin Chang","Yanan Niu","Yang Song","Kun Gai","Zhiheng Li","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2311.08302v1.pdf","comment":"WSDM 2024"},{"id":"http://arxiv.org/abs/2311.08272v1","updated":"2023-11-14T16:07:16Z","published":"2023-11-14T16:07:16Z","title":"Mixed Attention Network for Cross-domain Sequential Recommendation","summary":"  In modern recommender systems, sequential recommendation leverages\nchronological user behaviors to make effective next-item suggestions, which\nsuffers from data sparsity issues, especially for new users. One promising line\nof work is the cross-domain recommendation, which trains models with data\nacross multiple domains to improve the performance in data-scarce domains.\nRecent proposed cross-domain sequential recommendation models such as PiNet and\nDASL have a common drawback relying heavily on overlapped users in different\ndomains, which limits their usage in practical recommender systems. In this\npaper, we propose a Mixed Attention Network (MAN) with local and global\nattention modules to extract the domain-specific and cross-domain information.\nFirstly, we propose a local/global encoding layer to capture the\ndomain-specific/cross-domain sequential pattern. Then we propose a mixed\nattention layer with item similarity attention, sequence-fusion attention, and\ngroup-prototype attention to capture the local/global item similarity, fuse the\nlocal/global item sequence, and extract the user groups across different\ndomains, respectively. Finally, we propose a local/global prediction layer to\nfurther evolve and combine the domain-specific and cross-domain interests.\nExperimental results on two real-world datasets (each with two domains)\ndemonstrate the superiority of our proposed model. Further study also\nillustrates that our proposed method and components are model-agnostic and\neffective, respectively. The code and data are available at\nhttps://github.com/Guanyu-Lin/MAN.\n","authors":["Guanyu Lin","Chen Gao","Yu Zheng","Jianxin Chang","Yanan Niu","Yang Song","Kun Gai","Zhiheng Li","Depeng Jin","Yong Li","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08272v1.pdf","comment":"WSDM 2024"},{"id":"http://arxiv.org/abs/2311.08252v1","updated":"2023-11-14T15:43:47Z","published":"2023-11-14T15:43:47Z","title":"REST: Retrieval-Based Speculative Decoding","summary":"  We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.\n","authors":["Zhenyu He","Zexuan Zhong","Tianle Cai","Jason D Lee","Di He"],"pdf_url":"https://arxiv.org/pdf/2311.08252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08103v1","updated":"2023-11-14T12:03:26Z","published":"2023-11-14T12:03:26Z","title":"Exploring Semi-supervised Hierarchical Stacked Encoder for Legal\n  Judgement Prediction","summary":"  Predicting the judgment of a legal case from its unannotated case facts is a\nchallenging task. The lengthy and non-uniform document structure poses an even\ngreater challenge in extracting information for decision prediction. In this\nwork, we explore and propose a two-level classification mechanism; both\nsupervised and unsupervised; by using domain-specific pre-trained BERT to\nextract information from long documents in terms of sentence embeddings further\nprocessing with transformer encoder layer and use unsupervised clustering to\nextract hidden labels from these embeddings to better predict a judgment of a\nlegal case. We conduct several experiments with this mechanism and see higher\nperformance gains than the previously proposed methods on the ILDC dataset. Our\nexperimental results also show the importance of domain-specific pre-training\nof Transformer Encoders in legal information processing.\n","authors":["Nishchal Prasad","Mohand Boughanem","Taoufiq Dkaki"],"pdf_url":"https://arxiv.org/pdf/2311.08103v1.pdf","comment":"Published in the 1st International Workshop on Legal Information\n  Retrieval at ECIR 2023, April 2nd 2023, Dublin, Ireland.\n  (https://tmr.liacs.nl/legalIR/)"},{"id":"http://arxiv.org/abs/2311.08002v1","updated":"2023-11-14T08:57:01Z","published":"2023-11-14T08:57:01Z","title":"TempTabQA: Temporal Question Answering for Semi-Structured Tables","summary":"  Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.\n","authors":["Vivek Gupta","Pranshu Kandoi","Mahek Bhavesh Vora","Shuo Zhang","Yujie He","Ridho Reinanda","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2311.08002v1.pdf","comment":"EMNLP 2023(Main), 23 Figures, 32 Tables"},{"id":"http://arxiv.org/abs/2311.07994v1","updated":"2023-11-14T08:47:51Z","published":"2023-11-14T08:47:51Z","title":"Text Retrieval with Multi-Stage Re-Ranking Models","summary":"  The text retrieval is the task of retrieving similar documents to a search\nquery, and it is important to improve retrieval accuracy while maintaining a\ncertain level of retrieval speed. Existing studies have reported accuracy\nimprovements using language models, but many of these do not take into account\nthe reduction in search speed that comes with increased performance. In this\nstudy, we propose three-stage re-ranking model using model ensembles or larger\nlanguage models to improve search accuracy while minimizing the search delay.\nWe ranked the documents by BM25 and language models, and then re-ranks by a\nmodel ensemble or a larger language model for documents with high similarity to\nthe query. In our experiments, we train the MiniLM language model on the\nMS-MARCO dataset and evaluate it in a zero-shot setting. Our proposed method\nachieves higher retrieval accuracy while reducing the retrieval speed decay.\n","authors":["Yuichi Sasazawa","Kenichi Yokote","Osamu Imaichi","Yasuhiro Sogawa"],"pdf_url":"https://arxiv.org/pdf/2311.07994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07947v1","updated":"2023-11-14T06:57:20Z","published":"2023-11-14T06:57:20Z","title":"Towards a Technical Debt for Recommender System","summary":"  Balancing the management of technical debt within recommender systems\nrequires effectively juggling the introduction of new features with the ongoing\nmaintenance and enhancement of the current system. Within the realm of\nrecommender systems, technical debt encompasses the trade-offs and expedient\nchoices made during the development and upkeep of the recommendation system,\nwhich could potentially have adverse effects on its long-term performance,\nscalability, and maintainability. In this vision paper, our objective is to\nkickstart a research direction regarding Technical Debt in Recommender Systems.\nWe identified 15 potential factors, along with detailed explanations outlining\nwhy it is advisable to consider them.\n","authors":["Sergio Moreschini","Ludovik Coba","Valentina Lenarduzzi"],"pdf_url":"https://arxiv.org/pdf/2311.07947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12810v2","updated":"2023-11-14T04:52:03Z","published":"2023-07-24T14:00:07Z","title":"HeteFedRec: Federated Recommender Systems with Model Heterogeneity","summary":"  Owing to the nature of privacy protection, federated recommender systems\n(FedRecs) have garnered increasing interest in the realm of on-device\nrecommender systems. However, most existing FedRecs only allow participating\nclients to collaboratively train a recommendation model of the same public\nparameter size. Training a model of the same size for all clients can lead to\nsuboptimal performance since clients possess varying resources. For example,\nclients with limited training data may prefer to train a smaller recommendation\nmodel to avoid excessive data consumption, while clients with sufficient data\nwould benefit from a larger model to achieve higher recommendation accuracy. To\naddress the above challenge, this paper introduces HeteFedRec, a novel FedRec\nframework that enables the assignment of personalized model sizes to\nparticipants. In HeteFedRec, we present a heterogeneous recommendation model\naggregation strategy, including a unified dual-task learning mechanism and a\ndimensional decorrelation regularization, to allow knowledge aggregation among\nrecommender models of different sizes. Additionally, a relation-based ensemble\nknowledge distillation method is proposed to effectively distil knowledge from\nheterogeneous item embeddings. Extensive experiments conducted on three\nreal-world recommendation datasets demonstrate the effectiveness and efficiency\nof HeteFedRec in training federated recommender systems under heterogeneous\nsettings.\n","authors":["Wei Yuan","Liang Qu","Lizhen Cui","Yongxin Tong","Xiaofang Zhou","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2307.12810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07878v1","updated":"2023-11-14T03:15:48Z","published":"2023-11-14T03:15:48Z","title":"Evaluating LLMs on Document-Based QA: Exact Answer Selection and\n  Numerical Extraction using Cogtale datase","summary":"  Document-based Question-Answering (QA) tasks are crucial for precise\ninformation retrieval. While some existing work focus on evaluating large\nlanguage model's performance on retrieving and answering questions from\ndocuments, assessing the LLMs' performance on QA types that require exact\nanswer selection from predefined options and numerical extraction is yet to be\nfully assessed. In this paper, we specifically focus on this underexplored\ncontext and conduct empirical analysis of LLMs (GPT-4 and GPT 3.5) on question\ntypes, including single-choice, yes-no, multiple-choice, and number extraction\nquestions from documents. We use the Cogtale dataset for evaluation, which\nprovide human expert-tagged responses, offering a robust benchmark for\nprecision and factual grounding. We found that LLMs, particularly GPT-4, can\nprecisely answer many single-choice and yes-no questions given relevant\ncontext, demonstrating their efficacy in information retrieval tasks. However,\ntheir performance diminishes when confronted with multiple-choice and number\nextraction formats, lowering the overall performance of the model on this task,\nindicating that these models may not be reliable for the task. This limits the\napplications of LLMs on applications demanding precise information extraction\nfrom documents, such as meta-analysis tasks. However, these findings hinge on\nthe assumption that the retrievers furnish pertinent context necessary for\naccurate responses, emphasizing the need for further research on the efficacy\nof retriever mechanisms in enhancing question-answering performance. Our work\noffers a framework for ongoing dataset evaluation, ensuring that LLM\napplications for information retrieval and document analysis continue to meet\nevolving standards.\n","authors":["Zafaryab Rasool","Scott Barnett","Stefanus Kurniawan","Sherwin Balugo","Rajesh Vasa","Courtney Chesser","Alex Bahar-Fuchs"],"pdf_url":"https://arxiv.org/pdf/2311.07878v1.pdf","comment":"14 pages, 1 figure, 8 tables"},{"id":"http://arxiv.org/abs/2311.07870v1","updated":"2023-11-14T03:00:50Z","published":"2023-11-14T03:00:50Z","title":"AutoML for Large Capacity Modeling of Meta Ranking Systems","summary":"  Web-scale ranking systems at Meta serving billions of users is complex.\nImproving ranking models is essential but engineering heavy. Automated Machine\nLearning (AutoML) can release engineers from labor intensive work of tuning\nranking models; however, it is unknown if AutoML is efficient enough to meet\ntight production timeline in real-world and, at the same time, bring additional\nimprovements to the strong baselines. Moreover, to achieve higher ranking\nperformance, there is an ever-increasing demand to scale up ranking models to\neven larger capacity, which imposes more challenges on the efficiency. The\nlarge scale of models and tight production schedule requires AutoML to\noutperform human baselines by only using a small number of model evaluation\ntrials (around 100). We presents a sampling-based AutoML method, focusing on\nneural architecture search and hyperparameter optimization, addressing these\nchallenges in Meta-scale production when building large capacity models. Our\napproach efficiently handles large-scale data demands. It leverages a\nlightweight predictor-based searcher and reinforcement learning to explore vast\nsearch spaces, significantly reducing the number of model evaluations. Through\nexperiments in large capacity modeling for CTR and CVR applications, we show\nthat our method achieves outstanding Return on Investment (ROI) versus human\ntuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or\n$25\\%$ Query per Second (QPS) increase by only sampling one hundred models on\naverage from a curated search space. The proposed AutoML method has already\nmade real-world impact where a discovered Instagram CTR model with up to -0.36%\nNE gain (over existing production baseline) was selected for large-scale online\nA/B test and show statistically significant gain. These production results\nproved AutoML efficacy and accelerated its adoption in ranking systems at Meta.\n","authors":["Hang Yin","Kuang-Hung Liu","Mengying Sun","Yuxin Chen","Buyun Zhang","Jiang Liu","Vivek Sehgal","Rudresh Rajnikant Panchal","Eugen Hotaj","Xi Liu","Daifeng Guo","Jamey Zhang","Zhou Wang","Shali Jiang","Huayu Li","Zhengxing Chen","Wen-Yen Chen","Jiyan Yang","Wei Wen"],"pdf_url":"https://arxiv.org/pdf/2311.07870v1.pdf","comment":"Hang Yin and Kuang-Hung Liu contribute equally"},{"id":"http://arxiv.org/abs/2303.02867v3","updated":"2023-11-14T02:32:41Z","published":"2023-03-06T03:36:06Z","title":"Boundary-semantic collaborative guidance network with dual-stream\n  feedback mechanism for salient object detection in optical remote sensing\n  imagery","summary":"  With the increasing application of deep learning in various domains, salient\nobject detection in optical remote sensing images (ORSI-SOD) has attracted\nsignificant attention. However, most existing ORSI-SOD methods predominantly\nrely on local information from low-level features to infer salient boundary\ncues and supervise them using boundary ground truth, but fail to sufficiently\noptimize and protect the local information, and almost all approaches ignore\nthe potential advantages offered by the last layer of the decoder to maintain\nthe integrity of saliency maps. To address these issues, we propose a novel\nmethod named boundary-semantic collaborative guidance network (BSCGNet) with\ndual-stream feedback mechanism. First, we propose a boundary protection\ncalibration (BPC) module, which effectively reduces the loss of edge position\ninformation during forward propagation and suppresses noise in low-level\nfeatures without relying on boundary ground truth. Second, based on the BPC\nmodule, a dual feature feedback complementary (DFFC) module is proposed, which\naggregates boundary-semantic dual features and provides effective feedback to\ncoordinate features across different layers, thereby enhancing cross-scale\nknowledge communication. Finally, to obtain more complete saliency maps, we\nconsider the uniqueness of the last layer of the decoder for the first time and\npropose the adaptive feedback refinement (AFR) module, which further refines\nfeature representation and eliminates differences between features through a\nunique feedback mechanism. Extensive experiments on three benchmark datasets\ndemonstrate that BSCGNet exhibits distinct advantages in challenging scenarios\nand outperforms the 17 state-of-the-art (SOTA) approaches proposed in recent\nyears. Codes and results have been released on GitHub:\nhttps://github.com/YUHsss/BSCGNet.\n","authors":["Dejun Feng","Hongyu Chen","Suning Liu","Ziyang Liao","Xingyu Shen","Yakun Xie","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.02867v3.pdf","comment":"Accepted by TGRS"},{"id":"http://arxiv.org/abs/2311.07861v1","updated":"2023-11-14T02:25:18Z","published":"2023-11-14T02:25:18Z","title":"Overview of the TREC 2023 Product Product Search Track","summary":"  This is the first year of the TREC Product search track. The focus this year\nwas the creation of a reusable collection and evaluation of the impact of the\nuse of metadata and multi-modal data on retrieval accuracy. This year we\nleverage the new product search corpus, which includes contextual metadata. Our\nanalysis shows that in the product search domain, traditional retrieval systems\nare highly effective and commonly outperform general-purpose pretrained\nembedding models. Our analysis also evaluates the impact of using simplified\nand metadata-enhanced collections, finding no clear trend in the impact of the\nexpanded collection. We also see some surprising outcomes; despite their\nwidespread adoption and competitive performance on other tasks, we find\nsingle-stage dense retrieval runs can commonly be noncompetitive or generate\nlow-quality results both in the zero-shot and fine-tuned domain.\n","authors":["Daniel Campos","Surya Kallumadi","Corby Rosset","Cheng Xiang Zhai","Alessandro Magnani"],"pdf_url":"https://arxiv.org/pdf/2311.07861v1.pdf","comment":"14 pages, 4 figures, 11 tables - TREC 2023"},{"id":"http://arxiv.org/abs/2311.07838v1","updated":"2023-11-14T01:38:02Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with corresponding supporting documents, which enables the user to\nflexibly verify the answer and makes it more trustworthy. Its evaluation not\nonly measures the correctness of the answer, but also the answer's\nverifiability, i.e., how well the answer is supported by the corresponding\ndocuments. In typical, verifiable generation adopts the retrieval-read\npipeline, which is divided into two stages: 1) retrieve relevant documents of\nthe question. 2) according to the documents, generate the corresponding answer.\nSince the retrieved documents can supplement knowledge for the LLM to generate\nthe answer and serve as evidence, the retrieval stage is essential for the\ncorrectness and verifiability of the answer. However, the widely used\nretrievers become the bottleneck of the entire pipeline and limit the overall\nperformance. They often have fewer parameters than the large language model and\nhave not been proven to scale well to the size of LLMs. Since the LLM passively\nreceives the retrieval result, if the retriever does not correctly find the\nsupporting documents, the LLM can not generate the correct and verifiable\nanswer, which overshadows the LLM's remarkable abilities. In this paper, we\npropose LLatrieval (Large Language Model Verified Retrieval), where the LLM\nupdates the retrieval result until it verifies that the retrieved documents can\nsupport answering the question. Thus, the LLM can iteratively provide feedback\nto retrieval and facilitate the retrieval result to sufficiently support\nverifiable generation. Experimental results show that our method significantly\noutperforms extensive baselines and achieves new state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08597v1","updated":"2023-11-14T23:41:15Z","published":"2023-11-14T23:41:15Z","title":"Stopping Methods for Technology Assisted Reviews based on Point\n  Processes","summary":"  Technology Assisted Review (TAR), which aims to reduce the effort required to\nscreen collections of documents for relevance, is used to develop systematic\nreviews of medical evidence and identify documents that must be disclosed in\nresponse to legal proceedings. Stopping methods are algorithms which determine\nwhen to stop screening documents during the TAR process, helping to ensure that\nworkload is minimised while still achieving a high level of recall. This paper\nproposes a novel stopping method based on point processes, which are\nstatistical models that can be used to represent the occurrence of random\nevents. The approach uses rate functions to model the occurrence of relevant\ndocuments in the ranking and compares four candidates, including one that has\nnot previously been used for this purpose (hyperbolic). Evaluation is carried\nout using standard datasets (CLEF e-Health, TREC Total Recall, TREC Legal), and\nthis work is the first to explore stopping method robustness by reporting\nperformance on a range of rankings of varying effectiveness. Results show that\nthe proposed method achieves the desired level of recall without requiring an\nexcessive number of documents to be examined in the majority of cases and also\ncompares well against multiple alternative approaches.\n","authors":["Mark Stevenson","Reem Bin-Hezam"],"pdf_url":"https://arxiv.org/pdf/2311.08597v1.pdf","comment":"Accepted by ACM Transactions on Information Systems (TOIS)"},{"id":"http://arxiv.org/abs/2311.08593v1","updated":"2023-11-14T23:28:36Z","published":"2023-11-14T23:28:36Z","title":"ACID: Abstractive, Content-Based IDs for Document Retrieval with\n  Language Models","summary":"  Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a new approach\nfor end-to-end document retrieval that directly generates document identifiers\ngiven an input query. Techniques for designing effective, high-quality document\nIDs remain largely unexplored. We introduce ACID, in which each document's ID\nis composed of abstractive keyphrases generated by a large language model,\nrather than an integer ID sequence as done in past work. We compare our method\nwith the current state-of-the-art technique for ID generation, which produces\nIDs through hierarchical clustering of document embeddings. We also examine\nsimpler methods to generate natural-language document IDs, including the naive\napproach of using the first k words of each document as its ID or words with\nhigh BM25 scores in that document. We show that using ACID improves top-10 and\ntop-20 accuracy by 15.6% and 14.4% (relative) respectively versus the\nstate-of-the-art baseline on the MSMARCO 100k retrieval task, and 4.4% and 4.0%\nrespectively on the Natural Questions 100k retrieval task. Our results\ndemonstrate the effectiveness of human-readable, natural-language IDs in\ngenerative retrieval with LMs. The code for reproducing our results and the\nkeyword-augmented datasets will be released on formal publication.\n","authors":["Haoxin Li","Phillip Keung","Daniel Cheng","Jungo Kasai","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2311.08593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08430v1","updated":"2023-11-14T03:02:02Z","published":"2023-11-14T03:02:02Z","title":"Rankitect: Ranking Architecture Search Battling World-class Engineers at\n  Meta Scale","summary":"  Neural Architecture Search (NAS) has demonstrated its efficacy in computer\nvision and potential for ranking systems. However, prior work focused on\nacademic problems, which are evaluated at small scale under well-controlled\nfixed baselines. In industry system, such as ranking system in Meta, it is\nunclear whether NAS algorithms from the literature can outperform production\nbaselines because of: (1) scale - Meta ranking systems serve billions of users,\n(2) strong baselines - the baselines are production models optimized by\nhundreds to thousands of world-class engineers for years since the rise of deep\nlearning, (3) dynamic baselines - engineers may have established new and\nstronger baselines during NAS search, and (4) efficiency - the search pipeline\nmust yield results quickly in alignment with the productionization life cycle.\nIn this paper, we present Rankitect, a NAS software framework for ranking\nsystems at Meta. Rankitect seeks to build brand new architectures by composing\nlow level building blocks from scratch. Rankitect implements and improves\nstate-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under\nthe same search space, including sampling-based NAS, one-shot NAS, and\nDifferentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple\nproduction ranking models at Meta. We find that Rankitect can discover new\nmodels from scratch achieving competitive tradeoff between Normalized Entropy\nloss and FLOPs. When utilizing search space designed by engineers, Rankitect\ncan generate better models than engineers, achieving positive offline\nevaluation and online A/B test at Meta scale.\n","authors":["Wei Wen","Kuang-Hung Liu","Igor Fedorov","Xin Zhang","Hang Yin","Weiwei Chu","Kaveh Hassani","Mengying Sun","Jiang Liu","Xu Wang","Lin Jiang","Yuxin Chen","Buyun Zhang","Xi Liu","Dehua Cheng","Zhengxing Chen","Guang Zhao","Fangqiu Han","Jiyan Yang","Yuchen Hao","Liang Xiong","Wen-Yen Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08430v1.pdf","comment":"Wei Wen and Kuang-Hung Liu contribute equally"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.08403v1","updated":"2023-11-14T18:59:59Z","published":"2023-11-14T18:59:59Z","title":"Instant3D: Instant Text-to-3D Generation","summary":"  Text-to-3D generation, which aims to synthesize vivid 3D objects from text\nprompts, has attracted much attention from the computer vision community. While\nseveral existing works have achieved impressive results for this task, they\nmainly rely on a time-consuming optimization paradigm. Specifically, these\nmethods optimize a neural field from scratch for each text prompt, taking\napproximately one hour or more to generate one object. This heavy and\nrepetitive training cost impedes their practical deployment. In this paper, we\npropose a novel framework for fast text-to-3D generation, dubbed Instant3D.\nOnce trained, Instant3D is able to create a 3D object for an unseen text prompt\nin less than one second with a single run of a feedforward network. We achieve\nthis remarkable speed by devising a new network that directly constructs a 3D\ntriplane from a text prompt. The core innovation of our Instant3D lies in our\nexploration of strategies to effectively inject text conditions into the\nnetwork. Furthermore, we propose a simple yet effective activation function,\nthe scaled-sigmoid, to replace the original sigmoid function, which speeds up\nthe training convergence by more than ten times. Finally, to address the Janus\n(multi-head) problem in 3D generation, we propose an adaptive Perp-Neg\nalgorithm that can dynamically adjust its concept negation scales according to\nthe severity of the Janus problem during training, effectively reducing the\nmulti-head effect. Extensive experiments on a wide variety of benchmark\ndatasets demonstrate that the proposed algorithm performs favorably against the\nstate-of-the-art methods both qualitatively and quantitatively, while achieving\nsignificantly better efficiency. The project page is at\nhttps://ming1993li.github.io/Instant3DProj.\n","authors":["Ming Li","Pan Zhou","Jia-Wei Liu","Jussi Keppo","Min Lin","Shuicheng Yan","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2311.08403v1.pdf","comment":"Project page: https://ming1993li.github.io/Instant3DProj"},{"id":"http://arxiv.org/abs/2311.08401v1","updated":"2023-11-14T18:59:15Z","published":"2023-11-14T18:59:15Z","title":"Fine-tuning Language Models for Factuality","summary":"  The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.\n","authors":["Katherine Tian","Eric Mitchell","Huaxiu Yao","Christopher D. Manning","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2311.08401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08393v1","updated":"2023-11-14T18:53:28Z","published":"2023-11-14T18:53:28Z","title":"MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable\n  Trajectory Generation","summary":"  The learn-from-observation (LfO) paradigm is a human-inspired mode for a\nrobot to learn to perform a task simply by watching it being performed. LfO can\nfacilitate robot integration on factory floors by minimizing disruption and\nreducing tedious programming. A key component of the LfO pipeline is a\ntransformation of the depth camera frames to the corresponding task state and\naction pairs, which are then relayed to learning techniques such as imitation\nor inverse reinforcement learning for understanding the task parameters. While\nseveral existing computer vision models analyze videos for activity\nrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,\nSA-Net and many other models analyze frame data captured from a single\nviewpoint. Their analysis is therefore highly sensitive to occlusions of the\nobserved task, which are frequent in deployments. An obvious way of reducing\nocclusions is to simultaneously observe the task from multiple viewpoints and\nsynchronously fuse the multiple streams in the model. Toward this, we present\nmulti-view SA-Net, which generalizes the SA-Net model to allow the perception\nof multiple viewpoints of the task activity, integrate them, and better\nrecognize the state and action in each frame. Performance evaluations on two\ndistinct domains establish that MVSA-Net recognizes the state-action pairs\nunder occlusion more accurately compared to single-view MVSA-Net and other\nbaselines. Our ablation studies further evaluate its performance under\ndifferent ambient conditions and establish the contribution of the architecture\ncomponents. As such, MVSA-Net offers a significantly more robust and deployable\nstate-action trajectory generation compared to previous methods.\n","authors":["Ehsan Asali","Prashant Doshi","Jin Sun"],"pdf_url":"https://arxiv.org/pdf/2311.08393v1.pdf","comment":"Conference on Robot Learning 2023 (CoRL2023)"},{"id":"http://arxiv.org/abs/2311.08384v1","updated":"2023-11-14T18:45:56Z","published":"2023-11-14T18:45:56Z","title":"Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees","summary":"  Hybrid RL is the setting where an RL agent has access to both offline data\nand online data by interacting with the real-world environment. In this work,\nwe propose a new hybrid RL algorithm that combines an on-policy actor-critic\nmethod with offline data. On-policy methods such as policy gradient and natural\npolicy gradient (NPG) have shown to be more robust to model misspecification,\nthough sometimes it may not be as sample efficient as methods that rely on\noff-policy learning. On the other hand, offline methods that depend on\noff-policy training often require strong assumptions in theory and are less\nstable to train in practice. Our new approach integrates a procedure of\noff-policy training on the offline data into an on-policy NPG framework. We\nshow that our approach, in theory, can obtain a best-of-both-worlds type of\nresult -- it achieves the state-of-art theoretical guarantees of offline RL\nwhen offline RL-specific assumptions hold, while at the same time maintaining\nthe theoretical guarantees of on-policy NPG regardless of the offline RL\nassumptions' validity. Experimentally, in challenging rich-observation\nenvironments, we show that our approach outperforms a state-of-the-art hybrid\nRL baseline which only relies on off-policy policy optimization, demonstrating\nthe empirical benefit of combining on-policy and off-policy learning. Our code\nis publicly available at https://github.com/YifeiZhou02/HNPG.\n","authors":["Yifei Zhou","Ayush Sekhari","Yuda Song","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2311.08384v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2311.08379v1","updated":"2023-11-14T18:42:40Z","published":"2023-11-14T18:42:40Z","title":"Scheming AIs: Will AIs fake alignment during training in order to get\n  power?","summary":"  This report examines whether advanced AIs that perform well in training will\nbe doing so in order to gain power later -- a behavior I call \"scheming\" (also\nsometimes called \"deceptive alignment\"). I conclude that scheming is a\ndisturbingly plausible outcome of using baseline machine learning methods to\ntrain goal-directed AIs sophisticated enough to scheme (my subjective\nprobability on such an outcome, given these conditions, is roughly 25%). In\nparticular: if performing well in training is a good strategy for gaining power\n(as I think it might well be), then a very wide variety of goals would motivate\nscheming -- and hence, good training performance. This makes it plausible that\ntraining might either land on such a goal naturally and then reinforce it, or\nactively push a model's motivations towards such a goal as an easy way of\nimproving performance. What's more, because schemers pretend to be aligned on\ntests designed to reveal their motivations, it may be quite difficult to tell\nwhether this has occurred. However, I also think there are reasons for comfort.\nIn particular: scheming may not actually be such a good strategy for gaining\npower; various selection pressures in training might work against schemer-like\ngoals (for example, relative to non-schemers, schemers need to engage in extra\ninstrumental reasoning, which might harm their training performance); and we\nmay be able to increase such pressures intentionally. The report discusses\nthese and a wide variety of other considerations in detail, and it suggests an\narray of empirical research directions for probing the topic further.\n","authors":["Joe Carlsmith"],"pdf_url":"https://arxiv.org/pdf/2311.08379v1.pdf","comment":"127 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.08376v1","updated":"2023-11-14T18:41:28Z","published":"2023-11-14T18:41:28Z","title":"Ensemble sampling for linear bandits: small ensembles suffice","summary":"  We provide the first useful, rigorous analysis of ensemble sampling for the\nstochastic linear bandit setting. In particular, we show that, under standard\nassumptions, for a $d$-dimensional stochastic linear bandit with an interaction\nhorizon $T$, ensemble sampling with an ensemble of size $m$ on the order of $d\n\\log T$ incurs regret bounded by order $(d \\log T)^{5/2} \\sqrt{T}$. Ours is the\nfirst result in any structured setting not to require the size of the ensemble\nto scale linearly with $T$ -- which defeats the purpose of ensemble sampling --\nwhile obtaining near $\\sqrt{T}$ order regret. Ours is also the first result\nthat allows infinite action sets.\n","authors":["David Janz","Alexander E. Litvak","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2311.08376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08364v1","updated":"2023-11-14T18:14:56Z","published":"2023-11-14T18:14:56Z","title":"Plum: Prompt Learning using Metaheuristic","summary":"  Since the emergence of large language models, prompt learning has become a\npopular method for optimizing and customizing these models. Special prompts,\nsuch as Chain-of-Thought, have even revealed previously unknown reasoning\ncapabilities within these models. However, the progress of discovering\neffective prompts has been slow, driving a desire for general prompt\noptimization methods. Unfortunately, few existing prompt learning methods\nsatisfy the criteria of being truly \"general\", i.e., automatic, discrete,\nblack-box, gradient-free, and interpretable all at once. In this paper, we\nintroduce metaheuristics, a branch of discrete non-convex optimization methods\nwith over 100 options, as a promising approach to prompt learning. Within our\nparadigm, we test six typical methods: hill climbing, simulated annealing,\ngenetic algorithms with/without crossover, tabu search, and harmony search,\ndemonstrating their effectiveness in black-box prompt learning and\nChain-of-Thought prompt tuning. Furthermore, we show that these methods can be\nused to discover more human-understandable prompts that were previously\nunknown, opening the door to a cornucopia of possibilities in prompt\noptimization. We release all the codes in\n\\url{https://github.com/research4pan/Plum}.\n","authors":["Rui Pan","Shuo Xing","Shizhe Diao","Xiang Liu","Kashun Shum","Jipeng Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14421v3","updated":"2023-11-14T18:13:13Z","published":"2023-10-19T10:36:02Z","title":"On existence, uniqueness and scalability of adversarial robustness\n  measures for AI classifiers","summary":"  Simply-verifiable mathematical conditions for existence, uniqueness and\nexplicit analytical computation of minimal adversarial paths (MAP) and minimal\nadversarial distances (MAD) for (locally) uniquely-invertible classifiers, for\ngeneralized linear models (GLM), and for entropic AI (EAI) are formulated and\nproven. Practical computation of MAP and MAD, their comparison and\ninterpretations for various classes of AI tools (for neuronal networks, boosted\nrandom forests, GLM and EAI) are demonstrated on the common synthetic\nbenchmarks: on a double Swiss roll spiral and its extensions, as well as on the\ntwo biomedical data problems (for the health insurance claim predictions, and\nfor the heart attack lethality classification). On biomedical applications it\nis demonstrated how MAP provides unique minimal patient-specific\nrisk-mitigating interventions in the predefined subsets of accessible control\nvariables.\n","authors":["Illia Horenko"],"pdf_url":"https://arxiv.org/pdf/2310.14421v3.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.08362v1","updated":"2023-11-14T18:09:15Z","published":"2023-11-14T18:09:15Z","title":"Transformers can optimally learn regression mixture models","summary":"  Mixture models arise in many regression problems, but most methods have seen\nlimited adoption partly due to these algorithms' highly-tailored and\nmodel-specific nature. On the other hand, transformers are flexible, neural\nsequence models that present the intriguing possibility of providing\ngeneral-purpose prediction methods, even in this mixture setting. In this work,\nwe investigate the hypothesis that transformers can learn an optimal predictor\nfor mixtures of regressions. We construct a generative process for a mixture of\nlinear regressions for which the decision-theoretic optimal procedure is given\nby data-driven exponential weights on a finite set of parameters. We observe\nthat transformers achieve low mean-squared error on data generated via this\nprocess. By probing the transformer's output at inference time, we also show\nthat transformers typically make predictions that are close to the optimal\npredictor. Our experiments also demonstrate that transformers can learn\nmixtures of regressions in a sample-efficient fashion and are somewhat robust\nto distribution shifts. We complement our experimental observations by proving\nconstructively that the decision-theoretic optimal procedure is indeed\nimplementable by a transformer.\n","authors":["Reese Pathak","Rajat Sen","Weihao Kong","Abhimanyu Das"],"pdf_url":"https://arxiv.org/pdf/2311.08362v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.04972v2","updated":"2023-11-14T18:06:48Z","published":"2022-12-09T16:35:14Z","title":"MOPRD: A multidisciplinary open peer review dataset","summary":"  Open peer review is a growing trend in academic publications. Public access\nto peer review data can benefit both the academic and publishing communities.\nIt also serves as a great support to studies on review comment generation and\nfurther to the realization of automated scholarly paper review. However, most\nof the existing peer review datasets do not provide data that cover the whole\npeer review process. Apart from this, their data are not diversified enough as\nthe data are mainly collected from the field of computer science. These two\ndrawbacks of the currently available peer review datasets need to be addressed\nto unlock more opportunities for related studies. In response, we construct\nMOPRD, a multidisciplinary open peer review dataset. This dataset consists of\npaper metadata, multiple version manuscripts, review comments, meta-reviews,\nauthor's rebuttal letters, and editorial decisions. Moreover, we propose a\nmodular guided review comment generation method based on MOPRD. Experiments\nshow that our method delivers better performance as indicated by both automatic\nmetrics and human evaluation. We also explore other potential applications of\nMOPRD, including meta-review generation, editorial decision prediction, author\nrebuttal generation, and scientometric analysis. MOPRD is a strong endorsement\nfor further studies in peer review-related research and other applications.\n","authors":["Jialiang Lin","Jiaxin Song","Zhangping Zhou","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2212.04972v2.pdf","comment":"Please cite the version of Neural Computing and Applications"},{"id":"http://arxiv.org/abs/2311.08360v1","updated":"2023-11-14T18:03:20Z","published":"2023-11-14T18:03:20Z","title":"The Transient Nature of Emergent In-Context Learning in Transformers","summary":"  Transformer neural networks can exhibit a surprising capacity for in-context\nlearning (ICL) despite not being explicitly trained for it. Prior work has\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\nexamining the distributional properties of training data. However, in each of\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\nemergence of ICL during transformer training is, in fact, often transient. We\ntrain transformers on synthetic data designed so that both ICL and in-weights\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\nfirst emerges, then disappears and gives way to IWL, all while the training\nloss decreases, indicating an asymptotic preference for IWL. The transient\nnature of ICL is observed in transformers across a range of model sizes and\ndatasets, raising the question of how much to \"overtrain\" transformers when\nseeking compact, cheaper-to-run models. We find that L2 regularization may\noffer a path to more persistent ICL that removes the need for early stopping\nbased on ICL-style validation tasks. Finally, we present initial evidence that\nICL transience may be caused by competition between ICL and IWL circuits.\n","authors":["Aaditya K. Singh","Stephanie C. Y. Chan","Ted Moskovitz","Erin Grant","Andrew M. Saxe","Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2311.08360v1.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2311.08357v1","updated":"2023-11-14T17:59:51Z","published":"2023-11-14T17:59:51Z","title":"Sparsity-Preserving Differentially Private Training of Large Embedding\n  Models","summary":"  As the use of large embedding models in recommendation systems and language\napplications increases, concerns over user data privacy have also risen.\nDP-SGD, a training algorithm that combines differential privacy with stochastic\ngradient descent, has been the workhorse in protecting user privacy without\ncompromising model accuracy by much. However, applying DP-SGD naively to\nembedding models can destroy gradient sparsity, leading to reduced training\nefficiency. To address this issue, we present two new algorithms, DP-FEST and\nDP-AdaFEST, that preserve gradient sparsity during private training of large\nembedding models. Our algorithms achieve substantial reductions ($10^6 \\times$)\nin gradient size, while maintaining comparable levels of accuracy, on benchmark\nreal-world datasets.\n","authors":["Badih Ghazi","Yangsibo Huang","Pritish Kamath","Ravi Kumar","Pasin Manurangsi","Amer Sinha","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08357v1.pdf","comment":"Neural Information Processing Systems (NeurIPS) 2023"},{"id":"http://arxiv.org/abs/2301.09702v4","updated":"2023-11-14T17:44:53Z","published":"2023-01-23T20:11:24Z","title":"Illumination Variation Correction Using Image Synthesis For Unsupervised\n  Domain Adaptive Person Re-Identification","summary":"  Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to\nlearn identity information from labeled images in source domains and apply it\nto unlabeled images in a target domain. One major issue with many unsupervised\nre-identification methods is that they do not perform well relative to large\ndomain variations such as illumination, viewpoint, and occlusions. In this\npaper, we propose a Synthesis Model Bank (SMB) to deal with illumination\nvariation in unsupervised person re-ID. The proposed SMB consists of several\nconvolutional neural networks (CNN) for feature extraction and Mahalanobis\nmatrices for distance metrics. They are trained using synthetic data with\ndifferent illumination conditions such that their synergistic effect makes the\nSMB robust against illumination variation. To better quantify the illumination\nintensity and improve the quality of synthetic images, we introduce a new 3D\nvirtual-human dataset for GAN-based image synthesis. From our experiments, the\nproposed SMB outperforms other synthesis methods on several re-ID benchmarks.\n","authors":["Jiaqi Guo","Amy R. Reibman","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2301.09702v4.pdf","comment":"10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2310.19647v2","updated":"2023-11-14T17:39:27Z","published":"2023-10-30T15:35:24Z","title":"Fast swap regret minimization and applications to approximate correlated\n  equilibria","summary":"  We give a simple and computationally efficient algorithm that, for any\nconstant $\\varepsilon>0$, obtains $\\varepsilon T$-swap regret within only $T =\n\\mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the\nsuper-linear number of rounds required by the state-of-the-art algorithm, and\nresolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an\nexponential dependence on $\\varepsilon$, but we prove a new, matching lower\nbound.\n  Our algorithm for swap regret implies faster convergence to\n$\\varepsilon$-Correlated Equilibrium ($\\varepsilon$-CE) in several regimes: For\nnormal form two-player games with $n$ actions, it implies the first uncoupled\ndynamics that converges to the set of $\\varepsilon$-CE in polylogarithmic\nrounds; a $\\mathsf{polylog}(n)$-bit communication protocol for $\\varepsilon$-CE\nin two-player games (resolving an open problem mentioned by\n[Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]); and an\n$\\tilde{O}(n)$-query algorithm for $\\varepsilon$-CE (resolving an open problem\nof [Babichenko'2020] and obtaining the first separation between\n$\\varepsilon$-CE and $\\varepsilon$-Nash equilibrium in the query complexity\nmodel).\n  For extensive-form games, our algorithm implies a PTAS for $\\mathit{normal}$\n$\\mathit{form}$ $\\mathit{correlated}$ $\\mathit{equilibria}$, a solution concept\noften conjectured to be computationally intractable (e.g. [Stengel-Forges'08,\nFujii'23]).\n","authors":["Binghui Peng","Aviad Rubinstein"],"pdf_url":"https://arxiv.org/pdf/2310.19647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02712v2","updated":"2023-11-14T17:12:36Z","published":"2022-12-06T02:33:47Z","title":"Improved Beam Search for Hallucination Mitigation in Abstractive\n  Summarization","summary":"  Advancement in large pretrained language models has significantly improved\ntheir performance for conditional language generation tasks including\nsummarization albeit with hallucinations. To reduce hallucinations,\nconventional methods proposed improving beam search or using a fact checker as\na postprocessing step. In this paper, we investigate the use of the Natural\nLanguage Inference (NLI) entailment metric to detect and prevent hallucinations\nin summary generation. We propose an NLI-assisted beam re-ranking mechanism by\ncomputing entailment probability scores between the input context and\nsummarization model-generated beams during saliency-enhanced greedy decoding.\nMoreover, a diversity metric is introduced to compare its effectiveness against\nvanilla beam search. Our proposed algorithm significantly outperforms vanilla\nbeam decoding on XSum and CNN/DM datasets.\n","authors":["Arvind Krishna Sridhar","Erik Visser"],"pdf_url":"https://arxiv.org/pdf/2212.02712v2.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2306.00183v3","updated":"2023-11-14T17:00:52Z","published":"2023-05-31T21:00:50Z","title":"Diffused Redundancy in Pre-trained Representations","summary":"  Representations learned by pre-training a neural network on a large dataset\nare increasingly used successfully to perform a variety of downstream tasks. In\nthis work, we take a closer look at how features are encoded in such\npre-trained representations. We find that learned representations in a given\nlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of\nneurons in the layer that is larger than a threshold size shares a large degree\nof similarity with the full layer and is able to perform similarly as the whole\nlayer on a variety of downstream tasks. For example, a linear probe trained on\n$20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50\npre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe\ntrained on the full layer of neurons for downstream CIFAR10 classification. We\nconduct experiments on different neural architectures (including CNNs and\nTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a\nvariety of downstream tasks taken from the VTAB benchmark. We find that the\nloss and dataset used during pre-training largely govern the degree of diffuse\nredundancy and the \"critical mass\" of neurons needed often depends on the\ndownstream task, suggesting that there is a task-inherent\nredundancy-performance Pareto frontier. Our findings shed light on the nature\nof representations learned by pre-trained deep neural networks and suggest that\nentire layers might not be necessary to perform many downstream tasks. We\ninvestigate the potential for exploiting this redundancy to achieve efficient\ngeneralization for downstream tasks and also draw caution to certain possible\nunintended consequences. Our code is available at\n\\url{https://github.com/nvedant07/diffused-redundancy}.\n","authors":["Vedant Nanda","Till Speicher","John P. Dickerson","Soheil Feizi","Krishna P. Gummadi","Adrian Weller"],"pdf_url":"https://arxiv.org/pdf/2306.00183v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08309v1","updated":"2023-11-14T16:55:12Z","published":"2023-11-14T16:55:12Z","title":"Introducing an Improved Information-Theoretic Measure of Predictive\n  Uncertainty","summary":"  Applying a machine learning model for decision-making in the real world\nrequires to distinguish what the model knows from what it does not. A critical\nfactor in assessing the knowledge of a model is to quantify its predictive\nuncertainty. Predictive uncertainty is commonly measured by the entropy of the\nBayesian model average (BMA) predictive distribution. Yet, the properness of\nthis current measure of predictive uncertainty was recently questioned. We\nprovide new insights regarding those limitations. Our analyses show that the\ncurrent measure erroneously assumes that the BMA predictive distribution is\nequivalent to the predictive distribution of the true model that generated the\ndataset. Consequently, we introduce a theoretically grounded measure to\novercome these limitations. We experimentally verify the benefits of our\nintroduced measure of predictive uncertainty. We find that our introduced\nmeasure behaves more reasonably in controlled synthetic tasks. Moreover, our\nevaluations on ImageNet demonstrate that our introduced measure is advantageous\nin real-world applications utilizing predictive uncertainty.\n","authors":["Kajetan Schweighofer","Lukas Aichberger","Mykyta Ielanskyi","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2311.08309v1.pdf","comment":"M3L & InfoCog Workshops NeurIPS 23"},{"id":"http://arxiv.org/abs/2212.00768v3","updated":"2023-11-14T16:52:48Z","published":"2022-12-01T18:53:06Z","title":"Simplifying and Understanding State Space Models with Diagonal Linear\n  RNNs","summary":"  Sequence models based on linear state spaces (SSMs) have recently emerged as\na promising choice of architecture for modeling long range dependencies across\nvarious modalities. However, they invariably rely on discretization of a\ncontinuous state space, which complicates their presentation and understanding.\nIn this work, we dispose of the discretization step, and propose a model based\non vanilla Diagonal Linear RNNs ($\\mathrm{DLR}$). We empirically show that,\ndespite being conceptually much simpler, $\\mathrm{DLR}$ is as performant as\npreviously-proposed SSMs on a variety of tasks and benchmarks including Long\nRange Arena and raw speech classification. Moreover, we characterize the\nexpressivity of SSMs (including $\\mathrm{DLR}$) and attention-based models via\na suite of $13$ synthetic sequence-to-sequence tasks involving interactions\nover tens of thousands of tokens, ranging from simple operations, such as\nshifting an input sequence, to detecting co-dependent visual features over long\nspatial ranges in flattened images. We find that while SSMs report near-perfect\nperformance on tasks that can be modeled via $\\textit{few}$ convolutional\nkernels, they struggle on tasks requiring $\\textit{many}$ such kernels and\nespecially when the desired sequence manipulation is\n$\\textit{context-dependent}$. Despite these limitations, $\\mathrm{DLR}$ reaches\nhigh performance on two higher-order reasoning tasks $\\mathrm{ListOpsSubTrees}$\nand $\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{256}$ with input lengths\n$8K$ and $65K$ respectively, and gives encouraging performance on\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{512}$ with input length $262K$\nfor which attention is not a viable choice.\n","authors":["Ankit Gupta","Harsh Mehta","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2212.00768v3.pdf","comment":"added Long Range Arena, language modeling with mixture of experts"},{"id":"http://arxiv.org/abs/2306.06210v3","updated":"2023-11-14T16:46:00Z","published":"2023-05-26T13:06:38Z","title":"Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion","summary":"  Recent breakthroughs in generative modeling have sparked interest in\npractical single-model attribution. Such methods predict whether a sample was\ngenerated by a specific generator or not, for instance, to prove intellectual\nproperty theft. However, previous works are either limited to the closed-world\nsetting or require undesirable changes to the generative model. We address\nthese shortcomings by, first, viewing single-model attribution through the lens\nof anomaly detection. Arising from this change of perspective, we propose\nFLIPAD, a new approach for single-model attribution in the open-world setting\nbased on final-layer inversion and anomaly detection. We show that the utilized\nfinal-layer inversion can be reduced to a convex lasso optimization problem,\nmaking our approach theoretically sound and computationally efficient. The\ntheoretical findings are accompanied by an experimental study demonstrating the\neffectiveness of our approach and its flexibility to various domains.\n","authors":["Mike Laszkiewicz","Jonas Ricker","Johannes Lederer","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2306.06210v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08290v1","updated":"2023-11-14T16:37:28Z","published":"2023-11-14T16:37:28Z","title":"On-Policy Policy Gradient Reinforcement Learning Without On-Policy\n  Sampling","summary":"  On-policy reinforcement learning (RL) algorithms perform policy updates using\ni.i.d. trajectories collected by the current policy. However, after observing\nonly a finite number of trajectories, on-policy sampling may produce data that\nfails to match the expected on-policy data distribution. This sampling error\nleads to noisy updates and data inefficient on-policy learning. Recent work in\nthe policy evaluation setting has shown that non-i.i.d., off-policy sampling\ncan produce data with lower sampling error than on-policy sampling can produce.\nMotivated by this observation, we introduce an adaptive, off-policy sampling\nmethod to improve the data efficiency of on-policy policy gradient algorithms.\nOur method, Proximal Robust On-Policy Sampling (PROPS), reduces sampling error\nby collecting data with a behavior policy that increases the probability of\nsampling actions that are under-sampled with respect to the current policy.\nRather than discarding data from old policies -- as is commonly done in\non-policy algorithms -- PROPS uses data collection to adjust the distribution\nof previously collected data to be approximately on-policy. We empirically\nevaluate PROPS on both continuous-action MuJoCo benchmark tasks as well as\ndiscrete-action tasks and demonstrate that (1) PROPS decreases sampling error\nthroughout training and (2) improves the data efficiency of on-policy policy\ngradient algorithms. Our work improves the RL community's understanding of a\nnuance in the on-policy vs off-policy dichotomy: on-policy learning requires\non-policy data, not on-policy sampling.\n","authors":["Nicholas E. Corrado","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2311.08290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10485v2","updated":"2023-11-14T16:34:00Z","published":"2023-07-19T22:43:57Z","title":"FinGPT: Democratizing Internet-scale Data for Financial Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating human-like texts, which may potentially\nrevolutionize the finance industry. However, existing LLMs often fall short in\nthe financial field, which is mainly attributed to the disparities between\ngeneral text data and financial text data. Unfortunately, there is only a\nlimited number of financial text datasets available, and BloombergGPT, the\nfirst financial LLM (FinLLM), is close-sourced (only the training logs were\nreleased). In light of this, we aim to democratize Internet-scale financial\ndata for LLMs, which is an open challenge due to diverse data sources, low\nsignal-to-noise ratio, and high time-validity. To address the challenges, we\nintroduce an open-sourced and data-centric framework, Financial Generative\nPre-trained Transformer (FinGPT), that automates the collection and curation of\nreal-time financial data from 34 diverse sources on the Internet, providing\nresearchers and practitioners with accessible and transparent resources to\ndevelop their FinLLMs. Additionally, we propose a simple yet effective strategy\nfor fine-tuning FinLLM using the inherent feedback from the market, dubbed\nReinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank\nAdaptation (LoRA, QLoRA) method that enables users to customize their own\nFinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several\nFinGPT applications, including robo-advisor, sentiment analysis for algorithmic\ntrading, and low-code development. FinGPT aims to democratize FinLLMs,\nstimulate innovation, and unlock new opportunities in open finance. The codes\nhave been open-sourced.\n","authors":["Xiao-Yang Liu","Guoxuan Wang","Hongyang Yang","Daochen Zha"],"pdf_url":"https://arxiv.org/pdf/2307.10485v2.pdf","comment":"43 pages, 8 tables, and 2 figures"},{"id":"http://arxiv.org/abs/2311.05808v2","updated":"2023-11-14T16:33:21Z","published":"2023-11-10T00:53:22Z","title":"Scale-MIA: A Scalable Model Inversion Attack against Secure Federated\n  Learning via Latent Space Reconstruction","summary":"  Federated learning is known for its capability to safeguard participants'\ndata privacy. However, recently emerged model inversion attacks (MIAs) have\nshown that a malicious parameter server can reconstruct individual users' local\ndata samples through model updates. The state-of-the-art attacks either rely on\ncomputation-intensive search-based optimization processes to recover each input\nbatch, making scaling difficult, or they involve the malicious parameter server\nadding extra modules before the global model architecture, rendering the\nattacks too conspicuous and easily detectable.\n  To overcome these limitations, we propose Scale-MIA, a novel MIA capable of\nefficiently and accurately recovering training samples of clients from the\naggregated updates, even when the system is under the protection of a robust\nsecure aggregation protocol. Unlike existing approaches treating models as\nblack boxes, Scale-MIA recognizes the importance of the intricate architecture\nand inner workings of machine learning models. It identifies the latent space\nas the critical layer for breaching privacy and decomposes the complex recovery\ntask into an innovative two-step process to reduce computation complexity. The\nfirst step involves reconstructing the latent space representations (LSRs) from\nthe aggregated model updates using a closed-form inversion mechanism,\nleveraging specially crafted adversarial linear layers. In the second step, the\nwhole input batches are recovered from the LSRs by feeding them into a\nfine-tuned generative decoder.\n  We implemented Scale-MIA on multiple commonly used machine learning models\nand conducted comprehensive experiments across various settings. The results\ndemonstrate that Scale-MIA achieves excellent recovery performance on different\ndatasets, exhibiting high reconstruction rates, accuracy, and attack efficiency\non a larger scale compared to state-of-the-art MIAs.\n","authors":["Shanghao Shi","Ning Wang","Yang Xiao","Chaoyu Zhang","Yi Shi","Y. Thomas Hou","Wenjing Lou"],"pdf_url":"https://arxiv.org/pdf/2311.05808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17479v2","updated":"2023-11-14T16:20:33Z","published":"2023-05-27T13:57:26Z","title":"Inferring Causal Effects Under Heterogeneous Peer Influence","summary":"  Causal inference in networks should account for interference, which occurs\nwhen a unit's outcome is influenced by treatments or outcomes of peers.\nHeterogeneous peer influence (HPI) occurs when a unit's outcome is influenced\ndifferently by different peers based on their attributes and relationships, or\nwhen each unit has a different susceptibility to peer influence. Existing\nsolutions to estimating direct causal effects under interference consider\neither homogeneous influence from peers or specific heterogeneous influence\nmechanisms (e.g., based on local neighborhood structure). This paper presents a\nmethodology for estimating individual direct causal effects in the presence of\nHPI where the mechanism of influence is not known a priori. We propose a\nstructural causal model for networks that can capture different possible\nassumptions about network structure, interference conditions, and causal\ndependence and enables reasoning about identifiability in the presence of HPI.\nWe find potential heterogeneous contexts using the causal model and propose a\nnovel graph neural network-based estimator to estimate individual direct causal\neffects. We show that state-of-the-art methods for individual direct effect\nestimation produce biased results in the presence of HPI, and that our proposed\nestimator is robust.\n","authors":["Shishir Adhikari","Elena Zheleva"],"pdf_url":"https://arxiv.org/pdf/2305.17479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08272v1","updated":"2023-11-14T16:07:16Z","published":"2023-11-14T16:07:16Z","title":"Mixed Attention Network for Cross-domain Sequential Recommendation","summary":"  In modern recommender systems, sequential recommendation leverages\nchronological user behaviors to make effective next-item suggestions, which\nsuffers from data sparsity issues, especially for new users. One promising line\nof work is the cross-domain recommendation, which trains models with data\nacross multiple domains to improve the performance in data-scarce domains.\nRecent proposed cross-domain sequential recommendation models such as PiNet and\nDASL have a common drawback relying heavily on overlapped users in different\ndomains, which limits their usage in practical recommender systems. In this\npaper, we propose a Mixed Attention Network (MAN) with local and global\nattention modules to extract the domain-specific and cross-domain information.\nFirstly, we propose a local/global encoding layer to capture the\ndomain-specific/cross-domain sequential pattern. Then we propose a mixed\nattention layer with item similarity attention, sequence-fusion attention, and\ngroup-prototype attention to capture the local/global item similarity, fuse the\nlocal/global item sequence, and extract the user groups across different\ndomains, respectively. Finally, we propose a local/global prediction layer to\nfurther evolve and combine the domain-specific and cross-domain interests.\nExperimental results on two real-world datasets (each with two domains)\ndemonstrate the superiority of our proposed model. Further study also\nillustrates that our proposed method and components are model-agnostic and\neffective, respectively. The code and data are available at\nhttps://github.com/Guanyu-Lin/MAN.\n","authors":["Guanyu Lin","Chen Gao","Yu Zheng","Jianxin Chang","Yanan Niu","Yang Song","Kun Gai","Zhiheng Li","Depeng Jin","Yong Li","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08272v1.pdf","comment":"WSDM 2024"},{"id":"http://arxiv.org/abs/2206.03359v2","updated":"2023-11-14T16:06:45Z","published":"2022-06-07T14:53:35Z","title":"An efficient semi-supervised quality control system trained using\n  physics-based MRI-artefact generators and adversarial training","summary":"  Large medical imaging data sets are becoming increasingly available, but\nensuring sample quality without significant artefacts is challenging. Existing\nmethods for identifying imperfections in medical imaging rely on data-intensive\napproaches, compounded by a scarcity of artefact-rich scans for training\nmachine learning models in clinical research. To tackle this problem, we\npropose a framework with four main components: 1) artefact generators inspired\nby magnetic resonance physics to corrupt brain MRI scans and augment a training\ndataset, 2) abstract and engineered features to represent images compactly, 3)\na feature selection process depending on the artefact class to improve\nclassification, and 4) SVM classifiers to identify artefacts. Our contributions\nare threefold: first, physics-based artefact generators produce synthetic brain\nMRI scans with controlled artefacts for data augmentation. This will avoid the\nlabour-intensive collection and labelling process of scans with rare artefacts.\nSecond, we propose a pool of abstract and engineered image features to identify\n9 different artefacts for structural MRI. Finally, we use an artefact-based\nfeature selection block that, for each class of artefacts, finds the set of\nfeatures providing the best classification performance. We performed validation\nexperiments on a large data set of scans with artificially-generated artefacts,\nand in a multiple sclerosis clinical trial where real artefacts were identified\nby experts, showing that the proposed pipeline outperforms traditional methods.\nIn particular, our data augmentation increases performance by up to 12.5\npercentage points on accuracy, precision, and recall. The computational\nefficiency of our pipeline enables potential real-time deployment, promising\nhigh-throughput clinical applications through automated image-processing\npipelines driven by quality control systems.\n","authors":["Daniele Ravi","Frederik Barkhof","Daniel C. Alexander","Lemuel Puglisi","Geoffrey JM Parker","Arman Eshaghi"],"pdf_url":"https://arxiv.org/pdf/2206.03359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08271v1","updated":"2023-11-14T16:06:11Z","published":"2023-11-14T16:06:11Z","title":"Mobility-Induced Graph Learning for WiFi Positioning","summary":"  A smartphone-based user mobility tracking could be effective in finding\nhis/her location, while the unpredictable error therein due to low\nspecification of built-in inertial measurement units (IMUs) rejects its\nstandalone usage but demands the integration to another positioning technique\nlike WiFi positioning. This paper aims to propose a novel integration technique\nusing a graph neural network called Mobility-INduced Graph LEarning (MINGLE),\nwhich is designed based on two types of graphs made by capturing different user\nmobility features. Specifically, considering sequential measurement points\n(MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor\nMPs as edges, called time-driven mobility graph (TMG). Second, a user's\nrelatively straight transition at a constant pace when moving from one position\nto another can be captured by connecting the nodes on each path, called a\ndirection-driven mobility graph (DMG). Then, we can design graph convolution\nnetwork (GCN)-based cross-graph learning, where two different GCN models for\nTMG and DMG are jointly trained by feeding different input features created by\nWiFi RTTs yet sharing their weights. Besides, the loss function includes a\nmobility regularization term such that the differences between adjacent\nlocation estimates should be less variant due to the user's stable moving pace.\nNoting that the regularization term does not require ground-truth location,\nMINGLE can be designed under semi- and self-supervised learning frameworks. The\nproposed MINGLE's effectiveness is extensively verified through field\nexperiments, showing a better positioning accuracy than benchmarks, say root\nmean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and\nsemi-supervised learning cases, respectively.\n","authors":["Kyuwon Han","Seung Min Yu","Seong-Lyun Kim","Seung-Woo Ko"],"pdf_url":"https://arxiv.org/pdf/2311.08271v1.pdf","comment":"submitted to a possible IEEE journal"},{"id":"http://arxiv.org/abs/2310.07793v2","updated":"2023-11-14T15:51:18Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional carefully\ndesigned embedding-based and rule-based models dominate. The question remains\nopen of whether pre-trained LLMs can understand structured temporal relational\ndata and replace them as the foundation model for temporal relational\nforecasting. Therefore, we bring temporal knowledge forecasting into the\ngenerative setting. However, challenges occur in the huge chasms between\ncomplex temporal graph data structure and sequential natural expressions LLMs\ncan handle, and between the enormous data sizes of tKGs and heavy computation\ncosts of finetuning LLMs. To address these challenges, we propose a novel\nretrieval augmented generation framework that performs generative forecasting\non tKGs named GenTKG, which combines a temporal logical rule-based retrieval\nstrategy and lightweight parameter-efficient instruction tuning. Extensive\nexperiments have shown that GenTKG outperforms conventional methods of temporal\nrelational forecasting under low computation resources. GenTKG also highlights\nremarkable transferability with exceeding performance on unseen datasets\nwithout re-training. Our work reveals the huge potential of LLMs in the tKG\ndomain and opens a new frontier for generative forecasting on tKGs.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v2.pdf","comment":"8 pages, accepted to Temporal Graph Learning @ NeurIPS 2023"},{"id":"http://arxiv.org/abs/2301.12309v4","updated":"2023-11-14T15:48:48Z","published":"2023-01-28T23:22:49Z","title":"On the Lipschitz Constant of Deep Networks and Double Descent","summary":"  Existing bounds on the generalization error of deep networks assume some form\nof smooth or bounded dependence on the input variable, falling short of\ninvestigating the mechanisms controlling such factors in practice. In this\nwork, we present an extensive experimental study of the empirical Lipschitz\nconstant of deep networks undergoing double descent, and highlight\nnon-monotonic trends strongly correlating with the test error. Building a\nconnection between parameter-space and input-space gradients for SGD around a\ncritical point, we isolate two important factors -- namely loss landscape\ncurvature and distance of parameters from initialization -- respectively\ncontrolling optimization dynamics around a critical point and bounding model\nfunction complexity, even beyond the training data. Our study presents novels\ninsights on implicit regularization via overparameterization, and effective\nmodel complexity for networks trained in practice.\n","authors":["Matteo Gamba","Hossein Azizpour","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2301.12309v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08252v1","updated":"2023-11-14T15:43:47Z","published":"2023-11-14T15:43:47Z","title":"REST: Retrieval-Based Speculative Decoding","summary":"  We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.\n","authors":["Zhenyu He","Zexuan Zhong","Tianle Cai","Jason D Lee","Di He"],"pdf_url":"https://arxiv.org/pdf/2311.08252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.14350v2","updated":"2023-11-14T15:18:03Z","published":"2021-03-26T09:42:58Z","title":"The convergence of the Stochastic Gradient Descent (SGD) : a\n  self-contained proof","summary":"  We give here a proof of the convergence of the Stochastic Gradient Descent\n(SGD) in a self-contained manner.\n","authors":["Gabrel Turinici"],"pdf_url":"https://arxiv.org/pdf/2103.14350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08228v1","updated":"2023-11-14T15:08:14Z","published":"2023-11-14T15:08:14Z","title":"Counterfactual Explanation for Regression via Disentanglement in Latent\n  Space","summary":"  Counterfactual Explanations (CEs) help address the question: How can the\nfactors that influence the prediction of a predictive model be changed to\nachieve a more favorable outcome from a user's perspective? Thus, they bear the\npotential to guide the user's interaction with AI systems since they represent\neasy-to-understand explanations. To be applicable, CEs need to be realistic and\nactionable. In the literature, various methods have been proposed to generate\nCEs. However, the majority of research on CEs focuses on classification\nproblems where questions like ``What should I do to get my rejected loan\napproved?\" are raised. In practice, answering questions like ``What should I do\nto increase my salary?\" are of a more regressive nature. In this paper, we\nintroduce a novel method to generate CEs for a pre-trained regressor by first\ndisentangling the label-relevant from the label-irrelevant dimensions in the\nlatent space. CEs are then generated by combining the label-irrelevant\ndimensions and the predefined output. The intuition behind this approach is\nthat the ideal counterfactual search should focus on the label-irrelevant\ncharacteristics of the input and suggest changes toward target-relevant\ncharacteristics. Searching in the latent space could help achieve this goal. We\nshow that our method maintains the characteristics of the query sample during\nthe counterfactual search. In various experiments, we demonstrate that the\nproposed method is competitive based on different quality measures on image and\ntabular datasets in regression problem settings. It efficiently returns results\ncloser to the original data manifold compared to three state-of-the-art\nmethods, which is essential for realistic high-dimensional machine learning\napplications. Our code will be made available as an open-source package upon\nthe publication of this work.\n","authors":["Xuan Zhao","Klaus Broelemann","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2311.08228v1.pdf","comment":"CXAI workshop @ ICDM 2023"},{"id":"http://arxiv.org/abs/2311.07344v2","updated":"2023-11-14T14:39:58Z","published":"2023-11-13T14:01:04Z","title":"Missing Value Imputation for Multi-attribute Sensor Data Streams via\n  Message Propagation (Extended Version)","summary":"  Sensor data streams occur widely in various real-time applications in the\ncontext of the Internet of Things (IoT). However, sensor data streams feature\nmissing values due to factors such as sensor failures, communication errors, or\ndepleted batteries. Missing values can compromise the quality of real-time\nanalytics tasks and downstream applications. Existing imputation methods either\nmake strong assumptions about streams or have low efficiency. In this study, we\naim to accurately and efficiently impute missing values in data streams that\nsatisfy only general characteristics in order to benefit real-time applications\nmore widely. First, we propose a message propagation imputation network (MPIN)\nthat is able to recover the missing values of data instances in a time window.\nWe give a theoretical analysis of why MPIN is effective. Second, we present a\ncontinuous imputation framework that consists of data update and model update\nmechanisms to enable MPIN to perform continuous imputation both effectively and\nefficiently. Extensive experiments on multiple real datasets show that MPIN can\noutperform the existing data imputers by wide margins and that the continuous\nimputation framework is efficient and accurate.\n","authors":["Xiao Li","Huan Li","Hua Lu","Christian S. Jensen","Varun Pandey","Volker Markl"],"pdf_url":"https://arxiv.org/pdf/2311.07344v2.pdf","comment":"Accepted at VLDB 2024"},{"id":"http://arxiv.org/abs/2311.08202v1","updated":"2023-11-14T14:37:33Z","published":"2023-11-14T14:37:33Z","title":"Federated Skewed Label Learning with Logits Fusion","summary":"  Federated learning (FL) aims to collaboratively train a shared model across\nmultiple clients without transmitting their local data. Data heterogeneity is a\ncritical challenge in realistic FL settings, as it causes significant\nperformance deterioration due to discrepancies in optimization among local\nmodels. In this work, we focus on label distribution skew, a common scenario in\ndata heterogeneity, where the data label categories are imbalanced on each\nclient. To address this issue, we propose FedBalance, which corrects the\noptimization bias among local models by calibrating their logits. Specifically,\nwe introduce an extra private weak learner on the client side, which forms an\nensemble model with the local model. By fusing the logits of the two models,\nthe private weak learner can capture the variance of different data, regardless\nof their category. Therefore, the optimization direction of local models can be\nimproved by increasing the penalty for misclassifying minority classes and\nreducing the attention to majority classes, resulting in a better global model.\nExtensive experiments show that our method can gain 13\\% higher average\naccuracy compared with state-of-the-art methods.\n","authors":["Yuwei Wang","Runhan Li","Hao Tan","Xuefeng Jiang","Sheng Sun","Min Liu","Bo Gao","Zhiyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2311.08202v1.pdf","comment":"9 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2311.08199v1","updated":"2023-11-14T14:33:39Z","published":"2023-11-14T14:33:39Z","title":"Diffusion-based generation of Histopathological Whole Slide Images at a\n  Gigapixel scale","summary":"  We present a novel diffusion-based approach to generate synthetic\nhistopathological Whole Slide Images (WSIs) at an unprecedented gigapixel\nscale. Synthetic WSIs have many potential applications: They can augment\ntraining datasets to enhance the performance of many computational pathology\napplications. They allow the creation of synthesized copies of datasets that\ncan be shared without violating privacy regulations. Or they can facilitate\nlearning representations of WSIs without requiring data annotations. Despite\nthis variety of applications, no existing deep-learning-based method generates\nWSIs at their typically high resolutions. Mainly due to the high computational\ncomplexity. Therefore, we propose a novel coarse-to-fine sampling scheme to\ntackle image generation of high-resolution WSIs. In this scheme, we increase\nthe resolution of an initial low-resolution image to a high-resolution WSI.\nParticularly, a diffusion model sequentially adds fine details to images and\nincreases their resolution. In our experiments, we train our method with WSIs\nfrom the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also\nperformed a user study with pathologists. The study results suggest that our\ngenerated WSIs resemble the structure of real WSIs.\n","authors":["Robert Harb","Thomas Pock","Heimo Müller"],"pdf_url":"https://arxiv.org/pdf/2311.08199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08190v1","updated":"2023-11-14T14:23:09Z","published":"2023-11-14T14:23:09Z","title":"SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage\n  Segmentation","summary":"  Segment Anything Model (SAM), a vision foundation model trained on\nlarge-scale annotations, has recently continued raising awareness within\nmedical image segmentation. Despite the impressive capabilities of SAM on\nnatural scenes, it struggles with performance decline when confronted with\nmedical images, especially those involving blurry boundaries and highly\nirregular regions of low contrast. In this paper, a SAM-based\nparameter-efficient fine-tuning method, called SAMIHS, is proposed for\nintracranial hemorrhage segmentation, which is a crucial and challenging step\nin stroke diagnosis and surgical planning. Distinguished from previous SAM and\nSAM-based methods, SAMIHS incorporates parameter-refactoring adapters into\nSAM's image encoder and considers the efficient and flexible utilization of\nadapters' parameters. Additionally, we employ a combo loss that combines binary\ncross-entropy loss and boundary-sensitive loss to enhance SAMIHS's ability to\nrecognize the boundary regions. Our experimental results on two public datasets\ndemonstrate the effectiveness of our proposed method. Code is available at\nhttps://github.com/mileswyn/SAMIHS .\n","authors":["Yinuo Wang","Kai Chen","Weimin Yuan","Cai Meng","XiangZhi Bai"],"pdf_url":"https://arxiv.org/pdf/2311.08190v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.08182v1","updated":"2023-11-14T14:10:40Z","published":"2023-11-14T14:10:40Z","title":"Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning","summary":"  Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.\n","authors":["Shengguang Wu","Keming Lu","Benfeng Xu","Junyang Lin","Qi Su","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10167v4","updated":"2023-11-14T14:08:02Z","published":"2023-03-17T17:54:25Z","title":"Generalized partitioned local depth","summary":"  In this paper we provide a generalization of the concept of cohesion as\nintroduced recently by Berenhaut, Moore and Melvin [Proceedings of the National\nAcademy of Sciences, 119 (4) (2022)]. The formulation presented builds on the\ntechnique of partitioned local depth by distilling two key probabilistic\nconcepts: local relevance and support division. Earlier results are extended\nwithin the new context, and examples of applications to revealing communities\nin data with uncertainty are included. The work sheds light on the foundations\nof partitioned local depth, and extends the original ideas to enable\nprobabilistic consideration of uncertain, variable and potentially conflicting\ninformation.\n","authors":["Kenneth S. Berenhaut","John D. Foley","Liangdongsheng Lyu"],"pdf_url":"https://arxiv.org/pdf/2303.10167v4.pdf","comment":"Typos correct & clarifying comments, 19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.07439v2","updated":"2023-11-14T14:01:46Z","published":"2023-11-13T16:15:20Z","title":"Investigating Multi-Pivot Ensembling with Massively Multilingual Machine\n  Translation Models","summary":"  Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. As an alternative, we propose MaxEns, a\ncombination strategy that is biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n","authors":["Alireza Mohammadshahi","Jannis Vamvas","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2311.07439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08170v1","updated":"2023-11-14T13:54:35Z","published":"2023-11-14T13:54:35Z","title":"Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning\n  Approach","summary":"  Lattice reduction is a combinatorial optimization problem aimed at finding\nthe most orthogonal basis in a given lattice. In this work, we address lattice\nreduction via deep learning methods. We design a deep neural model outputting\nfactorized unimodular matrices and train it in a self-supervised manner by\npenalizing non-orthogonal lattice bases. We incorporate the symmetries of\nlattice reduction into the model by making it invariant and equivariant with\nrespect to appropriate continuous and discrete groups.\n","authors":["Giovanni Luca Marchetti","Gabriele Cesa","Kumar Pratik","Arash Behboodi"],"pdf_url":"https://arxiv.org/pdf/2311.08170v1.pdf","comment":"Symmetry and Geometry in Neural Representations - NeurReps Workshop @\n  NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08166v1","updated":"2023-11-14T13:49:03Z","published":"2023-11-14T13:49:03Z","title":"MechAgents: Large language model multi-agent collaborations can solve\n  mechanics problems, generate new data, and integrate knowledge","summary":"  Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.\n","authors":["Bo Ni","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2311.08166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00041v2","updated":"2023-11-14T13:36:53Z","published":"2023-05-31T13:13:45Z","title":"Causal Intervention for Measuring Confidence in Drug-Target Interaction\n  Prediction","summary":"  Identifying and discovering drug-target interactions(DTIs) are vital steps in\ndrug discovery and development. They play a crucial role in assisting\nscientists in finding new drugs and accelerating the drug development process.\nRecently, knowledge graph and knowledge graph embedding (KGE) models have made\nrapid advancements and demonstrated impressive performance in drug discovery.\nHowever, such models lack authenticity and accuracy in drug target\nidentification, leading to an increased misjudgment rate and reduced drug\ndevelopment efficiency. To address these issues, we focus on the problem of\ndrug-target interactions, with knowledge mapping as the core technology.\nSpecifically, a causal intervention-based confidence measure is employed to\nassess the triplet score to improve the accuracy of the drug-target interaction\nprediction model. Experimental results demonstrate that the developed\nconfidence measurement method based on causal intervention can significantly\nenhance the accuracy of DTI link prediction, particularly for high-precision\nmodels. The predicted results are more valuable in guiding the design and\ndevelopment of subsequent drug development experiments, thereby significantly\nimproving the efficiency of drug development.\n","authors":["Wenting Ye","Chen Li","Yang Xie","Wen Zhang","Hong-Yu Zhang","Bowen Wang","Debo Cheng","Zaiwen Feng"],"pdf_url":"https://arxiv.org/pdf/2306.00041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08652v2","updated":"2023-11-14T13:35:35Z","published":"2023-09-15T15:21:14Z","title":"Quantifying Credit Portfolio sensitivity to asset correlations with\n  interpretable generative neural networks","summary":"  In this research, we propose a novel approach for the quantification of\ncredit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the\nuse of synthetic financial correlation matrices generated with deep learning\nmodels. In previous work Generative Adversarial Networks (GANs) were employed\nto demonstrate the generation of plausible correlation matrices, that capture\nthe essential characteristics observed in empirical correlation matrices\nestimated on asset returns. Instead of GANs, we employ Variational Autoencoders\n(VAE) to achieve a more interpretable latent space representation. Through our\nanalysis, we reveal that the VAE latent space can be a useful tool to capture\nthe crucial factors impacting portfolio diversification, particularly in\nrelation to credit portfolio sensitivity to asset correlations changes.\n","authors":["Sergio Caprioli","Emanuele Cagliero","Riccardo Crupi"],"pdf_url":"https://arxiv.org/pdf/2309.08652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05525v2","updated":"2023-11-14T13:29:06Z","published":"2023-01-13T13:08:54Z","title":"Understanding Concept Identification as Consistent Data Clustering\n  Across Multiple Feature Spaces","summary":"  Identifying meaningful concepts in large data sets can provide valuable\ninsights into engineering design problems. Concept identification aims at\nidentifying non-overlapping groups of design instances that are similar in a\njoint space of all features, but which are also similar when considering only\nsubsets of features. These subsets usually comprise features that characterize\na design with respect to one specific context, for example, constructive design\nparameters, performance values, or operation modes. It is desirable to evaluate\nthe quality of design concepts by considering several of these feature subsets\nin isolation. In particular, meaningful concepts should not only identify\ndense, well separated groups of data instances, but also provide\nnon-overlapping groups of data that persist when considering pre-defined\nfeature subsets separately. In this work, we propose to view concept\nidentification as a special form of clustering algorithm with a broad range of\npotential applications beyond engineering design. To illustrate the differences\nbetween concept identification and classical clustering algorithms, we apply a\nrecently proposed concept identification algorithm to two synthetic data sets\nand show the differences in identified solutions. In addition, we introduce the\nmutual information measure as a metric to evaluate whether solutions return\nconsistent clusters across relevant subsets. To support the novel understanding\nof concept identification, we consider a simulated data set from a\ndecision-making problem in the energy management domain and show that the\nidentified clusters are more interpretable with respect to relevant feature\nsubsets than clusters found by common clustering algorithms and are thus more\nsuitable to support a decision maker.\n","authors":["Felix Lanfermann","Sebastian Schmitt","Patricia Wollstadt"],"pdf_url":"https://arxiv.org/pdf/2301.05525v2.pdf","comment":"10 pages, 6 figures, published in proceedings of 2022 IEEE\n  International Conference on Data Mining Workshops (ICDMW)"},{"id":"http://arxiv.org/abs/2311.08150v1","updated":"2023-11-14T13:26:49Z","published":"2023-11-14T13:26:49Z","title":"The Hyperdimensional Transform for Distributional Modelling, Regression\n  and Classification","summary":"  Hyperdimensional computing (HDC) is an increasingly popular computing\nparadigm with immense potential for future intelligent applications. Although\nthe main ideas already took form in the 1990s, HDC recently gained significant\nattention, especially in the field of machine learning and data science. Next\nto efficiency, interoperability and explainability, HDC offers attractive\nproperties for generalization as it can be seen as an attempt to combine\nconnectionist ideas from neural networks with symbolic aspects. In recent work,\nwe introduced the hyperdimensional transform, revealing deep theoretical\nfoundations for representing functions and distributions as high-dimensional\nholographic vectors. Here, we present the power of the hyperdimensional\ntransform to a broad data science audience. We use the hyperdimensional\ntransform as a theoretical basis and provide insight into state-of-the-art HDC\napproaches for machine learning. We show how existing algorithms can be\nmodified and how this transform can lead to a novel, well-founded toolbox. Next\nto the standard regression and classification tasks of machine learning, our\ndiscussion includes various aspects of statistical modelling, such as\nrepresentation, learning and deconvolving distributions, sampling, Bayesian\ninference, and uncertainty estimation.\n","authors":["Pieter Dewulf","Bernard De Baets","Michiel Stock"],"pdf_url":"https://arxiv.org/pdf/2311.08150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08148v1","updated":"2023-11-14T13:25:41Z","published":"2023-11-14T13:25:41Z","title":"Cattle Identification Using Muzzle Images and Deep Learning Techniques","summary":"  Traditional animal identification methods such as ear-tagging, ear notching,\nand branding have been effective but pose risks to the animal and have\nscalability issues. Electrical methods offer better tracking and monitoring but\nrequire specialized equipment and are susceptible to attacks. Biometric\nidentification using time-immutable dermatoglyphic features such as muzzle\nprints and iris patterns is a promising solution. This project explores cattle\nidentification using 4923 muzzle images collected from 268 beef cattle. Two\ndeep learning classification models are implemented - wide ResNet50 and\nVGG16\\_BN and image compression is done to lower the image quality and adapt\nthe models to work for the African context. From the experiments run, a maximum\naccuracy of 99.5\\% is achieved while using the wide ResNet50 model with a\ncompression retaining 25\\% of the original image. From the study, it is noted\nthat the time required by the models to train and converge as well as\nrecognition time are dependent on the machine used to run the model.\n","authors":["G. N. Kimani","P. Oluwadara","P. Fashingabo","M. Busogi","E. Luhanga","K. Sowon","L. Chacha"],"pdf_url":"https://arxiv.org/pdf/2311.08148v1.pdf","comment":"8 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.08149v1","updated":"2023-11-14T13:25:41Z","published":"2023-11-14T13:25:41Z","title":"Modeling Complex Disease Trajectories using Deep Generative Models with\n  Semi-Supervised Latent Processes","summary":"  In this paper, we propose a deep generative time series approach using latent\ntemporal processes for modeling and holistically analyzing complex disease\ntrajectories. We aim to find meaningful temporal latent representations of an\nunderlying generative process that explain the observed disease trajectories in\nan interpretable and comprehensive way. To enhance the interpretability of\nthese latent temporal processes, we develop a semi-supervised approach for\ndisentangling the latent space using established medical concepts. By combining\nthe generative approach with medical knowledge, we leverage the ability to\ndiscover novel aspects of the disease while integrating medical concepts into\nthe model. We show that the learned temporal latent processes can be utilized\nfor further data analysis and clinical hypothesis testing, including finding\nsimilar patients and clustering the disease into new sub-types. Moreover, our\nmethod enables personalized online monitoring and prediction of multivariate\ntime series including uncertainty quantification. We demonstrate the\neffectiveness of our approach in modeling systemic sclerosis, showcasing the\npotential of our machine learning model to capture complex disease trajectories\nand acquire new medical knowledge.\n","authors":["Cécile Trottet","Manuel Schürch","Ahmed Allam","Imon Barua","Liubov Petelytska","Oliver Distler","Anna-Maria Hoffmann-Vold","Michael Krauthammer","the EUSTAR collaborators"],"pdf_url":"https://arxiv.org/pdf/2311.08149v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 23 pages"},{"id":"http://arxiv.org/abs/2201.10859v2","updated":"2023-11-14T13:23:25Z","published":"2022-01-26T10:40:55Z","title":"Visualizing the Diversity of Representations Learned by Bayesian Neural\n  Networks","summary":"  Explainable Artificial Intelligence (XAI) aims to make learning machines less\nopaque, and offers researchers and practitioners various tools to reveal the\ndecision-making strategies of neural networks. In this work, we investigate how\nXAI methods can be used for exploring and visualizing the diversity of feature\nrepresentations learned by Bayesian Neural Networks (BNNs). Our goal is to\nprovide a global understanding of BNNs by making their decision-making\nstrategies a) visible and tangible through feature visualizations and b)\nquantitatively measurable with a distance measure learned by contrastive\nlearning. Our work provides new insights into the \\emph{posterior} distribution\nin terms of human-understandable feature information with regard to the\nunderlying decision making strategies. The main findings of our work are the\nfollowing: 1) global XAI methods can be applied to explain the diversity of\ndecision-making strategies of BNN instances, 2) Monte Carlo dropout with\ncommonly used Dropout rates exhibit increased diversity in feature\nrepresentations compared to the multimodal posterior approximation of\nMultiSWAG, 3) the diversity of learned feature representations highly\ncorrelates with the uncertainty estimate for the output and 4) the inter-mode\ndiversity of the multimodal posterior decreases as the network width increases,\nwhile the intra mode diversity increases. These findings are consistent with\nthe recent Deep Neural Networks theory, providing additional intuitions about\nwhat the theory implies in terms of humanly understandable concepts.\n","authors":["Dennis Grinwald","Kirill Bykov","Shinichi Nakajima","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2201.10859v2.pdf","comment":"16 pages, 18 figures"},{"id":"http://arxiv.org/abs/2311.08141v1","updated":"2023-11-14T13:12:47Z","published":"2023-11-14T13:12:47Z","title":"GMTR: Graph Matching Transformers","summary":"  Vision transformers (ViTs) have recently been used for visual matching beyond\nobject detection and segmentation. However, the original grid dividing strategy\nof ViTs neglects the spatial information of the keypoints, limiting the\nsensitivity to local information. Therefore, we propose \\textbf{QueryTrans}\n(Query Transformer), which adopts a cross-attention module and keypoints-based\ncenter crop strategy for better spatial information extraction. We further\nintegrate the graph attention module and devise a transformer-based graph\nmatching approach \\textbf{GMTR} (Graph Matching TRansformers) whereby the\ncombinatorial nature of GM is addressed by a graph transformer neural GM\nsolver. On standard GM benchmarks, GMTR shows competitive performance against\nthe SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves\n$\\mathbf{83.6\\%}$ accuracy, $\\mathbf{0.9\\%}$ higher than the SOTA framework. On\nSpair-71k, GMTR shows great potential and outperforms most of the previous\nworks. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from\n$80.1\\%$ to $\\mathbf{83.3\\%}$, and BBGM from $79.0\\%$ to $\\mathbf{84.5\\%}$. On\nSpair-71k, QueryTrans improves NGMv2 from $80.6\\%$ to $\\mathbf{82.5\\%}$, and\nBBGM from $82.1\\%$ to $\\mathbf{83.9\\%}$. Source code will be made publicly\navailable.\n","authors":["Jinpei Guo","Shaofeng Zhang","Runzhong Wang","Chang Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.08141v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.06440v4","updated":"2023-11-14T13:01:48Z","published":"2023-07-12T20:10:14Z","title":"No Train No Gain: Revisiting Efficient Training Algorithms For\n  Transformer-based Language Models","summary":"  The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n","authors":["Jean Kaddour","Oscar Key","Piotr Nawrot","Pasquale Minervini","Matt J. Kusner"],"pdf_url":"https://arxiv.org/pdf/2307.06440v4.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08125v1","updated":"2023-11-14T12:41:22Z","published":"2023-11-14T12:41:22Z","title":"Lite it fly: An All-Deformable-Butterfly Network","summary":"  Most deep neural networks (DNNs) consist fundamentally of convolutional\nand/or fully connected layers, wherein the linear transform can be cast as the\nproduct between a filter matrix and a data matrix obtained by arranging feature\ntensors into columns. The lately proposed deformable butterfly (DeBut)\ndecomposes the filter matrix into generalized, butterflylike factors, thus\nachieving network compression orthogonal to the traditional ways of pruning or\nlow-rank decomposition. This work reveals an intimate link between DeBut and a\nsystematic hierarchy of depthwise and pointwise convolutions, which explains\nthe empirically good performance of DeBut layers. By developing an automated\nDeBut chain generator, we show for the first time the viability of homogenizing\na DNN into all DeBut layers, thus achieving an extreme sparsity and\ncompression. Various examples and hardware benchmarks verify the advantages of\nAll-DeBut networks. In particular, we show it is possible to compress a\nPointNet to < 5% parameters with < 5% accuracy drop, a record not achievable by\nother compression schemes.\n","authors":["Rui Lin","Jason Chun Lok Li","Jiajun Zhou","Binxiao Huang","Jie Ran","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2311.08125v1.pdf","comment":"7 pages, 3 figures, accepted as a brief paper in IEEE Transactions on\n  Neural Networks and Learning Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2311.08123v1","updated":"2023-11-14T12:37:25Z","published":"2023-11-14T12:37:25Z","title":"Memory-efficient Stochastic methods for Memory-based Transformers","summary":"  Training Memory-based transformers can require a large amount of memory and\ncan be quite inefficient. We propose a novel two-phase training mechanism and a\nnovel regularization technique to improve the training efficiency of\nmemory-based transformers, which are often used for long-range context\nproblems. For our experiments, we consider transformer-XL as our baseline model\nwhich is one of memorybased transformer models. We show that our resultant\nmodel, Skip Cross-head TransformerXL, outperforms the baseline on character\nlevel language modeling task with similar parameters and outperforms the\nbaseline on word level language modelling task with almost 20% fewer\nparameters. Our proposed methods do not require any additional memory. We also\ndemonstrate the effectiveness of our regularization mechanism on BERT which\nshows similar performance with reduction in standard deviation of scores of\naround 30% on multiple GLUE tasks.\n","authors":["Vishwajit Kumar Vishnu","C. Chandra Sekhar"],"pdf_url":"https://arxiv.org/pdf/2311.08123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08118v1","updated":"2023-11-14T12:33:19Z","published":"2023-11-14T12:33:19Z","title":"Evaluating Neighbor Explainability for Graph Neural Networks","summary":"  Explainability in Graph Neural Networks (GNNs) is a new field growing in the\nlast few years. In this publication we address the problem of determining how\nimportant is each neighbor for the GNN when classifying a node and how to\nmeasure the performance for this specific task. To do this, various known\nexplainability methods are reformulated to get the neighbor importance and four\nnew metrics are presented. Our results show that there is almost no difference\nbetween the explanations provided by gradient-based techniques in the GNN\ndomain. In addition, many explainability techniques failed to identify\nimportant neighbors when GNNs without self-loops are used.\n","authors":["Oscar Llorente","Péter Vaderna","Sándor Laki","Roland Kotroczó","Rita Csoma","János Márk Szalai-Gindl"],"pdf_url":"https://arxiv.org/pdf/2311.08118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08113v1","updated":"2023-11-14T12:24:12Z","published":"2023-11-14T12:24:12Z","title":"Understanding learning from EEG data: Combining machine learning and\n  feature engineering based on hidden Markov models and mixed models","summary":"  Theta oscillations, ranging from 4-8 Hz, play a significant role in spatial\nlearning and memory functions during navigation tasks. Frontal theta\noscillations are thought to play an important role in spatial navigation and\nmemory. Electroencephalography (EEG) datasets are very complex, making any\nchanges in the neural signal related to behaviour difficult to interpret.\nHowever, multiple analytical methods are available to examine complex data\nstructure, especially machine learning based techniques. These methods have\nshown high classification performance and the combination with feature\nengineering enhances the capability of these methods. This paper proposes using\nhidden Markov and linear mixed effects models to extract features from EEG\ndata. Based on the engineered features obtained from frontal theta EEG data\nduring a spatial navigation task in two key trials (first, last) and between\ntwo conditions (learner and non-learner), we analysed the performance of six\nmachine learning methods (Polynomial Support Vector Machines, Non-linear\nSupport Vector Machines, Random Forests, K-Nearest Neighbours, Ridge, and Deep\nNeural Networks) on classifying learner and non-learner participants. We also\nanalysed how different standardisation methods used to pre-process the EEG data\ncontribute to classification performance. We compared the classification\nperformance of each trial with data gathered from the same subjects, including\nsolely coordinate-based features, such as idle time and average speed. We found\nthat more machine learning methods perform better classification using\ncoordinate-based data. However, only deep neural networks achieved an area\nunder the ROC curve higher than 80% using the theta EEG data alone. Our\nfindings suggest that standardising the theta EEG data and using deep neural\nnetworks enhances the classification of learner and non-learner subjects in a\nspatial learning task.\n","authors":["Gabriel Rodrigues Palma","Conor Thornberry","Seán Commins","Rafael de Andrade Moral"],"pdf_url":"https://arxiv.org/pdf/2311.08113v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2311.08105v1","updated":"2023-11-14T12:05:45Z","published":"2023-11-14T12:05:45Z","title":"DiLoCo: Distributed Low-Communication Training of Language Models","summary":"  Large language models (LLM) have become a critical component in many\napplications of machine learning. However, standard approaches to training LLM\nrequire a large number of tightly interconnected accelerators, with devices\nexchanging gradients and other intermediate states at each optimization step.\nWhile it is difficult to build and maintain a single computing cluster hosting\nmany accelerators, it might be easier to find several computing clusters each\nhosting a smaller number of devices. In this work, we propose a distributed\noptimization algorithm, Distributed Low-Communication (DiLoCo), that enables\ntraining of language models on islands of devices that are poorly connected.\nThe approach is a variant of federated averaging, where the number of inner\nsteps is large, the inner optimizer is AdamW, and the outer optimizer is\nNesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8\nworkers performs as well as fully synchronous optimization while communicating\n500 times less. DiLoCo exhibits great robustness to the data distribution of\neach worker. It is also robust to resources becoming unavailable over time, and\nvice versa, it can seamlessly leverage resources that become available during\ntraining.\n","authors":["Arthur Douillard","Qixuan Feng","Andrei A. Rusu","Rachita Chhaparia","Yani Donchev","Adhiguna Kuncoro","Marc'Aurelio Ranzato","Arthur Szlam","Jiajun Shen"],"pdf_url":"https://arxiv.org/pdf/2311.08105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15742v2","updated":"2023-11-14T12:02:43Z","published":"2023-10-24T11:34:15Z","title":"Improving Diffusion Models for ECG Imputation with an Augmented Template\n  Prior","summary":"  Pulsative signals such as the electrocardiogram (ECG) are extensively\ncollected as part of routine clinical care. However, noisy and poor-quality\nrecordings are a major issue for signals collected using mobile health systems,\ndecreasing the signal quality, leading to missing values, and affecting\nautomated downstream tasks. Recent studies have explored the imputation of\nmissing values in ECG with probabilistic time-series models. Nevertheless, in\ncomparison with the deterministic models, their performance is still limited,\nas the variations across subjects and heart-beat relationships are not\nexplicitly considered in the training objective. In this work, to improve the\nimputation and forecasting accuracy for ECG with probabilistic models, we\npresent a template-guided denoising diffusion probabilistic model (DDPM),\nPulseDiff, which is conditioned on an informative prior for a range of health\nconditions. Specifically, 1) we first extract a subject-level pulsative\ntemplate from the observed values to use as an informative prior of the missing\nvalues, which personalises the prior; 2) we then add beat-level stochastic\nshift terms to augment the prior, which considers variations in the position\nand amplitude of the prior at each beat; 3) we finally design a confidence\nscore to consider the health condition of the subject, which ensures our prior\nis provided safely. Experiments with the PTBXL dataset reveal that PulseDiff\nimproves the performance of two strong DDPM baseline models, CSDI and\nSSSD$^{S4}$, verifying that our method guides the generation of DDPMs while\nmanaging the uncertainty. When combined with SSSD$^{S4}$, PulseDiff outperforms\nthe leading deterministic model for short-interval missing data and is\ncomparable for long-interval data loss.\n","authors":["Alexander Jenkins","Zehua Chen","Fu Siong Ng","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2310.15742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08094v1","updated":"2023-11-14T11:38:38Z","published":"2023-11-14T11:38:38Z","title":"Act-VIT: A Representationally Robust Attention Architecture for Skeleton\n  Based Action Recognition Using Vision Transformer","summary":"  Skeleton-based action recognition receives the attention of many researchers\nas it is robust to viewpoint and illumination changes, and its processing is\nmuch more efficient than video frames. With the emergence of deep learning\nmodels, it has become very popular to represent the skeleton data in\npseudo-image form and apply Convolutional Neural Networks for action\nrecognition. Thereafter, studies concentrated on finding effective methods for\nforming pseudo-images. Recently, attention networks, more specifically\ntransformers have provided promising results in various vision problems. In\nthis study, the effectiveness of vision transformers for skeleton-based action\nrecognition is examined and its robustness on the pseudo-image representation\nscheme is investigated. To this end, a three-level architecture, Act-VIT is\nproposed, which forms a set of pseudo images apply a classifier on each of the\nrepresentation and combine their results to find the final action class. The\nclassifiers of Act-VIT are first realized by CNNs and then by VITs and their\nperformances are compared. Experimental studies reveal that the vision\ntransformer is less sensitive to the initial pseudo-image representation\ncompared to CNN. Nevertheless, even with the vision transformer, the\nrecognition performance can be further improved by consensus of classifiers.\n","authors":["Ozge Oztimur Karadag"],"pdf_url":"https://arxiv.org/pdf/2311.08094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15807v2","updated":"2023-11-14T11:37:11Z","published":"2023-07-28T20:58:00Z","title":"Anomaly Detection in Industrial Machinery using IoT Devices and Machine\n  Learning: a Systematic Mapping","summary":"  Anomaly detection is critical in the smart industry for preventing equipment\nfailure, reducing downtime, and improving safety. Internet of Things (IoT) has\nenabled the collection of large volumes of data from industrial machinery,\nproviding a rich source of information for Anomaly Detection. However, the\nvolume and complexity of data generated by the Internet of Things ecosystems\nmake it difficult for humans to detect anomalies manually. Machine learning\n(ML) algorithms can automate anomaly detection in industrial machinery by\nanalyzing generated data. Besides, each technique has specific strengths and\nweaknesses based on the data nature and its corresponding systems. However, the\ncurrent systematic mapping studies on Anomaly Detection primarily focus on\naddressing network and cybersecurity-related problems, with limited attention\ngiven to the industrial sector. Additionally, these studies do not cover the\nchallenges involved in using ML for Anomaly Detection in industrial machinery\nwithin the context of the IoT ecosystems. This paper presents a systematic\nmapping study on Anomaly Detection for industrial machinery using IoT devices\nand ML algorithms to address this gap. The study comprehensively evaluates 84\nrelevant studies spanning from 2016 to 2023, providing an extensive review of\nAnomaly Detection research. Our findings identify the most commonly used\nalgorithms, preprocessing techniques, and sensor types. Additionally, this\nreview identifies application areas and points to future challenges and\nresearch opportunities.\n","authors":["Sérgio F. Chevtchenko","Elisson da Silva Rocha","Monalisa Cristina Moura Dos Santos","Ricardo Lins Mota","Diego Moura Vieira","Ermeson Carneiro de Andrade","Danilo Ricardo Barbosa de Araújo"],"pdf_url":"https://arxiv.org/pdf/2307.15807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12570v2","updated":"2023-11-14T11:32:53Z","published":"2023-10-19T08:25:03Z","title":"DA-TransUNet: Integrating Spatial and Channel Dual Attention with\n  Transformer U-Net for Medical Image Segmentation","summary":"  Accurate medical image segmentation is critical for disease quantification\nand treatment evaluation. While traditional Unet architectures and their\ntransformer-integrated variants excel in automated segmentation tasks. However,\nthey lack the ability to harness the intrinsic position and channel features of\nimage. Existing models also struggle with parameter efficiency and\ncomputational complexity, often due to the extensive use of Transformers. To\naddress these issues, this study proposes a novel deep medical image\nsegmentation framework, called DA-TransUNet, aiming to integrate the\nTransformer and dual attention block(DA-Block) into the traditional U-shaped\narchitecture. Unlike earlier transformer-based U-net models, DA-TransUNet\nutilizes Transformers and DA-Block to integrate not only global and local\nfeatures, but also image-specific positional and channel features, improving\nthe performance of medical image segmentation. By incorporating a DA-Block at\nthe embedding layer and within each skip connection layer, we substantially\nenhance feature extraction capabilities and improve the efficiency of the\nencoder-decoder structure. DA-TransUNet demonstrates superior performance in\nmedical image segmentation tasks, consistently outperforming state-of-the-art\ntechniques across multiple datasets. In summary, DA-TransUNet offers a\nsignificant advancement in medical image segmentation, providing an effective\nand powerful alternative to existing techniques. Our architecture stands out\nfor its ability to improve segmentation accuracy, thereby advancing the field\nof automated medical image diagnostics. The codes and parameters of our model\nwill be publicly available at https://github.com/SUN-1024/DA-TransUnet.\n","authors":["Guanqun Sun","Yizhi Pan","Weikun Kong","Zichang Xu","Jianhua Ma","Teeradaj Racharak","Le-Minh Nguyen","Junyi Xin"],"pdf_url":"https://arxiv.org/pdf/2310.12570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07838v4","updated":"2023-11-14T11:26:10Z","published":"2023-10-11T19:30:08Z","title":"Towards the Fundamental Limits of Knowledge Transfer over Finite Domains","summary":"  We characterize the statistical efficiency of knowledge transfer through $n$\nsamples from a teacher to a probabilistic student classifier with input space\n$\\mathcal S$ over labels $\\mathcal A$. We show that privileged information at\nthree progressive levels accelerates the transfer. At the first level, only\nsamples with hard labels are known, via which the maximum likelihood estimator\nattains the minimax rate $\\sqrt{{|{\\mathcal S}||{\\mathcal A}|}/{n}}$. The\nsecond level has the teacher probabilities of sampled labels available in\naddition, which turns out to boost the convergence rate lower bound to\n${{|{\\mathcal S}||{\\mathcal A}|}/{n}}$. However, under this second data\nacquisition protocol, minimizing a naive adaptation of the cross-entropy loss\nresults in an asymptotically biased student. We overcome this limitation and\nachieve the fundamental limit by using a novel empirical variant of the squared\nerror logit loss. The third level further equips the student with the soft\nlabels (complete logits) on ${\\mathcal A}$ given every sampled input, thereby\nprovably enables the student to enjoy a rate ${|{\\mathcal S}|}/{n}$ free of\n$|{\\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be\noptimal in the last case. Numerical simulations distinguish the four learners\nand corroborate our theory.\n","authors":["Qingyue Zhao","Banghua Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07838v4.pdf","comment":"41 pages, 2 figures; Appendix polished"},{"id":"http://arxiv.org/abs/2211.08253v3","updated":"2023-11-14T11:16:59Z","published":"2022-11-15T15:59:43Z","title":"HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization","summary":"  Due to domain shifts, machine learning systems typically struggle to\ngeneralize well to new domains that differ from those of training data, which\nis what domain generalization (DG) aims to address. Although a variety of DG\nmethods have been proposed, most of them fall short in interpretability and\nrequire domain labels, which are not available in many real-world scenarios.\nThis paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture\nof Experts (MoE), which does not rely on domain labels and is more\ninterpretable. MoE proves effective in identifying heterogeneous patterns in\ndata. For the DG problem, heterogeneity arises exactly from domain shifts. HMOE\nemploys hypernetworks taking vectors as input to generate the weights of\nexperts, which promotes knowledge sharing among experts and enables the\nexploration of their similarities in a low-dimensional vector space. We\nbenchmark HMOE against other DG methods under a fair evaluation framework --\nDomainBed. Our extensive experiments show that HMOE can effectively separate\nmixed-domain data into distinct clusters that are surprisingly more consistent\nwith human intuition than original domain labels. Using self-learned domain\ninformation, HMOE achieves state-of-the-art results on most datasets and\nsignificantly surpasses other DG methods in average accuracy across all\ndatasets.\n","authors":["Jingang Qu","Thibault Faney","Ze Wang","Patrick Gallinari","Soleiman Yousef","Jean-Charles de Hemptinne"],"pdf_url":"https://arxiv.org/pdf/2211.08253v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08083v1","updated":"2023-11-14T11:10:46Z","published":"2023-11-14T11:10:46Z","title":"Solving ARC visual analogies with neural embeddings and vector\n  arithmetic: A generalized method","summary":"  Analogical reasoning derives information from known relations and generalizes\nthis information to similar yet unfamiliar situations. One of the first\ngeneralized ways in which deep learning models were able to solve verbal\nanalogies was through vector arithmetic of word embeddings, essentially\nrelating words that were mapped to a vector space (e.g., king - man + woman =\n__?). In comparison, most attempts to solve visual analogies are still\npredominantly task-specific and less generalizable. This project focuses on\nvisual analogical reasoning and applies the initial generalized mechanism used\nto solve verbal analogies to the visual realm. Taking the Abstraction and\nReasoning Corpus (ARC) as an example to investigate visual analogy solving, we\nuse a variational autoencoder (VAE) to transform ARC items into low-dimensional\nlatent vectors, analogous to the word embeddings used in the verbal approaches.\nThrough simple vector arithmetic, underlying rules of ARC items are discovered\nand used to solve them. Results indicate that the approach works well on simple\nitems with fewer dimensions (i.e., few colors used, uniform shapes), similar\ninput-to-output examples, and high reconstruction accuracy on the VAE.\nPredictions on more complex items showed stronger deviations from expected\noutputs, although, predictions still often approximated parts of the item's\nrule set. Error patterns indicated that the model works as intended. On the\nofficial ARC paradigm, the model achieved a score of 2% (cf. current world\nrecord is 21%) and on ConceptARC it scored 8.8%. Although the methodology\nproposed involves basic dimensionality reduction techniques and standard vector\narithmetic, this approach demonstrates promising outcomes on ARC and can easily\nbe generalized to other abstract visual reasoning tasks.\n","authors":["Luca H. Thoms","Karel A. Veldkamp","Hannes Rosenbusch","Claire E. Stevenson"],"pdf_url":"https://arxiv.org/pdf/2311.08083v1.pdf","comment":"Data and code can be found on\n  https://github.com/foger3/ARC_DeepLearning"},{"id":"http://arxiv.org/abs/2311.08081v1","updated":"2023-11-14T11:08:47Z","published":"2023-11-14T11:08:47Z","title":"Evolutionary-enhanced quantum supervised learning model","summary":"  Quantum supervised learning, utilizing variational circuits, stands out as a\npromising technology for NISQ devices due to its efficiency in hardware\nresource utilization during the creation of quantum feature maps and the\nimplementation of hardware-efficient ansatz with trainable parameters. Despite\nthese advantages, the training of quantum models encounters challenges, notably\nthe barren plateau phenomenon, leading to stagnation in learning during\noptimization iterations. This study proposes an innovative approach: an\nevolutionary-enhanced ansatz-free supervised learning model. In contrast to\nparametrized circuits, our model employs circuits with variable topology that\nevolves through an elitist method, mitigating the barren plateau issue.\nAdditionally, we introduce a novel concept, the superposition of multi-hot\nencodings, facilitating the treatment of multi-classification problems. Our\nframework successfully avoids barren plateaus, resulting in enhanced model\naccuracy. Comparative analysis with variational quantum classifiers from the\ntechnology's state-of-the-art reveal a substantial improvement in training\nefficiency and precision. Furthermore, we conduct tests on a challenging\ndataset class, traditionally problematic for conventional kernel machines,\ndemonstrating a potential alternative path for achieving quantum advantage in\nsupervised learning for NISQ era.\n","authors":["Anton Simen Albino","Rodrigo Bloot","Otto M. Pires","Erick G. S. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2311.08081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00305v2","updated":"2023-11-14T10:44:15Z","published":"2023-09-01T07:29:44Z","title":"Efficient Surrogate Models for Materials Science Simulations: Machine\n  Learning-based Prediction of Microstructure Properties","summary":"  Determining, understanding, and predicting the so-called structure-property\nrelation is an important task in many scientific disciplines, such as\nchemistry, biology, meteorology, physics, engineering, and materials science.\nStructure refers to the spatial distribution of, e.g., substances, material, or\nmatter in general, while property is a resulting characteristic that usually\ndepends in a non-trivial way on spatial details of the structure.\nTraditionally, forward simulations models have been used for such tasks.\nRecently, several machine learning algorithms have been applied in these\nscientific fields to enhance and accelerate simulation models or as surrogate\nmodels. In this work, we develop and investigate the applications of six\nmachine learning techniques based on two different datasets from the domain of\nmaterials science: data from a two-dimensional Ising model for predicting the\nformation of magnetic domains and data representing the evolution of dual-phase\nmicrostructures from the Cahn-Hilliard model. We analyze the accuracy and\nrobustness of all models and elucidate the reasons for the differences in their\nperformances. The impact of including domain knowledge through tailored\nfeatures is studied, and general recommendations based on the availability and\nquality of training data are derived from this.\n","authors":["Binh Duong Nguyen","Pavlo Potapenko","Aytekin Dermici","Kishan Govind","Sébastien Bompas","Stefan Sandfeld"],"pdf_url":"https://arxiv.org/pdf/2309.00305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08053v1","updated":"2023-11-14T10:23:00Z","published":"2023-11-14T10:23:00Z","title":"Communication-Constrained Bayesian Active Knowledge Distillation","summary":"  Consider an active learning setting in which a learner has a training set\nwith few labeled examples and a pool set with many unlabeled inputs, while a\nremote teacher has a pre-trained model that is known to perform well for the\nlearner's task. The learner actively transmits batches of unlabeled inputs to\nthe teacher through a constrained communication channel for labeling. This\npaper addresses the following key questions: (i) Active batch selection: Which\nbatch of inputs should be sent to the teacher to acquire the most useful\ninformation and thus reduce the number of required communication rounds? (ii)\nBatch encoding: How do we encode the batch of inputs for transmission to the\nteacher to reduce the communication resources required at each round? We\nintroduce Communication-Constrained Bayesian Active Knowledge Distillation\n(CC-BAKD), a novel protocol that integrates Bayesian active learning with\ncompression via a linear mix-up mechanism. Bayesian active learning selects the\nbatch of inputs based on their epistemic uncertainty, addressing the\n\"confirmation bias\" that is known to increase the number of required\ncommunication rounds. Furthermore, the proposed mix-up compression strategy is\nintegrated with the epistemic uncertainty-based active batch selection process\nto reduce the communication overhead per communication round.\n","authors":["Victor Croisfelt","Shashi Raj Pandey","Osvaldo Simeone","Petar Popovski"],"pdf_url":"https://arxiv.org/pdf/2311.08053v1.pdf","comment":"6 pages, 4 figures, conference version, submitted to IEEE ICC 2024"},{"id":"http://arxiv.org/abs/2311.08045v1","updated":"2023-11-14T10:10:31Z","published":"2023-11-14T10:10:31Z","title":"Adversarial Preference Optimization","summary":"  Human preference alignment is a crucial training step to improve the\ninteraction quality of large language models (LLMs). Existing aligning methods\ndepend on manually annotated preference data to guide the LLM optimization\ndirections. However, in practice, continuously updating LLMs raises a\ndistribution gap between model-generated samples and human-preferred responses,\nwhich hinders model fine-tuning efficiency. To mitigate this issue, previous\nmethods require additional preference annotation on generated samples to adapt\nthe shifted distribution, which consumes a large amount of annotation\nresources. Targeting more efficient human preference optimization, we propose\nan adversarial preference optimization (APO) framework, where the LLM agent and\nthe preference model update alternatively via a min-max game. Without\nadditional annotation, our APO method can make a self-adaption to the\ngeneration distribution gap through the adversarial learning process. In\nexperiments, we empirically verify the effectiveness of APO in improving LLM's\nhelpfulness and harmlessness compared with rejection sampling baselines.\n","authors":["Pengyu Cheng","Yifan Yang","Jian Li","Yong Dai","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2311.08045v1.pdf","comment":"In process"},{"id":"http://arxiv.org/abs/2311.08035v1","updated":"2023-11-14T09:55:03Z","published":"2023-11-14T09:55:03Z","title":"Data-driven building energy efficiency prediction based on envelope heat\n  losses using physics-informed neural networks","summary":"  The analytical prediction of building energy performance in residential\nbuildings based on the heat losses of its individual envelope components is a\nchallenging task. It is worth noting that this field is still in its infancy,\nwith relatively limited research conducted in this specific area to date,\nespecially when it comes for data-driven approaches. In this paper we introduce\na novel physics-informed neural network model for addressing this problem.\nThrough the employment of unexposed datasets that encompass general building\ninformation, audited characteristics, and heating energy consumption, we feed\nthe deep learning model with general building information, while the model's\noutput consists of the structural components and several thermal properties\nthat are in fact the basic elements of an energy performance certificate (EPC).\nOn top of this neural network, a function, based on physics equations,\ncalculates the energy consumption of the building based on heat losses and\nenhances the loss function of the deep learning model. This methodology is\ntested on a real case study for 256 buildings located in Riga, Latvia. Our\ninvestigation comes up with promising results in terms of prediction accuracy,\npaving the way for automated, and data-driven energy efficiency performance\nprediction based on basic properties of the building, contrary to exhaustive\nenergy efficiency audits led by humans, which are the current status quo.\n","authors":["Vasilis Michalakopoulos","Sotiris Pelekis","Giorgos Kormpakis","Vagelis Karakolis","Spiros Mouzakitis","Dimitris Askounis"],"pdf_url":"https://arxiv.org/pdf/2311.08035v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2311.08024v1","updated":"2023-11-14T09:33:33Z","published":"2023-11-14T09:33:33Z","title":"MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with\n  Semi Supervised Learning for Low Dose CT","summary":"  Image quality assessment (IQA) plays a critical role in optimizing radiation\ndose and developing novel medical imaging techniques in computed tomography\n(CT). Traditional IQA methods relying on hand-crafted features have limitations\nin summarizing the subjective perceptual experience of image quality. Recent\ndeep learning-based approaches have demonstrated strong modeling capabilities\nand potential for medical IQA, but challenges remain regarding model\ngeneralization and perceptual accuracy. In this work, we propose a multi-scale\ndistributions regression approach to predict quality scores by constraining the\noutput distribution, thereby improving model generalization. Furthermore, we\ndesign a dual-branch alignment network to enhance feature extraction\ncapabilities. Additionally, semi-supervised learning is introduced by utilizing\npseudo-labels for unlabeled data to guide model training. Extensive qualitative\nexperiments demonstrate the effectiveness of our proposed method for advancing\nthe state-of-the-art in deep learning-based medical IQA. Code is available at:\nhttps://github.com/zunzhumu/MD-IQA.\n","authors":["Tao Song","Ruizhi Hou","Lisong Dai","Lei Xiang"],"pdf_url":"https://arxiv.org/pdf/2311.08024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08022v1","updated":"2023-11-14T09:32:02Z","published":"2023-11-14T09:32:02Z","title":"Two-Stage Predict+Optimize for Mixed Integer Linear Programs with\n  Unknown Parameters in Constraints","summary":"  Consider the setting of constrained optimization, with some parameters\nunknown at solving time and requiring prediction from relevant features.\nPredict+Optimize is a recent framework for end-to-end training supervised\nlearning models for such predictions, incorporating information about the\noptimization problem in the training process in order to yield better\npredictions in terms of the quality of the predicted solution under the true\nparameters. Almost all prior works have focused on the special case where the\nunknowns appear only in the optimization objective and not the constraints. Hu\net al.~proposed the first adaptation of Predict+Optimize to handle unknowns\nappearing in constraints, but the framework has somewhat ad-hoc elements, and\nthey provided a training algorithm only for covering and packing linear\nprograms. In this work, we give a new \\emph{simpler} and \\emph{more powerful}\nframework called \\emph{Two-Stage Predict+Optimize}, which we believe should be\nthe canonical framework for the Predict+Optimize setting. We also give a\ntraining algorithm usable for all mixed integer linear programs, vastly\ngeneralizing the applicability of the framework. Experimental results\ndemonstrate the superior prediction performance of our training framework over\nall classical and state-of-the-art methods.\n","authors":["Xinyi Hu","Jasper C. H. Lee","Jimmy H. M. Lee"],"pdf_url":"https://arxiv.org/pdf/2311.08022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13816v2","updated":"2023-11-14T09:26:45Z","published":"2023-08-26T08:48:51Z","title":"Homological Convolutional Neural Networks","summary":"  Deep learning methods have demonstrated outstanding performances on\nclassification and regression tasks on homogeneous data types (e.g., image,\naudio, and text data). However, tabular data still pose a challenge, with\nclassic machine learning approaches being often computationally cheaper and\nequally effective than increasingly complex deep learning architectures. The\nchallenge arises from the fact that, in tabular data, the correlation among\nfeatures is weaker than the one from spatial or semantic relationships in\nimages or natural language, and the dependency structures need to be modeled\nwithout any prior information. In this work, we propose a novel deep learning\narchitecture that exploits the data structural organization through\ntopologically constrained network representations to gain relational\ninformation from sparse tabular inputs. The resulting model leverages the power\nof convolution and is centered on a limited number of concepts from network\ntopology to guarantee: (i) a data-centric and deterministic building pipeline;\n(ii) a high level of interpretability over the inference process; and (iii) an\nadequate room for scalability. We test our model on 18 benchmark datasets\nagainst 5 classic machine learning and 3 deep learning models, demonstrating\nthat our approach reaches state-of-the-art performances on these challenging\ndatasets. The code to reproduce all our experiments is provided at\nhttps://github.com/FinancialComputingUCL/HomologicalCNN.\n","authors":["Antonio Briola","Yuanrong Wang","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2308.13816v2.pdf","comment":"26 pages, 5 figures, 11 tables, 1 equation, 1 algorithm"},{"id":"http://arxiv.org/abs/2311.08016v1","updated":"2023-11-14T09:21:09Z","published":"2023-11-14T09:21:09Z","title":"Velocity-Based Channel Charting with Spatial Distribution Map Matching","summary":"  Fingerprint-based localization improves the positioning performance in\nchallenging, non-line-of-sight (NLoS) dominated indoor environments. However,\nfingerprinting models require an expensive life-cycle management including\nrecording and labeling of radio signals for the initial training and regularly\nat environmental changes. Alternatively, channel-charting avoids this labeling\neffort as it implicitly associates relative coordinates to the recorded radio\nsignals. Then, with reference real-world coordinates (positions) we can use\nsuch charts for positioning tasks. However, current channel-charting approaches\nlag behind fingerprinting in their positioning accuracy and still require\nreference samples for localization, regular data recording and labeling to keep\nthe models up to date. Hence, we propose a novel framework that does not\nrequire reference positions. We only require information from velocity\ninformation, e.g., from pedestrian dead reckoning or odometry to model the\nchannel charts, and topological map information, e.g., a building floor plan,\nto transform the channel charts into real coordinates. We evaluate our approach\non two different real-world datasets using 5G and distributed\nsingle-input/multiple-output system (SIMO) radio systems. Our experiments show\nthat even with noisy velocity estimates and coarse map information, we achieve\nsimilar position accuracies\n","authors":["Maximilian Stahlke","George Yammine","Tobias Feigl","Bjoern M. Eskofier","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2311.08016v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2303.06536v2","updated":"2023-11-14T09:18:43Z","published":"2023-03-12T01:45:05Z","title":"AutoOptLib: Tailoring Metaheuristic Optimizers via Automated Algorithm\n  Design","summary":"  Metaheuristics are prominent gradient-free optimizers for solving hard\nproblems that do not meet the rigorous mathematical assumptions of analytical\nsolvers. The canonical manual optimizer design could be laborious, untraceable\nand error-prone, let alone human experts are not always available. This arises\nincreasing interest and demand in automating the optimizer design process. In\nresponse, this paper proposes AutoOptLib, the first platform for accessible\nautomated design of metaheuristic optimizers. AutoOptLib leverages computing\nresources to conceive, build up, and verify the design choices of the\noptimizers. It requires much less labor resources and expertise than manual\ndesign, democratizing satisfactory metaheuristic optimizers to a much broader\nrange of researchers and practitioners. Furthermore, by fully exploring the\ndesign choices with computing resources, AutoOptLib has the potential to\nsurpass human experience, subsequently gaining enhanced performance compared\nwith human problem-solving. To realize the automated design, AutoOptLib\nprovides 1) a rich library of metaheuristic components for continuous,\ndiscrete, and permutation problems; 2) a flexible algorithm representation for\nevolving diverse algorithm structures; 3) different design objectives and\ntechniques for different optimization scenarios; and 4) a graphic user\ninterface for accessibility and practicability. AutoOptLib is fully written in\nMatlab/Octave; its source code and documentation are available at\nhttps://github.com/qz89/AutoOpt and https://AutoOpt.readthedocs.io/,\nrespectively.\n","authors":["Qi Zhao","Bai Yan","Taiwei Hu","Xianglong Chen","Qiqi Duan","Jian Yang","Yuhui Shi"],"pdf_url":"https://arxiv.org/pdf/2303.06536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08005v1","updated":"2023-11-14T09:03:33Z","published":"2023-11-14T09:03:33Z","title":"Iterative missing value imputation based on feature importance","summary":"  Many datasets suffer from missing values due to various reasons,which not\nonly increases the processing difficulty of related tasks but also reduces the\naccuracy of classification. To address this problem, the mainstream approach is\nto use missing value imputation to complete the dataset. Existing imputation\nmethods estimate the missing parts based on the observed values in the original\nfeature space, and they treat all features as equally important during data\ncompletion, while in fact different features have different importance.\nTherefore, we have designed an imputation method that considers feature\nimportance. This algorithm iteratively performs matrix completion and feature\nimportance learning, and specifically, matrix completion is based on a filling\nloss that incorporates feature importance. Our experimental analysis involves\nthree types of datasets: synthetic datasets with different noisy features and\nmissing values, real-world datasets with artificially generated missing values,\nand real-world datasets originally containing missing values. The results on\nthese datasets consistently show that the proposed method outperforms the\nexisting five imputation algorithms.To the best of our knowledge, this is the\nfirst work that considers feature importance in the imputation model.\n","authors":["Cong Guo","Chun Liu","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2311.08005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12945v2","updated":"2023-11-14T08:29:06Z","published":"2021-11-25T07:01:06Z","title":"Low-rank variational Bayes correction to the Laplace method","summary":"  Approximate inference methods like the Laplace method, Laplace approximations\nand variational methods, amongst others, are popular methods when exact\ninference is not feasible due to the complexity of the model or the abundance\nof data. In this paper we propose a hybrid approximate method called Low-Rank\nVariational Bayes correction (VBC), that uses the Laplace method and\nsubsequently a Variational Bayes correction in a lower dimension, to the joint\nposterior mean. The cost is essentially that of the Laplace method which\nensures scalability of the method, in both model complexity and data size.\nModels with fixed and unknown hyperparameters are considered, for simulated and\nreal examples, for small and large datasets.\n","authors":["Janet van Niekerk","Haavard Rue"],"pdf_url":"https://arxiv.org/pdf/2111.12945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08100v2","updated":"2023-11-14T08:25:37Z","published":"2023-10-12T07:50:37Z","title":"Generative Intrinsic Optimization: Intrinsic Control with Model Learning","summary":"  Future sequence represents the outcome after executing the action into the\nenvironment (i.e. the trajectory onwards). When driven by the\ninformation-theoretic concept of mutual information, it seeks maximally\ninformative consequences. Explicit outcomes may vary across state, return, or\ntrajectory serving different purposes such as credit assignment or imitation\nlearning. However, the inherent nature of incorporating intrinsic motivation\nwith reward maximization is often neglected. In this work, we propose a policy\niteration scheme that seamlessly incorporates the mutual information, ensuring\nconvergence to the optimal policy. Concurrently, a variational approach is\nintroduced, which jointly learns the necessary quantity for estimating the\nmutual information and the dynamics model, providing a general framework for\nincorporating different forms of outcomes of interest. While we mainly focus on\ntheoretical analysis, our approach opens the possibilities of leveraging\nintrinsic control with model learning to enhance sample efficiency and\nincorporate uncertainty of the environment into decision-making.\n","authors":["Jianfei Ma"],"pdf_url":"https://arxiv.org/pdf/2310.08100v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07534v2","updated":"2023-11-14T08:15:25Z","published":"2023-11-13T18:21:33Z","title":"Unsupervised Musical Object Discovery from Audio","summary":"  Current object-centric learning models such as the popular SlotAttention\narchitecture allow for unsupervised visual scene decomposition. Our novel\nMusicSlots method adapts SlotAttention to the audio domain, to achieve\nunsupervised music decomposition. Since concepts of opacity and occlusion in\nvision have no auditory analogues, the softmax normalization of alpha masks in\nthe decoders of visual object-centric models is not well-suited for decomposing\naudio objects. MusicSlots overcomes this problem. We introduce a\nspectrogram-based multi-object music dataset tailored to evaluate\nobject-centric learning on western tonal music. MusicSlots achieves good\nperformance on unsupervised note discovery and outperforms several established\nbaselines on supervised note property prediction tasks.\n","authors":["Joonsu Gha","Vincent Herrmann","Benjamin Grewe","Jürgen Schmidhuber","Anand Gopalakrishnan"],"pdf_url":"https://arxiv.org/pdf/2311.07534v2.pdf","comment":"Accepted to Machine Learning for Audio Workshop, NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07978v1","updated":"2023-11-14T08:10:14Z","published":"2023-11-14T08:10:14Z","title":"How good are Large Language Models on African Languages?","summary":"  Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on unseen tasks and\nlanguages. Additionally, they have been widely adopted as\nlanguage-model-as-a-service commercial APIs like GPT-4 API. However, their\nperformance on African languages is largely unknown. We present an analysis of\nthree popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks\n(news topic classification, sentiment classification, machine translation,\nquestion answering, and named entity recognition) across 30 African languages,\nspanning different language families and geographical regions. Our results\nsuggest that all LLMs produce below-par performance on African languages, and\nthere is a large gap in performance compared to high-resource languages like\nEnglish most tasks. We find that GPT-4 has an average or impressive performance\non classification tasks but very poor results on generative tasks like machine\ntranslation. Surprisingly, we find that mT0 had the best overall on\ncross-lingual QA, better than the state-of-the-art supervised model (i.e.\nfine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the\nworst performance due to its limited multilingual capabilities and\nEnglish-centric pre-training corpus. In general, our findings present a\ncall-to-action to ensure African languages are well represented in large\nlanguage models, given their growing popularity.\n","authors":["Jessica Ojo","Kelechi Ogueji","Pontus Stenetorp","David I. Adelani"],"pdf_url":"https://arxiv.org/pdf/2311.07978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07975v1","updated":"2023-11-14T08:05:02Z","published":"2023-11-14T08:05:02Z","title":"Out-of-Distribution Knowledge Distillation via Confidence Amendment","summary":"  Out-of-distribution (OOD) detection is essential in identifying test samples\nthat deviate from the in-distribution (ID) data upon which a standard network\nis trained, ensuring network robustness and reliability. This paper introduces\nOOD knowledge distillation, a pioneering learning framework applicable whether\nor not training ID data is available, given a standard network. This framework\nharnesses OOD-sensitive knowledge from the standard network to craft a binary\nclassifier adept at distinguishing between ID and OOD samples. To accomplish\nthis, we introduce Confidence Amendment (CA), an innovative methodology that\ntransforms an OOD sample into an ID one while progressively amending prediction\nconfidence derived from the standard network. This approach enables the\nsimultaneous synthesis of both ID and OOD samples, each accompanied by an\nadjusted prediction confidence, thereby facilitating the training of a binary\nclassifier sensitive to OOD. Theoretical analysis provides bounds on the\ngeneralization error of the binary classifier, demonstrating the pivotal role\nof confidence amendment in enhancing OOD sensitivity. Extensive experiments\nspanning various datasets and network architectures confirm the efficacy of the\nproposed method in detecting OOD samples.\n","authors":["Zhilin Zhao","Longbing Cao","Yixuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07967v1","updated":"2023-11-14T07:46:03Z","published":"2023-11-14T07:46:03Z","title":"Comparison of two data fusion approaches for land use classification","summary":"  Accurate land use maps, describing the territory from an anthropic\nutilisation point of view, are useful tools for land management and planning.\nTo produce them, the use of optical images alone remains limited. It is\ntherefore necessary to make use of several heterogeneous sources, each carrying\ncomplementary or contradictory information due to their imperfections or their\ndifferent specifications. This study compares two different approaches i.e. a\npre-classification and a post-classification fusion approach for combining\nseveral sources of spatial data in the context of land use classification. The\napproaches are applied on authoritative land use data located in the Gers\ndepartment in the southwest of France. Pre-classification fusion, while not\nexplicitly modeling imperfections, has the best final results, reaching an\noverall accuracy of 97% and a macro-mean F1 score of 88%.\n","authors":["Martin Cubaud","Arnaud Le Bris","Laurence Jolivet","Ana-Maria Olteanu-Raimond"],"pdf_url":"https://arxiv.org/pdf/2311.07967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07966v1","updated":"2023-11-14T07:44:46Z","published":"2023-11-14T07:44:46Z","title":"Higher-Order Expander Graph Propagation","summary":"  Graph neural networks operate on graph-structured data via exchanging\nmessages along edges. One limitation of this message passing paradigm is the\nover-squashing problem. Over-squashing occurs when messages from a node's\nexpanded receptive field are compressed into fixed-size vectors, potentially\ncausing information loss. To address this issue, recent works have explored\nusing expander graphs, which are highly-connected sparse graphs with low\ndiameters, to perform message passing. However, current methods on expander\ngraph propagation only consider pair-wise interactions, ignoring higher-order\nstructures in complex data. To explore the benefits of capturing these\nhigher-order correlations while still leveraging expander graphs, we introduce\nhigher-order expander graph propagation. We propose two methods for\nconstructing bipartite expanders and evaluate their performance on both\nsynthetic and real-world datasets.\n","authors":["Thomas Christie","Yu He"],"pdf_url":"https://arxiv.org/pdf/2311.07966v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2212.12989v3","updated":"2023-11-14T07:41:33Z","published":"2022-12-26T02:32:20Z","title":"Improved Kernel Alignment Regret Bound for Online Kernel Learning","summary":"  In this paper, we improve the kernel alignment regret bound for online kernel\nlearning in the regime of the Hinge loss function. Previous algorithm achieves\na regret of $O((\\mathcal{A}_TT\\ln{T})^{\\frac{1}{4}})$ at a computational\ncomplexity (space and per-round time) of $O(\\sqrt{\\mathcal{A}_TT\\ln{T}})$,\nwhere $\\mathcal{A}_T$ is called \\textit{kernel alignment}. We propose an\nalgorithm whose regret bound and computational complexity are better than\nprevious results. Our results depend on the decay rate of eigenvalues of the\nkernel matrix. If the eigenvalues of the kernel matrix decay exponentially,\nthen our algorithm enjoys a regret of $O(\\sqrt{\\mathcal{A}_T})$ at a\ncomputational complexity of $O(\\ln^2{T})$. Otherwise, our algorithm enjoys a\nregret of $O((\\mathcal{A}_TT)^{\\frac{1}{4}})$ at a computational complexity of\n$O(\\sqrt{\\mathcal{A}_TT})$. We extend our algorithm to batch learning and\nobtain a $O(\\frac{1}{T}\\sqrt{\\mathbb{E}[\\mathcal{A}_T]})$ excess risk bound\nwhich improves the previous $O(1/\\sqrt{T})$ bound.\n","authors":["Junfan Li","Shizhong Liao"],"pdf_url":"https://arxiv.org/pdf/2212.12989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09345v2","updated":"2023-11-14T07:39:32Z","published":"2023-08-18T07:07:15Z","title":"Denoising diffusion-based MRI to CT image translation enables automated\n  spinal segmentation","summary":"  Background: Automated segmentation of spinal MR images plays a vital role\nboth scientifically and clinically. However, accurately delineating posterior\nspine structures presents challenges.\n  Methods: This retrospective study, approved by the ethical committee,\ninvolved translating T1w and T2w MR image series into CT images in a total of\nn=263 pairs of CT/MR series. Landmark-based registration was performed to align\nimage pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit\nmodels (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired\ntranslation, SynDiff) image-to-image translation using \"peak signal to noise\nratio\" (PSNR) as quality measure. A publicly available segmentation network\nsegmented the synthesized CT datasets, and Dice scores were evaluated on\nin-house test sets and the \"MRSpineSeg Challenge\" volumes. The 2D findings were\nextended to 3D Pix2Pix and DDIM.\n  Results: 2D paired methods and SynDiff exhibited similar translation\nperformance and Dice scores on paired data. DDIM image mode achieved the\nhighest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated\nsimilar Dice scores (0.77). For craniocaudal axis rotations, at least two\nlandmarks per vertebra were required for registration. The 3D translation\noutperformed the 2D approach, resulting in improved Dice scores (0.80) and\nanatomically accurate segmentations in a higher resolution than the original MR\nimage.\n  Conclusion: Two landmarks per vertebra registration enabled paired\nimage-to-image translation from MR to CT and outperformed all unpaired\napproaches. The 3D techniques provided anatomically correct segmentations,\navoiding underprediction of small structures like the spinous process.\n","authors":["Robert Graf","Joachim Schmitt","Sarah Schlaeger","Hendrik Kristian Möller","Vasiliki Sideri-Lampretsa","Anjany Sekuboyina","Sandro Manuel Krieg","Benedikt Wiestler","Bjoern Menze","Daniel Rueckert","Jan Stefan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2308.09345v2.pdf","comment":"35 pages, 7 figures, Code and a model weights available\n  https://doi.org/10.5281/zenodo.8221159 and\n  https://doi.org/10.5281/zenodo.8198697"},{"id":"http://arxiv.org/abs/2311.07247v2","updated":"2023-11-14T07:36:39Z","published":"2023-11-13T11:29:38Z","title":"Simultaneous Clutter Detection and Semantic Segmentation of Moving\n  Objects for Automotive Radar Data","summary":"  The unique properties of radar sensors, such as their robustness to adverse\nweather conditions, make them an important part of the environment perception\nsystem of autonomous vehicles. One of the first steps during the processing of\nradar point clouds is often the detection of clutter, i.e. erroneous points\nthat do not correspond to real objects. Another common objective is the\nsemantic segmentation of moving road users. These two problems are handled\nstrictly separate from each other in literature. The employed neural networks\nare always focused entirely on only one of the tasks. In contrast to this, we\nexamine ways to solve both tasks at the same time with a single jointly used\nmodel. In addition to a new augmented multi-head architecture, we also devise a\nmethod to represent a network's predictions for the two tasks with only one\noutput value. This novel approach allows us to solve the tasks simultaneously\nwith the same inference time as a conventional task-specific model. In an\nextensive evaluation, we show that our setup is highly effective and\noutperforms every existing network for semantic segmentation on the RadarScenes\ndataset.\n","authors":["Johannes Kopp","Dominik Kellner","Aldi Piroli","Vinzenz Dallabetta","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2311.07247v2.pdf","comment":"Published at IEEE International Conference on Intelligent\n  Transportation Systems (ITSC), Bilbao, ESP, 2023"},{"id":"http://arxiv.org/abs/2310.00566v2","updated":"2023-11-14T07:30:18Z","published":"2023-10-01T03:50:34Z","title":"Empowering Many, Biasing a Few: Generalist Credit Scoring through Large\n  Language Models","summary":"  In the financial industry, credit scoring is a fundamental element, shaping\naccess to credit and determining the terms of loans for individuals and\nbusinesses alike. Traditional credit scoring methods, however, often grapple\nwith challenges such as narrow knowledge scope and isolated evaluation of\ncredit tasks. Our work posits that Large Language Models (LLMs) have great\npotential for credit scoring tasks, with strong generalization ability across\nmultiple tasks. To systematically explore LLMs for credit scoring, we propose\nthe first open-source comprehensive framework. We curate a novel benchmark\ncovering 9 datasets with 14K samples, tailored for credit assessment and a\ncritical examination of potential biases within LLMs, and the novel instruction\ntuning data with over 45k samples. We then propose the first Credit and Risk\nAssessment Large Language Model (CALM) by instruction tuning, tailored to the\nnuanced demands of various financial risk assessment tasks. We evaluate CALM,\nand existing state-of-art (SOTA) open source and close source LLMs on the build\nbenchmark. Our empirical results illuminate the capability of LLMs to not only\nmatch but surpass conventional models, pointing towards a future where credit\nscoring can be more inclusive, comprehensive, and unbiased. We contribute to\nthe industry's transformation by sharing our pioneering instruction-tuning\ndatasets, credit and risk assessment LLM, and benchmarks with the research\ncommunity and the financial industry.\n","authors":["Duanyu Feng","Yongfu Dai","Jimin Huang","Yifang Zhang","Qianqian Xie","Weiguang Han","Alejandro Lopez-Lira","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.00566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.03461v4","updated":"2023-11-14T07:25:48Z","published":"2022-10-07T11:16:36Z","title":"FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using\n  Style Representations","summary":"  In recent years, language-driven artistic style transfer has emerged as a new\ntype of style transfer technique, eliminating the need for a reference style\nimage by using natural language descriptions of the style. The first model to\nachieve this, called CLIPstyler, has demonstrated impressive stylisation\nresults. However, its lengthy optimisation procedure at runtime for each query\nlimits its suitability for many practical applications. In this work, we\npresent FastCLIPstyler, a generalised text-based image style transfer model\ncapable of stylising images in a single forward pass for arbitrary text inputs.\nFurthermore, we introduce EdgeCLIPstyler, a lightweight model designed for\ncompatibility with resource-constrained devices. Through quantitative and\nqualitative comparisons with state-of-the-art approaches, we demonstrate that\nour models achieve superior stylisation quality based on measurable metrics\nwhile offering significantly improved runtime efficiency, particularly on edge\ndevices.\n","authors":["Ananda Padhmanabhan Suresh","Sanjana Jain","Pavit Noinongyao","Ankush Ganguly","Ukrit Watchareeruetai","Aubin Samacoits"],"pdf_url":"https://arxiv.org/pdf/2210.03461v4.pdf","comment":"Accepted at the 2024 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2024)"},{"id":"http://arxiv.org/abs/2311.07957v1","updated":"2023-11-14T07:20:57Z","published":"2023-11-14T07:20:57Z","title":"Language Models are Better Bug Detector Through Code-Pair Classification","summary":"  Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful\nmodels for code generation and understanding. Fine-tuning these models comes\nwith a high computational cost and requires a large labeled dataset.\nAlternatively, in-context learning techniques allow models to learn downstream\ntasks with only a few examples. Recently, researchers have shown how in-context\nlearning performs well in bug detection and repair. In this paper, we propose\ncode-pair classification task in which both the buggy and non-buggy versions\nare given to the model, and the model identifies the buggy ones. We evaluate\nour task in real-world dataset of bug detection and two most powerful LLMs. Our\nexperiments indicate that an LLM can often pick the buggy from the non-buggy\nversion of the code, and the code-pair classification task is much easier\ncompared to be given a snippet and deciding if and where a bug exists.\n","authors":["Kamel Alrashedy"],"pdf_url":"https://arxiv.org/pdf/2311.07957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15794v2","updated":"2023-11-14T07:09:04Z","published":"2023-06-27T20:46:34Z","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide\n  Resolution","summary":"  Genomic (DNA) sequences encode an enormous amount of information for gene\nregulation and protein synthesis. Similar to natural language models,\nresearchers have proposed foundation models in genomics to learn generalizable\nfeatures from unlabeled genome data that can then be fine-tuned for downstream\ntasks such as identifying regulatory elements. Due to the quadratic scaling of\nattention, previous Transformer-based genomic models have used 512 to 4k tokens\nas context (<0.001% of the human genome), significantly limiting the modeling\nof long-range interactions in DNA. In addition, these methods rely on\ntokenizers or fixed k-mers to aggregate meaningful DNA units, losing single\nnucleotide resolution where subtle genetic variations can completely alter\nprotein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a\nlarge language model based on implicit convolutions was shown to match\nattention in quality while allowing longer context lengths and lower time\ncomplexity. Leveraging Hyena's new long-range capabilities, we present\nHyenaDNA, a genomic foundation model pretrained on the human reference genome\nwith context lengths of up to 1 million tokens at the single nucleotide-level -\nan up to 500x increase over previous dense attention-based models. HyenaDNA\nscales sub-quadratically in sequence length (training up to 160x faster than\nTransformer), uses single nucleotide tokens, and has full global context at\neach layer. We explore what longer context enables - including the first use of\nin-context learning in genomics. On fine-tuned benchmarks from the Nucleotide\nTransformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets\nusing a model with orders of magnitude less parameters and pretraining data. On\nthe GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by\n+10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.\n","authors":["Eric Nguyen","Michael Poli","Marjan Faizi","Armin Thomas","Callum Birch-Sykes","Michael Wornow","Aman Patel","Clayton Rabideau","Stefano Massaroli","Yoshua Bengio","Stefano Ermon","Stephen A. Baccus","Chris Ré"],"pdf_url":"https://arxiv.org/pdf/2306.15794v2.pdf","comment":"NeurIPS 2023 (Spotlight)"},{"id":"http://arxiv.org/abs/2311.07951v1","updated":"2023-11-14T07:04:47Z","published":"2023-11-14T07:04:47Z","title":"A Fast and Simple Algorithm for computing the MLE of Amplitude Density\n  Function Parameters","summary":"  Over the last decades, the family of $\\alpha$-stale distributions has proven\nto be useful for modelling in telecommunication systems. Particularly, in the\ncase of radar applications, finding a fast and accurate estimation for the\namplitude density function parameters appears to be very important. In this\nwork, the maximum likelihood estimator (MLE) is proposed for parameters of the\namplitude distribution. To do this, the amplitude data are \\emph{projected} on\nthe horizontal and vertical axes using two simple transformations. It is proved\nthat the \\emph{projected} data follow a zero-location symmetric $\\alpha$-stale\ndistribution for which the MLE can be computed quite fast. The average of\ncomputed MLEs based on two \\emph{projections} is considered as estimator for\nparameters of the amplitude distribution. Performance of the proposed\n\\emph{projection} method is demonstrated through simulation study and analysis\nof two sets of real radar data.\n","authors":["Mahdi Teimouri"],"pdf_url":"https://arxiv.org/pdf/2311.07951v1.pdf","comment":"5pages, 1 figure,"},{"id":"http://arxiv.org/abs/2311.07948v1","updated":"2023-11-14T06:58:09Z","published":"2023-11-14T06:58:09Z","title":"Finding Inductive Loop Invariants using Large Language Models","summary":"  Loop invariants are fundamental to reasoning about programs with loops. They\nestablish properties about a given loop's behavior. When they additionally are\ninductive, they become useful for the task of formal verification that seeks to\nestablish strong mathematical guarantees about program's runtime behavior. The\ninductiveness ensures that the invariants can be checked locally without\nconsulting the entire program, thus are indispensable artifacts in a formal\nproof of correctness. Finding inductive loop invariants is an undecidable\nproblem, and despite a long history of research towards practical solutions, it\nremains far from a solved problem. This paper investigates the capabilities of\nthe Large Language Models (LLMs) in offering a new solution towards this old,\nyet important problem. To that end, we first curate a dataset of verification\nproblems on programs with loops. Next, we design a prompt for exploiting LLMs,\nobtaining inductive loop invariants, that are checked for correctness using\nsound symbolic tools. Finally, we explore the effectiveness of using an\nefficient combination of a symbolic tool and an LLM on our dataset and compare\nit against a purely symbolic baseline. Our results demonstrate that LLMs can\nhelp improve the state-of-the-art in automated program verification.\n","authors":["Adharsh Kamath","Aditya Senthilnathan","Saikat Chakraborty","Pantazis Deligiannis","Shuvendu K. Lahiri","Akash Lal","Aseem Rastogi","Subhajit Roy","Rahul Sharma"],"pdf_url":"https://arxiv.org/pdf/2311.07948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00754v2","updated":"2023-11-14T06:55:30Z","published":"2023-07-03T04:57:40Z","title":"ImDiffusion: Imputed Diffusion Models for Multivariate Time Series\n  Anomaly Detection","summary":"  Anomaly detection in multivariate time series data is of paramount importance\nfor ensuring the efficient operation of large-scale systems across diverse\ndomains. However, accurately detecting anomalies in such data poses significant\nchallenges. Existing approaches, including forecasting and reconstruction-based\nmethods, struggle to address these challenges effectively. To overcome these\nlimitations, we propose a novel anomaly detection framework named ImDiffusion,\nwhich combines time series imputation and diffusion models to achieve accurate\nand robust anomaly detection. The imputation-based approach employed by\nImDiffusion leverages the information from neighboring values in the time\nseries, enabling precise modeling of temporal and inter-correlated\ndependencies, reducing uncertainty in the data, thereby enhancing the\nrobustness of the anomaly detection process. ImDiffusion further leverages\ndiffusion models as time series imputers to accurately capturing complex\ndependencies. We leverage the step-by-step denoised outputs generated during\nthe inference process to serve as valuable signals for anomaly prediction,\nresulting in improved accuracy and robustness of the detection process.\n  We evaluate the performance of ImDiffusion via extensive experiments on\nbenchmark datasets. The results demonstrate that our proposed framework\nsignificantly outperforms state-of-the-art approaches in terms of detection\naccuracy and timeliness. ImDiffusion is further integrated into the real\nproduction system in Microsoft and observe a remarkable 11.4% increase in\ndetection F1 score compared to the legacy approach. To the best of our\nknowledge, ImDiffusion represents a pioneering approach that combines\nimputation-based techniques with time series anomaly detection, while\nintroducing the novel use of diffusion models to the field.\n","authors":["Yuhang Chen","Chaoyun Zhang","Minghua Ma","Yudong Liu","Ruomeng Ding","Bowen Li","Shilin He","Saravan Rajmohan","Qingwei Lin","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.00754v2.pdf","comment":"To appear in VLDB 2024.Code:\n  https://github.com/17000cyh/IMDiffusion.git"},{"id":"http://arxiv.org/abs/2306.08230v2","updated":"2023-11-14T06:52:21Z","published":"2023-06-14T03:59:21Z","title":"Unbiased Learning of Deep Generative Models with Structured Discrete\n  Representations","summary":"  By composing graphical models with deep learning architectures, we learn\ngenerative models with the strengths of both frameworks. The structured\nvariational autoencoder (SVAE) inherits structure and interpretability from\ngraphical models, and flexible likelihoods for high-dimensional data from deep\nlearning, but poses substantial optimization challenges. We propose novel\nalgorithms for learning SVAEs, and are the first to demonstrate the SVAE's\nability to handle multimodal uncertainty when data is missing by incorporating\ndiscrete latent variables. Our memory-efficient implicit differentiation scheme\nmakes the SVAE tractable to learn via gradient descent, while demonstrating\nrobustness to incomplete optimization. To more rapidly learn accurate graphical\nmodel parameters, we derive a method for computing natural gradients without\nmanual derivations, which avoids biases found in prior work. These optimization\ninnovations enable the first comparisons of the SVAE to state-of-the-art time\nseries models, where the SVAE performs competitively while learning\ninterpretable and structured discrete data representations.\n","authors":["Harry Bendekgey","Gabriel Hope","Erik B. Sudderth"],"pdf_url":"https://arxiv.org/pdf/2306.08230v2.pdf","comment":"38 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.14550v2","updated":"2023-11-14T06:52:17Z","published":"2023-10-23T04:07:26Z","title":"Corruption-Robust Offline Reinforcement Learning with General Function\n  Approximation","summary":"  We investigate the problem of corruption robustness in offline reinforcement\nlearning (RL) with general function approximation, where an adversary can\ncorrupt each sample in the offline dataset, and the corruption level\n$\\zeta\\geq0$ quantifies the cumulative corruption amount over $n$ episodes and\n$H$ steps. Our goal is to find a policy that is robust to such corruption and\nminimizes the suboptimality gap with respect to the optimal policy for the\nuncorrupted Markov decision processes (MDPs). Drawing inspiration from the\nuncertainty-weighting technique from the robust online RL setting\n\\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight\niteration procedure to efficiently compute on batched samples and propose a\ncorruption-robust algorithm for offline RL. Notably, under the assumption of\nsingle policy coverage and the knowledge of $\\zeta$, our proposed algorithm\nachieves a suboptimality bound that is worsened by an additive factor of\n$\\mathcal O(\\zeta \\cdot (\\text{CC}(\\lambda,\\hat{\\mathcal F},\\mathcal\nZ_n^H))^{1/2} (C(\\hat{\\mathcal F},\\mu))^{-1/2} n^{-1})$ due to the corruption.\nHere $\\text{CC}(\\lambda,\\hat{\\mathcal F},\\mathcal Z_n^H)$ is the coverage\ncoefficient that depends on the regularization parameter $\\lambda$, the\nconfidence set $\\hat{\\mathcal F}$, and the dataset $\\mathcal Z_n^H$, and\n$C(\\hat{\\mathcal F},\\mu)$ is a coefficient that depends on $\\hat{\\mathcal F}$\nand the underlying data distribution $\\mu$. When specialized to linear MDPs,\nthe corruption-dependent error term reduces to $\\mathcal O(\\zeta d n^{-1})$\nwith $d$ being the dimension of the feature map, which matches the existing\nlower bound for corrupted linear MDPs. This suggests that our analysis is tight\nin terms of the corruption-dependent term.\n","authors":["Chenlu Ye","Rui Yang","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.14550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05511v2","updated":"2023-11-14T06:46:06Z","published":"2023-11-09T16:51:26Z","title":"Anytime-Constrained Reinforcement Learning","summary":"  We introduce and study constrained Markov Decision Processes (cMDPs) with\nanytime constraints. An anytime constraint requires the agent to never violate\nits budget at any point in time, almost surely. Although Markovian policies are\nno longer sufficient, we show that there exist optimal deterministic policies\naugmented with cumulative costs. In fact, we present a fixed-parameter\ntractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our\nreduction yields planning and learning algorithms that are time and\nsample-efficient for tabular cMDPs so long as the precision of the costs is\nlogarithmic in the size of the cMDP. However, we also show that computing\nnon-trivial approximately optimal policies is NP-hard in general. To circumvent\nthis bottleneck, we design provable approximation algorithms that efficiently\ncompute or learn an arbitrarily accurate approximately feasible policy with\noptimal value so long as the maximum supported cost is bounded by a polynomial\nin the cMDP or the absolute budget. Given our hardness results, our\napproximation guarantees are the best possible under worst-case analysis.\n","authors":["Jeremy McMahan","Xiaojin Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.05511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05949v3","updated":"2023-11-14T06:45:27Z","published":"2022-12-12T15:04:56Z","title":"Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear\n  Contextual Bandits and Markov Decision Processes","summary":"  Despite the significant interest and progress in reinforcement learning (RL)\nproblems with adversarial corruption, current works are either confined to the\nlinear setting or lead to an undesired $\\tilde{O}(\\sqrt{T}\\zeta)$ regret bound,\nwhere $T$ is the number of rounds and $\\zeta$ is the total amount of\ncorruption. In this paper, we consider the contextual bandit with general\nfunction approximation and propose a computationally efficient algorithm to\nachieve a regret of $\\tilde{O}(\\sqrt{T}+\\zeta)$. The proposed algorithm relies\non the recently developed uncertainty-weighted least-squares regression from\nlinear contextual bandit and a new weighted estimator of uncertainty for the\ngeneral function class. In contrast to the existing analysis that heavily\nrelies on the linear structure, we develop a novel technique to control the sum\nof weighted uncertainty, thus establishing the final regret bounds. We then\ngeneralize our algorithm to the episodic MDP setting and first achieve an\nadditive dependence on the corruption level $\\zeta$ in the scenario of general\nfunction approximation. Notably, our algorithms achieve regret bounds either\nnearly match the performance lower bound or improve the existing methods for\nall the corruption levels and in both known and unknown $\\zeta$ cases.\n","authors":["Chenlu Ye","Wei Xiong","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.05949v3.pdf","comment":"We study the corruption-robust MDPs and contextual bandits with\n  general function approximation"},{"id":"http://arxiv.org/abs/2311.07939v1","updated":"2023-11-14T06:33:41Z","published":"2023-11-14T06:33:41Z","title":"Discretized Distributed Optimization over Dynamic Digraphs","summary":"  We consider a discrete-time model of continuous-time distributed optimization\nover dynamic directed-graphs (digraphs) with applications to distributed\nlearning. Our optimization algorithm works over general strongly connected\ndynamic networks under switching topologies, e.g., in mobile multi-agent\nsystems and volatile networks due to link failures. Compared to many existing\nlines of work, there is no need for bi-stochastic weight designs on the links.\nThe existing literature mostly needs the link weights to be stochastic using\nspecific weight-design algorithms needed both at the initialization and at all\ntimes when the topology of the network changes. This paper eliminates the need\nfor such algorithms and paves the way for distributed optimization over\ntime-varying digraphs. We derive the bound on the gradient-tracking step-size\nand discrete time-step for convergence and prove dynamic stability using\narguments from consensus algorithms, matrix perturbation theory, and Lyapunov\ntheory. This work, particularly, is an improvement over existing\nstochastic-weight undirected networks in case of link removal or packet drops.\nThis is because the existing literature may need to rerun time-consuming and\ncomputationally complex algorithms for stochastic design, while the proposed\nstrategy works as long as the underlying network is weight-symmetric and\nbalanced. The proposed optimization framework finds applications to distributed\nclassification and learning.\n","authors":["Mohammadreza Doostmohammadian","Wei Jiang","Muwahida Liaquat","Alireza Aghasi","Houman Zarrabi"],"pdf_url":"https://arxiv.org/pdf/2311.07939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07929v1","updated":"2023-11-14T06:15:16Z","published":"2023-11-14T06:15:16Z","title":"Self-supervised Heterogeneous Graph Variational Autoencoders","summary":"  Heterogeneous Information Networks (HINs), which consist of various types of\nnodes and edges, have recently demonstrated excellent performance in graph\nmining. However, most existing heterogeneous graph neural networks (HGNNs)\nignore the problems of missing attributes, inaccurate attributes and scarce\nlabels for nodes, which limits their expressiveness. In this paper, we propose\na generative self-supervised model SHAVA to address these issues\nsimultaneously. Specifically, SHAVA first initializes all the nodes in the\ngraph with a low-dimensional representation matrix. After that, based on the\nvariational graph autoencoder framework, SHAVA learns both node-level and\nattribute-level embeddings in the encoder, which can provide fine-grained\nsemantic information to construct node attributes. In the decoder, SHAVA\nreconstructs both links and attributes. Instead of directly reconstructing raw\nfeatures for attributed nodes, SHAVA generates the initial low-dimensional\nrepresentation matrix for all the nodes, based on which raw features of\nattributed nodes are further reconstructed to leverage accurate attributes. In\nthis way, SHAVA can not only complete informative features for non-attributed\nnodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct\nextensive experiments to show the superiority of SHAVA in tackling HINs with\nmissing and inaccurate attributes.\n","authors":["Yige Zhao","Jianxiang Yu","Yao Cheng","Chengcheng Yu","Yiding Liu","Xiang Li","Shuaiqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07928v1","updated":"2023-11-14T06:13:52Z","published":"2023-11-14T06:13:52Z","title":"Towards Improving Robustness Against Common Corruptions in Object\n  Detectors Using Adversarial Contrastive Learning","summary":"  Neural networks have revolutionized various domains, exhibiting remarkable\naccuracy in tasks like natural language processing and computer vision.\nHowever, their vulnerability to slight alterations in input samples poses\nchallenges, particularly in safety-critical applications like autonomous\ndriving. Current approaches, such as introducing distortions during training,\nfall short in addressing unforeseen corruptions. This paper proposes an\ninnovative adversarial contrastive learning framework to enhance neural network\nrobustness simultaneously against adversarial attacks and common corruptions.\nBy generating instance-wise adversarial examples and optimizing contrastive\nloss, our method fosters representations that resist adversarial perturbations\nand remain robust in real-world scenarios. Subsequent contrastive learning then\nstrengthens the similarity between clean samples and their adversarial\ncounterparts, fostering representations resistant to both adversarial attacks\nand common distortions. By focusing on improving performance under adversarial\nand real-world conditions, our approach aims to bolster the robustness of\nneural networks in safety-critical applications, such as autonomous vehicles\nnavigating unpredictable weather conditions. We anticipate that this framework\nwill contribute to advancing the reliability of neural networks in challenging\nenvironments, facilitating their widespread adoption in mission-critical\nscenarios.\n","authors":["Shashank Kotyan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2311.07928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10627v2","updated":"2023-11-14T06:00:04Z","published":"2022-11-19T09:08:43Z","title":"EGRC-Net: Embedding-induced Graph Refinement Clustering Network","summary":"  Existing graph clustering networks heavily rely on a predefined yet fixed\ngraph, which can lead to failures when the initial graph fails to accurately\ncapture the data topology structure of the embedding space. In order to address\nthis issue, we propose a novel clustering network called Embedding-Induced\nGraph Refinement Clustering Network (EGRC-Net), which effectively utilizes the\nlearned embedding to adaptively refine the initial graph and enhance the\nclustering performance. To begin, we leverage both semantic and topological\ninformation by employing a vanilla auto-encoder and a graph convolution\nnetwork, respectively, to learn a latent feature representation. Subsequently,\nwe utilize the local geometric structure within the feature embedding space to\nconstruct an adjacency matrix for the graph. This adjacency matrix is\ndynamically fused with the initial one using our proposed fusion architecture.\nTo train the network in an unsupervised manner, we minimize the Jeffreys\ndivergence between multiple derived distributions. Additionally, we introduce\nan improved approximate personalized propagation of neural predictions to\nreplace the standard graph convolution network, enabling EGRC-Net to scale\neffectively. Through extensive experiments conducted on nine widely-used\nbenchmark datasets, we demonstrate that our proposed methods consistently\noutperform several state-of-the-art approaches. Notably, EGRC-Net achieves an\nimprovement of more than 11.99\\% in Adjusted Rand Index (ARI) over the best\nbaseline on the DBLP dataset. Furthermore, our scalable approach exhibits a\n10.73% gain in ARI while reducing memory usage by 33.73% and decreasing running\ntime by 19.71%. The code for EGRC-Net will be made publicly available at\n\\url{https://github.com/ZhihaoPENG-CityU/EGRC-Net}.\n","authors":["Zhihao Peng","Hui Liu","Yuheng Jia","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2211.10627v2.pdf","comment":"This paper has been accepted by IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2311.07919v1","updated":"2023-11-14T05:34:50Z","published":"2023-11-14T05:34:50Z","title":"Qwen-Audio: Advancing Universal Audio Understanding via Unified\n  Large-Scale Audio-Language Models","summary":"  Recently, instruction-following audio-language models have received broad\nattention for audio interaction with humans. However, the absence of\npre-trained audio models capable of handling diverse audio types and tasks has\nhindered progress in this field. Consequently, most existing works have only\nbeen able to support a limited range of interaction capabilities. In this\npaper, we develop the Qwen-Audio model and address this limitation by scaling\nup audio-language pre-training to cover over 30 tasks and various audio types,\nsuch as human speech, natural sounds, music, and songs, to facilitate universal\naudio understanding abilities. However, directly co-training all tasks and\ndatasets can lead to interference issues, as the textual labels associated with\ndifferent datasets exhibit considerable variations due to differences in task\nfocus, language, granularity of annotation, and text structure. To overcome the\none-to-many interference, we carefully design a multi-task training framework\nby conditioning on a sequence of hierarchical tags to the decoder for\nencouraging knowledge sharing and avoiding interference through shared and\nspecified tags respectively. Remarkably, Qwen-Audio achieves impressive\nperformance across diverse benchmark tasks without requiring any task-specific\nfine-tuning, surpassing its counterparts. Building upon the capabilities of\nQwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from\nvarious audios and text inputs, enabling multi-turn dialogues and supporting\nvarious audio-central scenarios.\n","authors":["Yunfei Chu","Jin Xu","Xiaohuan Zhou","Qian Yang","Shiliang Zhang","Zhijie Yan","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.07919v1.pdf","comment":"The code is released at https://github.com/QwenLM/Qwen-Audio"},{"id":"http://arxiv.org/abs/2306.09299v2","updated":"2023-11-14T05:24:06Z","published":"2023-06-15T17:27:20Z","title":"Can Language Models Teach Weaker Agents? Teacher Explanations Improve\n  Students via Personalization","summary":"  A hallmark property of explainable AI models is the ability to teach other\nagents, communicating knowledge of how to perform a task. While Large Language\nModels perform complex reasoning by generating explanations for their\npredictions, it is unclear whether they also make good teachers for weaker\nagents. To address this, we consider a student-teacher framework between two\nLLM agents and study if, when, and how the teacher should intervene with\nnatural language explanations to improve the student's performance. Since\ncommunication is expensive, we define a budget such that the teacher only\ncommunicates explanations for a fraction of the data, after which the student\nshould perform well on its own. We decompose the teaching problem along four\naxes: (1) if teacher's test time intervention improve student predictions, (2)\nwhen it is worth explaining a data point, (3) how the teacher should\npersonalize explanations to better teach the student, and (4) if teacher\nexplanations also improve students on future unexplained data. We first show\nthat teacher LLMs can indeed intervene on student reasoning to improve their\nperformance. Next, inspired by the Theory of Mind abilities of effective\nteachers, we propose building two few-shot mental models of the student. The\nfirst model defines an Intervention Function that simulates the utility of an\nintervention, allowing the teacher to intervene when this utility is the\nhighest and improving student performance at lower budgets. The second model\nenables the teacher to personalize explanations for a particular student and\noutperform unpersonalized teachers. We also demonstrate that in multi-turn\ninteractions, teacher explanations generalize and learning from explained data\nimproves student performance on future unexplained data. Finally, we verify\nthat misaligned teachers can lower student performance to random chance by\nintentionally misleading them.\n","authors":["Swarnadeep Saha","Peter Hase","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2306.09299v2.pdf","comment":"NeurIPS 2023 (23 pages, 12 figures). Our code is available at\n  https://github.com/swarnaHub/ExplanationIntervention"},{"id":"http://arxiv.org/abs/2311.07914v1","updated":"2023-11-14T05:21:57Z","published":"2023-11-14T05:21:57Z","title":"Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey","summary":"  The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\nconduct a comprehensive review of these knowledge-graph-based knowledge\naugmentation techniques in LLMs, focusing on their efficacy in mitigating\nhallucinations. We systematically categorize these methods into three\noverarching groups, offering both methodological comparisons and empirical\nevaluations of their performance. Lastly, the paper explores the challenges\nassociated with these techniques and outlines potential avenues for future\nresearch in this emerging field.\n","authors":["Garima Agrawal","Tharindu Kumarage","Zeyad Alghami","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2311.07914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07911v1","updated":"2023-11-14T05:13:55Z","published":"2023-11-14T05:13:55Z","title":"Instruction-Following Evaluation for Large Language Models","summary":"  One core capability of Large Language Models (LLMs) is to follow natural\nlanguage instructions. However, the evaluation of such abilities is not\nstandardized: Human evaluations are expensive, slow, and not objectively\nreproducible, while LLM-based auto-evaluation is potentially biased or limited\nby the ability of the evaluator LLM. To overcome these issues, we introduce\nInstruction-Following Eval (IFEval) for large language models. IFEval is a\nstraightforward and easy-to-reproduce evaluation benchmark. It focuses on a set\nof \"verifiable instructions\" such as \"write in more than 400 words\" and\n\"mention the keyword of AI at least 3 times\". We identified 25 types of those\nverifiable instructions and constructed around 500 prompts, with each prompt\ncontaining one or more verifiable instructions. We show evaluation results of\ntwo widely available LLMs on the market. Our code and data can be found at\nhttps://github.com/google-research/google-research/tree/master/instruction_following_eval\n","authors":["Jeffrey Zhou","Tianjian Lu","Swaroop Mishra","Siddhartha Brahma","Sujoy Basu","Yi Luan","Denny Zhou","Le Hou"],"pdf_url":"https://arxiv.org/pdf/2311.07911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13243v2","updated":"2023-11-14T05:10:51Z","published":"2023-05-22T17:13:33Z","title":"Chip-Chat: Challenges and Opportunities in Conversational Hardware\n  Design","summary":"  Modern hardware design starts with specifications provided in natural\nlanguage. These are then translated by hardware engineers into appropriate\nHardware Description Languages (HDLs) such as Verilog before synthesizing\ncircuit elements. Automating this translation could reduce sources of human\nerror from the engineering process. But, it is only recently that artificial\nintelligence (AI) has demonstrated capabilities for machine-based end-to-end\ndesign translations. Commercially-available instruction-tuned Large Language\nModels (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to\nproduce code in a variety of programming languages; but studies examining them\nfor hardware are still lacking. In this work, we thus explore the challenges\nfaced and opportunities presented when leveraging these recent advances in LLMs\nfor hardware design. Given that these `conversational' LLMs perform best when\nused interactively, we perform a case study where a hardware engineer\nco-architects a novel 8-bit accumulator-based microprocessor architecture with\nthe LLM according to real-world hardware constraints. We then sent the\nprocessor to tapeout in a Skywater 130nm shuttle, meaning that this `Chip-Chat'\nresulted in what we believe to be the world's first wholly-AI-written HDL for\ntapeout.\n","authors":["Jason Blocklove","Siddharth Garg","Ramesh Karri","Hammond Pearce"],"pdf_url":"https://arxiv.org/pdf/2305.13243v2.pdf","comment":"6 pages, 8 figures. Accepted in 2023 ACM/IEEE 5th Workshop on Machine\n  Learning for CAD (MLCAD)"},{"id":"http://arxiv.org/abs/2308.10874v2","updated":"2023-11-14T05:06:01Z","published":"2023-08-21T17:21:23Z","title":"Analyzing Transformer Dynamics as Movement through Embedding Space","summary":"  Transformer based language models exhibit intelligent behaviors such as\nunderstanding natural language, recognizing patterns, acquiring knowledge,\nreasoning, planning, reflecting and using tools. This paper explores how their\nunderlying mechanics give rise to intelligent behaviors. Towards that end, we\npropose framing Transformer dynamics as movement through embedding space.\nExamining Transformers through this perspective reveals key insights,\nestablishing a Theory of Transformers: 1) Intelligent behaviours map to paths\nin Embedding Space which, the Transformer random-walks through during\ninferencing. 2) LM training learns a probability distribution over all possible\npaths. `Intelligence' is learnt by assigning higher probabilities to paths\nrepresenting intelligent behaviors. No learning can take place in-context;\ncontext only narrows the subset of paths sampled during decoding. 5) The\nTransformer is a self-mapping composition function, folding a context sequence\ninto a context-vector such that it's proximity to a token-vector reflects its\nco-occurrence and conditioned probability. Thus, the physical arrangement of\nvectors in Embedding Space determines path probabilities. 6) Context vectors\nare composed by aggregating features of the sequence's tokens via a process we\ncall the encoding walk. Attention contributes a - potentially redundant -\nassociation-bias to this process. 7) This process is comprised of two principal\noperation types: filtering (data independent) and aggregation (data dependent).\nThis generalization unifies Transformers with other sequence models. Building\nupon this foundation, we formalize a popular semantic interpretation of\nembeddings into a ``concept-space theory'' and find some evidence of it's\nvalidity.\n","authors":["Sumeet S. Singh"],"pdf_url":"https://arxiv.org/pdf/2308.10874v2.pdf","comment":"V2. Rewrote abstract. Rewrote / re-organized the entire paper into a\n  more formal proposition/argument/result format. To shorten main paper length:\n  Wrote more compact text in general, moved \"negative self bias\" and \"encoder\n  v/s decoder walks\" sections to the appendix and packed figures. Styled as\n  TMLR"},{"id":"http://arxiv.org/abs/2306.08698v4","updated":"2023-11-14T05:04:24Z","published":"2023-06-14T18:38:32Z","title":"Phase Transitions of Civil Unrest across Countries and Time","summary":"  Phase transitions, characterized by abrupt shifts between macroscopic\npatterns of organization, are ubiquitous in complex systems. Despite\nconsiderable research in the physical and natural sciences, the empirical study\nof this phenomenon in societal systems is relatively underdeveloped. The goal\nof this study is to explore whether the dynamics of collective civil unrest can\nbe plausibly characterized as a sequence of recurrent phase shifts, with each\nphase having measurable and identifiable latent characteristics. Building on\nprevious efforts to characterize civil unrest as a self-organized critical\nsystem, we introduce a macro-level statistical model of civil unrest and\nevaluate its plausibility using a comprehensive dataset of civil unrest events\nin 170 countries from 1946 to 2017. Our findings demonstrate that the\nmacro-level phase model effectively captures the characteristics of civil\nunrest data from diverse countries globally and that universal mechanisms may\nunderlie certain aspects of the dynamics of civil unrest. We also introduce a\nscale to quantify a country's long-term unrest per unit of time and show that\ncivil unrest events tend to cluster geographically, with the magnitude of civil\nunrest concentrated in specific regions. Our approach has the potential to\nidentify and measure phase transitions in various collective human phenomena\nbeyond civil unrest, contributing to a better understanding of complex social\nsystems.\n","authors":["Dan Braha"],"pdf_url":"https://arxiv.org/pdf/2306.08698v4.pdf","comment":"Main paper (57 pages); Supporting Information (144 pages) will be\n  available upon request. To appear in npj Complexity"},{"id":"http://arxiv.org/abs/2301.13737v4","updated":"2023-11-14T04:44:08Z","published":"2023-01-31T16:17:18Z","title":"Self-Consistent Velocity Matching of Probability Flows","summary":"  We present a discretization-free scalable framework for solving a large class\nof mass-conserving partial differential equations (PDEs), including the\ntime-dependent Fokker-Planck equation and the Wasserstein gradient flow. The\nmain observation is that the time-varying velocity field of the PDE solution\nneeds to be self-consistent: it must satisfy a fixed-point equation involving\nthe probability flow characterized by the same velocity field. Instead of\ndirectly minimizing the residual of the fixed-point equation with neural\nparameterization, we use an iterative formulation with a biased gradient\nestimator that bypasses significant computational obstacles with strong\nempirical performance. Compared to existing approaches, our method does not\nsuffer from temporal or spatial discretization, covers a wider range of PDEs,\nand scales to high dimensions. Experimentally, our method recovers analytical\nsolutions accurately when they are available and achieves superior performance\nin high dimensions with less training time compared to alternatives.\n","authors":["Lingxiao Li","Samuel Hurault","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2301.13737v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07896v1","updated":"2023-11-14T04:08:14Z","published":"2023-11-14T04:08:14Z","title":"Bayesian Conditional Diffusion Models for Versatile Spatiotemporal\n  Turbulence Generation","summary":"  Turbulent flows have historically presented formidable challenges to\npredictive computational modeling. Traditional numerical simulations often\nrequire vast computational resources, making them infeasible for numerous\nengineering applications. As an alternative, deep learning-based surrogate\nmodels have emerged, offering data-drive solutions. However, these are\ntypically constructed within deterministic settings, leading to shortfall in\ncapturing the innate chaotic and stochastic behaviors of turbulent dynamics. We\nintroduce a novel generative framework grounded in probabilistic diffusion\nmodels for versatile generation of spatiotemporal turbulence. Our method\nunifies both unconditional and conditional sampling strategies within a\nBayesian framework, which can accommodate diverse conditioning scenarios,\nincluding those with a direct differentiable link between specified conditions\nand generated unsteady flow outcomes, and scenarios lacking such explicit\ncorrelations. A notable feature of our approach is the method proposed for\nlong-span flow sequence generation, which is based on autoregressive\ngradient-based conditional sampling, eliminating the need for cumbersome\nretraining processes. We showcase the versatile turbulence generation\ncapability of our framework through a suite of numerical experiments,\nincluding: 1) the synthesis of LES simulated instantaneous flow sequences from\nURANS inputs; 2) holistic generation of inhomogeneous, anisotropic wall-bounded\nturbulence, whether from given initial conditions, prescribed turbulence\nstatistics, or entirely from scratch; 3) super-resolved generation of\nhigh-speed turbulent boundary layer flows from low-resolution data across a\nrange of input resolutions. Collectively, our numerical experiments highlight\nthe merit and transformative potential of the proposed methods, making a\nsignificant advance in the field of turbulence generation.\n","authors":["Han Gao","Xu Han","Xiantao Fan","Luning Sun","Li-Ping Liu","Lian Duan","Jian-Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07896v1.pdf","comment":"37 pages, 31 figures"},{"id":"http://arxiv.org/abs/2311.06315v2","updated":"2023-11-14T03:55:35Z","published":"2023-11-09T22:26:03Z","title":"ShipGen: A Diffusion Model for Parametric Ship Hull Generation with\n  Multiple Objectives and Constraints","summary":"  Ship design is a years-long process that requires balancing complex design\ntrade-offs to create a ship that is efficient and effective. Finding new ways\nto improve the ship design process can lead to significant cost savings for\nship building and operation. One promising technology is generative artificial\nintelligence, which has been shown to reduce design cycle time and create\nnovel, high-performing designs. In literature review, generative artificial\nintelligence has been shown to generate ship hulls; however, ship design is\nparticularly difficult as the hull of a ship requires the consideration of many\nobjectives. This paper presents a study on the generation of parametric ship\nhull designs using a parametric diffusion model that considers multiple\nobjectives and constraints for the hulls. This denoising diffusion\nprobabilistic model (DDPM) generates the tabular parametric design vectors of a\nship hull for evaluation. In addition to a tabular DDPM, this paper details\nadding guidance to improve the quality of generated ship hull designs. By\nleveraging classifier guidance, the DDPM produced feasible parametric ship\nhulls that maintain the coverage of the initial training dataset of ship hulls\nwith a 99.5% rate, a 149x improvement over random sampling of the design vector\nparameters across the design space. Parametric ship hulls produced with\nperformance guidance saw an average of 91.4% reduction in wave drag\ncoefficients and an average of a 47.9x relative increase in the total displaced\nvolume of the hulls compared to the mean performance of the hulls in the\ntraining dataset. The use of a DDPM to generate parametric ship hulls can\nreduce design time by generating high-performing hull designs for future\nanalysis. These generated hulls have low drag and high volume, which can reduce\nthe cost of operating a ship and increase its potential to generate revenue.\n","authors":["Noah J. Bagazinski","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2311.06315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07888v1","updated":"2023-11-14T03:49:20Z","published":"2023-11-14T03:49:20Z","title":"RoboSense At Edge: Detecting Slip, Crumple and Shape of the Object in\n  Robotic Hand for Teleoprations","summary":"  Slip and crumple detection is essential for performing robust manipulation\ntasks with a robotic hand (RH) like remote surgery. It has been one of the\nchallenging problems in the robotics manipulation community. In this work, we\npropose a technique based on machine learning (ML) based techniques to detect\nthe slip, and crumple as well as the shape of an object that is currently held\nin the robotic hand. We proposed ML model will detect the slip, crumple, and\nshape using the force/torque exerted and the angular positions of the actuators\npresent in the RH. The proposed model would be integrated into the loop of a\nrobotic hand(RH) and haptic glove(HG). This would help us to reduce the latency\nin case of teleoperation\n","authors":["Sudev Kumar Padhi","Mohit Kumar","Debanka Giri","Subidh Ali"],"pdf_url":"https://arxiv.org/pdf/2311.07888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07336v3","updated":"2023-11-14T03:14:49Z","published":"2023-08-11T13:15:35Z","title":"Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic","summary":"  We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.\n","authors":["Terufumi Morishita","Gaku Morio","Atsuki Yamaguchi","Yasuhiro Sogawa"],"pdf_url":"https://arxiv.org/pdf/2308.07336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07876v1","updated":"2023-11-14T03:12:43Z","published":"2023-11-14T03:12:43Z","title":"Learning Adversarial Low-rank Markov Decision Processes with Unknown\n  Transition and Full-information Feedback","summary":"  In this work, we study the low-rank MDPs with adversarially changed losses in\nthe full-information feedback setting. In particular, the unknown transition\nprobability kernel admits a low-rank matrix decomposition \\citep{REPUCB22}, and\nthe loss functions may change adversarially but are revealed to the learner at\nthe end of each episode. We propose a policy optimization-based algorithm POLO,\nand we prove that it attains the\n$\\widetilde{O}(K^{\\frac{5}{6}}A^{\\frac{1}{2}}d\\ln(1+M)/(1-\\gamma)^2)$ regret\nguarantee, where $d$ is rank of the transition kernel (and hence the dimension\nof the unknown representations), $A$ is the cardinality of the action space,\n$M$ is the cardinality of the model class, and $\\gamma$ is the discounted\nfactor. Notably, our algorithm is oracle-efficient and has a regret guarantee\nwith no dependence on the size of potentially arbitrarily large state space.\nFurthermore, we also prove an $\\Omega(\\frac{\\gamma^2}{1-\\gamma} \\sqrt{d A K})$\nregret lower bound for this problem, showing that low-rank MDPs are\nstatistically more difficult to learn than linear MDPs in the regret\nminimization setting. To the best of our knowledge, we present the first\nalgorithm that interleaves representation learning, exploration, and\nexploitation to achieve the sublinear regret guarantee for RL with nonlinear\nfunction approximation and adversarial losses.\n","authors":["Canzhe Zhao","Ruofeng Yang","Baoxiang Wang","Xuezhou Zhang","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2311.07876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05583v2","updated":"2023-11-14T03:09:38Z","published":"2023-06-08T22:54:48Z","title":"Gibbs-Based Information Criteria and the Over-Parameterized Regime","summary":"  Double-descent refers to the unexpected drop in test loss of a learning\nalgorithm beyond an interpolating threshold with over-parameterization, which\nis not predicted by information criteria in their classical forms due to the\nlimitations in the standard asymptotic approach. We update these analyses using\nthe information risk minimization framework and provide Akaike Information\nCriterion (AIC) and Bayesian Information Criterion (BIC) for models learned by\nthe Gibbs algorithm. Notably, the penalty terms for the Gibbs-based AIC and BIC\ncorrespond to specific information measures, i.e., symmetrized KL information\nand KL divergence. We extend this information-theoretic analysis to\nover-parameterized models by providing two different Gibbs-based BICs to\ncompute the marginal likelihood of random feature models in the regime where\nthe number of parameters $p$ and the number of samples $n$ tend to infinity,\nwith $p/n$ fixed. Our experiments demonstrate that the Gibbs-based BIC can\nselect the high-dimensional model and reveal the mismatch between marginal\nlikelihood and population risk in the over-parameterized regime, providing new\ninsights to understand double-descent.\n","authors":["Haobo Chen","Yuheng Bu","Gregory W. Wornell"],"pdf_url":"https://arxiv.org/pdf/2306.05583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07868v1","updated":"2023-11-14T02:57:37Z","published":"2023-11-14T02:57:37Z","title":"Multi-Signal Reconstruction Using Masked Autoencoder From EEG During\n  Polysomnography","summary":"  Polysomnography (PSG) is an indispensable diagnostic tool in sleep medicine,\nessential for identifying various sleep disorders. By capturing physiological\nsignals, including EEG, EOG, EMG, and cardiorespiratory metrics, PSG presents a\npatient's sleep architecture. However, its dependency on complex equipment and\nexpertise confines its use to specialized clinical settings. Addressing these\nlimitations, our study aims to perform PSG by developing a system that requires\nonly a single EEG measurement. We propose a novel system capable of\nreconstructing multi-signal PSG from a single-channel EEG based on a masked\nautoencoder. The masked autoencoder was trained and evaluated using the\nSleep-EDF-20 dataset, with mean squared error as the metric for assessing the\nsimilarity between original and reconstructed signals. The model demonstrated\nproficiency in reconstructing multi-signal data. Our results present promise\nfor the development of more accessible and long-term sleep monitoring systems.\nThis suggests the expansion of PSG's applicability, enabling its use beyond the\nconfines of clinics.\n","authors":["Young-Seok Kweon","Gi-Hwan Shin","Heon-Gyu Kwak","Ha-Na Jo","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2311.07868v1.pdf","comment":"Proc. 12th IEEE International Winter Conference on Brain-Computer\n  Interface"},{"id":"http://arxiv.org/abs/2311.07867v1","updated":"2023-11-14T02:55:37Z","published":"2023-11-14T02:55:37Z","title":"Mixture of Coupled HMMs for Robust Modeling of Multivariate Healthcare\n  Time Series","summary":"  Analysis of multivariate healthcare time series data is inherently\nchallenging: irregular sampling, noisy and missing values, and heterogeneous\npatient groups with different dynamics violating exchangeability. In addition,\ninterpretability and quantification of uncertainty are critically important.\nHere, we propose a novel class of models, a mixture of coupled hidden Markov\nmodels (M-CHMM), and demonstrate how it elegantly overcomes these challenges.\nTo make the model learning feasible, we derive two algorithms to sample the\nsequences of the latent variables in the CHMM: samplers based on (i) particle\nfiltering and (ii) factorized approximation. Compared to existing inference\nmethods, our algorithms are computationally tractable, improve mixing, and\nallow for likelihood estimation, which is necessary to learn the mixture model.\nExperiments on challenging real-world epidemiological and semi-synthetic data\ndemonstrate the advantages of the M-CHMM: improved data fit, capacity to\nefficiently handle missing and noisy measurements, improved prediction\naccuracy, and ability to identify interpretable subsets in the data.\n","authors":["Onur Poyraz","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2311.07867v1.pdf","comment":"9 pages, 7 figures, Proceedings of Machine Learning Research, Machine\n  Learning for Health (ML4H) 2023"},{"id":"http://arxiv.org/abs/2310.17784v2","updated":"2023-11-14T02:41:40Z","published":"2023-10-07T04:53:31Z","title":"Data-Centric Financial Large Language Models","summary":"  Large language models (LLMs) show promise for natural language tasks but\nstruggle when applied directly to complex domains like finance. LLMs have\ndifficulty reasoning about and integrating all relevant information. We propose\na data-centric approach to enable LLMs to better handle financial tasks. Our\nkey insight is that rather than overloading the LLM with everything at once, it\nis more effective to preprocess and pre-understand the data. We create a\nfinancial LLM (FLLM) using multitask prompt-based finetuning to achieve data\npre-processing and pre-understanding. However, labeled data is scarce for each\ntask. To overcome manual annotation costs, we employ abductive augmentation\nreasoning (AAR) to automatically generate training data by modifying the pseudo\nlabels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR\nsubstantially outperforms baseline financial LLMs designed for raw text,\nachieving state-of-the-art on financial analysis and interpretation tasks. We\nalso open source a new benchmark for financial analysis and interpretation. Our\nmethodology provides a promising path to unlock LLMs' potential for complex\nreal-world domains.\n","authors":["Zhixuan Chu","Huaiyu Guo","Xinyuan Zhou","Yijia Wang","Fei Yu","Hong Chen","Wanqing Xu","Xin Lu","Qing Cui","Longfei Li","Jun Zhou","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2310.17784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07864v1","updated":"2023-11-14T02:33:54Z","published":"2023-11-14T02:33:54Z","title":"Probing clustering in neural network representations","summary":"  Neural network representations contain structure beyond what was present in\nthe training labels. For instance, representations of images that are visually\nor semantically similar tend to lie closer to each other than to dissimilar\nimages, regardless of their labels. Clustering these representations can thus\nprovide insights into dataset properties as well as the network internals. In\nthis work, we study how the many design choices involved in neural network\ntraining affect the clusters formed in the hidden representations. To do so, we\nestablish an evaluation setup based on the BREEDS hierarchy, for the task of\nsubclass clustering after training models with only superclass information. We\nisolate the training dataset and architecture as important factors affecting\nclusterability. Datasets with labeled classes consisting of unrelated\nsubclasses yield much better clusterability than those following a natural\nhierarchy. When using pretrained models to cluster representations on\ndownstream datasets, models pretrained on subclass labels provide better\nclusterability than models pretrained on superclass labels, but only when there\nis a high degree of domain overlap between the pretraining and downstream data.\nArchitecturally, we find that normalization strategies affect which layers\nyield the best clustering performance, and, surprisingly, Vision Transformers\nattain lower subclass clusterability than ResNets.\n","authors":["Thao Nguyen","Simon Kornblith"],"pdf_url":"https://arxiv.org/pdf/2311.07864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13810v2","updated":"2023-11-14T02:30:34Z","published":"2023-10-20T20:49:06Z","title":"A Better Match for Drivers and Riders: Reinforcement Learning at Lyft","summary":"  To better match drivers to riders in our ridesharing application, we revised\nLyft's core matching algorithm. We use a novel online reinforcement learning\napproach that estimates the future earnings of drivers in real time and use\nthis information to find more efficient matches. This change was the first\ndocumented implementation of a ridesharing matching algorithm that can learn\nand improve in real time. We evaluated the new approach during weeks of\nswitchback experimentation in most Lyft markets, and estimated how it benefited\ndrivers, riders, and the platform. In particular, it enabled our drivers to\nserve millions of additional riders each year, leading to more than $30 million\nper year in incremental revenue. Lyft rolled out the algorithm globally in\n2021.\n","authors":["Xabi Azagirre","Akshay Balwally","Guillaume Candeli","Nicholas Chamandy","Benjamin Han","Alona King","Hyungjun Lee","Martin Loncaric","Sebastien Martin","Vijay Narasiman"," Zhiwei"," Qin","Baptiste Richard","Sara Smoot","Sean Taylor","Garrett van Ryzin","Di Wu","Fei Yu","Alex Zamoshchin"],"pdf_url":"https://arxiv.org/pdf/2310.13810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13475v2","updated":"2023-11-14T02:20:44Z","published":"2023-09-23T20:33:38Z","title":"Detecting and Mitigating System-Level Anomalies of Vision-Based\n  Controllers","summary":"  Autonomous systems, such as self-driving cars and drones, have made\nsignificant strides in recent years by leveraging visual inputs and machine\nlearning for decision-making and control. Despite their impressive performance,\nthese vision-based controllers can make erroneous predictions when faced with\nnovel or out-of-distribution inputs. Such errors can cascade to catastrophic\nsystem failures and compromise system safety. In this work, we introduce a\nrun-time anomaly monitor to detect and mitigate such closed-loop, system-level\nfailures. Specifically, we leverage a reachability-based framework to\nstress-test the vision-based controller offline and mine its system-level\nfailures. This data is then used to train a classifier that is leveraged online\nto flag inputs that might cause system breakdowns. The anomaly detector\nhighlights issues that transcend individual modules and pertain to the safety\nof the overall system. We also design a fallback controller that robustly\nhandles these detected anomalies to preserve system safety. We validate the\nproposed approach on an autonomous aircraft taxiing system that uses a\nvision-based controller for taxiing. Our results show the efficacy of the\nproposed approach in identifying and handling system-level anomalies,\noutperforming methods such as prediction error-based detection, and ensembling,\nthereby enhancing the overall safety and robustness of autonomous systems.\n","authors":["Aryaman Gupta","Kaustav Chakraborty","Somil Bansal"],"pdf_url":"https://arxiv.org/pdf/2309.13475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07853v1","updated":"2023-11-14T02:09:10Z","published":"2023-11-14T02:09:10Z","title":"Learning Mutually Informed Representations for Characters and Subwords","summary":"  Most pretrained language models rely on subword tokenization, which processes\ntext as a sequence of subword tokens. However, different granularities of text,\nsuch as characters, subwords, and words, can contain different kinds of\ninformation. Previous studies have shown that incorporating multiple input\ngranularities improves model generalization, yet very few of them outputs\nuseful representations for each granularity. In this paper, we introduce the\nentanglement model, aiming to combine character and subword language models.\nInspired by vision-language models, our model treats characters and subwords as\nseparate modalities, and it generates mutually informed representations for\nboth granularities as output. We evaluate our model on text classification,\nnamed entity recognition, and POS-tagging tasks. Notably, the entanglement\nmodel outperforms its backbone language models, particularly in the presence of\nnoisy texts and low-resource languages. Furthermore, the entanglement model\neven outperforms larger pre-trained models on all English sequence labeling\ntasks and classification tasks. Our anonymized code is available at\nhttps://anonymous.4open.science/r/noisy-IE-A673\n","authors":["Yilin Wang","Xinyi Hu","Matthew R. Gormley"],"pdf_url":"https://arxiv.org/pdf/2311.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07850v1","updated":"2023-11-14T02:05:29Z","published":"2023-11-14T02:05:29Z","title":"Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA","summary":"  We present BYOKG, a universal question-answering (QA) system that can operate\non any knowledge graph (KG), requires no human-annotated training data, and can\nbe ready to use within a day -- attributes that are out-of-scope for current\nKGQA systems. BYOKG draws inspiration from the remarkable ability of humans to\ncomprehend information present in an unseen KG through exploration -- starting\nat random nodes, inspecting the labels of adjacent nodes and edges, and\ncombining them with their prior world knowledge. In BYOKG, exploration\nleverages an LLM-backed symbolic agent that generates a diverse set of\nquery-program exemplars, which are then used to ground a retrieval-augmented\nreasoning procedure to predict programs for arbitrary questions. BYOKG is\neffective over both small- and large-scale graphs, showing dramatic gains in QA\naccuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,\nrespectively. On GrailQA, we further show that our unsupervised BYOKG\noutperforms a supervised in-context learning method, demonstrating the\neffectiveness of exploration. Lastly, we find that performance of BYOKG\nreliably improves with continued exploration as well as improvements in the\nbase LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1\non a sub-sampled zero-shot split of GrailQA.\n","authors":["Dhruv Agarwal","Rajarshi Das","Sopan Khosla","Rashmi Gangadharaiah"],"pdf_url":"https://arxiv.org/pdf/2311.07850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02192v2","updated":"2023-11-14T02:01:38Z","published":"2023-04-05T02:13:42Z","title":"A Diffusion-based Method for Multi-turn Compositional Image Generation","summary":"  Multi-turn compositional image generation (M-CIG) is a challenging task that\naims to iteratively manipulate a reference image given a modification text.\nWhile most of the existing methods for M-CIG are based on generative\nadversarial networks (GANs), recent advances in image generation have\ndemonstrated the superiority of diffusion models over GANs. In this paper, we\npropose a diffusion-based method for M-CIG named conditional denoising\ndiffusion with image compositional matching (CDD-ICM). We leverage CLIP as the\nbackbone of image and text encoders, and incorporate a gated fusion mechanism,\noriginally proposed for question answering, to compositionally fuse the\nreference image and the modification text at each turn of M-CIG. We introduce a\nconditioning scheme to generate the target image based on the fusion results.\nTo prioritize the semantic quality of the generated target image, we learn an\nauxiliary image compositional match (ICM) objective, along with the conditional\ndenoising diffusion (CDD) objective in a multi-task learning framework.\nAdditionally, we also perform ICM guidance and classifier-free guidance to\nimprove performance. Experimental results show that CDD-ICM achieves\nstate-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and\ni-CLEVR.\n","authors":["Chao Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02192v2.pdf","comment":"WACV 2024 3rd Workshop on Image/Video/Audio Quality in Computer\n  Vision and Generative AI"},{"id":"http://arxiv.org/abs/2311.03062v2","updated":"2023-11-14T02:00:20Z","published":"2023-11-06T12:46:29Z","title":"Imaging through multimode fibres with physical prior","summary":"  Imaging through perturbed multimode fibres based on deep learning has been\nwidely researched. However, existing methods mainly use target-speckle pairs in\ndifferent configurations. It is challenging to reconstruct targets without\ntrained networks. In this paper, we propose a physics-assisted, unsupervised,\nlearning-based fibre imaging scheme. The role of the physical prior is to\nsimplify the mapping relationship between the speckle pattern and the target\nimage, thereby reducing the computational complexity. The unsupervised network\nlearns target features according to the optimized direction provided by the\nphysical prior. Therefore, the reconstruction process of the online learning\nonly requires a few speckle patterns and unpaired targets. The proposed scheme\nalso increases the generalization ability of the learning-based method in\nperturbed multimode fibres. Our scheme has the potential to extend the\napplication of multimode fibre imaging.\n","authors":["Chuncheng Zhang","Yingjie Shi","Zheyi Yao","Xiubao Sui","Qian Chen"],"pdf_url":"https://arxiv.org/pdf/2311.03062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07841v1","updated":"2023-11-14T01:40:21Z","published":"2023-11-14T01:40:21Z","title":"PEMS: Pre-trained Epidmic Time-series Models","summary":"  Providing accurate and reliable predictions about the future of an epidemic\nis an important problem for enabling informed public health decisions. Recent\nworks have shown that leveraging data-driven solutions that utilize advances in\ndeep learning methods to learn from past data of an epidemic often outperform\ntraditional mechanistic models. However, in many cases, the past data is sparse\nand may not sufficiently capture the underlying dynamics. While there exists a\nlarge amount of data from past epidemics, leveraging prior knowledge from\ntime-series data of other diseases is a non-trivial challenge. Motivated by the\nsuccess of pre-trained models in language and vision tasks, we tackle the\nproblem of pre-training epidemic time-series models to learn from multiple\ndatasets from different diseases and epidemics. We introduce Pre-trained\nEpidemic Time-Series Models (PEMS) that learn from diverse time-series datasets\nof a variety of diseases by formulating pre-training as a set of\nself-supervised learning (SSL) tasks. We tackle various important challenges\nspecific to pre-training for epidemic time-series such as dealing with\nheterogeneous dynamics and efficiently capturing useful patterns from multiple\nepidemic datasets by carefully designing the SSL tasks to learn important\npriors about the epidemic dynamics that can be leveraged for fine-tuning to\nmultiple downstream tasks. The resultant PEM outperforms previous\nstate-of-the-art methods in various downstream time-series tasks across\ndatasets of varying seasonal patterns, geography, and mechanism of contagion\nincluding the novel Covid-19 pandemic unseen in pre-trained data with better\nefficiency using smaller fraction of datasets.\n","authors":["Harshavardhan Kamarthi","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2311.07841v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2311.07833v1","updated":"2023-11-14T01:26:20Z","published":"2023-11-14T01:26:20Z","title":"Toward Efficient and Incremental Spectral Clustering via Parametric\n  Spectral Clustering","summary":"  Spectral clustering is a popular method for effectively clustering\nnonlinearly separable data. However, computational limitations, memory\nrequirements, and the inability to perform incremental learning challenge its\nwidespread application. To overcome these limitations, this paper introduces a\nnovel approach called parametric spectral clustering (PSC). By extending the\ncapabilities of spectral clustering, PSC addresses the challenges associated\nwith big data and real-time scenarios and enables efficient incremental\nclustering with new data points. Experimental evaluations conducted on various\nopen datasets demonstrate the superiority of PSC in terms of computational\nefficiency while achieving clustering quality mostly comparable to standard\nspectral clustering. The proposed approach has significant potential for\nincremental and real-time data analysis applications, facilitating timely and\naccurate clustering in dynamic and evolving datasets. The findings of this\nresearch contribute to the advancement of clustering techniques and open new\navenues for efficient and effective data analysis. We publish the experimental\ncode at https://github.com/109502518/PSC_BigData.\n","authors":["Jo-Chun Chen","Hung-Hsuan Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07821v1","updated":"2023-11-14T00:45:53Z","published":"2023-11-14T00:45:53Z","title":"Statistical Parameterized Physics-Based Machine Learning Digital Twin\n  Models for Laser Powder Bed Fusion Process","summary":"  A digital twin (DT) is a virtual representation of physical process, products\nand/or systems that requires a high-fidelity computational model for continuous\nupdate through the integration of sensor data and user input. In the context of\nlaser powder bed fusion (LPBF) additive manufacturing, a digital twin of the\nmanufacturing process can offer predictions for the produced parts, diagnostics\nfor manufacturing defects, as well as control capabilities. This paper\nintroduces a parameterized physics-based digital twin (PPB-DT) for the\nstatistical predictions of LPBF metal additive manufacturing process. We\naccomplish this by creating a high-fidelity computational model that accurately\nrepresents the melt pool phenomena and subsequently calibrating and validating\nit through controlled experiments. In PPB-DT, a mechanistic reduced-order\nmethod-driven stochastic calibration process is introduced, which enables the\nstatistical predictions of the melt pool geometries and the identification of\ndefects such as lack-of-fusion porosity and surface roughness, specifically for\ndiagnostic applications. Leveraging data derived from this physics-based model\nand experiments, we have trained a machine learning-based digital twin\n(PPB-ML-DT) model for predicting, monitoring, and controlling melt pool\ngeometries. These proposed digital twin models can be employed for predictions,\ncontrol, optimization, and quality assurance within the LPBF process,\nultimately expediting product development and certification in LPBF-based metal\nadditive manufacturing.\n","authors":["Yangfan Li","Satyajit Mojumder","Ye Lu","Abdullah Al Amin","Jiachen Guo","Xiaoyu Xie","Wei Chen","Gregory J. Wagner","Jian Cao","Wing Kam Liu"],"pdf_url":"https://arxiv.org/pdf/2311.07821v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2208.02907"},{"id":"http://arxiv.org/abs/2311.01797v2","updated":"2023-11-14T00:09:58Z","published":"2023-11-03T09:20:20Z","title":"On the Generalization Properties of Diffusion Models","summary":"  Diffusion models are a class of generative models that serve to establish a\nstochastic transport map between an empirically observed, yet unknown, target\ndistribution and a known prior. Despite their remarkable success in real-world\napplications, a theoretical understanding of their generalization capabilities\nremains underdeveloped. This work embarks on a comprehensive theoretical\nexploration of the generalization attributes of diffusion models. We establish\ntheoretical estimates of the generalization gap that evolves in tandem with the\ntraining dynamics of score-based diffusion models, suggesting a polynomially\nsmall generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$\nand the model capacity $m$, evading the curse of dimensionality (i.e., not\nexponentially large in the data dimension) when early-stopped. Furthermore, we\nextend our quantitative analysis to a data-dependent scenario, wherein target\ndistributions are portrayed as a succession of densities with progressively\nincreasing distances between modes. This precisely elucidates the adverse\neffect of \"modes shift\" in ground truths on the model generalization. Moreover,\nthese estimates are not solely theoretical constructs but have also been\nconfirmed through numerical simulations. Our findings contribute to the\nrigorous understanding of diffusion models' generalization properties and\nprovide insights that may guide practical applications.\n","authors":["Puheng Li","Zhong Li","Huishuai Zhang","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2311.01797v2.pdf","comment":"42 pages, 11 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2311.08403v1","updated":"2023-11-14T18:59:59Z","published":"2023-11-14T18:59:59Z","title":"Instant3D: Instant Text-to-3D Generation","summary":"  Text-to-3D generation, which aims to synthesize vivid 3D objects from text\nprompts, has attracted much attention from the computer vision community. While\nseveral existing works have achieved impressive results for this task, they\nmainly rely on a time-consuming optimization paradigm. Specifically, these\nmethods optimize a neural field from scratch for each text prompt, taking\napproximately one hour or more to generate one object. This heavy and\nrepetitive training cost impedes their practical deployment. In this paper, we\npropose a novel framework for fast text-to-3D generation, dubbed Instant3D.\nOnce trained, Instant3D is able to create a 3D object for an unseen text prompt\nin less than one second with a single run of a feedforward network. We achieve\nthis remarkable speed by devising a new network that directly constructs a 3D\ntriplane from a text prompt. The core innovation of our Instant3D lies in our\nexploration of strategies to effectively inject text conditions into the\nnetwork. Furthermore, we propose a simple yet effective activation function,\nthe scaled-sigmoid, to replace the original sigmoid function, which speeds up\nthe training convergence by more than ten times. Finally, to address the Janus\n(multi-head) problem in 3D generation, we propose an adaptive Perp-Neg\nalgorithm that can dynamically adjust its concept negation scales according to\nthe severity of the Janus problem during training, effectively reducing the\nmulti-head effect. Extensive experiments on a wide variety of benchmark\ndatasets demonstrate that the proposed algorithm performs favorably against the\nstate-of-the-art methods both qualitatively and quantitatively, while achieving\nsignificantly better efficiency. The project page is at\nhttps://ming1993li.github.io/Instant3DProj.\n","authors":["Ming Li","Pan Zhou","Jia-Wei Liu","Jussi Keppo","Min Lin","Shuicheng Yan","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2311.08403v1.pdf","comment":"Project page: https://ming1993li.github.io/Instant3DProj"},{"id":"http://arxiv.org/abs/2311.08172v1","updated":"2023-11-14T14:02:32Z","published":"2023-11-14T14:02:32Z","title":"Vision-Language Instruction Tuning: A Review and Analysis","summary":"  Instruction tuning is an essential supervised training phase for Large\nLanguage Models (LLMs), with the goal of enhancing LLMs' capacity to generalize\ninstruction execution and adapt to user preferences. With the growing\nincorporation of multi-modal data into LLMs, there is an increasing interest in\nthe performance of vision-language instruction tuning which presents more\ncomplex features in comparison to pure text instructions. In this paper, we\nsystematically review the latest vision-language instruction tuning settings\nand datasets in multi-modal LLMs and summarize the characteristics that\nhigh-quality vision-language tuning data should have. We consider these\ncharacteristics as the foundational principles for constructing vision-language\ninstruction data and propose a complete construction pipeline consisting of\ndata collection, instruction generation, and quality control modules that\nincorporate meticulously designed instruction property evaluation indicators.\nWe perform vision-language instruction tuning on three widely used multi-modal\nLLMs based on the instruction data we constructed and conduct extensive\nexperiments on the corresponding metrics to demonstrate the rationality of the\nconstruction principles proposed in this paper. The code and dataset related to\nthis paper have been open-sourced at\n\\url{https://github.com/palchenli/VL-Instruction-Tuning}.\n","authors":["Chen Li","Yixiao Ge","Dian Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.08172v1.pdf","comment":"36 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08074v1","updated":"2023-11-14T10:58:57Z","published":"2023-11-14T10:58:57Z","title":"Content-Adaptive Variable Framerate Encoding Scheme for Green Live\n  Streaming","summary":"  Adaptive live video streaming applications use a fixed predefined\nconfiguration for the bitrate ladder with constant framerate and encoding\npresets in a session. However, selecting optimized framerates and presets for\nevery bitrate ladder representation can enhance perceptual quality, improve\ncomputational resource allocation, and thus, the streaming energy efficiency.\nIn particular, low framerates for low-bitrate representations reduce\ncompression artifacts and decrease encoding energy consumption. In addition, an\noptimized preset may lead to improved compression efficiency. To this light,\nthis paper proposes a Content-adaptive Variable Framerate (CVFR) encoding\nscheme, which offers two modes of operation: ecological (ECO) and high-quality\n(HQ). CVFR-ECO optimizes for the highest encoding energy savings by predicting\nthe optimized framerate for each representation in the bitrate ladder. CVFR-HQ\ntakes it further by predicting each representation's optimized\nframerate-encoding preset pair using low-complexity discrete cosine transform\nenergy-based spatial and temporal features for compression efficiency and\nsustainable storage. We demonstrate the advantage of CVFR using the x264\nopen-source video encoder. The results show that CVFR-ECO yields an average\nPSNR and VMAF increase of 0.02 dB and 2.50 points, respectively, for the same\nbitrate, compared to the fastest preset highest framerate encoding. CVFR-ECO\nalso yields an average encoding and storage energy consumption reduction of\n34.54% and 76.24%, considering a just noticeable difference (JND) of six VMAF\npoints. In comparison, CVFR-HQ yields an average increase in PSNR and VMAF of\n2.43 dB and 10.14 points, respectively, for the same bitrate. Finally, CVFR-HQ\nresulted in an average reduction in storage energy consumption of 83.18%,\nconsidering a JND of six VMAF points.\n","authors":["Vignesh V Menon","Samira Afzal","Prajit T Rajendran","Klaus Schoeffmann","Radu Prodan","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2311.08074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02143v2","updated":"2023-11-14T10:02:00Z","published":"2023-05-03T14:22:48Z","title":"GANonymization: A GAN-based Face Anonymization Framework for Preserving\n  Emotional Expressions","summary":"  In recent years, the increasing availability of personal data has raised\nconcerns regarding privacy and security. One of the critical processes to\naddress these concerns is data anonymization, which aims to protect individual\nprivacy and prevent the release of sensitive information. This research focuses\non the importance of face anonymization. Therefore, we introduce\nGANonymization, a novel face anonymization framework with facial\nexpression-preserving abilities. Our approach is based on a high-level\nrepresentation of a face, which is synthesized into an anonymized version based\non a generative adversarial network (GAN). The effectiveness of the approach\nwas assessed by evaluating its performance in removing identifiable facial\nattributes to increase the anonymity of the given individual face.\nAdditionally, the performance of preserving facial expressions was evaluated on\nseveral affect recognition datasets and outperformed the state-of-the-art\nmethods in most categories. Finally, our approach was analyzed for its ability\nto remove various facial traits, such as jewelry, hair color, and multiple\nothers. Here, it demonstrated reliable performance in removing these\nattributes. Our results suggest that GANonymization is a promising approach for\nanonymizing faces while preserving facial expressions.\n","authors":["Fabio Hellmann","Silvan Mertes","Mohamed Benouis","Alexander Hustinx","Tzung-Chien Hsieh","Cristina Conati","Peter Krawitz","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2305.02143v2.pdf","comment":"26 pages, 11 figures, 6 tables, ACM Transactions on Multimedia\n  Computing, Communications, and Applications"},{"id":"http://arxiv.org/abs/2308.04156v3","updated":"2023-11-14T09:43:45Z","published":"2023-08-08T09:37:18Z","title":"Towards Top-Down Stereo Image Quality Assessment via Stereo Attention","summary":"  Stereo image quality assessment (SIQA) plays a crucial role in evaluating and\nimproving the visual experience of 3D content. Existing visual properties-based\nmethods for SIQA have achieved promising performance. However, these approaches\nignore the top-down philosophy, leading to a lack of a comprehensive grasp of\nthe human visual system (HVS) and SIQA. This paper presents a novel Stereo\nAttenTion Network (SATNet), which employs a top-down perspective to guide the\nquality assessment process. Specifically, our generalized Stereo AttenTion\n(SAT) structure adapts components and input/output for stereo scenarios. It\nleverages the fusion-generated attention map as a higher-level binocular\nmodulator to influence two lower-level monocular features, allowing progressive\nrecalibration of both throughout the pipeline. Additionally, we introduce an\nEnergy Coefficient (EC) to flexibly tune the magnitude of binocular response,\naccounting for the fact that binocular responses in the primate primary visual\ncortex are less than the sum of monocular responses. To extract the most\ndiscriminative quality information from the summation and subtraction of the\ntwo branches of monocular features, we utilize a dual-pooling strategy that\napplies min-pooling and max-pooling operations to the respective branches.\nExperimental results highlight the superiority of our top-down method in\nadvancing the state-of-the-art in the SIQA field. The code is available at\nhttps://github.com/Fanning-Zhang/SATNet.\n","authors":["Huilin Zhang","Sumei Li","Haoxiang Chang","Peiming Lin"],"pdf_url":"https://arxiv.org/pdf/2308.04156v3.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2306.09126v2","updated":"2023-11-14T08:29:23Z","published":"2023-06-15T13:37:14Z","title":"STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes\n  with Spatiotemporal Annotations of Sound Events","summary":"  While direction of arrival (DOA) of sound events is generally estimated from\nmultichannel audio data recorded in a microphone array, sound events usually\nderive from visually perceptible source objects, e.g., sounds of footsteps come\nfrom the feet of a walker. This paper proposes an audio-visual sound event\nlocalization and detection (SELD) task, which uses multichannel audio and video\ninformation to estimate the temporal activation and DOA of target sound events.\nAudio-visual SELD systems can detect and localize sound events using signals\nfrom a microphone array and audio-visual correspondence. We also introduce an\naudio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23),\nwhich consists of multichannel audio data recorded with a microphone array,\nvideo data, and spatiotemporal annotation of sound events. Sound scenes in\nSTARSS23 are recorded with instructions, which guide recording participants to\nensure adequate activity and occurrences of sound events. STARSS23 also serves\nhuman-annotated temporal activation labels and human-confirmed DOA labels,\nwhich are based on tracking results of a motion capture system. Our benchmark\nresults demonstrate the benefits of using visual object positions in\naudio-visual SELD tasks. The data is available at\nhttps://zenodo.org/record/7880637.\n","authors":["Kazuki Shimada","Archontis Politis","Parthasaarathy Sudarsanam","Daniel Krause","Kengo Uchida","Sharath Adavanne","Aapo Hakala","Yuichiro Koyama","Naoya Takahashi","Shusuke Takahashi","Tuomas Virtanen","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2306.09126v2.pdf","comment":"27 pages, 9 figures, accepted for publication in NeurIPS 2023 Track\n  on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2211.10627v2","updated":"2023-11-14T06:00:04Z","published":"2022-11-19T09:08:43Z","title":"EGRC-Net: Embedding-induced Graph Refinement Clustering Network","summary":"  Existing graph clustering networks heavily rely on a predefined yet fixed\ngraph, which can lead to failures when the initial graph fails to accurately\ncapture the data topology structure of the embedding space. In order to address\nthis issue, we propose a novel clustering network called Embedding-Induced\nGraph Refinement Clustering Network (EGRC-Net), which effectively utilizes the\nlearned embedding to adaptively refine the initial graph and enhance the\nclustering performance. To begin, we leverage both semantic and topological\ninformation by employing a vanilla auto-encoder and a graph convolution\nnetwork, respectively, to learn a latent feature representation. Subsequently,\nwe utilize the local geometric structure within the feature embedding space to\nconstruct an adjacency matrix for the graph. This adjacency matrix is\ndynamically fused with the initial one using our proposed fusion architecture.\nTo train the network in an unsupervised manner, we minimize the Jeffreys\ndivergence between multiple derived distributions. Additionally, we introduce\nan improved approximate personalized propagation of neural predictions to\nreplace the standard graph convolution network, enabling EGRC-Net to scale\neffectively. Through extensive experiments conducted on nine widely-used\nbenchmark datasets, we demonstrate that our proposed methods consistently\noutperform several state-of-the-art approaches. Notably, EGRC-Net achieves an\nimprovement of more than 11.99\\% in Adjusted Rand Index (ARI) over the best\nbaseline on the DBLP dataset. Furthermore, our scalable approach exhibits a\n10.73% gain in ARI while reducing memory usage by 33.73% and decreasing running\ntime by 19.71%. The code for EGRC-Net will be made publicly available at\n\\url{https://github.com/ZhihaoPENG-CityU/EGRC-Net}.\n","authors":["Zhihao Peng","Hui Liu","Yuheng Jia","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2211.10627v2.pdf","comment":"This paper has been accepted by IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2301.13591v5","updated":"2023-11-14T03:53:55Z","published":"2023-01-31T12:43:54Z","title":"Zero3D: Semantic-Driven Multi-Category 3D Shape Generation","summary":"  Semantic-driven 3D shape generation aims to generate 3D objects conditioned\non text. Previous works face problems with single-category generation,\nlow-frequency 3D details, and requiring a large number of paired datasets for\ntraining. To tackle these challenges, we propose a multi-category conditional\ndiffusion model. Specifically, 1) to alleviate the problem of lack of\nlarge-scale paired data, we bridge the text, 2D image and 3D shape based on the\npre-trained CLIP model, and 2) to obtain the multi-category 3D shape feature,\nwe apply the conditional flow model to generate 3D shape vector conditioned on\nCLIP embedding. 3) to generate multi-category 3D shape, we employ the\nhidden-layer diffusion model conditioned on the multi-category shape vector,\nwhich greatly reduces the training time and memory consumption.\n","authors":["Bo Han","Yitong Fu","Yixuan Shen"],"pdf_url":"https://arxiv.org/pdf/2301.13591v5.pdf","comment":"work in progress"}]},"2023-11-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.09216v1","updated":"2023-11-15T18:58:19Z","published":"2023-11-15T18:58:19Z","title":"Assessing Translation capabilities of Large Language Models involving\n  English and Indian Languages","summary":"  Generative Large Language Models (LLMs) have achieved remarkable advancements\nin various NLP tasks. In this work, our aim is to explore the multilingual\ncapabilities of large language models by using machine translation as a task\ninvolving English and 22 Indian languages. We first investigate the translation\ncapabilities of raw large language models, followed by exploring the in-context\nlearning capabilities of the same raw models. We fine-tune these large language\nmodels using parameter efficient fine-tuning methods such as LoRA and\nadditionally with full fine-tuning. Through our study, we have identified the\nbest performing large language model for the translation task involving LLMs,\nwhich is based on LLaMA.\n  Our results demonstrate significant progress, with average BLEU scores of\n13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99,\n42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for\nEnglish to Indian languages on IN22 (conversational), IN22 (general),\nflores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for\nIndian languages to English, we achieved average BLEU scores of 14.03, 16.65,\n16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51,\nand 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational),\nIN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets.\nOverall, our findings highlight the potential and strength of large language\nmodels for machine translation capabilities, including for languages that are\ncurrently underrepresented in LLMs.\n","authors":["Vandan Mujadia","Ashok Urlana","Yash Bhaskar","Penumalla Aditya Pavani","Kukkapalli Shravya","Parameswari Krishnamurthy","Dipti Misra Sharma"],"pdf_url":"https://arxiv.org/pdf/2311.09216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14336v2","updated":"2023-11-15T18:56:34Z","published":"2023-05-23T17:58:10Z","title":"Schema-Driven Information Extraction from Heterogeneous Tables","summary":"  In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we develop a benchmark composed of tables from\nfour diverse domains: machine learning papers, chemistry literature, material\nscience journals, and webpages. Alongside the benchmark, we present an\nextraction method based on instruction-tuned LLMs. Our approach shows\ncompetitive performance without task-specific labels, achieving F1 scores\nranging from 74.2 to 96.1, while maintaining great cost efficiency. Moreover,\nwe validate the possibility of distilling compact table-extraction models to\nreduce API reliance, as well as extraction from image tables using multi-modal\nmodels. By developing a benchmark and demonstrating the feasibility of this\ntask using proprietary models, we aim to support future work on open-source\nschema-driven IE models.\n","authors":["Fan Bai","Junmo Kang","Gabriel Stanovsky","Dayne Freitag","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2305.14336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09214v1","updated":"2023-11-15T18:56:23Z","published":"2023-11-15T18:56:23Z","title":"Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive\n  Thinking from Large Language Models","summary":"  Large language models (LLMs) have achieved remarkable advancements in the\nfield of natural language processing. However, the sheer scale and\ncomputational demands of these models present formidable challenges when\nconsidering their practical deployment in resource-constrained contexts. While\ntechniques such as chain-of-thought (CoT) distillation have displayed promise\nin distilling LLMs into small language models (SLMs), there is a risk that\ndistilled SLMs may still carry over flawed reasoning or hallucinations\ninherited from their LLM counterparts. To address these issues, we propose a\ntwofold methodology: First, we introduce a novel method for distilling the\nself-evaluation capability inherent in LLMs into SLMs, which aims to mitigate\nthe adverse effects of erroneous reasoning and reduce hallucinations. Second,\nwe advocate for a comprehensive distillation process that incorporates multiple\ndistinct chain-of-thought and self-evaluation paradigms and ensures a more\nholistic and robust knowledge transfer into SLMs. Experiments on three NLP\nbenchmarks demonstrate that our method significantly improves the performance\nof distilled SLMs and sheds light on the path towards developing smaller models\nclosely aligned with human cognition.\n","authors":["Weize Liu","Guocong Li","Kai Zhang","Bang Du","Qiyuan Chen","Xuming Hu","Hongxia Xu","Jintai Chen","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2311.09214v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09213v1","updated":"2023-11-15T18:55:45Z","published":"2023-11-15T18:55:45Z","title":"GRIM: GRaph-based Interactive narrative visualization for gaMes","summary":"  Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The\nnarratives of these may take years to write and typically involve a large\ncreative team. In this work, we demonstrate the potential of large generative\ntext models to assist this process. \\textbf{GRIM}, a prototype\n\\textbf{GR}aph-based \\textbf{I}nteractive narrative visualization system for\nga\\textbf{M}es, generates a rich narrative graph with branching storylines that\nmatch a high-level narrative description and constraints provided by the\ndesigner. Game designers can interactively edit the graph by automatically\ngenerating new sub-graphs that fit the edits within the original narrative and\nconstraints. We illustrate the use of \\textbf{GRIM} in conjunction with GPT-4,\ngenerating branching narratives for four well-known stories with different\ncontextual constraints.\n","authors":["Jorge Leandro","Sudha Rao","Michael Xu","Weijia Xu","Nebosja Jojic","Chris Brockett","Bill Dolan"],"pdf_url":"https://arxiv.org/pdf/2311.09213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09212v1","updated":"2023-11-15T18:55:43Z","published":"2023-11-15T18:55:43Z","title":"Controllable Text Summarization: Unraveling Challenges, Approaches, and\n  Prospects -- A Survey","summary":"  Generic text summarization approaches often fail to address the specific\nintent and needs of individual users. Recently, scholarly attention has turned\nto the development of summarization methods that are more closely tailored and\ncontrolled to align with specific objectives and user needs. While a growing\ncorpus of research is devoted towards a more controllable summarization, there\nis no comprehensive survey available that thoroughly explores the diverse\ncontrollable aspects or attributes employed in this context, delves into the\nassociated challenges, and investigates the existing solutions. In this survey,\nwe formalize the Controllable Text Summarization (CTS) task, categorize\ncontrollable aspects according to their shared characteristics and objectives,\nand present a thorough examination of existing methods and datasets within each\ncategory. Moreover, based on our findings, we uncover limitations and research\ngaps, while also delving into potential solutions and future directions for\nCTS.\n","authors":["Ashok Urlana","Pruthwik Mishra","Tathagato Roy","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2311.09212v1.pdf","comment":"19 pages, 1 figure"},{"id":"http://arxiv.org/abs/2311.09210v1","updated":"2023-11-15T18:54:53Z","published":"2023-11-15T18:54:53Z","title":"Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language\n  Models","summary":"  Retrieval-augmented language models (RALMs) represent a substantial\nadvancement in the capabilities of large language models, notably in reducing\nfactual hallucination by leveraging external knowledge sources. However, the\nreliability of the retrieved information is not always guaranteed. The\nretrieval of irrelevant data can lead to misguided responses, and potentially\ncausing the model to overlook its inherent knowledge, even when it possesses\nadequate information to address the query. Moreover, standard RALMs often\nstruggle to assess whether they possess adequate knowledge, both intrinsic and\nretrieved, to provide an accurate answer. In situations where knowledge is\nlacking, these systems should ideally respond with \"unknown\" when the answer is\nunattainable. In response to these challenges, we introduces Chain-of-Noting\n(CoN), a novel approach aimed at improving the robustness of RALMs in facing\nnoisy, irrelevant documents and in handling unknown scenarios. The core idea of\nCoN is to generate sequential reading notes for retrieved documents, enabling a\nthorough evaluation of their relevance to the given question and integrating\nthis information to formulate the final answer. We employed ChatGPT to create\ntraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.\nOur experiments across four open-domain QA benchmarks show that RALMs equipped\nwith CoN significantly outperform standard RALMs. Notably, CoN achieves an\naverage improvement of +7.9 in EM score given entirely noisy retrieved\ndocuments and +10.5 in rejection rates for real-time questions that fall\noutside the pre-training knowledge scope.\n","authors":["Wenhao Yu","Hongming Zhang","Xiaoman Pan","Kaixin Ma","Hongwei Wang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.09210v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09206v1","updated":"2023-11-15T18:47:52Z","published":"2023-11-15T18:47:52Z","title":"TableLlama: Towards Open Large Generalist Models for Tables","summary":"  Semi-structured tables are ubiquitous. There has been a variety of tasks that\naim to automatically interpret, augment, and query tables. Current methods\noften require pretraining on tables or special model architecture design, are\nrestricted to specific table types, or have simplifying assumptions about\ntables and tasks. This paper makes the first step towards developing\nopen-source large language models (LLMs) as generalists for a diversity of\ntable-based tasks. Towards that end, we construct TableInstruct, a new dataset\nwith a variety of realistic tables and tasks, for instruction tuning and\nevaluating LLMs. We further develop the first open-source generalist model for\ntables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the\nlong context challenge. We experiment under both in-domain setting and\nout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves\ncomparable or better performance than the SOTA for each task, despite the\nlatter often has task-specific design. On 6 out-of-domain datasets, it achieves\n6-48 absolute point gains compared with the base model, showing that training\non TableInstruct enhances the model's generalizability. We will open-source our\ndataset and trained model to boost future work on developing open generalist\nmodels for tables.\n","authors":["Tianshu Zhang","Xiang Yue","Yifei Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09205v1","updated":"2023-11-15T18:47:42Z","published":"2023-11-15T18:47:42Z","title":"When Is Multilinguality a Curse? Language Modeling for 250 High- and\n  Low-Resource Languages","summary":"  Multilingual language models are widely used to extend NLP systems to\nlow-resource languages. However, concrete evidence for the effects of\nmultilinguality on language modeling performance in individual languages\nremains scarce. Here, we pre-train over 10,000 monolingual and multilingual\nlanguage models for over 250 languages, including multiple language families\nthat are under-studied in NLP. We assess how language modeling performance in\neach language varies as a function of (1) monolingual dataset size, (2) added\nmultilingual dataset size, (3) linguistic similarity of the added languages,\nand (4) model size (up to 45M parameters). We find that in moderation, adding\nmultilingual data improves low-resource language modeling performance, similar\nto increasing low-resource dataset sizes by up to 33%. Improvements depend on\nthe syntactic similarity of the added multilingual data, with marginal\nadditional effects of vocabulary overlap. However, high-resource languages\nconsistently perform worse in multilingual pre-training scenarios. As dataset\nsizes increase, adding multilingual data begins to hurt performance for both\nlow-resource and high-resource languages, likely due to limited model capacity\n(the \"curse of multilinguality\"). These results suggest that massively\nmultilingual pre-training may not be optimal for any languages involved, but\nthat more targeted models can significantly improve performance.\n","authors":["Tyler A. Chang","Catherine Arnett","Zhuowen Tu","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2311.09205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09204v1","updated":"2023-11-15T18:46:56Z","published":"2023-11-15T18:46:56Z","title":"Fusion-Eval: Integrating Evaluators with LLMs","summary":"  Evaluating Large Language Models (LLMs) is a complex task, especially\nconsidering the intricacies of natural language understanding and the\nexpectations for high-level reasoning. Traditional evaluations typically lean\non human-based, model-based, or automatic-metrics-based paradigms, each with\nits own advantages and shortcomings. We introduce \"Fusion-Eval\", a system that\nemploys LLMs not solely for direct evaluations, but to skillfully integrate\ninsights from diverse evaluators. This gives Fusion-Eval flexibility, enabling\nit to work effectively across diverse tasks and make optimal use of multiple\nreferences. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman\ncorrelation of 0.96, outperforming other evaluators. The success of Fusion-Eval\nunderscores the potential of LLMs to produce evaluations that closely align\nhuman perspectives, setting a new standard in the field of LLM evaluation.\n","authors":["Lei Shu","Nevan Wichers","Liangchen Luo","Yun Zhu","Yinxiao Liu","Jindong Chen","Lei Meng"],"pdf_url":"https://arxiv.org/pdf/2311.09204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09198v1","updated":"2023-11-15T18:42:44Z","published":"2023-11-15T18:42:44Z","title":"Never Lost in the Middle: Improving Large Language Models via Attention\n  Strengthening Question Answering","summary":"  While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community.\n","authors":["He Junqing","Pan Kunhao","Dong Xiaoqun","Song Zhuoyang","Liu Yibo","Liang Yuxin","Wang Hao","Sun Qianguo","Zhang Songxin","Xie Zejian","Zhang Jiaxing"],"pdf_url":"https://arxiv.org/pdf/2311.09198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09194v1","updated":"2023-11-15T18:39:56Z","published":"2023-11-15T18:39:56Z","title":"Structural Priming Demonstrates Abstract Grammatical Representations in\n  Multilingual Language Models","summary":"  Abstract grammatical knowledge - of parts of speech and grammatical patterns\n- is key to the capacity for linguistic generalization in humans. But how\nabstract is grammatical knowledge in large language models? In the human\nliterature, compelling evidence for grammatical abstraction comes from\nstructural priming. A sentence that shares the same grammatical structure as a\npreceding sentence is processed and produced more readily. Because confounds\nexist when using stimuli in a single language, evidence of abstraction is even\nmore compelling from crosslingual structural priming, where use of a syntactic\nstructure in one language primes an analogous structure in another language. We\nmeasure crosslingual structural priming in large language models, comparing\nmodel behavior to human experimental results from eight crosslingual\nexperiments covering six languages, and four monolingual structural priming\nexperiments in three non-English languages. We find evidence for abstract\nmonolingual and crosslingual grammatical representations in the models that\nfunction similarly to those found in humans. These results demonstrate that\ngrammatical representations in multilingual language models are not only\nsimilar across languages, but they can causally influence text produced in\ndifferent languages.\n","authors":["James A. Michaelov","Catherine Arnett","Tyler A. Chang","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2311.09194v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09193v1","updated":"2023-11-15T18:39:21Z","published":"2023-11-15T18:39:21Z","title":"The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task","summary":"  The study explores the effectiveness of the Chain-of-Thought approach, known\nfor its proficiency in language tasks by breaking them down into sub-tasks and\nintermediate steps, in improving vision-language tasks that demand\nsophisticated perception and reasoning. We present the \"Description then\nDecision\" strategy, which is inspired by how humans process signals. This\nstrategy significantly improves probing task performance by 50%, establishing\nthe groundwork for future research on reasoning paradigms in complex\nvision-language tasks.\n","authors":["Yifan Wu","Pengchuan Zhang","Wenhan Xiong","Barlas Oguz","James C. Gee","Yixin Nie"],"pdf_url":"https://arxiv.org/pdf/2311.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02017v6","updated":"2023-11-15T18:34:35Z","published":"2023-03-27T21:27:58Z","title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its\n  Applications, Advantages, Limitations, and Future Directions in Natural\n  Language Processing","summary":"  Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n","authors":["Walid Hariri"],"pdf_url":"https://arxiv.org/pdf/2304.02017v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09189v1","updated":"2023-11-15T18:32:27Z","published":"2023-11-15T18:32:27Z","title":"PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for\n  Mental Health","summary":"  Recently, there has been a growing interest in utilizing large language\nmodels (LLMs) in mental health research, with studies showcasing their\nremarkable capabilities, such as disease detection. However, there is currently\na lack of a comprehensive benchmark for evaluating the capability of LLMs in\nthis domain. Therefore, we address this gap by introducing the first\ncomprehensive benchmark tailored to the unique characteristics of the mental\nhealth domain. This benchmark encompasses a total of six sub-tasks, covering\nthree dimensions, to systematically assess the capabilities of LLMs in the\nrealm of mental health. We have designed corresponding concise prompts for each\nsub-task. And we comprehensively evaluate a total of eight advanced LLMs using\nour benchmark. Experiment results not only demonstrate significant room for\nimprovement in current LLMs concerning mental health but also unveil potential\ndirections for future model optimization.\n","authors":["Haoan Jin","Siyuan Chen","Mengyue Wu","Kenny Q. Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.09189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09188v1","updated":"2023-11-15T18:28:29Z","published":"2023-11-15T18:28:29Z","title":"Towards Verifiable Text Generation with Symbolic References","summary":"  Large language models (LLMs) have demonstrated an impressive ability to\nsynthesize plausible and fluent text. However they remain vulnerable to\nhallucinations, and thus their outputs generally require manual human\nverification for high-stakes applications, which can be time-consuming and\ndifficult. This paper proposes symbolically grounded generation (SymGen) as a\nsimple approach for enabling easier validation of an LLM's output. SymGen\nprompts an LLM to interleave its regular output text with explicit symbolic\nreferences to fields present in some conditioning data (e.g., a table in JSON\nformat). The references can be used to display the provenance of different\nspans of text in the generation, reducing the effort required for manual\nverification. Across data-to-text and question answering experiments, we find\nthat LLMs are able to directly output text that makes use of symbolic\nreferences while maintaining fluency and accuracy.\n","authors":["Lucas Torroba Hennigen","Shannon Shen","Aniruddha Nrusimha","Bernhard Gapp","David Sontag","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.09188v1.pdf","comment":"46 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2311.09184v1","updated":"2023-11-15T18:25:26Z","published":"2023-11-15T18:25:26Z","title":"Benchmarking Generation and Evaluation Capabilities of Large Language\n  Models for Instruction Controllable Summarization","summary":"  While large language models (LLMs) already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for the desired\nsummary characteristics. To this end, we curate an evaluation-only dataset for\nthis task setting and conduct human evaluation on 5 LLM-based summarization\nsystems. We then benchmark LLM-based automatic evaluation for this task with 4\ndifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\nin total. Our study reveals that instruction controllable text summarization\nremains a challenging task for LLMs, since (1) all LLMs evaluated still make\nfactual and other types of errors in their summaries; (2) all LLM-based\nevaluation methods cannot achieve a strong alignment with human annotators when\njudging the quality of candidate summaries; (3) different LLMs show large\nperformance gaps in summary generation and evaluation. We make our collected\nbenchmark, InstruSum, publicly available to facilitate future research in this\ndirection.\n","authors":["Yixin Liu","Alexander R. Fabbri","Jiawen Chen","Yilun Zhao","Simeng Han","Shafiq Joty","Pengfei Liu","Dragomir Radev","Chien-Sheng Wu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09184v1.pdf","comment":"GitHub Repo: https://github.com/yale-nlp/InstruSum"},{"id":"http://arxiv.org/abs/2310.12963v2","updated":"2023-11-15T18:23:40Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available in various sizes and\nconfigurations from cloud API providers. While this diversity offers a broad\nspectrum of choices, effectively leveraging the options to optimize\ncomputational cost and performance remains challenging. In this work, we\npresent AutoMix, an approach that strategically routes queries to larger LMs,\nbased on the approximate correctness of outputs from a smaller LM. Central to\nAutoMix is a few-shot self-verification mechanism, which estimates the\nreliability of its own outputs without requiring training. Given that\nverifications can be noisy, we employ a meta verifier in AutoMix to refine the\naccuracy of these assessments. Our experiments using LLAMA2-13/70B, on five\ncontext-grounded reasoning datasets demonstrate that AutoMix surpasses\nestablished baselines, improving the incremental benefit per cost by up to 89%.\nOur code and data are available at https://github.com/automix-llm/automix.\n","authors":["Aman Madaan","Pranjal Aggarwal","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay"," Mausam","Manaal Faruqui"],"pdf_url":"https://arxiv.org/pdf/2310.12963v2.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on mixing\n  3 models, and will be presented at the workshop on robustness of\n  zero/few-shot learning in foundation models, Neurips 2023"},{"id":"http://arxiv.org/abs/2311.09182v1","updated":"2023-11-15T18:23:17Z","published":"2023-11-15T18:23:17Z","title":"ContraDoc: Understanding Self-Contradictions in Documents with Large\n  Language Models","summary":"  In recent times, large language models (LLMs) have shown impressive\nperformance on various document-level tasks such as document classification,\nsummarization, and question-answering. However, research on understanding their\ncapabilities on the task of self-contradictions in long documents has been very\nlimited. In this work, we introduce ContraDoc, the first human-annotated\ndataset to study self-contradictions in long documents across multiple domains,\nvarying document lengths, self-contradictions types, and scope. We then analyze\nthe current capabilities of four state-of-the-art open-source and commercially\navailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\nperforms the best and can outperform humans on this task, we find that it is\nstill unreliable and struggles with self-contradictions that require more\nnuance and context. We release the dataset and all the code associated with the\nexperiments.\n","authors":["Jierui Li","Vipul Raheja","Dhruv Kumar"],"pdf_url":"https://arxiv.org/pdf/2311.09182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09180v1","updated":"2023-11-15T18:19:58Z","published":"2023-11-15T18:19:58Z","title":"PEARL: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers","summary":"  Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style\nand specialized knowledge. In this paper, we address this challenge by\nproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\na generation-calibrated retriever. Our retriever is trained to select historic\nuser-authored documents for prompt augmentation, such that they are likely to\nbest personalize LLM generations for a user request. We propose two key\nnovelties for training our retriever: 1) A training data selection method that\nidentifies user requests likely to benefit from personalization and documents\nthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\nthat ensures that our retriever closely tracks the benefit of a document for\npersonalized generation. We demonstrate the effectiveness of PEARL in\ngenerating personalized workplace social media posts and Reddit comments.\nFinally, we showcase the potential of a generation-calibrated retriever to\ndouble as a performance predictor and further improve low-quality generations\nvia LLM chaining.\n","authors":["Sheshera Mysore","Zhuoran Lu","Mengting Wan","Longqi Yang","Steve Menezes","Tina Baghaee","Emmanuel Barajas Gonzalez","Jennifer Neville","Tara Safavi"],"pdf_url":"https://arxiv.org/pdf/2311.09180v1.pdf","comment":"Pre-print, work in progress"},{"id":"http://arxiv.org/abs/2311.09179v1","updated":"2023-11-15T18:15:37Z","published":"2023-11-15T18:15:37Z","title":"SiRA: Sparse Mixture of Low Rank Adaptation","summary":"  Parameter Efficient Tuning has been an prominent approach to adapt the Large\nLanguage Model to downstream tasks. Most previous works considers adding the\ndense trainable parameters, where all parameters are used to adapt certain\ntask. We found this less effective empirically using the example of LoRA that\nintroducing more trainable parameters does not help. Motivated by this we\ninvestigate the importance of leveraging \"sparse\" computation and propose SiRA:\nsparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of\nExpert(SMoE) to boost the performance of LoRA. Specifically it enforces the top\n$k$ experts routing with a capacity limit restricting the maximum number of\ntokens each expert can process. We propose a novel and simple expert dropout on\ntop of gating network to reduce the over-fitting issue. Through extensive\nexperiments, we verify SiRA performs better than LoRA and other mixture of\nexpert approaches across different single tasks and multitask settings.\n","authors":["Yun Zhu","Nevan Wichers","Chu-Cheng Lin","Xinyi Wang","Tianlong Chen","Lei Shu","Han Lu","Canoee Liu","Liangchen Luo","Jindong Chen","Lei Meng"],"pdf_url":"https://arxiv.org/pdf/2311.09179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09174v1","updated":"2023-11-15T18:11:23Z","published":"2023-11-15T18:11:23Z","title":"AbsPyramid: Benchmarking the Abstraction Ability of Language Models with\n  a Unified Entailment Graph","summary":"  Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.\n","authors":["Zhaowei Wang","Haochen Shi","Weiqi Wang","Tianqing Fang","Hongming Zhang","Sehyun Choi","Xin Liu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2311.09174v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.02457v2","updated":"2023-11-15T18:02:03Z","published":"2023-10-03T22:02:17Z","title":"The Empty Signifier Problem: Towards Clearer Paradigms for\n  Operationalising \"Alignment\" in Large Language Models","summary":"  In this paper, we address the concept of \"alignment\" in large language models\n(LLMs) through the lens of post-structuralist socio-political theory,\nspecifically examining its parallels to empty signifiers. To establish a shared\nvocabulary around how abstract concepts of alignment are operationalised in\nempirical datasets, we propose a framework that demarcates: 1) which dimensions\nof model behaviour are considered important, then 2) how meanings and\ndefinitions are ascribed to these dimensions, and by whom. We situate existing\nempirical literature and provide guidance on deciding which paradigm to follow.\nThrough this framework, we aim to foster a culture of transparency and critical\nevaluation, aiding the community in navigating the complexities of aligning\nLLMs with human populations.\n","authors":["Hannah Rose Kirk","Bertie Vidgen","Paul Röttger","Scott A. Hale"],"pdf_url":"https://arxiv.org/pdf/2310.02457v2.pdf","comment":"Socially Responsible Language Modelling Research (SoLaR) @ NeurIPs\n  2023"},{"id":"http://arxiv.org/abs/2311.09154v1","updated":"2023-11-15T17:50:30Z","published":"2023-11-15T17:50:30Z","title":"CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models","summary":"  We are currently in an era of fierce competition among various large language\nmodels (LLMs) continuously pushing the boundaries of benchmark performance.\nHowever, genuinely assessing the capabilities of these LLMs has become a\nchallenging and critical issue due to potential data contamination, and it\nwastes dozens of time and effort for researchers and engineers to download and\ntry those contaminated models. To save our precious time, we propose a novel\nand useful method, Clean-Eval, which mitigates the issue of data contamination\nand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\nparaphrase and back-translate the contaminated data into a candidate set,\ngenerating expressions with the same meaning but in different surface forms. A\nsemantic detector is then used to filter the generated low-quality samples to\nnarrow down this candidate set. The best candidate is finally selected from\nthis set based on the BLEURT score. According to human assessment, this best\ncandidate is semantically similar to the original contamination data but\nexpressed differently. All candidates can form a new benchmark to evaluate the\nmodel. Our experiments illustrate that Clean-Eval substantially restores the\nactual evaluation results on contaminated LLMs under both few-shot learning and\nfine-tuning scenarios.\n","authors":["Wenhong Zhu","Hongkun Hao","Zhiwei He","Yunze Song","Yumeng Zhang","Hanxu Hu","Yiran Wei","Rui Wang","Hongyuan Lu"],"pdf_url":"https://arxiv.org/pdf/2311.09154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09149v1","updated":"2023-11-15T17:46:39Z","published":"2023-11-15T17:46:39Z","title":"Temporal Knowledge Question Answering via Abstract Reasoning Induction","summary":"  In this paper, we tackle the significant challenge of temporal knowledge\nreasoning in Large Language Models (LLMs), an area where such models frequently\nencounter difficulties. These difficulties often result in the generation of\nmisleading or incorrect information, primarily due to their limited capacity to\nprocess evolving factual knowledge and complex temporal logic. In response, we\npropose a novel, constructivism-based approach that advocates for a paradigm\nshift in LLM learning towards an active, ongoing process of knowledge synthesis\nand customization. At the heart of our proposal is the Abstract Reasoning\nInduction ARI framework, which divides temporal reasoning into two distinct\nphases: Knowledge-agnostic and Knowledge-based. This division aims to reduce\ninstances of hallucinations and improve LLMs' capacity for integrating abstract\nmethodologies derived from historical data. Our approach achieves remarkable\nimprovements, with relative gains of 29.7\\% and 9.27\\% on two temporal QA\ndatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.\nThe code will be released at https://github.com/czy1999/ARI.\n","authors":["Ziyang Chen","Dongfang Li","Xiang Zhao","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09149v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.09144v1","updated":"2023-11-15T17:40:27Z","published":"2023-11-15T17:40:27Z","title":"Grounding or Guesswork? Large Language Models are Presumptive Grounders","summary":"  Effective conversation requires common ground: a shared understanding between\nthe participants. Common ground, however, does not emerge spontaneously in\nconversation. Speakers and listeners work together to both identify and\nconstruct a shared basis while avoiding misunderstanding. To accomplish\ngrounding, humans rely on a range of dialogue acts, like clarification (What do\nyou mean?) and acknowledgment (I understand.). In domains like teaching and\nemotional support, carefully constructing grounding prevents misunderstanding.\nHowever, it is unclear whether large language models (LLMs) leverage these\ndialogue acts in constructing common ground. To this end, we curate a set of\ngrounding acts and propose corresponding metrics that quantify attempted\ngrounding. We study whether LLMs use these grounding acts, simulating them\ntaking turns from several dialogue datasets, and comparing the results to\nhumans. We find that current LLMs are presumptive grounders, biased towards\nassuming common ground without using grounding acts. To understand the roots of\nthis behavior, we examine the role of instruction tuning and reinforcement\nlearning with human feedback (RLHF), finding that RLHF leads to less grounding.\nAltogether, our work highlights the need for more research investigating\ngrounding in human-AI interaction.\n","authors":["Omar Shaikh","Kristina Gligorić","Ashna Khetan","Matthias Gerstgrasser","Diyi Yang","Dan Jurafsky"],"pdf_url":"https://arxiv.org/pdf/2311.09144v1.pdf","comment":"16 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.08329v2","updated":"2023-11-15T17:36:55Z","published":"2023-11-14T17:18:08Z","title":"KTRL+F: Knowledge-Augmented In-Document Search","summary":"  We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. This task addresses following unique challenges for in-document search:\n1) utilizing knowledge outside the document for extended use of additional\ninformation about targets to bridge the semantic gap between the query and the\ntargets, and 2) balancing between real-time applicability with the performance.\nWe analyze various baselines in KTRL+F and find there are limitations of\nexisting models, such as hallucinations, low latency, or difficulties in\nleveraging external knowledge. Therefore we propose a Knowledge-Augmented\nPhrase Retrieval model that shows a promising balance between speed and\nperformance by simply augmenting external knowledge embedding in phrase\nembedding. Additionally, we conduct a user study to verify whether solving\nKTRL+F can enhance search experience of users. It demonstrates that even with\nour simple model users can reduce the time for searching with less queries and\nreduced extra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.\n","authors":["Hanseok Oh","Haebin Shin","Miyoung Ko","Hyunji Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.08329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04978v2","updated":"2023-11-15T17:34:56Z","published":"2023-05-08T18:20:36Z","title":"NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge","summary":"  Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is\nan essential component of our world knowledge, yet understudied in prior\nliterature. In this paper, we study the task of comparative knowledge\nacquisition, motivated by the dramatic improvements in the capabilities of\nextreme-scale language models like GPT-4, which have fueled efforts towards\nharvesting their knowledge into knowledge bases. While acquisition of such\ncomparative knowledge is much easier from models like GPT-4, compared to their\nconsiderably smaller and weaker counterparts such as GPT-2, not even the most\npowerful models are exempt from making errors. We thus ask: to what extent are\nmodels at different scales able to generate valid and diverse comparative\nknowledge?\n  We introduce NeuroComparatives, a novel framework for comparative knowledge\ndistillation overgenerated from language models such as GPT-variants and Llama,\nfollowed by stringent filtering of the generated knowledge. Our framework\nacquires comparative knowledge between everyday objects, producing a corpus of\nup to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more\ndiverse than existing resources. Moreover, human evaluations show that\nNeuroComparatives outperform existing resources (up to 32% absolute\nimprovement). We also demonstrate the utility of our distilled\nNeuroComparatives on three downstream tasks. Our results show that\nneuro-symbolic manipulation of smaller models offer complementary benefits to\nthe currently dominant practice of prompting extreme-scale language models for\nknowledge distillation.\n","authors":["Phillip Howard","Junlin Wang","Vasudev Lal","Gadi Singer","Yejin Choi","Swabha Swayamdipta"],"pdf_url":"https://arxiv.org/pdf/2305.04978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06851v2","updated":"2023-11-15T17:33:39Z","published":"2023-11-12T14:01:38Z","title":"Automatic Textual Normalization for Hate Speech Detection","summary":"  Social media data is a valuable resource for research, yet it contains a wide\nrange of non-standard words (NSW). These irregularities hinder the effective\noperation of NLP tools. Current state-of-the-art methods for the Vietnamese\nlanguage address this issue as a problem of lexical normalization, involving\nthe creation of manual rules or the implementation of multi-staged deep\nlearning frameworks, which necessitate extensive efforts to craft intricate\nrules. In contrast, our approach is straightforward, employing solely a\nsequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset\nfor textual normalization, comprising 2,181 human-annotated comments with an\ninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for\ntextual normalization, our results reveal that the accuracy achieved falls\nslightly short of 70%. Nevertheless, textual normalization enhances the\naccuracy of the Hate Speech Detection (HSD) task by approximately 2%,\ndemonstrating its potential to improve the performance of complex NLP tasks.\nOur dataset is accessible for research purposes.\n","authors":["Anh Thi-Hoang Nguyen","Dung Ha Nguyen","Nguyet Thi Nguyen","Khanh Thanh-Duy Ho","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.06851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09136v1","updated":"2023-11-15T17:27:14Z","published":"2023-11-15T17:27:14Z","title":"RRescue: Ranking LLM Responses to Enhance Reasoning Over Context","summary":"  Effectively using a given context is paramount for large language models. A\ncontext window can include task specifications, retrieved documents, previous\nconversations, and even model self-reflections, functioning similarly to\nepisodic memory. While efforts are being made to expand the context window,\nstudies indicate that LLMs do not use their context optimally for response\ngeneration. In this paper, we present a novel approach to optimize LLMs using\nranking metrics, which teaches LLMs to rank a collection of\ncontextually-grounded candidate responses. Rather than a traditional full\nordering, we advocate for a partial ordering. This is because achieving\nconsensus on the perfect order for system responses can be challenging. Our\npartial ordering is more robust, less sensitive to noise, and can be acquired\nthrough human labelers, heuristic functions, or model distillation. We test our\nsystem's improved contextual understanding using the latest benchmarks,\nincluding a new multi-document question answering dataset. We conduct ablation\nstudies to understand crucial factors, such as how to gather candidate\nresponses, determine their most suitable order, and balance supervised\nfine-tuning with ranking metrics. Our approach, named RRescue, suggests a\npromising avenue for enhancing LLMs' contextual understanding via response\nranking.\n","authors":["Yikun Wang","Rui Zheng","Haoming Li","Qi Zhang","Tao Gui","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09132v1","updated":"2023-11-15T17:21:58Z","published":"2023-11-15T17:21:58Z","title":"Aligning Neural Machine Translation Models: Human Feedback in Training\n  and Inference","summary":"  Reinforcement learning from human feedback (RLHF) is a recent technique to\nimprove the quality of the text generated by a language model, making it closer\nto what humans would generate. A core ingredient in RLHF's success in aligning\nand improving large language models (LLMs) is its reward model, trained using\nhuman feedback on model outputs. In machine translation (MT), where metrics\ntrained from human annotations can readily be used as reward models, recent\nmethods using minimum Bayes risk decoding and reranking have succeeded in\nimproving the final quality of translation. In this study, we comprehensively\nexplore and compare techniques for integrating quality metrics as reward models\ninto the MT pipeline. This includes using the reward model for data filtering,\nduring the training phase through RL, and at inference time by employing\nreranking techniques, and we assess the effects of combining these in a unified\napproach. Our experimental results, conducted across multiple translation\ntasks, underscore the crucial role of effective data filtering, based on\nestimated quality, in harnessing the full potential of RL in enhancing MT\nquality. Furthermore, our findings demonstrate the effectiveness of combining\nRL training with reranking techniques, showcasing substantial improvements in\ntranslation quality.\n","authors":["Miguel Moura Ramos","Patrick Fernandes","António Farinhas","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2311.09132v1.pdf","comment":"14 pages, work-in-progress"},{"id":"http://arxiv.org/abs/2311.09130v1","updated":"2023-11-15T17:20:20Z","published":"2023-11-15T17:20:20Z","title":"Social Meme-ing: Measuring Linguistic Variation in Memes","summary":"  Much work in the space of NLP has used computational methods to explore\nsociolinguistic variation in text. In this paper, we argue that memes, as\nmultimodal forms of language comprised of visual templates and text, also\nexhibit meaningful social variation. We construct a computational pipeline to\ncluster individual instances of memes into templates and semantic variables,\ntaking advantage of their multimodal structure in doing so. We apply this\nmethod to a large collection of meme images from Reddit and make available the\nresulting \\textsc{SemanticMemes} dataset of 3.8M images clustered by their\nsemantic function. We use these clusters to analyze linguistic variation in\nmemes, discovering not only that socially meaningful variation in meme usage\nexists between subreddits, but that patterns of meme innovation and\nacculturation within these communities align with previous findings on written\nlanguage.\n","authors":["Naitian Zhou","David Jurgens","David Bamman"],"pdf_url":"https://arxiv.org/pdf/2311.09130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09863v2","updated":"2023-11-15T17:19:10Z","published":"2023-05-17T00:29:18Z","title":"Explaining black box text modules in natural language with language\n  models","summary":"  Large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their rapid proliferation\nand increasing opaqueness have created a growing need for interpretability.\nHere, we ask whether we can automatically obtain natural language explanations\nfor black box text modules. A \"text module\" is any function that maps text to a\nscalar continuous value, such as a submodule within an LLM or a fitted model of\na brain region. \"Black box\" indicates that we only have access to the module's\ninputs/outputs.\n  We introduce Summarize and Score (SASC), a method that takes in a text module\nand returns a natural language explanation of the module's selectivity along\nwith a score for how reliable the explanation is. We study SASC in 3 contexts.\nFirst, we evaluate SASC on synthetic modules and find that it often recovers\nground truth explanations. Second, we use SASC to explain modules found within\na pre-trained BERT model, enabling inspection of the model's internals.\nFinally, we show that SASC can generate explanations for the response of\nindividual fMRI voxels to language stimuli, with potential applications to\nfine-grained brain mapping. All code for using SASC and reproducing results is\nmade available on Github.\n","authors":["Chandan Singh","Aliyah R. Hsu","Richard Antonello","Shailee Jain","Alexander G. Huth","Bin Yu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2305.09863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14332v2","updated":"2023-11-15T17:15:31Z","published":"2023-05-23T17:57:46Z","title":"Evaluating and Modeling Attribution for Cross-Lingual Question Answering","summary":"  Trustworthy answer content is abundant in many high-resource languages and is\ninstantly accessible through question answering systems, yet this content can\nbe hard to access for those that do not speak these languages. The leap forward\nin cross-lingual modeling quality offered by generative language models offers\nmuch promise, yet their raw generations often fall short in factuality. To\nimprove trustworthiness in these systems, a promising direction is to attribute\nthe answer to a retrieved source, possibly in a content-rich language different\nfrom the query. Our work is the first to study attribution for cross-lingual\nquestion answering. First, we collect data in 5 languages to assess the\nattribution level of a state-of-the-art cross-lingual QA system. To our\nsurprise, we find that a substantial portion of the answers is not attributable\nto any retrieved passages (up to 50% of answers exactly matching a gold\nreference) despite the system being able to attend directly to the retrieved\ntext. Second, to address this poor attribution level, we experiment with a wide\nrange of attribution detection techniques. We find that Natural Language\nInference models and PaLM 2 fine-tuned on a very small amount of attribution\ndata can accurately detect attribution. Based on these models, we improve the\nattribution level of a cross-lingual question-answering system. Overall, we\nshow that current academic generative cross-lingual QA systems have substantial\nshortcomings in attribution and we build tooling to mitigate these issues.\n","authors":["Benjamin Muller","John Wieting","Jonathan H. Clark","Tom Kwiatkowski","Sebastian Ruder","Livio Baldini Soares","Roee Aharoni","Jonathan Herzig","Xinyi Wang"],"pdf_url":"https://arxiv.org/pdf/2305.14332v2.pdf","comment":"Published as a long paper at EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09122v1","updated":"2023-11-15T17:09:54Z","published":"2023-11-15T17:09:54Z","title":"Universal NER: A Gold-Standard Multilingual Named Entity Recognition\n  Benchmark","summary":"  We introduce Universal NER (UNER), an open, community-driven project to\ndevelop gold-standard NER benchmarks in many languages. The overarching goal of\nUNER is to provide high-quality, cross-lingually consistent annotations to\nfacilitate and standardize multilingual NER research. UNER v1 contains 18\ndatasets annotated with named entities in a cross-lingual consistent schema\nacross 12 diverse languages. In this paper, we detail the dataset creation and\ncomposition of UNER; we also provide initial modeling baselines on both\nin-language and cross-lingual learning settings. We release the data, code, and\nfitted models to the public.\n","authors":["Stephen Mayhew","Terra Blevins","Shuheng Liu","Marek Šuppa","Hila Gonen","Joseph Marvin Imperial","Börje F. Karlsson","Peiqin Lin","Nikola Ljubešić","LJ Miranda","Barbara Plank","Arij Riabi","Yuval Pinter"],"pdf_url":"https://arxiv.org/pdf/2311.09122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09117v1","updated":"2023-11-15T17:07:44Z","published":"2023-11-15T17:07:44Z","title":"R-Spin: Efficient Speaker and Noise-invariant Representation Learning\n  with Acoustic Pieces","summary":"  This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised\nfine-tuning framework for speaker and noise-invariant speech representations by\nlearning discrete acoustic units with speaker-invariant clustering (Spin).\nR-Spin resolves Spin's issues and enhances content representations by learning\nto predict acoustic pieces. R-Spin offers a 12X reduction in computational\nresources compared to previous state-of-the-art methods while outperforming\nthem in severely distorted speech scenarios. This paper provides detailed\nanalyses to show how discrete units contribute to speech encoder training and\nimproving robustness in diverse acoustic environments.\n","authors":["Heng-Jui Chang","James Glass"],"pdf_url":"https://arxiv.org/pdf/2311.09117v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2311.09114v1","updated":"2023-11-15T17:04:56Z","published":"2023-11-15T17:04:56Z","title":"Ever: Mitigating Hallucination in Large Language Models through\n  Real-Time Verification and Rectification","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.\n","authors":["Haoqiang Kang","Juntong Ni","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2311.09114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07705v2","updated":"2023-11-15T17:02:17Z","published":"2023-07-15T04:37:11Z","title":"CPET: Effective Parameter-Efficient Tuning for Compressed Large Language\n  Models","summary":"  Parameter-efficient tuning (PET) has been widely explored in recent years\nbecause it tunes much fewer parameters (PET modules) than full-parameter\nfine-tuning (FT) while still stimulating sufficient knowledge from large\nlanguage models (LLMs) for downstream tasks. Moreover, when PET is employed to\nserve multiple tasks, different task-specific PET modules can be built on a\nfrozen LLM, avoiding redundant LLM deployments. Although PET significantly\nreduces the cost of tuning and deploying LLMs, its inference still suffers from\nthe computational bottleneck of LLMs. To address the above issue, we propose an\neffective PET framework based on compressed LLMs, named \"CPET\". In CPET, we\nevaluate the impact of mainstream LLM compression techniques on PET performance\nand then introduce knowledge inheritance and recovery strategies to restore the\nknowledge loss caused by these compression techniques. Our experimental results\ndemonstrate that, owing to the restoring strategies of CPET, collaborating\ntask-specific PET modules with a compressed LLM can achieve comparable\nperformance to collaborating PET modules with the original version of the\ncompressed LLM and outperform directly applying vanilla PET methods to the\ncompressed LLM.\n","authors":["Weilin Zhao","Yuxiang Huang","Xu Han","Zhiyuan Liu","Zhengyan Zhang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2307.07705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09109v1","updated":"2023-11-15T16:56:49Z","published":"2023-11-15T16:56:49Z","title":"Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge\n  Graph Completion?","summary":"  Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.\n","authors":["Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2311.09109v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.09106v1","updated":"2023-11-15T16:53:35Z","published":"2023-11-15T16:53:35Z","title":"\"We Demand Justice!\": Towards Grounding Political Text in Social Context","summary":"  Social media discourse from US politicians frequently consists of 'seemingly\nsimilar language used by opposing sides of the political spectrum'. But often,\nit translates to starkly contrasting real-world actions. For instance, \"We need\nto keep our students safe from mass shootings\" may signal either \"arming\nteachers to stop the shooter\" or \"banning guns to reduce mass shootings\"\ndepending on who says it and their political stance on the issue. In this\npaper, we define and characterize the context that is required to fully\nunderstand such ambiguous statements in a computational setting and ground them\nin real-world entities, actions, and attitudes. To that end, we propose two\nchallenging datasets that require an understanding of the real-world context of\nthe text to be solved effectively. We benchmark these datasets against\nbaselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3,\netc. Additionally, we develop and benchmark more structured baselines building\nupon existing 'Discourse Contextualization Framework' and 'Political Actor\nRepresentation' models. We perform analysis of the datasets and baseline\npredictions to obtain further insights into the pragmatic language\nunderstanding challenges posed by the proposed social grounding tasks.\n","authors":["Rajkumar Pujari","Chengfei Wu","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2311.09106v1.pdf","comment":"Was accepted to and withdrawn from Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09105v1","updated":"2023-11-15T16:52:14Z","published":"2023-11-15T16:52:14Z","title":"MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding\n  Dataset with Event Argument Annotation","summary":"  Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and our code can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.\n","authors":["Xiaozhi Wang","Hao Peng","Yong Guan","Kaisheng Zeng","Jianhui Chen","Lei Hou","Xu Han","Yankai Lin","Zhiyuan Liu","Ruobing Xie","Jie Zhou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2311.09105v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2311.09101v1","updated":"2023-11-15T16:47:57Z","published":"2023-11-15T16:47:57Z","title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning","summary":"  Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. Usually,\nanswer calibration strategies such as step-level or path-level calibration play\na vital role in multi-step reasoning. While effective, there remains a\nsignificant gap in our understanding of the key factors that drive their\nsuccess. In this paper, we break down the design of recent answer calibration\nstrategies and present a unified view which establishes connections between\nthem. We then conduct a thorough evaluation on these strategies from a unified\nview, systematically scrutinizing step-level and path-level answer calibration\nacross multiple paths. Our study holds the potential to illuminate key insights\nfor optimizing multi-step reasoning with answer calibration.\n","authors":["Shumin Deng","Ningyu Zhang","Nay Oo","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2311.09101v1.pdf","comment":"Working in Progress"},{"id":"http://arxiv.org/abs/2305.14493v3","updated":"2023-11-15T16:44:04Z","published":"2023-05-23T19:45:45Z","title":"Do prompt positions really matter?","summary":"  Prompt-based models have gathered a lot of attention from researchers due to\ntheir remarkable advancements in the fields of zero-shot and few-shot learning.\nDeveloping an effective prompt template plays a critical role. However, prior\nstudies have mainly focused on prompt vocabulary selection or embedding\ninitialization within a predefined template with the prompt position fixed. In\nthis empirical study, we conduct the most comprehensive analysis to date of\nprompt position for diverse natural language process tasks. Our findings\nquantify the substantial impact prompt position has on model performance. We\nobserve that the prompt position used in prior studies is often sub-optimal.\nThese findings suggest prompt position optimisation as a valuable research\ndirection to fill the gap in existing prompt engineering methodologies.\n","authors":["Junyu Mao","Stuart E. Middleton","Mahesan Niranjan"],"pdf_url":"https://arxiv.org/pdf/2305.14493v3.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.09096v1","updated":"2023-11-15T16:42:29Z","published":"2023-11-15T16:42:29Z","title":"Defending Large Language Models Against Jailbreaking Attacks Through\n  Goal Prioritization","summary":"  Large Language Models (LLMs) continue to advance in their capabilities, yet\nthis progress is accompanied by a growing array of safety risks. While\nsignificant attention has been dedicated to exploiting weaknesses in LLMs\nthrough jailbreaking attacks, there remains a paucity of exploration into\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the inherent conflict between the goals of being\nhelpful and ensuring safety. To counter jailbreaking attacks, we propose to\nintegrate goal prioritization at both training and inference stages.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to\n2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising\ngeneral performance. Furthermore, integrating the concept of goal\nprioritization into the training phase reduces the ASR from 71.0% to 6.6% for\nLLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are\nincluded during training, our approach slashes the ASR by half, decreasing it\nfrom 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs\nface greater safety risks, they also possess a greater capacity to be steered\ntowards defending against such attacks. We hope our work could contribute to\nthe comprehension of jailbreaking attacks and defenses, and shed light on the\nrelationship between LLMs' capability and safety. Our code will be available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.\n","authors":["Zhexin Zhang","Junxiao Yang","Pei Ke","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2311.09096v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2311.08385v2","updated":"2023-11-15T16:40:13Z","published":"2023-11-14T18:48:27Z","title":"ChOiRe: Characterizing and Predicting Human Opinions with Chain of\n  Opinion Reasoning","summary":"  Aligning language models (LMs) with human opinion is challenging yet vital to\nenhance their grasp of human values, preferences, and beliefs. We present\nChOiRe, a four-step solution framework to predict human opinion that\ndifferentiates between the user explicit personae (i.e. demographic or\nideological attributes) that are manually declared and implicit personae\ninferred from user historical opinions. Specifically, it consists of (i) an LM\nanalyzing the user explicit personae to filter out irrelevant attributes; (ii)\nthe LM ranking the implicit persona opinions into a preferential list; (iii)\nChain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the\nexplicit personae and the most relevant implicit personae to perform opinion\nprediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with\nincreasingly larger lists of implicit personae to overcome insufficient\npersonae information to infer a final result. ChOiRe achieves new\nstate-of-the-art effectiveness with limited inference calls, improving previous\nLLM-based techniques significantly by 3.22%.\n","authors":["Xuan Long Do","Kenji Kawaguchi","Min-Yen Kan","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08385v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2311.09090v1","updated":"2023-11-15T16:35:59Z","published":"2023-11-15T16:35:59Z","title":"Social Bias Probing: Fairness Benchmarking for Language Models","summary":"  Large language models have been shown to encode a variety of social biases,\nwhich carries the risk of downstream harms. While the impact of these biases\nhas been recognized, prior methods for bias evaluation have been limited to\nbinary association tests on small datasets, offering a constrained view of the\nnature of societal biases within language models. In this paper, we propose an\noriginal framework for probing language models for societal biases. We collect\na probing dataset to analyze language models' general associations, as well as\nalong the axes of societal categories, identities, and stereotypes. To this\nend, we leverage a novel perplexity-based fairness score. We curate a\nlarge-scale benchmarking dataset addressing drawbacks and limitations of\nexisting fairness collections, expanding to a variety of different identities\nand stereotypes. When comparing our methodology with prior work, we demonstrate\nthat biases within language models are more nuanced than previously\nacknowledged. In agreement with recent findings, we find that larger model\nvariants exhibit a higher degree of bias. Moreover, we expose how identities\nexpressing different religions lead to the most pronounced disparate treatments\nacross all models.\n","authors":["Marta Marchiori Manerba","Karolina Stańczak","Riccardo Guidotti","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2311.09090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09086v1","updated":"2023-11-15T16:30:44Z","published":"2023-11-15T16:30:44Z","title":"The Uli Dataset: An Exercise in Experience Led Annotation of oGBV","summary":"  Online gender based violence has grown concomitantly with adoption of the\ninternet and social media. Its effects are worse in the Global majority where\nmany users use social media in languages other than English. The scale and\nvolume of conversations on the internet has necessitated the need for automated\ndetection of hate speech, and more specifically gendered abuse. There is,\nhowever, a lack of language specific and contextual data to build such\nautomated tools. In this paper we present a dataset on gendered abuse in three\nlanguages- Hindi, Tamil and Indian English. The dataset comprises of tweets\nannotated along three questions pertaining to the experience of gender abuse,\nby experts who identify as women or a member of the LGBTQIA community in South\nAsia. Through this dataset we demonstrate a participatory approach to creating\ndatasets that drive AI systems.\n","authors":["Arnav Arora","Maha Jinadoss","Cheshta Arora","Denny George"," Brindaalakshmi","Haseena Dawood Khan","Kirti Rawat"," Div"," Ritash","Seema Mathur","Shivani Yadav","Shehla Rashid Shora","Rie Raut","Sumit Pawar","Apurva Paithane"," Sonia"," Vivek","Dharini Priscilla"," Khairunnisha","Grace Banu","Ambika Tandon","Rishav Thakker","Rahul Dev Korra","Aatman Vaidya","Tarunima Prabhakar"],"pdf_url":"https://arxiv.org/pdf/2311.09086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11483v4","updated":"2023-11-15T16:21:58Z","published":"2022-11-21T14:18:25Z","title":"Deanthropomorphising NLP: Can a Language Model Be Conscious?","summary":"  This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.\n","authors":["Matthew Shardlow","Piotr Przybyła"],"pdf_url":"https://arxiv.org/pdf/2211.11483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09071v1","updated":"2023-11-15T16:13:14Z","published":"2023-11-15T16:13:14Z","title":"How Multilingual is Multilingual LLM?","summary":"  Large Language Models (LLMs), trained predominantly on extensive English\ndata, often exhibit limitations when applied to other languages. Current\nresearch is primarily focused on enhancing the multilingual capabilities of\nthese models by employing various tuning strategies. Despite their\neffectiveness in certain languages, the understanding of the multilingual\nabilities of LLMs remains incomplete. This study endeavors to evaluate the\nmultilingual capacity of LLMs by conducting an exhaustive analysis across 101\nlanguages, and classifies languages with similar characteristics into four\ndistinct quadrants. By delving into each quadrant, we shed light on the\nrationale behind their categorization and offer actionable guidelines for\ntuning these languages. Extensive experiments reveal that existing LLMs possess\nmultilingual capabilities that surpass our expectations, and we can\nsignificantly improve the multilingual performance of LLMs by focusing on these\ndistinct attributes present in each quadrant.\n","authors":["Fei Yuan","Shuai Yuan","Zhiyong Wu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2311.09071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09069v1","updated":"2023-11-15T16:11:27Z","published":"2023-11-15T16:11:27Z","title":"How Well Do Large Language Models Truly Ground?","summary":"  Reliance on the inherent knowledge of Large Language Models (LLMs) can cause\nissues such as hallucinations, lack of control, and difficulties in integrating\nvariable knowledge. To mitigate this, LLMs can be probed to generate responses\nby grounding on external context, often given as input (knowledge-augmented\nmodels). Yet, previous research is often confined to a narrow view of the term\n\"grounding\", often only focusing on whether the response contains the correct\nanswer or not, which does not ensure the reliability of the entire response. To\naddress this limitation, we introduce a strict definition of grounding: a model\nis considered truly grounded when its responses (1) fully utilize necessary\nknowledge from the provided context, and (2) don't exceed the knowledge within\nthe contexts. We introduce a new dataset and a grounding metric to assess this\nnew definition and perform experiments across 13 LLMs of different sizes and\ntraining methods to provide insights into the factors that influence grounding\nperformance. Our findings contribute to a better understanding of how to\nimprove grounding capabilities and suggest an area of improvement toward more\nreliable and controllable LLM applications.\n","authors":["Hyunji Lee","Sejune Joo","Chaeeun Kim","Joel Jang","Doyoung Kim","Kyoung-Woon On","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.09069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09066v1","updated":"2023-11-15T16:05:55Z","published":"2023-11-15T16:05:55Z","title":"Identifying Self-Disclosures of Use, Misuse and Addiction in\n  Community-based Social Media Posts","summary":"  In the last decade, the United States has lost more than 500,000 people from\nan overdose involving prescription and illicit opioids\n(https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national\npublic health emergency (USDHHS, 2017). To more effectively prevent\nunintentional opioid overdoses, medical practitioners require robust and timely\ntools that can effectively identify at-risk patients. Community-based social\nmedia platforms such as Reddit allow self-disclosure for users to discuss\notherwise sensitive drug-related behaviors, often acting as indicators for\nopioid use disorder. Towards this, we present a moderate size corpus of 2500\nopioid-related posts from various subreddits spanning 6 different phases of\nopioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For\nevery post, we annotate span-level extractive explanations and crucially study\ntheir role both in annotation quality and model development. We evaluate\nseveral state-of-the-art models in a supervised, few-shot, or zero-shot\nsetting. Experimental results and error analysis show that identifying the\nphases of opioid use disorder is highly contextual and challenging. However, we\nfind that using explanations during modeling leads to a significant boost in\nclassification accuracy demonstrating their beneficial role in a high-stakes\ndomain such as studying the opioid use disorder continuum. The dataset will be\nmade available for research on Github in the formal version.\n","authors":["Chenghao Yang","Tuhin Chakrabarty","Karli R Hochstatter","Melissa N Slavin","Nabila El-Bassel","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2311.09066v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.00684v2","updated":"2023-11-15T15:55:02Z","published":"2023-11-01T17:43:35Z","title":"Attention Alignment and Flexible Positional Embeddings Improve\n  Transformer Length Extrapolation","summary":"  An ideal length-extrapolatable Transformer language model can handle\nsequences longer than the training length without any fine-tuning. Such\nlong-context utilization capability relies heavily on a flexible positional\nembedding design. Upon investigating the flexibility of existing large\npre-trained Transformer language models, we find that the T5 family deserves a\ncloser look, as its positional embeddings capture rich and flexible attention\npatterns. However, T5 suffers from the dispersed attention issue: the longer\nthe input sequence, the flatter the attention distribution. To alleviate the\nissue, we propose two attention alignment strategies via temperature scaling.\nOur findings show improvement on the long-context utilization capability of T5\non language modeling, retrieval, multi-document question answering, and code\ncompletion tasks without any fine-tuning. This suggests that a flexible\npositional embedding design and attention alignment can go a long way toward\nTransformer length extrapolation.\n","authors":["Ta-Chung Chi","Ting-Han Fan","Alexander I. Rudnicky"],"pdf_url":"https://arxiv.org/pdf/2311.00684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09060v1","updated":"2023-11-15T15:52:40Z","published":"2023-11-15T15:52:40Z","title":"Do Localization Methods Actually Localize Memorized Data in LLMs?","summary":"  Large language models (LLMs) can memorize many pretrained sequences verbatim.\nThis paper studies if we can locate a small set of neurons in LLMs responsible\nfor memorizing a given sequence. While the concept of localization is often\nmentioned in prior work, methods for localization have never been\nsystematically and directly evaluated; we address this with two benchmarking\napproaches. In our INJ Benchmark, we actively inject a piece of new information\ninto a small subset of LLM weights and measure whether localization methods can\nidentify these \"ground truth\" weights. In the DEL Benchmark, we study\nlocalization of pretrained data that LLMs have already memorized; while this\nsetting lacks ground truth, we can still evaluate localization by measuring\nwhether dropping out located neurons erases a memorized sequence from the\nmodel. We evaluate five localization methods on our two benchmarks, and both\nshow similar rankings. All methods exhibit promising localization ability,\nespecially for pruning-based methods, though the neurons they identify are not\nnecessarily specific to a single memorized sequence.\n","authors":["Ting-Yun Chang","Jesse Thomason","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2311.09060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14463v2","updated":"2023-11-15T15:50:31Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a systematic study and comprehensive evaluation of large language\nmodels for automatic multilingual readability assessment. In particular, we\nconstruct ReadMe++, a multilingual multi-domain dataset with human annotations\nof 9757 sentences in Arabic, English, French, Hindi, and Russian collected from\n112 different data sources. ReadMe++ offers more domain and language diversity\nthan existing readability datasets, making it ideal for benchmarking\nmultilingual and non-English language models (including mBERT, XLM-R, mT5,\nLlama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot prompting\nsettings. Our experiments reveal that models fine-tuned on ReadMe++ outperform\nthose trained on single-domain datasets, showcasing superior performance on\nmulti-domain readability assessment and cross-lingual transfer capabilities. We\nalso compare to traditional readability metrics (such as Flesch-Kincaid Grade\nLevel and Open Source Metric for Measuring Arabic Narratives), as well as the\nstate-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will make\nour data and code publicly available at: https://github.com/tareknaous/readme.\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v2.pdf","comment":"We have added French and Russian as two new languages to the corpus"},{"id":"http://arxiv.org/abs/2311.09053v1","updated":"2023-11-15T15:44:42Z","published":"2023-11-15T15:44:42Z","title":"Assessing Knowledge Editing in Language Models via Relation Perspective","summary":"  Knowledge Editing (KE) for modifying factual knowledge in Large Language\nModels (LLMs) has been receiving increasing attention. However, existing\nknowledge editing methods are entity-centric, and it is unclear whether this\napproach is suitable for a relation-centric perspective. To address this gap,\nthis paper constructs a new benchmark named RaKE, which focuses on Relation\nbased Knowledge Editing. In this paper, we establish a suite of innovative\nmetrics for evaluation and conduct comprehensive experiments involving various\nknowledge editing baselines. We notice that existing knowledge editing methods\nexhibit the potential difficulty in their ability to edit relations. Therefore,\nwe further explore the role of relations in factual triplets within the\ntransformer. Our research results confirm that knowledge related to relations\nis not only stored in the FFN network but also in the attention layers. This\nprovides experimental support for future relation-based knowledge editing\nmethods.\n","authors":["Yifan Wei","Xiaoyan Yu","Huanhuan Ma","Fangyu Lei","Yixuan Weng","Ran Song","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09053v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09048v1","updated":"2023-11-15T15:38:28Z","published":"2023-11-15T15:38:28Z","title":"GRASP: A novel benchmark for evaluating language GRounding And Situated\n  Physics understanding in multimodal language models","summary":"  This paper presents GRASP, a novel benchmark to evaluate the language\ngrounding and physical understanding capabilities of video-based multimodal\nlarge language models (LLMs). This evaluation is accomplished via a two-tier\napproach leveraging Unity simulations. The initial level tests for language\ngrounding by assessing a model's ability to relate simple textual descriptions\nwith visual information. The second level evaluates the model's understanding\nof 'Intuitive Physics' principles, such as object permanence and continuity. In\naddition to releasing the benchmark, we use it to evaluate several\nstate-of-the-art multimodal LLMs. Our evaluation reveals significant\nshortcomings in current models' language grounding and intuitive physics. These\nidentified limitations underline the importance of benchmarks like GRASP to\nmonitor the progress of future models in developing these competencies.\n","authors":["Serwan Jassim","Mario Holubar","Annika Richter","Cornelius Wolff","Xenia Ohmer","Elia Bruni"],"pdf_url":"https://arxiv.org/pdf/2311.09048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07772v2","updated":"2023-11-15T15:29:05Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n","authors":["Tomer Bar Natan","Gilad Deutch","Nadav Magar","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09033v1","updated":"2023-11-15T15:25:28Z","published":"2023-11-15T15:25:28Z","title":"MELA: Multilingual Evaluation of Linguistic Acceptability","summary":"  Recent benchmarks for Large Language Models (LLMs) have mostly focused on\napplication-driven tasks such as complex reasoning and code generation, and\nthis has led to a scarcity in purely linguistic evaluation of LLMs. Against\nthis background, we introduce Multilingual Evaluation of Linguistic\nAcceptability -- MELA, the first multilingual benchmark on linguistic\nacceptability with 48K samples covering 10 languages from a diverse set of\nlanguage families. We establish baselines of commonly used LLMs along with\nsupervised models, and conduct cross-lingual transfer and multi-task learning\nexperiments with XLM-R. In pursuit of multilingual interpretability, we analyze\nthe weights of fine-tuned XLM-R to explore the possibility of identifying\ntransfer difficulty between languages. Our results show that ChatGPT benefits\nmuch from in-context examples but still lags behind fine-tuned XLM-R, while the\nperformance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.\nCross-lingual and multi-task learning experiments show that unlike semantic\ntasks, in-language training data is crucial in acceptability judgements.\nResults in layerwise probing indicate that the upper layers of XLM-R become a\ntask-specific but language-agnostic region for multilingual acceptability\njudgment. We also introduce the concept of conflicting weight, which could be a\npotential indicator for the difficulty of cross-lingual transfer between\nlanguages. Our data will be available at https://github.com/sjtu-compling/MELA.\n","authors":["Ziyin Zhang","Yikang Liu","Weifang Huang","Junyu Mao","Rui Wang","Hai Hu"],"pdf_url":"https://arxiv.org/pdf/2311.09033v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09022v1","updated":"2023-11-15T15:12:15Z","published":"2023-11-15T15:12:15Z","title":"Exploring the Potential of Large Language Models in Computational\n  Argumentation","summary":"  Computational argumentation has become an essential tool in various fields,\nincluding artificial intelligence, law, and public policy. It is an emerging\nresearch field in natural language processing (NLP) that attracts increasing\nattention. Research on computational argumentation mainly involves two types of\ntasks: argument mining and argument generation. As large language models (LLMs)\nhave demonstrated strong abilities in understanding context and generating\nnatural language, it is worthwhile to evaluate the performance of LLMs on\nvarious computational argumentation tasks. This work aims to embark on an\nassessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under\nzero-shot and few-shot settings within the realm of computational\nargumentation. We organize existing tasks into 6 main classes and standardise\nthe format of 14 open-sourced datasets. In addition, we present a new benchmark\ndataset on counter speech generation, that aims to holistically evaluate the\nend-to-end performance of LLMs on argument mining and argument generation.\nExtensive experiments show that LLMs exhibit commendable performance across\nmost of these datasets, demonstrating their capabilities in the field of\nargumentation. We also highlight the limitations in evaluating computational\nargumentation and provide suggestions for future research directions in this\nfield.\n","authors":["Guizhen Chen","Liying Cheng","Luu Anh Tuan","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2311.09022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09008v1","updated":"2023-11-15T14:50:16Z","published":"2023-11-15T14:50:16Z","title":"End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and\n  Future Directions","summary":"  End-to-end task-oriented dialogue (EToD) can directly generate responses in\nan end-to-end fashion without modular training, which attracts escalating\npopularity. The advancement of deep neural networks, especially the successful\nuse of large pre-trained models, has further led to significant progress in\nEToD research in recent years. In this paper, we present a thorough review and\nprovide a unified perspective to summarize existing approaches as well as\nrecent trends to advance the development of EToD research. The contributions of\nthis paper can be summarized: (1) \\textbf{\\textit{First survey}}: to our\nknowledge, we take the first step to present a thorough survey of this research\nfield; (2) \\textbf{\\textit{New taxonomy}}: we first introduce a unified\nperspective for EToD, including (i) \\textit{Modularly EToD} and (ii)\n\\textit{Fully EToD}; (3) \\textbf{\\textit{New Frontiers}}: we discuss some\npotential frontier areas as well as the corresponding challenges, hoping to\nspur breakthrough research in EToD field; (4) \\textbf{\\textit{Abundant\nresources}}: we build a public website\\footnote{We collect the related papers,\nbaseline projects, and leaderboards for the community at\n\\url{https://etods.net/}.}, where EToD researchers could directly access the\nrecent progress. We hope this work can serve as a thorough reference for the\nEToD research community.\n","authors":["Libo Qin","Wenbo Pan","Qiguang Chen","Lizi Liao","Zhou Yu","Yue Zhang","Wanxiang Che","Min Li"],"pdf_url":"https://arxiv.org/pdf/2311.09008v1.pdf","comment":"Accepted at EMNLP2023"},{"id":"http://arxiv.org/abs/2311.09006v1","updated":"2023-11-15T14:48:08Z","published":"2023-11-15T14:48:08Z","title":"Data Similarity is Not Enough to Explain Language Model Performance","summary":"  Large language models achieve high performance on many but not all downstream\ntasks. The interaction between pretraining data and task data is commonly\nassumed to determine this variance: a task with data that is more similar to a\nmodel's pretraining data is assumed to be easier for that model. We test\nwhether distributional and example-specific similarity measures (embedding-,\ntoken- and model-based) correlate with language model performance through a\nlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\nbenchmarks. Similarity correlates with performance for multilingual datasets,\nbut in other benchmarks, we surprisingly find that similarity metrics are not\ncorrelated with accuracy or even each other. This suggests that the\nrelationship between pretraining data and downstream tasks is more complex than\noften assumed.\n","authors":["Gregory Yauney","Emily Reif","David Mimno"],"pdf_url":"https://arxiv.org/pdf/2311.09006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09000v1","updated":"2023-11-15T14:41:57Z","published":"2023-11-15T14:41:57Z","title":"Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and\n  Correction of LLM Output","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We design and\nbuild an annotation tool to speed up the labelling procedure and ease the\nworkload of raters. It allows flexible incorporation of automatic results in\nany stage, e.g. automatically-retrieved evidence. We further construct an\nopen-domain document-level factuality benchmark in three-level granularity:\nclaim, sentence and document. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims with the\nbest F1=0.53. Annotation tool, benchmark and code are available at\nhttps://github.com/yuxiaw/Factcheck-GPT.\n","authors":["Yuxia Wang","Revanth Gangi Reddy","Zain Muhammad Mujahid","Arnav Arora","Aleksandr Rubashevskii","Jiahui Geng","Osama Mohammed Afzal","Liangming Pan","Nadav Borenstein","Aditya Pillai","Isabelle Augenstein","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2311.09000v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2311.08993v1","updated":"2023-11-15T14:26:30Z","published":"2023-11-15T14:26:30Z","title":"When does In-context Learning Fall Short and Why? A Study on\n  Specification-Heavy Tasks","summary":"  In-context learning (ICL) has become the default method for using large\nlanguage models (LLMs), making the exploration of its limitations and\nunderstanding the underlying causes crucial. In this paper, we find that ICL\nfalls short of handling specification-heavy tasks, which are tasks with\ncomplicated and extensive task specifications, requiring several hours for\nordinary humans to master, such as traditional information extraction tasks.\nThe performance of ICL on these tasks mostly cannot reach half of the\nstate-of-the-art results. To explore the reasons behind this failure, we\nconduct comprehensive experiments on 18 specification-heavy tasks with various\nLLMs and identify three primary reasons: inability to specifically understand\ncontext, misalignment in task schema comprehension with humans, and inadequate\nlong-text understanding ability. Furthermore, we demonstrate that through\nfine-tuning, LLMs can achieve decent performance on these tasks, indicating\nthat the failure of ICL is not an inherent flaw of LLMs, but rather a drawback\nof existing alignment methods that renders LLMs incapable of handling\ncomplicated specification-heavy tasks via ICL. To substantiate this, we perform\ndedicated instruction tuning on LLMs for these tasks and observe a notable\nimprovement. We hope the analyses in this paper could facilitate advancements\nin alignment methods enabling LLMs to meet more sophisticated human demands.\n","authors":["Hao Peng","Xiaozhi Wang","Jianhui Chen","Weikai Li","Yunjia Qi","Zimu Wang","Zhili Wu","Kaisheng Zeng","Bin Xu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2311.08993v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2311.08982v1","updated":"2023-11-15T14:15:41Z","published":"2023-11-15T14:15:41Z","title":"SentAlign: Accurate and Scalable Sentence Alignment","summary":"  We present SentAlign, an accurate sentence alignment tool designed to handle\nvery large parallel document pairs. Given user-defined parameters, the\nalignment algorithm evaluates all possible alignment paths in fairly large\ndocuments of thousands of sentences and uses a divide-and-conquer approach to\nalign documents containing tens of thousands of sentences. The scoring function\nis based on LaBSE bilingual sentence representations. SentAlign outperforms\nfive other sentence alignment tools when evaluated on two different evaluation\nsets, German-French and English-Icelandic, and on a downstream machine\ntranslation task.\n","authors":["Steinþór Steingrímsson","Hrafn Loftsson","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2311.08982v1.pdf","comment":"EMNLP 2023 System Demonstration paper"},{"id":"http://arxiv.org/abs/2311.08981v1","updated":"2023-11-15T14:15:30Z","published":"2023-11-15T14:15:30Z","title":"Speculative Contrastive Decoding","summary":"  Large language models (LLMs) have shown extraordinary performance in various\nlanguage tasks, but high computational requirements hinder their widespread\ndeployment. Speculative decoding, which uses amateur models to predict the\ngeneration of expert models, has been proposed as a way to accelerate LLM\ninference. However, speculative decoding focuses on acceleration instead of\nmaking the best use of the token distribution from amateur models. We proposed\nSpeculative Contrastive Decoding (SCD), an accelerated decoding method\nleveraging the natural contrast between expert and amateur models in\nspeculative decoding. Comprehensive evaluations on four benchmarks show that\nSCD can achieve similar acceleration factors as speculative decoding while\nfurther improving the generation quality as the contrastive decoding. The\nanalysis of token probabilities further demonstrates the compatibility between\nspeculative and contrastive decoding. Overall, SCD provides an effective\napproach to enhance the decoding quality of LLMs while saving computational\nresources.\n","authors":["Hongyi Yuan","Keming Lu","Fei Huang","Zheng Yuan","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08981v1.pdf","comment":"Working in Progress"},{"id":"http://arxiv.org/abs/2308.15452v4","updated":"2023-11-15T14:06:30Z","published":"2023-08-29T17:22:39Z","title":"When Do Program-of-Thoughts Work for Reasoning?","summary":"  In the realm of embodied artificial intelligence, the reasoning capabilities\nof Large Language Models (LLMs) play a pivotal role. Although there are\neffective methods like program-of-thought prompting for LLMs which uses\nprogramming language to tackle complex reasoning tasks, the specific impact of\ncode data on the improvement of reasoning capabilities remains under-explored.\nTo address this gap, we propose complexity-impacted reasoning score (CIRS),\nwhich combines structural and logical attributes, to measure the correlation\nbetween code and reasoning abilities. Specifically, we use the abstract syntax\ntree to encode the structural information and calculate logical complexity by\nconsidering the difficulty and the cyclomatic complexity. Through an empirical\nanalysis, we find not all code data of complexity can be learned or understood\nby LLMs. Optimal level of complexity is critical to the improvement of\nreasoning abilities by program-aided prompting. Then we design an\nauto-synthesizing and stratifying algorithm, and apply it to instruction\ngeneration for mathematical reasoning and code data filtering for code\ngeneration tasks. Extensive results demonstrates the effectiveness of our\nproposed approach. Code will be integrated into the EasyInstruct framework at\nhttps://github.com/zjunlp/EasyInstruct.\n","authors":["Zhen Bi","Ningyu Zhang","Yinuo Jiang","Shumin Deng","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.15452v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.08968v1","updated":"2023-11-15T14:01:41Z","published":"2023-11-15T14:01:41Z","title":"Identifying Linear Relational Concepts in Large Language Models","summary":"  Transformer language models (LMs) have been shown to represent concepts as\ndirections in the latent space of hidden activations. However, for any given\nhuman-interpretable concept, how can we find its direction in the latent space?\nWe present a technique called linear relational concepts (LRC) for finding\nconcept directions corresponding to human-interpretable concepts at a given\nhidden layer in a transformer LM by first modeling the relation between subject\nand object as a linear relational embedding (LRE). While the LRE work was\nmainly presented as an exercise in understanding model representations, we find\nthat inverting the LRE while using earlier object layers results in a powerful\ntechnique to find concept directions that both work well as a classifier and\ncausally influence model outputs.\n","authors":["David Chanin","Anthony Hunter","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2311.08968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10405v6","updated":"2023-11-15T14:00:17Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hyper network to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Ningyu Zhang","Bozhong Tian","Xi Chen","Qingbing Liu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v6.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2311.08966v1","updated":"2023-11-15T13:53:28Z","published":"2023-11-15T13:53:28Z","title":"Improving Large-scale Deep Biasing with Phoneme Features and Text-only\n  Data in Streaming Transducer","summary":"  Deep biasing for the Transducer can improve the recognition performance of\nrare words or contextual entities, which is essential in practical\napplications, especially for streaming Automatic Speech Recognition (ASR).\nHowever, deep biasing with large-scale rare words remains challenging, as the\nperformance drops significantly when more distractors exist and there are words\nwith similar grapheme sequences in the bias list. In this paper, we combine the\nphoneme and textual information of rare words in Transducers to distinguish\nwords with similar pronunciation or spelling. Moreover, the introduction of\ntraining with text-only data containing more rare words benefits large-scale\ndeep biasing. The experiments on the LibriSpeech corpus demonstrate that the\nproposed method achieves state-of-the-art performance on rare word error rate\nfor different scales and levels of bias lists.\n","authors":["Jin Qiu","Lu Huang","Boyu Li","Jun Zhang","Lu Lu","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2311.08966v1.pdf","comment":"Submitted to ASRU 2023"},{"id":"http://arxiv.org/abs/2311.08941v1","updated":"2023-11-15T13:23:24Z","published":"2023-11-15T13:23:24Z","title":"Reasoning over Description Logic-based Contexts with Transformers","summary":"  One way that the current state of the art measures the reasoning ability of\ntransformer-based models is by evaluating accuracy in downstream tasks like\nlogical question answering or proof generation over synthetic contexts\nexpressed in natural language. However, most of the contexts used are in\npractice very simple; in most cases, they are generated from short first-order\nlogic sentences with only a few logical operators and quantifiers. In this\nwork, we seek to answer the question how well a transformer-based model will\nperform reasoning over expressive contexts. For this purpose, we construct a\nsynthetic natural language question-answering dataset, generated by description\nlogic knowledge bases. For the generation of the knowledge bases, we use the\nexpressive language $\\mathcal{ALCQ}$. The resulting dataset contains 384K\nexamples, and increases in two dimensions: i) reasoning depth, and ii) length\nof sentences. We show that the performance of our DeBERTa-based model,\nDELTA$_M$, is marginally affected when the reasoning depth is increased and it\nis not affected at all when the length of the sentences is increasing. We also\nevaluate the generalization ability of the model on reasoning depths unseen at\ntraining, both increasing and decreasing, revealing interesting insights into\nthe model's adaptive generalization abilities.\n","authors":["Angelos Poulis","Eleni Tsalapati","Manolis Koubarakis"],"pdf_url":"https://arxiv.org/pdf/2311.08941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08703v4","updated":"2023-11-15T12:55:56Z","published":"2023-05-15T15:06:20Z","title":"Schema-adaptable Knowledge Graph Construction","summary":"  Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n","authors":["Hongbin Ye","Honghao Gui","Xin Xu","Xi Chen","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08703v4.pdf","comment":"EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2311.08921v1","updated":"2023-11-15T12:47:52Z","published":"2023-11-15T12:47:52Z","title":"Self-Improving for Zero-Shot Named Entity Recognition with Large\n  Language Models","summary":"  Exploring the application of powerful large language models (LLMs) on the\nfundamental named entity recognition (NER) task has drawn much attention\nrecently. This work aims to investigate the possibilities of pushing the\nboundary of zero-shot NER with LLM via a training-free self-improving strategy.\nWe propose a self-improving framework, which utilize an unlabeled corpus to\nstimulate the self-learning ability of LLMs on NER. First, we use LLM to make\npredictions on the unlabeled corpus and obtain the self-annotated data. Second,\nwe explore various strategies to select reliable samples from the\nself-annotated dataset as demonstrations, considering the similarity, diversity\nand reliability of demonstrations. Finally, we conduct inference for the test\nquery via in-context learning with the selected self-annotated demonstrations.\nThrough comprehensive experimental analysis, our study yielded the following\nfindings: (1) The self-improving framework further pushes the boundary of\nzero-shot NER with LLMs, and achieves an obvious performance improvement; (2)\nIterative self-improving or naively increasing the size of unlabeled corpus\ndoes not guarantee improvements; (3) There might still be space for improvement\nvia more advanced strategy for reliable entity selection.\n","authors":["Tingyu Xie","Qi Li","Yan Zhang","Zuozhu Liu","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10191v4","updated":"2023-11-15T12:41:57Z","published":"2023-10-16T08:53:57Z","title":"VIBE: Topic-Driven Temporal Adaptation for Twitter Classification","summary":"  Language features are evolving in real-world social media, resulting in the\ndeteriorating performance of text classification in dynamics. To address this\nchallenge, we study temporal adaptation, where models trained on past data are\ntested in the future. Most prior work focused on continued pretraining or\nknowledge updating, which may compromise their performance on noisy social\nmedia data. To tackle this issue, we reflect feature change via modeling latent\ntopic evolution and propose a novel model, VIBE: Variational Information\nBottleneck for Evolutions. Concretely, we first employ two Information\nBottleneck (IB) regularizers to distinguish past and future topics. Then, the\ndistinguished topics work as adaptive features via multi-task training with\ntimestamp and class label prediction. In adaptive learning, VIBE utilizes\nretrieved unlabeled data from online streams created posterior to training data\ntime. Substantial Twitter experiments on three classification tasks show that\nour model, with only 3% of data, significantly outperforms previous\nstate-of-the-art continued-pretraining methods.\n","authors":["Yuji Zhang","Jing Li","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2310.10191v4.pdf","comment":"accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.13062v3","updated":"2023-11-15T12:18:39Z","published":"2023-05-22T14:23:46Z","title":"GPT4Table: Can Large Language Models Understand Structured Table Data? A\n  Benchmark and Empirical Study","summary":"  Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve Natural Language (NL)-related tasks. However, there is still much to\nlearn about how well LLMs understand structured data, such as tables. While it\nis true that tables can be used as inputs to LLMs with serialization, there is\na lack of comprehensive studies examining whether LLMs can truly comprehend\nsuch data. In this paper, we try to understand this by designing a benchmark to\nevaluate the structural understanding capabilities (SUC) of LLMs. The benchmark\nwe create includes seven tasks, each with its own unique challenges, \\eg, cell\nlookup, row retrieval, and size detection. We conduct a series of evaluations\non GPT-3.5 and GPT-4. We find that the performance varied depending on several\ninput choices, including table input format, content order, role prompting, and\npartition marks. Drawing from the insights gained through the benchmark\nevaluations, we propose \\textit{self-augmentation} for effective structural\nprompting, such as critical value / range identification using LLMs' internal\nknowledge. When combined with carefully chosen input choices, these structural\nprompting methods lead to promising improvements in LLM performance on a\nvariety of tabular tasks, \\eg, TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe that our benchmark and proposed\nprompting methods can serve as a simple yet generic selection for future\nresearch.\n","authors":["Yuan Sui","Mengyu Zhou","Mingjie Zhou","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13062v3.pdf","comment":"This paper has been accepted as a full paper at WSDM 2024"},{"id":"http://arxiv.org/abs/2311.08896v1","updated":"2023-11-15T12:02:52Z","published":"2023-11-15T12:02:52Z","title":"HELLaMA: LLaMA-based Table to Text Generation by Highlighting the\n  Important Evidence","summary":"  Large models have demonstrated significant progress across various domains,\nparticularly in tasks related to text generation. In the domain of Table to\nText, many Large Language Model (LLM)-based methods currently resort to\nmodifying prompts to invoke public APIs, incurring potential costs and\ninformation leaks. With the advent of open-source large models, fine-tuning\nLLMs has become feasible. In this study, we conducted parameter-efficient\nfine-tuning on the LLaMA2 model. Distinguishing itself from previous\nfine-tuning-based table-to-text methods, our approach involves injecting\nreasoning information into the input by emphasizing table-specific row data.\nOur model consists of two modules: 1) a table reasoner that identifies relevant\nrow evidence, and 2) a table summarizer that generates sentences based on the\nhighlighted table. To facilitate this, we propose a search strategy to\nconstruct reasoning labels for training the table reasoner. On both the FetaQA\nand QTSumm datasets, our approach achieved state-of-the-art results.\nAdditionally, we observed that highlighting input tables significantly enhances\nthe model's performance and provides valuable interpretability.\n","authors":["Junyi Bian","Xiaolei Qin","Wuhe Zou","Mengzuo Huang","Weidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08894v1","updated":"2023-11-15T11:56:56Z","published":"2023-11-15T11:56:56Z","title":"Combining Transfer Learning with In-context Learning using Blackbox LLMs\n  for Zero-shot Knowledge Base Question Answering","summary":"  We address the zero-shot transfer learning setting for the knowledge base\nquestion answering (KBQA) problem, where a large volume of labeled training\ndata is available for the source domain, but no such labeled examples are\navailable for the target domain. Transfer learning for KBQA makes use of large\nvolumes of unlabeled data in the target in addition to the labeled data in the\nsource. More recently, few-shot in-context learning using Black-box Large\nLanguage Models (BLLMs) has been adapted for KBQA without considering any\nsource domain data. In this work, we show how to meaningfully combine these two\nparadigms for KBQA so that their benefits add up. Specifically, we preserve the\ntwo stage retrieve-then-generate pipeline of supervised KBQA and introduce\ninteraction between in-context learning using BLLMs and transfer learning from\nthe source for both stages. In addition, we propose execution-guided\nself-refinement using BLLMs, decoupled from the transfer setting. With the help\nof experiments using benchmark datasets GrailQA as the source and WebQSP as the\ntarget, we show that the proposed combination brings significant improvements\nto both stages and also outperforms by a large margin state-of-the-art\nsupervised KBQA models trained on the source. We also show that in the\nin-domain setting, the proposed BLLM augmentation significantly outperforms\nstate-of-the-art supervised models, when the volume of labeled data is limited,\nand also outperforms these marginally even when using the entire large training\ndataset.\n","authors":["Mayur Patidar","Avinash Singh","Riya Sawhney","Indrajit Bhattacharya"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2311.08894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08890v1","updated":"2023-11-15T11:50:10Z","published":"2023-11-15T11:50:10Z","title":"Large Language Models are legal but they are not: Making the case for a\n  powerful LegalLLM","summary":"  Realizing the recent advances in Natural Language Processing (NLP) to the\nlegal sector poses challenging problems such as extremely long sequence\nlengths, specialized vocabulary that is usually only understood by legal\nprofessionals, and high amounts of data imbalance. The recent surge of Large\nLanguage Models (LLMs) has begun to provide new opportunities to apply NLP in\nthe legal domain due to their ability to handle lengthy, complex sequences.\nMoreover, the emergence of domain-specific LLMs has displayed extremely\npromising results on various tasks. In this study, we aim to quantify how\ngeneral LLMs perform in comparison to legal-domain models (be it an LLM or\notherwise). Specifically, we compare the zero-shot performance of three\ngeneral-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR\nsubset of the LexGLUE benchmark for contract provision classification. Although\nthe LLMs were not explicitly trained on legal data, we observe that they are\nstill able to classify the theme correctly in most cases. However, we find that\ntheir mic-F1/mac-F1 performance is up to 19.2/26.8\\% lesser than smaller models\nfine-tuned on the legal domain, thus underscoring the need for more powerful\nlegal-domain LLMs.\n","authors":["Thanmay Jayakumar","Fauzan Farooqui","Luqman Farooqui"],"pdf_url":"https://arxiv.org/pdf/2311.08890v1.pdf","comment":"7 pages, Accepted at Natural Legal Language Processing Workshop,\n  EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.08886v1","updated":"2023-11-15T11:48:16Z","published":"2023-11-15T11:48:16Z","title":"CLIMB: Curriculum Learning for Infant-inspired Model Building","summary":"  We describe our team's contribution to the STRICT-SMALL track of the BabyLM\nChallenge. The challenge requires training a language model from scratch using\nonly a relatively small training dataset of ten million words. We experiment\nwith three variants of cognitively-motivated curriculum learning and analyze\ntheir effect on the performance of the model on linguistic evaluation tasks. In\nthe vocabulary curriculum, we analyze methods for constraining the vocabulary\nin the early stages of training to simulate cognitively more plausible learning\ncurves. In the data curriculum experiments, we vary the order of the training\ninstances based on i) infant-inspired expectations and ii) the learning\nbehavior of the model. In the objective curriculum, we explore different\nvariations of combining the conventional masked language modeling task with a\nmore coarse-grained word class prediction task to reinforce linguistic\ngeneralization capabilities. Our results did not yield consistent improvements\nover our own non-curriculum learning baseline across a range of linguistic\nbenchmarks; however, we do find marginal gains on select tasks. Our analysis\nhighlights key takeaways for specific combinations of tasks and settings which\nbenefit from our proposed curricula. We moreover determine that careful\nselection of model architecture, and training hyper-parameters yield\nsubstantial improvements over the default baselines provided by the BabyLM\nchallenge.\n","authors":["Richard Diehl Martinez","Zebulon Goriely","Hope McGovern","Christopher Davis","Andrew Caines","Paula Buttery","Lisa Beinborn"],"pdf_url":"https://arxiv.org/pdf/2311.08886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08883v1","updated":"2023-11-15T11:42:41Z","published":"2023-11-15T11:42:41Z","title":"Enabling Large Language Models to Learn from Rules","summary":"  Large language models (LLMs) have shown incredible performance in completing\nvarious real-world tasks. The current knowledge learning paradigm of LLMs is\nmainly based on learning from examples, in which LLMs learn the internal rule\nimplicitly from a certain number of supervised examples. However, the learning\nparadigm may not well learn those complicated rules, especially when the\ntraining examples are limited. We are inspired that humans can learn the new\ntasks or knowledge in another way by learning from rules. That is, humans can\ngrasp the new tasks or knowledge quickly and generalize well given only a\ndetailed rule and a few optional examples. Therefore, in this paper, we aim to\nexplore the feasibility of this new learning paradigm, which encodes the\nrule-based knowledge into LLMs. We propose rule distillation, which first uses\nthe strong in-context abilities of LLMs to extract the knowledge from the\ntextual rules and then explicitly encode the knowledge into LLMs' parameters by\nlearning from the above in-context signals produced inside the model. Our\nexperiments show that making LLMs learn from rules by our method is much more\nefficient than example-based learning in both the sample size and\ngeneralization ability.\n","authors":["Wenkai Yang","Yankai Lin","Jie Zhou","Jirong Wen"],"pdf_url":"https://arxiv.org/pdf/2311.08883v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2206.06807v3","updated":"2023-11-15T11:42:28Z","published":"2022-06-14T12:56:34Z","title":"The Causal Structure of Semantic Ambiguities","summary":"  Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eye-tracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.\n","authors":["Daphne Wang","Mehrnoosh Sadrzadeh"],"pdf_url":"https://arxiv.org/pdf/2206.06807v3.pdf","comment":"In Proceedings QPL 2022, arXiv:2311.08375"},{"id":"http://arxiv.org/abs/2311.08877v1","updated":"2023-11-15T11:27:44Z","published":"2023-11-15T11:27:44Z","title":"Llamas Know What GPTs Don't Show: Surrogate Models for Confidence\n  Estimation","summary":"  To maintain user trust, large language models (LLMs) should signal low\nconfidence on examples where they are incorrect, instead of misleading the\nuser. The standard approach of estimating confidence is to use the softmax\nprobabilities of these models, but as of November 2023, state-of-the-art LLMs\nsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\nfirst study eliciting confidence linguistically -- asking an LLM for its\nconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\naveraged across 12 question-answering datasets -- 7% above a random baseline)\nbut leaves room for improvement. We then explore using a surrogate confidence\nmodel -- using a model where we do have probabilities to evaluate the original\nmodel's confidence in a given question. Surprisingly, even though these\nprobabilities come from a different and often weaker model, this method leads\nto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\nmethod composing linguistic confidences and surrogate model probabilities gives\nstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\nGPT-4).\n","authors":["Vaishnavi Shrivastava","Percy Liang","Ananya Kumar"],"pdf_url":"https://arxiv.org/pdf/2311.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08849v1","updated":"2023-11-15T10:40:45Z","published":"2023-11-15T10:40:45Z","title":"OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient\n  Large-scale Multilingual Continued Pretraining","summary":"  Pretraining multilingual language models from scratch requires considerable\ncomputational resources and substantial training data. Therefore, a more\nefficient method is to adapt existing pretrained language models (PLMs) to new\nlanguages via vocabulary extension and continued pretraining. However, this\nmethod usually randomly initializes the embeddings of new subwords and\nintroduces substantially more embedding parameters to the language model, thus\nweakening the efficiency. To address these issues, we propose a novel\nframework: \\textbf{O}ne \\textbf{F}or \\textbf{A}ll (\\textbf{\\textsc{Ofa}}),\nwhich wisely initializes the embeddings of unseen subwords from target\nlanguages and thus can adapt a PLM to multiple languages efficiently and\neffectively. \\textsc{Ofa} takes advantage of external well-aligned multilingual\nword embeddings and injects the alignment knowledge into the new embeddings. In\naddition, \\textsc{Ofa} applies matrix factorization and replaces the cumbersome\nembeddings with two lower-dimensional matrices, which significantly reduces the\nnumber of parameters while not sacrificing the performance. Through extensive\nexperiments, we show models initialized by \\textsc{Ofa} are efficient and\noutperform several baselines. \\textsc{Ofa} not only accelerates the convergence\nof continued pretraining, which is friendly to a limited computation budget,\nbut also improves the zero-shot crosslingual transfer on a wide range of\ndownstream tasks. We make our code and models publicly available.\n","authors":["Yihong Liu","Peiqin Lin","Mingyang Wang","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2311.08849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08844v1","updated":"2023-11-15T10:34:14Z","published":"2023-11-15T10:34:14Z","title":"Violet: A Vision-Language Model for Arabic Image Captioning with Gemini\n  Decoder","summary":"  Although image captioning has a vast array of applications, it has not\nreached its full potential in languages other than English. Arabic, for\ninstance, although the native language of more than 400 million people, remains\nlargely underrepresented in this area. This is due to the lack of labeled data\nand powerful Arabic generative models. We alleviate this issue by presenting a\nnovel vision-language model dedicated to Arabic, dubbed \\textit{Violet}. Our\nmodel is based on a vision encoder and a Gemini text decoder that maintains\ngeneration fluency while allowing fusion between the vision and language\ncomponents. To train our model, we introduce a new method for automatically\nacquiring data from available English datasets. We also manually prepare a new\ndataset for evaluation. \\textit{Violet} performs sizeably better than our\nbaselines on all of our evaluation datasets. For example, it reaches a CIDEr\nscore of $61.2$ on our manually annotated dataset and achieves an improvement\nof $13$ points on Flickr8k.\n","authors":["Abdelrahman Mohamed","Fakhraddin Alwajih","El Moatez Billah Nagoudi","Alcides Alcoba Inciarte","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2311.08844v1.pdf","comment":"Accepted in ArabicNLP Conference"},{"id":"http://arxiv.org/abs/2311.08838v1","updated":"2023-11-15T10:25:30Z","published":"2023-11-15T10:25:30Z","title":"Disinformation Capabilities of Large Language Models","summary":"  Automated disinformation generation is often listed as one of the risks of\nlarge language models (LLMs). The theoretical ability to flood the information\nspace with disinformation content might have dramatic consequences for\ndemocratic societies around the world. This paper presents a comprehensive\nstudy of the disinformation capabilities of the current generation of LLMs to\ngenerate false news articles in English language. In our study, we evaluated\nthe capabilities of 10 LLMs using 20 disinformation narratives. We evaluated\nseveral aspects of the LLMs: how well they are at generating news articles, how\nstrongly they tend to agree or disagree with the disinformation narratives, how\noften they generate safety warnings, etc. We also evaluated the abilities of\ndetection models to detect these articles as LLM-generated. We conclude that\nLLMs are able to generate convincing news articles that agree with dangerous\ndisinformation narratives.\n","authors":["Ivan Vykopal","Matúš Pikuliak","Ivan Srba","Robert Moro","Dominik Macko","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2311.08838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08836v1","updated":"2023-11-15T10:25:14Z","published":"2023-11-15T10:25:14Z","title":"Evaluating Gender Bias in the Translation of Gender-Neutral Languages\n  into English","summary":"  Machine Translation (MT) continues to improve in quality and adoption, yet\nthe inadvertent perpetuation of gender bias remains a significant concern.\nDespite numerous studies into gender bias in translations from gender-neutral\nlanguages such as Turkish into more strongly gendered languages like English,\nthere are no benchmarks for evaluating this phenomenon or for assessing\nmitigation strategies. To address this gap, we introduce GATE X-E, an extension\nto the GATE (Rarrick et al., 2023) corpus, that consists of human translations\nfrom Turkish, Hungarian, Finnish, and Persian into English. Each translation is\naccompanied by feminine, masculine, and neutral variants for each possible\ngender interpretation. The dataset, which contains between 1250 and 1850\ninstances for each of the four language pairs, features natural sentences with\na wide range of sentence lengths and domains, challenging translation rewriters\non various linguistic phenomena. Additionally, we present an English gender\nrewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We\nopen source our contributions to encourage further research on gender\ndebiasing.\n","authors":["Spencer Rarrick","Ranjita Naik","Sundar Poudel","Vishal Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2311.08836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08817v1","updated":"2023-11-15T09:38:53Z","published":"2023-11-15T09:38:53Z","title":"MAP's not dead yet: Uncovering true language model modes by conditioning\n  away degeneracy","summary":"  It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has\ngenerally been attributed to either a fundamental inadequacy of modes in models\nor weaknesses in language modeling. Contrastingly in this work, we emphasize\nthat degenerate modes can even occur in the absence of any model error, due to\ncontamination of the training data. Specifically, we show that mixing even a\ntiny amount of low-entropy noise with a population text distribution can cause\nthe data distribution's mode to become degenerate, implying that any models\ntrained on it will be as well. As the unconditional mode of NLG models will\noften be degenerate, we therefore propose to apply MAP decoding to the model's\ndistribution conditional on avoiding specific degeneracies. Using exact-search,\nwe empirically verify that the length-conditional modes of machine translation\nmodels and language models are indeed more fluent and topical than their\nunconditional modes. For the first time, we also share many examples of exact\nmodal sequences from these models, and from several variants of the LLaMA-7B\nmodel. Notably, the modes of the LLaMA models are still degenerate, showing\nthat improvements in modeling have not fixed this issue. Because of the cost of\nexact mode finding algorithms, we develop an approximate mode finding approach,\nACBS, which finds sequences that are both high-likelihood and high-quality. We\napply this approach to LLaMA-7B, a model which was not trained for instruction\nfollowing, and find that we are able to elicit reasonable outputs without any\nfinetuning.\n","authors":["Davis Yoshida","Kartik Goyal","Kevin Gimpel"],"pdf_url":"https://arxiv.org/pdf/2311.08817v1.pdf","comment":"49 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.08803v1","updated":"2023-11-15T09:18:09Z","published":"2023-11-15T09:18:09Z","title":"StrategyLLM: Large Language Models as Strategy Generators, Executors,\n  Optimizers, and Evaluators for Problem Solving","summary":"  Most existing chain-of-thought (CoT) prompting methods suffer from the issues\nof generalizability and consistency, as they often rely on instance-specific\nsolutions that may not be applicable to other cases and lack task-level\nconsistency in their reasoning steps. To address these limitations, we propose\na comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to\ntackle various tasks. The framework improves generalizability by formulating\ngeneral problem-solving strategies and enhances consistency by producing\nconsistent solutions using these strategies. StrategyLLM employs four LLM-based\nagents: strategy generator, executor, optimizer, and evaluator, working\ntogether to generate, evaluate, and select promising strategies for a given\ntask automatically. The experimental results demonstrate that StrategyLLM\noutperforms the competitive baseline CoT-SC that requires human-annotated\nsolutions on 13 datasets across 4 challenging tasks without human involvement,\nincluding math reasoning (39.2% $\\rightarrow$ 43.3%), commonsense reasoning\n(70.3% $\\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\\rightarrow$ 62.0%),\nand symbolic reasoning (30.0% $\\rightarrow$ 79.2%).\n","authors":["Chang Gao","Haiyun Jiang","Deng Cai","Shuming Shi","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2311.08803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08793v1","updated":"2023-11-15T09:07:29Z","published":"2023-11-15T09:07:29Z","title":"German FinBERT: A German Pre-trained Language Model","summary":"  This study presents German FinBERT, a novel pre-trained German language model\ntailored for financial textual data. The model is trained through a\ncomprehensive pre-training process, leveraging a substantial corpus comprising\nfinancial reports, ad-hoc announcements and news related to German companies.\nThe corpus size is comparable to the data sets commonly used for training\nstandard BERT models. I evaluate the performance of German FinBERT on\ndownstream tasks, specifically sentiment prediction, topic recognition and\nquestion answering against generic German language models. My results\ndemonstrate improved performance on finance-specific data, indicating the\nefficacy of German FinBERT in capturing domain-specific nuances. The presented\nfindings suggest that German FinBERT holds promise as a valuable tool for\nfinancial text analysis, potentially benefiting various applications in the\nfinancial domain.\n","authors":["Moritz Scherrmann"],"pdf_url":"https://arxiv.org/pdf/2311.08793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08788v1","updated":"2023-11-15T09:01:55Z","published":"2023-11-15T09:01:55Z","title":"X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented\n  Instruction Tuning with Auxiliary Evaluation Aspects","summary":"  Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n","authors":["Minqian Liu","Ying Shen","Zhiyang Xu","Yixin Cao","Eunah Cho","Vaibhav Kumar","Reza Ghanadan","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08788v1.pdf","comment":"17 pages, 5 figures, 14 tables"},{"id":"http://arxiv.org/abs/2308.00755v2","updated":"2023-11-15T08:51:11Z","published":"2023-08-01T18:00:08Z","title":"The Bias Amplification Paradox in Text-to-Image Generation","summary":"  Bias amplification is a phenomenon in which models exacerbate biases or\nstereotypes present in the training data. In this paper, we study bias\namplification in the text-to-image domain using Stable Diffusion by comparing\ngender ratios in training vs. generated images. We find that the model appears\nto amplify gender-occupation biases found in the training data (LAION)\nconsiderably. However, we discover that amplification can be largely attributed\nto discrepancies between training captions and model prompts. For example, an\ninherent difference is that captions from the training data often contain\nexplicit gender information while our prompts do not, which leads to a\ndistribution shift and consequently inflates bias measures. Once we account for\ndistributional differences between texts used for training and generation when\nevaluating amplification, we observe that amplification decreases drastically.\nOur findings illustrate the challenges of comparing biases in models and their\ntraining data, and highlight confounding factors that impact analyses.\n","authors":["Preethi Seshadri","Sameer Singh","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2308.00755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15612v3","updated":"2023-11-15T08:47:28Z","published":"2023-10-24T08:27:56Z","title":"Machine Translation for Nko: Tools, Corpora and Baseline Results","summary":"  Currently, there is no usable machine translation system for Nko, a language\nspoken by tens of millions of people across multiple West African countries,\nwhich holds significant cultural and educational value.\n  To address this issue, we present a set of tools, resources, and baseline\nresults aimed towards the development of usable machine translation systems for\nNko and other languages that do not currently have sufficiently large parallel\ntext corpora available.\n  (1) Fria$\\parallel$el: A novel collaborative parallel text curation software\nthat incorporates quality control through copyedit-based workflows.\n  (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193\nhigh-quality Nko translations in parallel with 204 and 40 other languages.\n  (3) nicolingua-0005: A collection of trilingual and bilingual corpora with\n130,850 parallel segments and monolingual corpora containing over 3 million Nko\nwords.\n  (4) Baseline bilingual and multilingual neural machine translation results\nwith the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.\n","authors":["Moussa Koulako Bala Doumbouya","Baba Mamadi Diané","Solo Farabado Cissé","Djibrila Diané","Abdoulaye Sow","Séré Moussa Doumbouya","Daouda Bangoura","Fodé Moriba Bayo","Ibrahima Sory 2. Condé","Kalo Mory Diané","Chris Piech","Christopher Manning"],"pdf_url":"https://arxiv.org/pdf/2310.15612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08768v1","updated":"2023-11-15T08:24:41Z","published":"2023-11-15T08:24:41Z","title":"Three Conjectures on Unexpectedeness","summary":"  Unexpectedness is a central concept in Simplicity Theory, a theory of\ncognition relating various inferential processes to the computation of\nKolmogorov complexities, rather than probabilities. Its predictive power has\nbeen confirmed by several experiments with human subjects, yet its theoretical\nbasis remains largely unexplored: why does it work? This paper lays the\ngroundwork for three theoretical conjectures. First, unexpectedness can be seen\nas a generalization of Bayes' rule. Second, the frequentist core of\nunexpectedness can be connected to the function of tracking ergodic properties\nof the world. Third, unexpectedness can be seen as constituent of various\nmeasures of divergence between the entropy of the world (environment) and the\nvariety of the observer (system). The resulting framework hints to research\ndirections that go beyond the division between probabilistic and logical\napproaches, potentially bringing new insights into the extraction of causal\nrelations, and into the role of descriptive mechanisms in learning.\n","authors":["Giovanni Sileno","Jean-Louis Dessalles"],"pdf_url":"https://arxiv.org/pdf/2311.08768v1.pdf","comment":"Working paper"},{"id":"http://arxiv.org/abs/2307.02185v2","updated":"2023-11-15T07:59:16Z","published":"2023-07-05T10:25:45Z","title":"Citation: A Key to Building Responsible and Accountable Large Language\n  Models","summary":"  Large Language Models (LLMs) bring transformative benefits alongside unique\nchallenges, including intellectual property (IP) and ethical concerns. This\nposition paper explores a novel angle to mitigate these risks, drawing\nparallels between LLMs and established web systems. We identify \"citation\" -\nthe acknowledgement or reference to a source or evidence - as a crucial yet\nmissing component in LLMs. Incorporating citation could enhance content\ntransparency and verifiability, thereby confronting the IP and ethical issues\nin the deployment of LLMs. We further propose that a comprehensive citation\nmechanism for LLMs should account for both non-parametric and parametric\ncontent. Despite the complexity of implementing such a citation mechanism,\nalong with the potential pitfalls, we advocate for its development. Building on\nthis foundation, we outline several research problems in this area, aiming to\nguide future explorations towards building more responsible and accountable\nLLMs.\n","authors":["Jie Huang","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2307.02185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08756v1","updated":"2023-11-15T07:50:57Z","published":"2023-11-15T07:50:57Z","title":"Accelerating Toeplitz Neural Network with Constant-time Inference\n  Complexity","summary":"  Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in\nvarious sequence modeling tasks. They outperform commonly used\nTransformer-based models while benefiting from log-linear space-time\ncomplexities. On the other hand, State Space Models (SSMs) achieve lower\nperformance than TNNs in language modeling but offer the advantage of constant\ninference complexity. In this paper, we aim to combine the strengths of TNNs\nand SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to\nachieve the same constant inference complexities as SSMs. To accomplish this,\nwe formulate the conversion process as an optimization problem and provide a\nclosed-form solution. We demonstrate how to transform the target equation into\na Vandermonde linear system problem, which can be efficiently solved using the\nDiscrete Fourier Transform (DFT). Notably, our method requires no training and\nmaintains numerical stability. It can be also applied to any LongConv-based\nmodel. To assess its effectiveness, we conduct extensive experiments on\nlanguage modeling tasks across various settings. Additionally, we compare our\nmethod to other gradient-descent solutions, highlighting the superior numerical\nstability of our approach. The source code is available at\nhttps://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.\n","authors":["Zhen Qin","Yiran Zhong"],"pdf_url":"https://arxiv.org/pdf/2311.08756v1.pdf","comment":"Accepted to EMNLP 2023. Yiran Zhong is the corresponding author. The\n  source code is available at\n  https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion"},{"id":"http://arxiv.org/abs/2311.08734v1","updated":"2023-11-15T06:54:44Z","published":"2023-11-15T06:54:44Z","title":"Thread of Thought Unraveling Chaotic Contexts","summary":"  Large Language Models (LLMs) have ushered in a transformative era in the\nfield of natural language processing, excelling in tasks related to text\ncomprehension and generation. Nevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\ncontext), leading to the inadvertent omission of certain details within the\nchaotic context. In response to these challenges, we introduce the \"Thread of\nThought\" (ThoT) strategy, which draws inspiration from human cognitive\nprocesses. ThoT systematically segments and analyzes extended contexts while\nadeptly selecting pertinent information. This strategy serves as a versatile\n\"plug-and-play\" module, seamlessly integrating with various LLMs and prompting\ntechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\nwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\nillustrate that ThoT significantly improves reasoning performance compared to\nother prompting techniques.\n","authors":["Yucheng Zhou","Xiubo Geng","Tao Shen","Chongyang Tao","Guodong Long","Jian-Guang Lou","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2311.08734v1.pdf","comment":"11 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2311.08732v1","updated":"2023-11-15T06:48:50Z","published":"2023-11-15T06:48:50Z","title":"Enhancing Emergency Decision-making with Knowledge Graphs and Large\n  Language Models","summary":"  Emergency management urgently requires comprehensive knowledge while having a\nhigh possibility to go beyond individuals' cognitive scope. Therefore,\nartificial intelligence(AI) supported decision-making under that circumstance\nis of vital importance. Recent emerging large language models (LLM) provide a\nnew direction for enhancing targeted machine intelligence. However, the\nutilization of LLM directly would inevitably introduce unreliable output for\nits inherent issue of hallucination and poor reasoning skills. In this work, we\ndevelop a system called Enhancing Emergency decision-making with Knowledge\nGraph and LLM (E-KELL), which provides evidence-based decision-making in\nvarious emergency stages. The study constructs a structured emergency knowledge\ngraph and guides LLMs to reason over it via a prompt chain. In real-world\nevaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\ncomprehensibility, accuracy, conciseness, and instructiveness from a group of\nemergency commanders and firefighters, demonstrating a significant improvement\nacross various situations compared to baseline models. This work introduces a\nnovel approach to providing reliable emergency decision support.\n","authors":["Minze Chen","Zhenxiang Tao","Weitong Tang","Tingxin Qin","Rui Yang","Chunli Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.08732v1.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08726v1","updated":"2023-11-15T06:36:29Z","published":"2023-11-15T06:36:29Z","title":"Uncertainty Estimation on Sequential Labeling via Uncertainty\n  Transmission","summary":"  Sequential labeling is a task predicting labels for each token in a sequence,\nsuch as Named Entity Recognition (NER). NER tasks aim to extract entities and\npredict their labels given a text, which is important in information\nextraction. Although previous works have shown great progress in improving NER\nperformance, uncertainty estimation on NER (UE-NER) is still underexplored but\nessential. This work focuses on UE-NER, which aims to estimate uncertainty\nscores for the NER predictions. Previous uncertainty estimation models often\noverlook two unique characteristics of NER: the connection between entities\n(i.e., one entity embedding is learned based on the other ones) and wrong span\ncases in the entity extraction subtask. Therefore, we propose a Sequential\nLabeling Posterior Network (SLPN) to estimate uncertainty scores for the\nextracted entities, considering uncertainty transmitted from other tokens.\nMoreover, we have defined an evaluation strategy to address the specificity of\nwrong-span cases. Our SLPN has achieved significant improvements on two\ndatasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant\ndataset.\n","authors":["Jianfeng He","Linlin Yu","Shuo Lei","Chang-Tien Lu","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08726v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.08724v1","updated":"2023-11-15T06:35:01Z","published":"2023-11-15T06:35:01Z","title":"Method for Text Entity Linking in Power Distribution Scheduling Oriented\n  to Power Distribution Network Knowledge Graph","summary":"  The proposed method for linking entities in power distribution dispatch texts\nto a power distribution network knowledge graph is based on a deep\nunderstanding of these networks. This method leverages the unique features of\nentities in both the power distribution network's knowledge graph and the\ndispatch texts, focusing on their semantic, phonetic, and syntactic\ncharacteristics. An enhanced model, the Lexical Semantic Feature-based Skip\nConvolutional Neural Network (LSF-SCNN), is utilized for effectively matching\ndispatch text entities with those in the knowledge graph. The efficacy of this\nmodel, compared to a control model, is evaluated through cross-validation\nmethods in real-world power distribution dispatch scenarios. The results\nindicate that the LSF-SCNN model excels in accurately linking a variety of\nentity types, demonstrating high overall accuracy in entity linking when the\nprocess is conducted in English.\n","authors":["Xiang Li","Che Wang","Bing Li","Hao Chen","Sizhe Li"],"pdf_url":"https://arxiv.org/pdf/2311.08724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08723v1","updated":"2023-11-15T06:33:52Z","published":"2023-11-15T06:33:52Z","title":"Token Prediction as Implicit Classification to Identify LLM-Generated\n  Text","summary":"  This paper introduces a novel approach for identifying the possible large\nlanguage models (LLMs) involved in text generation. Instead of adding an\nadditional classification layer to a base LM, we reframe the classification\ntask as a next-token prediction task and directly fine-tune the base LM to\nperform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the\nbackbone for our experiments. We compared our approach to the more direct\napproach of utilizing hidden states for classification. Evaluation shows the\nexceptional performance of our method in the text classification task,\nhighlighting its simplicity and efficiency. Furthermore, interpretability\nstudies on the features extracted by our model reveal its ability to\ndifferentiate distinctive writing styles among various LLMs even in the absence\nof an explicit classifier. We also collected a dataset named OpenLLMText,\ncontaining approximately 340k text samples from human and LLMs, including\nGPT3.5, PaLM, LLaMA, and GPT2.\n","authors":["Yutian Chen","Hao Kang","Vivian Zhai","Liangze Li","Rita Singh","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2311.08723v1.pdf","comment":"EMNLP 2023, Main Conference"},{"id":"http://arxiv.org/abs/2311.08719v1","updated":"2023-11-15T06:08:35Z","published":"2023-11-15T06:08:35Z","title":"Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term\n  Memory","summary":"  Memory-augmented Large Language Models (LLMs) have demonstrated remarkable\nperformance in long-term human-machine interactions, which basically relies on\niterative recalling and reasoning of history to generate high-quality\nresponses. However, such repeated recall-reason steps easily produce biased\nthoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same\nhistory for different questions. On the contrary, humans can keep thoughts in\nthe memory and recall them without repeated reasoning. Motivated by this human\ncapability, we propose a novel memory mechanism called TiM (Think-in-Memory)\nthat enables LLMs to maintain an evolved memory for storing historical thoughts\nalong the conversation stream. The TiM framework consists of two crucial\nstages: (1) before generating a response, a LLM agent recalls relevant thoughts\nfrom memory, and (2) after generating a response, the LLM agent post-thinks and\nincorporates both historical and new thoughts to update the memory. Thus, TiM\ncan eliminate the issue of repeated reasoning by saving the post-thinking\nthoughts as the history. Besides, we formulate the basic principles to organize\nthe thoughts in memory based on the well-established operations,\n(\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic\nupdates and evolution of the thoughts. Furthermore, we introduce\nLocality-Sensitive Hashing into TiM to achieve efficient retrieval for the\nlong-term conversations. We conduct qualitative and quantitative experiments on\nreal-world and simulated dialogues covering a wide range of topics,\ndemonstrating that equipping existing LLMs with TiM significantly enhances\ntheir performance in generating responses for long-term interactions.\n","authors":["Lei Liu","Xiaoyan Yang","Yue Shen","Binbin Hu","Zhiqiang Zhang","Jinjie Gu","Guannan Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08718v1","updated":"2023-11-15T05:58:35Z","published":"2023-11-15T05:58:35Z","title":"Decomposing Uncertainty for Large Language Models through Input\n  Clarification Ensembling","summary":"  Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a model into data (aleatoric) uncertainty, resulting from the\ninherent complexity or ambiguity of the data, and model (epistemic)\nuncertainty, resulting from the lack of knowledge in the model. Performing\nuncertainty decomposition for large language models (LLMs) is an important step\ntoward improving the reliability, trustworthiness, and interpretability of\nLLMs, but this research task is very challenging and remains unresolved. The\nexisting canonical method, Bayesian Neural Network (BNN), cannot be applied to\nLLMs, because BNN requires training and ensembling multiple variants of models,\nwhich is infeasible or prohibitively expensive for LLMs. In this paper, we\nintroduce an uncertainty decomposition framework for LLMs, called input\nclarifications ensemble, which bypasses the need to train new models. Rather\nthan ensembling models with different parameters, our approach generates a set\nof clarifications for the input, feeds them into the fixed LLMs, and ensembles\nthe corresponding predictions. We show that our framework shares a symmetric\ndecomposition structure with BNN. Empirical evaluations demonstrate that the\nproposed framework provides accurate and reliable uncertainty quantification on\nvarious tasks. Code will be made publicly available at\nhttps://github.com/UCSB-NLP-Chang/llm_uncertainty .\n","authors":["Bairu Hou","Yujian Liu","Kaizhi Qian","Jacob Andreas","Shiyu Chang","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08718v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.08711v1","updated":"2023-11-15T05:28:07Z","published":"2023-11-15T05:28:07Z","title":"PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning","summary":"  Instruction tuning has remarkably advanced large language models (LLMs) in\nunderstanding and responding to diverse human instructions. Despite the success\nin high-resource languages, its application in lower-resource ones faces\nchallenges due to the imbalanced foundational abilities of LLMs across\ndifferent languages, stemming from the uneven language distribution in their\npre-training data. To tackle this issue, we propose pivot language guided\ngeneration (PLUG), an approach that utilizes a high-resource language,\nprimarily English, as the pivot to enhance instruction tuning in lower-resource\nlanguages. It trains the model to first process instructions in the pivot\nlanguage, and then produce responses in the target language. To evaluate our\napproach, we introduce a benchmark, X-AlpacaEval, of instructions in 4\nlanguages (Chinese, Korean, Italian, and Spanish), each annotated by\nprofessional translators. Our approach demonstrates a significant improvement\nin the instruction-following abilities of LLMs by 29% on average, compared to\ndirectly responding in the target language alone. Further experiments validate\nthe versatility of our approach by employing alternative pivot languages beyond\nEnglish to assist languages where LLMs exhibit lower proficiency.\n","authors":["Zhihan Zhang","Dong-Ho Lee","Yuwei Fang","Wenhao Yu","Mengzhao Jia","Meng Jiang","Francesco Barbieri"],"pdf_url":"https://arxiv.org/pdf/2311.08711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05922v2","updated":"2023-11-15T05:23:04Z","published":"2023-11-10T08:12:00Z","title":"Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation\n  Extraction","summary":"  Few-shot relation extraction involves identifying the type of relationship\nbetween two specific entities within a text, using a limited number of\nannotated samples. A variety of solutions to this problem have emerged by\napplying meta-learning and neural graph techniques which typically necessitate\na training process for adaptation. Recently, the strategy of in-context\nlearning has been demonstrating notable results without the need of training.\nFew studies have already utilized in-context learning for zero-shot information\nextraction. Unfortunately, the evidence for inference is either not considered\nor implicitly modeled during the construction of chain-of-thought prompts. In\nthis paper, we propose a novel approach for few-shot relation extraction using\nlarge language models, named CoT-ER, chain-of-thought with explicit evidence\nreasoning. In particular, CoT-ER first induces large language models to\ngenerate evidences using task-specific and concept-level knowledge. Then these\nevidences are explicitly incorporated into chain-of-thought prompting for\nrelation extraction. Experimental results demonstrate that our CoT-ER approach\n(with 0% training data) achieves competitive performance compared to the\nfully-supervised (with 100% training data) state-of-the-art approach on the\nFewRel1.0 and FewRel2.0 datasets.\n","authors":["Xilai Ma","Jing Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.05922v2.pdf","comment":"An error example is in Table 14 on Page 18. Need to carefully correct\n  and evaluate the error"},{"id":"http://arxiv.org/abs/2311.08705v1","updated":"2023-11-15T05:11:43Z","published":"2023-11-15T05:11:43Z","title":"Evaluating Robustness of Dialogue Summarization Models in the Presence\n  of Naturally Occurring Variations","summary":"  Dialogue summarization task involves summarizing long conversations while\npreserving the most salient information. Real-life dialogues often involve\nnaturally occurring variations (e.g., repetitions, hesitations) and existing\ndialogue summarization models suffer from performance drop on such\nconversations. In this study, we systematically investigate the impact of such\nvariations on state-of-the-art dialogue summarization models using publicly\navailable datasets. To simulate real-life variations, we introduce two types of\nperturbations: utterance-level perturbations that modify individual utterances\nwith errors and language variations, and dialogue-level perturbations that add\nnon-informative exchanges (e.g., repetitions, greetings). We conduct our\nanalysis along three dimensions of robustness: consistency, saliency, and\nfaithfulness, which capture different aspects of the summarization model's\nperformance. We find that both fine-tuned and instruction-tuned models are\naffected by input variations, with the latter being more susceptible,\nparticularly to dialogue-level perturbations. We also validate our findings via\nhuman evaluation. Finally, we investigate if the robustness of fine-tuned\nmodels can be improved by training them with a fraction of perturbed data and\nobserve that this approach is insufficient to address robustness challenges\nwith current models and thus warrants a more thorough investigation to identify\nbetter solutions. Overall, our work highlights robustness challenges in\ndialogue summarization and provides insights for future research.\n","authors":["Ankita Gupta","Chulaka Gunasekara","Hui Wan","Jatin Ganhotra","Sachindra Joshi","Marina Danilevsky"],"pdf_url":"https://arxiv.org/pdf/2311.08705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08704v1","updated":"2023-11-15T05:11:26Z","published":"2023-11-15T05:11:26Z","title":"Can Large Language Models Follow Concept Annotation Guidelines? A Case\n  Study on Scientific and Financial Domains","summary":"  Although large language models (LLMs) exhibit remarkable capacity to leverage\nin-context demonstrations, it is still unclear to what extent they can learn\nnew concepts or facts from ground-truth labels. To address this question, we\nexamine the capacity of instruction-tuned LLMs to follow in-context concept\nguidelines for sentence labeling tasks. We design guidelines that present\ndifferent types of factual and counterfactual concept definitions, which are\nused as prompts for zero-shot sentence classification tasks. Our results show\nthat although concept definitions consistently help in task performance, only\nthe larger models (with 70B parameters or more) have limited ability to work\nunder counterfactual contexts. Importantly, only proprietary models such as\nGPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is\ndue to more sophisticated alignment methods. Finally, we find that\nFalcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which\nindicates that careful fine-tuning is more effective than increasing model\nscale. Altogether, our simple evaluation method reveals significant gaps in\nconcept understanding between the most capable open-source language models and\nthe leading proprietary APIs.\n","authors":["Marcio Fonseca","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2311.08704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08702v1","updated":"2023-11-15T05:05:40Z","published":"2023-11-15T05:05:40Z","title":"Debate Helps Supervise Unreliable Experts","summary":"  As AI systems are used to answer more difficult questions and potentially\nhelp create new knowledge, judging the truthfulness of their outputs becomes\nmore difficult and more important. How can we supervise unreliable experts,\nwhich have access to the truth but may not accurately report it, to give\nanswers that are systematically true and don't just superficially seem true,\nwhen the supervisor can't tell the difference between the two on their own? In\nthis work, we show that debate between two unreliable experts can help a\nnon-expert judge more reliably identify the truth. We collect a dataset of\nhuman-written debates on hard reading comprehension questions where the judge\nhas not read the source passage, only ever seeing expert arguments and short\nquotes selectively revealed by 'expert' debaters who have access to the\npassage. In our debates, one expert argues for the correct answer, and the\nother for an incorrect answer. Comparing debate to a baseline we call\nconsultancy, where a single expert argues for only one answer which is correct\nhalf of the time, we find that debate performs significantly better, with 84%\njudge accuracy compared to consultancy's 74%. Debates are also more efficient,\nbeing 68% of the length of consultancies. By comparing human to AI debaters, we\nfind evidence that with more skilled (in this case, human) debaters, the\nperformance of debate goes up but the performance of consultancy goes down. Our\nerror analysis also supports this trend, with 46% of errors in human debate\nattributable to mistakes by the honest debater (which should go away with\nincreased skill); whereas 52% of errors in human consultancy are due to\ndebaters obfuscating the relevant evidence from the judge (which should become\nworse with increased skill). Overall, these results show that debate is a\npromising approach for supervising increasingly capable but potentially\nunreliable AI systems.\n","authors":["Julian Michael","Salsabila Mahdi","David Rein","Jackson Petty","Julien Dirani","Vishakh Padmakumar","Samuel R. Bowman"],"pdf_url":"https://arxiv.org/pdf/2311.08702v1.pdf","comment":"84 pages, 13 footnotes, 5 figures, 4 tables, 28 debate transcripts;\n  data and code at\n  https://github.com/julianmichael/debate/tree/2023-nyu-experiments"},{"id":"http://arxiv.org/abs/2305.03111v3","updated":"2023-11-15T04:56:25Z","published":"2023-05-04T19:02:29Z","title":"Can LLM Already Serve as A Database Interface? A BIg Bench for\n  Large-Scale Database Grounded Text-to-SQLs","summary":"  Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.\n","authors":["Jinyang Li","Binyuan Hui","Ge Qu","Jiaxi Yang","Binhua Li","Bowen Li","Bailin Wang","Bowen Qin","Rongyu Cao","Ruiying Geng","Nan Huo","Xuanhe Zhou","Chenhao Ma","Guoliang Li","Kevin C. C. Chang","Fei Huang","Reynold Cheng","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2305.03111v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2305.14489v2","updated":"2023-11-15T04:51:27Z","published":"2023-05-23T19:38:28Z","title":"Are Large Language Models Robust Coreference Resolvers?","summary":"  Recent work on extending coreference resolution across domains and languages\nrelies on annotated data in both the target domain and language. At the same\ntime, pre-trained large language models (LMs) have been reported to exhibit\nstrong zero- and few-shot learning abilities across a wide range of NLP tasks.\nHowever, prior work mostly studied this ability using artificial sentence-level\ndatasets such as the Winograd Schema Challenge. In this paper, we assess the\nfeasibility of prompt-based coreference resolution by evaluating\ninstruction-tuned language models on difficult, linguistically-complex\ncoreference benchmarks (e.g., CoNLL-2012). We show that prompting for\ncoreference can outperform current unsupervised coreference systems, although\nthis approach appears to be reliant on high-quality mention detectors. Further\ninvestigations reveal that instruction-tuned LMs generalize surprisingly well\nacross domains, languages, and time periods; yet continued fine-tuning of\nneural models should still be preferred if small amounts of annotated examples\nare available.\n","authors":["Nghia T. Le","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2305.14489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08695v1","updated":"2023-11-15T04:50:30Z","published":"2023-11-15T04:50:30Z","title":"Attribute Diversity Determines the Systematicity Gap in VQA","summary":"  The degree to which neural networks can generalize to new combinations of\nfamiliar concepts, and the conditions under which they are able to do so, has\nlong been an open question. In this work, we study the systematicity gap in\nvisual question answering: the performance difference between reasoning on\npreviously seen and unseen combinations of object attributes. To test, we\nintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\nquantity of training data does not reduce the systematicity gap, increased\ntraining data diversity of the attributes in the unseen combination does. In\nall, our experiments suggest that the more distinct attribute type combinations\nare seen during training, the more systematic we can expect the resulting model\nto be.\n","authors":["Ian Berlot-Attwell","A. Michael Carrell","Kumar Krishna Agrawal","Yash Sharma","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2311.08695v1.pdf","comment":"18 pages, 20 figures"},{"id":"http://arxiv.org/abs/2311.08692v1","updated":"2023-11-15T04:40:43Z","published":"2023-11-15T04:40:43Z","title":"Routing to the Expert: Efficient Reward-guided Ensemble of Large\n  Language Models","summary":"  The complementary potential of Large Language Models (LLM) assumes\noff-the-shelf LLMs have heterogeneous expertise in a wide range of domains and\ntasks so that an ensemble of LLMs can achieve consistently better performance.\nExisting ensemble methods for LLMs mainly focus on reward model ranking of\noutputs, leading to significant computation overhead. To combat this issue, we\nrevisit the complementary potential of LLMs and further elaborate it by mining\nlatent expertise with off-the-shelf reward models. We propose Zooter, a\nreward-guided routing method distilling rewards on training queries to train a\nrouting function, which can precisely distribute each query to the LLM with\nexpertise about it. We also integrate a tag-based label enhancement to mitigate\nnoise from uncertainty when using rewards as silver supervision. Zooter shows\ncomputation efficiency in inference as it introduces only a minor computation\noverhead of a routing function compared with reward model ranking methods. We\nevaluate Zooter on a comprehensive benchmark collection with 26 subsets on\ndifferent domains and tasks. Zooter outperforms the best single model on\naverage and ranks first on 44% of tasks, even surpassing multiple reward model\nranking methods.\n","authors":["Keming Lu","Hongyi Yuan","Runji Lin","Junyang Lin","Zheng Yuan","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08687v1","updated":"2023-11-15T04:30:20Z","published":"2023-11-15T04:30:20Z","title":"An Eye on Clinical BERT: Investigating Language Model Generalization for\n  Diabetic Eye Disease Phenotyping","summary":"  Diabetic eye disease is a major cause of blindness worldwide. The ability to\nmonitor relevant clinical trajectories and detect lapses in care is critical to\nmanaging the disease and preventing blindness. Alas, much of the information\nnecessary to support these goals is found only in the free text of the\nelectronic medical record. To fill this information gap, we introduce a system\nfor extracting evidence from clinical text of 19 clinical concepts related to\ndiabetic eye disease and inferring relevant attributes for each. In developing\nthis ophthalmology phenotyping system, we are also afforded a unique\nopportunity to evaluate the effectiveness of clinical language models at\nadapting to new clinical domains. Across multiple training paradigms, we find\nthat BERT language models pretrained on out-of-distribution clinical data offer\nno significant improvement over BERT language models pretrained on non-clinical\ndata for our domain. Our study tempers recent claims that language models\npretrained on clinical data are necessary for clinical NLP tasks and highlights\nthe importance of not treating clinical language data as a single homogeneous\ndomain.\n","authors":["Keith Harrigian","Tina Tang","Anthony Gonzales","Cindy X. Cai","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2311.08687v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 24 pages"},{"id":"http://arxiv.org/abs/2311.08685v1","updated":"2023-11-15T04:22:22Z","published":"2023-11-15T04:22:22Z","title":"Safer-Instruct: Aligning Language Models with Automated Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for\nenhancing model safety in language models. However, annotating preference data\nfor RLHF is a resource-intensive and creativity-demanding process, while\nautomatic generation methods face limitations in data diversity and quality. In\nresponse, we present Safer-Instruct, a novel pipeline for semi-automatically\nconstructing large-scale preference datasets. Our approach leverages reversed\ninstruction tuning, instruction induction, and expert model evaluation to\nefficiently generate high-quality preference data without human annotators. We\nevaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an\nexpert model, generating approximately 10K preference samples. Finetuning an\nAlpaca model on this dataset demonstrates improved harmlessness while\nmaintaining competitive performance on conversation and downstream tasks.\nSafer-Instruct addresses the challenges in preference data acquisition,\nadvancing the development of safer and more responsible AI systems. Our code\nand data are available at https://github.com/uscnlp-lime/safer-instruct\n","authors":["Taiwei Shi","Kai Chen","Jieyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.08685v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2311.08360v2","updated":"2023-11-15T04:02:44Z","published":"2023-11-14T18:03:20Z","title":"The Transient Nature of Emergent In-Context Learning in Transformers","summary":"  Transformer neural networks can exhibit a surprising capacity for in-context\nlearning (ICL) despite not being explicitly trained for it. Prior work has\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\nexamining the distributional properties of training data. However, in each of\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\nemergence of ICL during transformer training is, in fact, often transient. We\ntrain transformers on synthetic data designed so that both ICL and in-weights\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\nfirst emerges, then disappears and gives way to IWL, all while the training\nloss decreases, indicating an asymptotic preference for IWL. The transient\nnature of ICL is observed in transformers across a range of model sizes and\ndatasets, raising the question of how much to \"overtrain\" transformers when\nseeking compact, cheaper-to-run models. We find that L2 regularization may\noffer a path to more persistent ICL that removes the need for early stopping\nbased on ICL-style validation tasks. Finally, we present initial evidence that\nICL transience may be caused by competition between ICL and IWL circuits.\n","authors":["Aaditya K. Singh","Stephanie C. Y. Chan","Ted Moskovitz","Erin Grant","Andrew M. Saxe","Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2311.08360v2.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2310.08130v4","updated":"2023-11-15T03:59:42Z","published":"2023-10-12T08:38:12Z","title":"Fine-grained Conversational Decoding via Isotropic and Proximal Search","summary":"  General-purpose text decoding approaches are usually adopted for dialogue\nresponse generation. Although the quality of the generated responses can be\nimproved with dialogue-specific encoding methods, conversational decoding\nmethods are still under-explored. Inspired by \\citet{wu2023learning} that a\ngood dialogue feature space should follow the rules of locality and isotropy,\nwe present a fine-grained conversational decoding method, termed\n\\textit{isotropic and proximal search (IPS)}. Our method is designed to\ngenerate the semantic-concentrated response, while still maintaining\ninformativeness and discrimination against the context. Experiments show that\nour approach outperforms existing decoding strategies in the dialogue field\nacross both automatic and human evaluation metrics. More in-depth analyses\nfurther confirm the effectiveness of our approach.\n","authors":["Yuxuan Yao","Han Wu","Qiling Xu","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2310.08130v4.pdf","comment":"Accepted to EMNLP 2023 Main Conference"},{"id":"http://arxiv.org/abs/2311.08669v1","updated":"2023-11-15T03:29:02Z","published":"2023-11-15T03:29:02Z","title":"Understanding Calibration for Multilingual Question Answering Models","summary":"  Multilingual pre-trained language models are incredibly effective at Question\nAnswering (QA), a core task in Natural Language Understanding, achieving high\naccuracies on several multilingual benchmarks. However, little is known about\nhow well they are calibrated. In this paper, we study the calibration\nproperties of several pre-trained multilingual large language models (LLMs) on\na variety of question-answering tasks. We perform extensive experiments,\nspanning both extractive and generative QA model designs and diverse languages,\nspanning both high-resource and low-resource ones. We study different\ndimensions of calibration in in-distribution, out-of-distribution, and\ncross-lingual transfer settings, and investigate strategies to improve it,\nincluding post-hoc methods and regularized fine-tuning. We demonstrate\nautomatically translated data augmentation as a highly effective technique to\nimprove model calibration. We also conduct a number of ablation experiments to\nstudy the effect of model size on calibration and how multilingual models\ncompare with their monolingual counterparts for diverse tasks and languages.\n","authors":["Yahan Yang","Soham Dan","Dan Roth","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2311.08669v1.pdf","comment":"Preprint. Under Submission"},{"id":"http://arxiv.org/abs/2210.13034v2","updated":"2023-11-15T03:24:03Z","published":"2022-10-24T08:34:10Z","title":"Beyond Vectors: Subspace Representations for Set Operations of\n  Embeddings","summary":"  In natural language processing (NLP), the role of embeddings in representing\nlinguistic semantics is crucial. Despite the prevalence of vector\nrepresentations in embedding sets, they exhibit limitations in expressiveness\nand lack comprehensive set operations. To address this, we attempt to formulate\nand apply sets and their operations within pre-trained embedding spaces.\nInspired by quantum logic, we propose to go beyond the conventional vector set\nrepresentation with our novel subspace-based approach. This methodology\nconstructs subspaces using pre-trained embedding sets, effectively preserving\nsemantic nuances previously overlooked, and consequently consistently improving\nperformance in downstream tasks.\n","authors":["Yoichi Ishibashi","Sho Yokoi","Katsuhito Sudoh","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2210.13034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08666v1","updated":"2023-11-15T03:21:04Z","published":"2023-11-15T03:21:04Z","title":"It Takes Two to Negotiate: Modeling Social Exchange in Online\n  Multiplayer Games","summary":"  Online games are dynamic environments where players interact with each other,\nwhich offers a rich setting for understanding how players negotiate their way\nthrough the game to an ultimate victory. This work studies online player\ninteractions during the turn-based strategy game, Diplomacy. We annotated a\ndataset of over 10,000 chat messages for different negotiation strategies and\nempirically examined their importance in predicting long- and short-term game\noutcomes. Although negotiation strategies can be predicted reasonably\naccurately through the linguistic modeling of the chat messages, more is needed\nfor predicting short-term outcomes such as trustworthiness. On the other hand,\nthey are essential in graph-aware reinforcement learning approaches to predict\nlong-term outcomes, such as a player's success, based on their prior\nnegotiation history. We close with a discussion of the implications and impact\nof our work. The dataset is available at\nhttps://github.com/kj2013/claff-diplomacy.\n","authors":["Kokil Jaidka","Hansin Ahuja","Lynnette Ng"],"pdf_url":"https://arxiv.org/pdf/2311.08666v1.pdf","comment":"28 pages, 11 figures. Accepted to CSCW '24 and forthcoming the\n  Proceedings of ACM HCI '24"},{"id":"http://arxiv.org/abs/2311.08662v1","updated":"2023-11-15T02:59:10Z","published":"2023-11-15T02:59:10Z","title":"Multi-Set Inoculation: Assessing Model Robustness Across Multiple\n  Challenge Sets","summary":"  Language models, given their black-box nature, often exhibit sensitivity to\ninput perturbations, leading to trust issues due to hallucinations. To bolster\ntrust, it's essential to understand these models' failure modes and devise\nstrategies to enhance their performance. In this study, we propose a framework\nto study the effect of input perturbations on language models of different\nscales, from pre-trained models to large language models (LLMs). We use\nfine-tuning to train a robust model to perturbations, and we investigate\nwhether exposure to one perturbation improves or degrades the model's\nperformance on other perturbations. To address multi-perturbation robustness,\nwe suggest three distinct training strategies. We also extend the framework to\nLLMs via a chain of thought(COT) prompting with exemplars. We instantiate our\nframework for the Tabular-NLI task and show that the proposed strategies train\nthe model robust to different perturbations without losing accuracy on a given\ndataset.\n","authors":["Vatsal Gupta","Pranshu Pandya","Tushar Kataria","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2311.08662v1.pdf","comment":"13 pages, 2 Figure, 12 Tables"},{"id":"http://arxiv.org/abs/2311.08189v2","updated":"2023-11-15T02:57:01Z","published":"2023-11-14T14:22:47Z","title":"Unlocking Science: Novel Dataset and Benchmark for Cross-Modality\n  Scientific Information Extraction","summary":"  Extracting key information from scientific papers has the potential to help\nresearchers work more efficiently and accelerate the pace of scientific\nprogress. Over the last few years, research on Scientific Information\nExtraction (SciIE) witnessed the release of several new systems and benchmarks.\nHowever, existing paper-focused datasets mostly focus only on specific parts of\na manuscript (e.g., abstracts) and are single-modality (i.e., text- or\ntable-only), due to complex processing and expensive annotations. Moreover,\ncore information can be present in either text or tables or across both. To\nclose this gap in data availability and enable cross-modality IE, while\nalleviating labeling costs, we propose a semi-supervised pipeline for\nannotating entities in text, as well as entities and relations in tables, in an\niterative procedure. Based on this pipeline, we release novel resources for the\nscientific community, including a high-quality benchmark, a large-scale corpus,\nand a semi-supervised annotation pipeline. We further report the performance of\nstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.\nLastly, we explore the potential capability of large language models such as\nChatGPT for the current task. Our new dataset, results, and analysis validate\nthe effectiveness and efficiency of our semi-supervised pipeline, and we\ndiscuss its remaining limitations.\n","authors":["Yuhan Li","Jian Wu","Zhiwei Yu","Börje F. Karlsson","Wei Shen","Manabu Okumura","Chin-Yew Lin"],"pdf_url":"https://arxiv.org/pdf/2311.08189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14613v2","updated":"2023-11-15T02:15:02Z","published":"2023-05-24T01:25:38Z","title":"Selectively Answering Ambiguous Questions","summary":"  Trustworthy language models should abstain from answering questions when they\ndo not know the answer. However, the answer to a question can be unknown for a\nvariety of reasons. Prior research has focused on the case in which the\nquestion is clear and the answer is unambiguous but possibly unknown, but the\nanswer to a question can also be unclear due to uncertainty of the questioner's\nintent or context. We investigate question answering from this perspective,\nfocusing on answering a subset of questions with a high degree of accuracy,\nfrom a set of questions in which many are inherently ambiguous. In this\nsetting, we find that the most reliable approach to decide when to abstain\ninvolves quantifying repetition within sampled model outputs, rather than the\nmodel's likelihood or self-verification as used in prior work. We find this to\nbe the case across different types of uncertainty and model scales,and with or\nwithout instruction tuning. Our results suggest that sampling-based confidence\nscores help calibrate answers to relatively unambiguous questions, with more\ndramatic improvements on ambiguous questions.\n","authors":["Jeremy R. Cole","Michael J. Q. Zhang","Daniel Gillick","Julian Martin Eisenschlos","Bhuwan Dhingra","Jacob Eisenstein"],"pdf_url":"https://arxiv.org/pdf/2305.14613v2.pdf","comment":"To appear in EMNLP 2023. 9 pages, 5 figures, 2 pages of appendix"},{"id":"http://arxiv.org/abs/2311.08648v1","updated":"2023-11-15T01:58:54Z","published":"2023-11-15T01:58:54Z","title":"Explore Spurious Correlations at the Concept Level in Language Models\n  for Text Classification","summary":"  Language models (LMs) have gained great achievement in various NLP tasks for\nboth fine-tuning and in-context learning (ICL) methods. Despite its outstanding\nperformance, evidence shows that spurious correlations caused by imbalanced\nlabel distributions in training data (or exemplars in ICL) lead to robustness\nissues. However, previous studies mostly focus on word- and phrase-level\nfeatures and fail to tackle it from the concept level, partly due to the lack\nof concept labels and subtle and diverse expressions of concepts in text. In\nthis paper, we first use the LLM to label the concept for each text and then\nmeasure the concept bias of models for fine-tuning or ICL on the test data.\nSecond, we propose a data rebalancing method to mitigate the spurious\ncorrelations by adding the LLM-generated counterfactual data to make a balanced\nlabel distribution for each concept. We verify the effectiveness of our\nmitigation method and show its superiority over the token removal method.\nOverall, our results show that there exist label distribution biases in\nconcepts across multiple text classification datasets, and LMs will utilize\nthese shortcuts to make predictions in both fine-tuning and ICL methods.\n","authors":["Yuhang Zhou","Paiheng Xu","Xiaoyu Liu","Bang An","Wei Ai","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08648v1.pdf","comment":"14 pages, 3 page appendix"},{"id":"http://arxiv.org/abs/2311.08640v1","updated":"2023-11-15T01:28:28Z","published":"2023-11-15T01:28:28Z","title":"Multistage Collaborative Knowledge Distillation from Large Language\n  Models","summary":"  We study semi-supervised sequence prediction tasks where labeled data are too\nscarce to effectively finetune a model and at the same time few-shot prompting\nof a large language model (LLM) has suboptimal performance. This happens when a\ntask, such as parsing, is expensive to annotate and also unfamiliar to a\npretrained LLM. In this paper, we present a discovery that student models\ndistilled from a prompted LLM can often generalize better than their teacher on\nsuch tasks. Leveraging this finding, we propose a new distillation method,\nmultistage collaborative knowledge distillation from an LLM (MCKD), for such\ntasks. MCKD first prompts an LLM using few-shot in-context learning to produce\npseudolabels for unlabeled data. Then, at each stage of distillation, a pair of\nstudents are trained on disjoint partitions of the pseudolabeled data. Each\nstudent subsequently produces new and improved pseudolabels for the unseen\npartition to supervise the next round of student(s) with. We show the benefit\nof multistage cross-partition labeling on two constituency parsing tasks. On\nCRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the\nperformance of supervised finetuning with 500 examples and outperforms the\nprompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.\n","authors":["Jiachen Zhao","Wenlong Zhao","Andrew Drozdov","Benjamin Rozonoyer","Md Arafat Sultan","Jay-Yoon Lee","Mohit Iyyer","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2311.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08637v1","updated":"2023-11-15T01:24:09Z","published":"2023-11-15T01:24:09Z","title":"Formal Proofs as Structured Explanations: Proposing Several Tasks on\n  Explainable Natural Language Inference","summary":"  In this position paper, we propose a way of exploiting formal proofs to put\nforward several explainable natural language inference (NLI) tasks. The formal\nproofs will be produced by a reliable and high-performing logic-based NLI\nsystem. Taking advantage of the in-depth information available in the generated\nformal proofs, we show how it can be used to define NLI tasks with structured\nexplanations. The proposed tasks can be ordered according to difficulty defined\nin terms of the granularity of explanations. We argue that the tasks will\nsuffer with substantially fewer shortcomings than the existing explainable NLI\ntasks (or datasets).\n","authors":["Lasha Abzianidze"],"pdf_url":"https://arxiv.org/pdf/2311.08637v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.08623v1","updated":"2023-11-15T01:01:02Z","published":"2023-11-15T01:01:02Z","title":"DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder\n  Transformer Models","summary":"  Encoder-decoder transformer models have achieved great success on various\nvision-language (VL) tasks, but they suffer from high inference latency.\nTypically, the decoder takes up most of the latency because of the\nauto-regressive decoding. To accelerate the inference, we propose an approach\nof performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit\nencoder-decoder transformer model which is trained with deep supervision so\nthat each of its decoder layers is capable of generating plausible predictions.\nIn addition, we leverage simple yet practical techniques, including shared\ngeneration head and adaptation modules, to keep accuracy when exiting at\nshallow decoder layers. Based on the multi-exit model, we perform step-level\ndynamic early exit during inference, where the model may decide to use fewer\ndecoder layers based on its confidence of the current layer at each individual\ndecoding step. Considering different number of decoder layers may be used at\ndifferent decoding steps, we compute deeper-layer decoder features of previous\ndecoding steps just-in-time, which ensures the features from different decoding\nsteps are semantically aligned. We evaluate our approach with two\nstate-of-the-art encoder-decoder transformer models on various VL tasks. We\nshow our approach can reduce overall inference latency by 30%-60% with\ncomparable or even higher accuracy compared to baselines.\n","authors":["Peng Tang","Pengkai Zhu","Tian Li","Srikar Appalaraju","Vijay Mahadevan","R. Manmatha"],"pdf_url":"https://arxiv.org/pdf/2311.08623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08622v1","updated":"2023-11-15T01:00:02Z","published":"2023-11-15T01:00:02Z","title":"Multiple-Question Multiple-Answer Text-VQA","summary":"  We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do\ntext-VQA in encoder-decoder transformer models. The text-VQA task requires a\nmodel to answer a question by understanding multi-modal content: text\n(typically from OCR) and an associated image. To the best of our knowledge,\nalmost all previous approaches for text-VQA process a single question and its\nassociated content to predict a single answer. In order to answer multiple\nquestions from the same image, each question and content are fed into the model\nmultiple times. In contrast, our proposed MQMA approach takes multiple\nquestions and content as input at the encoder and predicts multiple answers at\nthe decoder in an auto-regressive manner at the same time. We make several\nnovel architectural modifications to standard encoder-decoder transformers to\nsupport MQMA. We also propose a novel MQMA denoising pre-training task which is\ndesigned to teach the model to align and delineate multiple questions and\ncontent with associated answers. MQMA pre-trained model achieves\nstate-of-the-art results on multiple text-VQA datasets, each with strong\nbaselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),\nDocVQA (+1.1%) absolute improvements over the previous state-of-the-art\napproaches.\n","authors":["Peng Tang","Srikar Appalaraju","R. Manmatha","Yusheng Xie","Vijay Mahadevan"],"pdf_url":"https://arxiv.org/pdf/2311.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12342v2","updated":"2023-11-15T00:59:54Z","published":"2023-10-18T21:42:16Z","title":"Eliminating Reasoning via Inferring with Planning: A New Framework to\n  Guide LLMs' Non-linear Thinking","summary":"  Chain-of-Thought(CoT) prompting and its variants explore equipping large\nlanguage models (LLMs) with high-level reasoning abilities by emulating\nhuman-like linear cognition and logic. However, the human mind is complicated\nand mixed with both linear and nonlinear thinking. In this work, we propose\n\\textbf{I}nferential \\textbf{E}xclusion \\textbf{P}rompting (IEP), a novel\nprompting that combines the principles of elimination and inference in order to\nguide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize\nNatural Language Inference (NLI) to deduce each possible solution's entailment\nrelation with context, commonsense, or facts, therefore yielding a broader\nperspective by thinking back for inferring. This forward planning and backward\neliminating process allows IEP to better simulate the complex human thinking\nprocesses compared to other CoT-based methods, which only reflect linear\ncognitive processes. We conducted a series of empirical studies and have\ncorroborated that IEP consistently outperforms CoT across various tasks.\nAdditionally, we observe that integrating IEP and CoT further improves the\nLLMs' performance on certain tasks, highlighting the necessity of equipping\nLLMs with mixed logic processes. Moreover, to better evaluate comprehensive\nfeatures inherent in human logic, we introduce \\textbf{M}ental-\\textbf{A}bility\n\\textbf{R}easoning \\textbf{B}enchmark (MARB). The benchmark comprises six novel\nsubtasks with a total of 9,115 questions, among which 1,685 are developed with\nhand-crafted rationale references. We believe both \\textsc{IEP} and\n\\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and\nverbal reasoning abilities and drive further advancements. \\textsc{MARB} will\nbe available at ~\\texttt{anonymity link} soon.\n","authors":["Yongqi Tong","Yifan Wang","Dawei Li","Sizhe Wang","Zi Lin","Simeng Han","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2310.12342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08620v1","updated":"2023-11-15T00:57:51Z","published":"2023-11-15T00:57:51Z","title":"Toucan: Token-Aware Character Level Language Modeling","summary":"  Character-level language models obviate the need for separately trained\ntokenizers, but efficiency suffers from longer sequence lengths. Learning to\ncombine character representations into tokens has made training these models\nmore efficient, but they still require decoding characters individually. We\npropose Toucan, an augmentation to character-level models to make them\n\"token-aware\". Comparing our method to prior work, we demonstrate significant\nspeed-ups in character generation without a loss in language modeling\nperformance. We then explore differences between our learned dynamic\ntokenization of character sequences with popular fixed vocabulary solutions\nsuch as Byte-Pair Encoding and WordPiece, finding our approach leads to a\ngreater amount of longer sequences tokenized as single items. Our project and\ncode are available at https://nlp.jhu.edu/nuggets/.\n","authors":["William Fleshman","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2311.08620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08614v1","updated":"2023-11-15T00:34:28Z","published":"2023-11-15T00:34:28Z","title":"XplainLLM: A QA Explanation Dataset for Understanding LLM\n  Decision-Making","summary":"  Large Language Models (LLMs) have recently made impressive strides in natural\nlanguage understanding tasks. Despite their remarkable performance,\nunderstanding their decision-making process remains a big challenge. In this\npaper, we look into bringing some transparency to this process by introducing a\nnew explanation dataset for question answering (QA) tasks that integrates\nknowledge graphs (KGs) in a novel way. Our dataset includes 12,102\nquestion-answer-explanation (QAE) triples. Each explanation in the dataset\nlinks the LLM's reasoning to entities and relations in the KGs. The explanation\ncomponent includes a why-choose explanation, a why-not-choose explanation, and\na set of reason-elements that underlie the LLM's decision. We leverage KGs and\ngraph attention networks (GAT) to find the reason-elements and transform them\ninto why-choose and why-not-choose explanations that are comprehensible to\nhumans. Through quantitative and qualitative evaluations, we demonstrate the\npotential of our dataset to improve the in-context learning of LLMs, and\nenhance their interpretability and explainability. Our work contributes to the\nfield of explainable AI by enabling a deeper understanding of the LLMs\ndecision-making process to make them more transparent and thereby, potentially\nmore reliable, to researchers and practitioners alike. Our dataset is available\nat: https://github.com/chen-zichen/XplainLLM_dataset.git\n","authors":["Zichen Chen","Jianda Chen","Mitali Gaidhani","Ambuj Singh","Misha Sra"],"pdf_url":"https://arxiv.org/pdf/2311.08614v1.pdf","comment":"17 pages, 6 figures, 7 tables. Our dataset is available at:\n  https://github.com/chen-zichen/XplainLLM_dataset.git"},{"id":"http://arxiv.org/abs/2311.04378v2","updated":"2023-11-15T00:21:29Z","published":"2023-11-07T22:52:54Z","title":"Watermarks in the Sand: Impossibility of Strong Watermarking for\n  Generative Models","summary":"  Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.\n","authors":["Hanlin Zhang","Benjamin L. Edelman","Danilo Francati","Daniele Venturi","Giuseppe Ateniese","Boaz Barak"],"pdf_url":"https://arxiv.org/pdf/2311.04378v2.pdf","comment":"Blog post:\n  https://www.harvard.edu/kempner-institute/2023/11/09/watermarking-in-the-sand/"},{"id":"http://arxiv.org/abs/2308.10103v2","updated":"2023-11-15T00:14:08Z","published":"2023-08-19T20:18:15Z","title":"ASPIRE: Language-Guided Augmentation for Robust Image Classification","summary":"  Neural image classifiers can often learn to make predictions by overly\nrelying on non-predictive features that are spuriously correlated with the\nclass labels in the training data. This leads to poor performance in real-world\natypical scenarios where such features are absent. Supplementing the training\ndataset with images without such spurious features can aid robust learning\nagainst spurious correlations via better generalization. This paper presents\nASPIRE (Language-guided data Augmentation for SPurIous correlation REmoval), a\nsimple yet effective solution for expanding the training dataset with synthetic\nimages without spurious features. ASPIRE, guided by language, generates these\nimages without requiring any form of additional supervision or existing\nexamples. Precisely, we employ LLMs to first extract foreground and background\nfeatures from textual descriptions of an image, followed by advanced\nlanguage-guided image editing to discover the features that are spuriously\ncorrelated with the class label. Finally, we personalize a text-to-image\ngeneration model to generate diverse in-domain images without spurious\nfeatures. We demonstrate the effectiveness of ASPIRE on 4 datasets, including\nthe very challenging Hard ImageNet dataset, and 9 baselines and show that\nASPIRE improves the classification accuracy of prior methods by 1% - 38%. Code\nsoon at: https://github.com/Sreyan88/ASPIRE.\n","authors":["Sreyan Ghosh","Chandra Kiran Reddy Evuru","Sonal Kumar","Utkarsh Tyagi","Sakshi Singh","Sanjoy Chowdhury","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2308.10103v2.pdf","comment":"Pre-print Under Review"},{"id":"http://arxiv.org/abs/2311.08607v1","updated":"2023-11-15T00:09:21Z","published":"2023-11-15T00:09:21Z","title":"Towards Generalizable SER: Soft Labeling and Data Augmentation for\n  Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech","summary":"  Recognizing emotions in spoken communication is crucial for advanced\nhuman-machine interaction. Current emotion detection methodologies often\ndisplay biases when applied cross-corpus. To address this, our study\namalgamates 16 diverse datasets, resulting in 375 hours of data across\nlanguages like English, Chinese, and Japanese. We propose a soft labeling\nsystem to capture gradational emotional intensities. Using the Whisper encoder\nand data augmentation methods inspired by contrastive learning, our method\nemphasizes the temporal dynamics of emotions. Our validation on four\nmultilingual datasets demonstrates notable zero-shot generalization. We publish\nour open source model weights and initial promising results after fine-tuning\non Hume-Prosody.\n","authors":["Mohamed Osman","Tamer Nadeem","Ghada Khoriba"],"pdf_url":"https://arxiv.org/pdf/2311.08607v1.pdf","comment":"Accepted as talk at NeurIPS ML for Audio workshop"},{"id":"http://arxiv.org/abs/2311.08605v1","updated":"2023-11-15T00:02:25Z","published":"2023-11-15T00:02:25Z","title":"Navigating the Ocean of Biases: Political Bias Attribution in Language\n  Models via Causal Structures","summary":"  The rapid advancement of Large Language Models (LLMs) has sparked intense\ndebate regarding their ability to perceive and interpret complex\nsocio-political landscapes. In this study, we undertake an exploration of\ndecision-making processes and inherent biases within LLMs, exemplified by\nChatGPT, specifically contextualizing our analysis within political debates. We\naim not to critique or validate LLMs' values, but rather to discern how they\ninterpret and adjudicate \"good arguments.\" By applying Activity Dependency\nNetworks (ADNs), we extract the LLMs' implicit criteria for such assessments\nand illustrate how normative values influence these perceptions. We discuss the\nconsequences of our findings for human-AI alignment and bias mitigation. Our\ncode and data at https://github.com/david-jenny/LLM-Political-Study.\n","authors":["David F. Jenny","Yann Billeter","Mrinmaya Sachan","Bernhard Schölkopf","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2311.08605v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2311.09221v1","updated":"2023-11-15T18:59:56Z","published":"2023-11-15T18:59:56Z","title":"Single-Image 3D Human Digitization with Shape-Guided Diffusion","summary":"  We present an approach to generate a 360-degree view of a person with a\nconsistent, high-resolution appearance from a single input image. NeRF and its\nvariants typically require videos or images from different viewpoints. Most\nexisting approaches taking monocular input either rely on ground-truth 3D scans\nfor supervision or lack 3D consistency. While recent 3D generative models show\npromise of 3D consistent human digitization, these approaches do not generalize\nwell to diverse clothing appearances, and the results lack photorealism. Unlike\nexisting work, we utilize high-capacity 2D diffusion models pretrained for\ngeneral image synthesis tasks as an appearance prior of clothed humans. To\nachieve better 3D consistency while retaining the input identity, we\nprogressively synthesize multiple views of the human in the input image by\ninpainting missing regions with shape-guided diffusion conditioned on\nsilhouette and surface normal. We then fuse these synthesized multi-view images\nvia inverse rendering to obtain a fully textured high-resolution 3D mesh of the\ngiven person. Experiments show that our approach outperforms prior methods and\nachieves photorealistic 360-degree synthesis of a wide range of clothed humans\nwith complex textures from a single image.\n","authors":["Badour AlBahar","Shunsuke Saito","Hung-Yu Tseng","Changil Kim","Johannes Kopf","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2311.09221v1.pdf","comment":"SIGGRAPH Asia 2023. Project website: https://human-sgd.github.io/"},{"id":"http://arxiv.org/abs/2311.09217v1","updated":"2023-11-15T18:58:41Z","published":"2023-11-15T18:58:41Z","title":"DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction\n  Model","summary":"  We propose \\textbf{DMV3D}, a novel 3D generation approach that uses a\ntransformer-based 3D large reconstruction model to denoise multi-view\ndiffusion. Our reconstruction model incorporates a triplane NeRF representation\nand can denoise noisy multi-view images via NeRF reconstruction and rendering,\nachieving single-stage 3D generation in $\\sim$30s on single A100 GPU. We train\n\\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse\nobjects using only image reconstruction losses, without accessing 3D assets. We\ndemonstrate state-of-the-art results for the single-image reconstruction\nproblem where probabilistic modeling of unseen object parts is required for\ngenerating diverse reconstructions with sharp textures. We also show\nhigh-quality text-to-3D generation results outperforming previous 3D diffusion\nmodels. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .\n","authors":["Yinghao Xu","Hao Tan","Fujun Luan","Sai Bi","Peng Wang","Jiahao Li","Zifan Shi","Kalyan Sunkavalli","Gordon Wetzstein","Zexiang Xu","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09217v1.pdf","comment":"Project Page: https://justimyhxu.github.io/projects/dmv3d/"},{"id":"http://arxiv.org/abs/2311.09215v1","updated":"2023-11-15T18:56:51Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09193v1","updated":"2023-11-15T18:39:21Z","published":"2023-11-15T18:39:21Z","title":"The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task","summary":"  The study explores the effectiveness of the Chain-of-Thought approach, known\nfor its proficiency in language tasks by breaking them down into sub-tasks and\nintermediate steps, in improving vision-language tasks that demand\nsophisticated perception and reasoning. We present the \"Description then\nDecision\" strategy, which is inspired by how humans process signals. This\nstrategy significantly improves probing task performance by 50%, establishing\nthe groundwork for future research on reasoning paradigms in complex\nvision-language tasks.\n","authors":["Yifan Wu","Pengchuan Zhang","Wenhan Xiong","Barlas Oguz","James C. Gee","Yixin Nie"],"pdf_url":"https://arxiv.org/pdf/2311.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09191v1","updated":"2023-11-15T18:34:26Z","published":"2023-11-15T18:34:26Z","title":"Domain Aligned CLIP for Few-shot Classification","summary":"  Large vision-language representation learning models like CLIP have\ndemonstrated impressive performance for zero-shot transfer to downstream tasks\nwhile largely benefiting from inter-modal (image-text) alignment via\ncontrastive objectives. This downstream performance can further be enhanced by\nfull-scale fine-tuning which is often compute intensive, requires large\nlabelled data, and can reduce out-of-distribution (OOD) robustness.\nFurthermore, sole reliance on inter-modal alignment might overlook the rich\ninformation embedded within each individual modality. In this work, we\nintroduce a sample-efficient domain adaptation strategy for CLIP, termed Domain\nAligned CLIP (DAC), which improves both intra-modal (image-image) and\ninter-modal alignment on target distributions without fine-tuning the main\nmodel. For intra-modal alignment, we introduce a lightweight adapter that is\nspecifically trained with an intra-modal contrastive objective. To improve\ninter-modal alignment, we introduce a simple framework to modulate the\nprecomputed class text embeddings. The proposed few-shot fine-tuning framework\nis computationally efficient, robust to distribution shifts, and does not alter\nCLIP's parameters. We study the effectiveness of DAC by benchmarking on 11\nwidely used image classification tasks with consistent improvements in 16-shot\nclassification upon strong baselines by about 2.3% and demonstrate competitive\nperformance on 4 OOD robustness benchmarks.\n","authors":["Muhammad Waleed Gondal","Jochen Gast","Inigo Alonso Ruiz","Richard Droste","Tommaso Macri","Suren Kumar","Luitpold Staudigl"],"pdf_url":"https://arxiv.org/pdf/2311.09191v1.pdf","comment":"To appear at WACV 2024"},{"id":"http://arxiv.org/abs/2311.09190v1","updated":"2023-11-15T18:34:03Z","published":"2023-11-15T18:34:03Z","title":"On the Computation of the Gaussian Rate-Distortion-Perception Function","summary":"  In this paper, we study the computation of the rate-distortion-perception\nfunction (RDPF) for a multivariate Gaussian source under mean squared error\n(MSE) distortion and, respectively, Kullback-Leibler divergence, geometric\nJensen-Shannon divergence, squared Hellinger distance, and squared\nWasserstein-2 distance perception metrics. To this end, we first characterize\nthe analytical bounds of the scalar Gaussian RDPF for the aforementioned\ndivergence functions, also providing the RDPF-achieving forward \"test-channel\"\nrealization. Focusing on the multivariate case, we establish that, for\ntensorizable distortion and perception metrics, the optimal solution resides on\nthe vector space spanned by the eigenvector of the source covariance matrix.\nConsequently, the multivariate optimization problem can be expressed as a\nfunction of the scalar Gaussian RDPFs of the source marginals, constrained by\nglobal distortion and perception levels. Leveraging this characterization, we\ndesign an alternating minimization scheme based on the block nonlinear\nGauss-Seidel method, which optimally solves the problem while identifying the\nGaussian RDPF-achieving realization. Furthermore, the associated algorithmic\nembodiment is provided, as well as the convergence and the rate of convergence\ncharacterization. Lastly, for the \"perfect realism\" regime, the analytical\nsolution for the multivariate Gaussian RDPF is obtained. We corroborate our\nresults with numerical simulations and draw connections to existing results.\n","authors":["Giuseppe Serra","Photios A. Stavrou","Marios Kountouris"],"pdf_url":"https://arxiv.org/pdf/2311.09190v1.pdf","comment":"This paper has been submitted for journal publication"},{"id":"http://arxiv.org/abs/2311.09178v1","updated":"2023-11-15T18:15:30Z","published":"2023-11-15T18:15:30Z","title":"RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution","summary":"  Recently, video super resolution (VSR) has become a very impactful task in\nthe area of Computer Vision due to its various applications. In this paper, we\npropose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for\nVSR in an attempt to generate temporally coherent solutions while preserving\nspatial details. RBPGAN integrates two state-of-the-art models to get the best\nin both worlds without compromising the accuracy of produced video. The\ngenerator of the model is inspired by RBPN system, while the discriminator is\ninspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal\nconsistency over time. Our contribution together results in a model that\noutperforms earlier work in terms of temporally consistent details, as we will\ndemonstrate qualitatively and quantitatively using different datasets.\n","authors":["Dareen Hussein","Hesham Eraqi","Israa Fahmy","Marwah Sulaiman","Mohammed Barakat","Mohammed El-Naggar","Moustafa Youssef","Zahraa Shehabeldin"],"pdf_url":"https://arxiv.org/pdf/2311.09178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10368v2","updated":"2023-11-15T18:08:57Z","published":"2022-12-20T15:49:56Z","title":"Masked Event Modeling: Self-Supervised Pretraining for Event Cameras","summary":"  Event cameras offer the capacity to asynchronously capture brightness changes\nwith low latency, high temporal resolution, and high dynamic range. Deploying\ndeep learning methods for classification or other tasks to these sensors\ntypically requires large labeled datasets. Since the amount of labeled event\ndata is tiny compared to the bulk of labeled RGB imagery, the progress of\nevent-based vision has remained limited. To reduce the dependency on labeled\nevent data, we introduce Masked Event Modeling (MEM), a self-supervised\npretraining framework for events. Our method pretrains a neural network on\nunlabeled events, which can originate from any event camera recording.\nSubsequently, the pretrained model is finetuned on a downstream task leading to\nan overall better performance while requiring fewer labels. Our method\noutperforms the state-of-the-art on N-ImageNet, N-Cars, and N-Caltech101,\nincreasing the object classification accuracy on N-ImageNet by 7.96%. We\ndemonstrate that Masked Event Modeling is superior to RGB-based pretraining on\na real world dataset.\n","authors":["Simon Klenk","David Bonello","Lukas Koestler","Nikita Araslanov","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2212.10368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09118v1","updated":"2023-11-15T17:08:09Z","published":"2023-11-15T17:08:09Z","title":"WildlifeDatasets: An open-source toolkit for animal re-identification","summary":"  In this paper, we present WildlifeDatasets\n(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source\ntoolkit intended primarily for ecologists and computer-vision /\nmachine-learning researchers. The WildlifeDatasets is written in Python, allows\nstraightforward access to publicly available wildlife datasets, and provides a\nwide variety of methods for dataset pre-processing, performance analysis, and\nmodel fine-tuning. We showcase the toolkit in various scenarios and baseline\nexperiments, including, to the best of our knowledge, the most comprehensive\nexperimental comparison of datasets and methods for wildlife re-identification,\nincluding both local descriptors and deep learning approaches. Furthermore, we\nprovide the first-ever foundation model for individual re-identification within\na wide range of species - MegaDescriptor - that provides state-of-the-art\nperformance on animal re-identification datasets and outperforms other\npre-trained models such as CLIP and DINOv2 by a significant margin. To make the\nmodel available to the general public and to allow easy integration with any\nexisting wildlife monitoring applications, we provide multiple MegaDescriptor\nflavors (i.e., Small, Medium, and Large) through the HuggingFace hub\n(https://huggingface.co/BVRA).\n","authors":["Vojtěch Čermák","Lukas Picek","Lukáš Adam","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2311.09118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09104v1","updated":"2023-11-15T16:51:18Z","published":"2023-11-15T16:51:18Z","title":"Cross-view and Cross-pose Completion for 3D Human Understanding","summary":"  Human perception and understanding is a major domain of computer vision\nwhich, like many other vision subdomains recently, stands to gain from the use\nof large models pre-trained on large datasets. We hypothesize that the most\ncommon pre-training strategy of relying on general purpose, object-centric\nimage datasets such as ImageNet, is limited by an important domain shift. On\nthe other hand, collecting domain specific ground truth such as 2D or 3D labels\ndoes not scale well. Therefore, we propose a pre-training approach based on\nself-supervised learning that works on human-centric data using only images.\nOur method uses pairs of images of humans: the first is partially masked and\nthe model is trained to reconstruct the masked parts given the visible ones and\na second image. It relies on both stereoscopic (cross-view) pairs, and temporal\n(cross-pose) pairs taken from videos, in order to learn priors about 3D as well\nas human motion. We pre-train a model for body-centric tasks and one for\nhand-centric tasks. With a generic transformer architecture, these models\noutperform existing self-supervised pre-training methods on a wide set of\nhuman-centric downstream tasks, and obtain state-of-the-art performance for\ninstance when fine-tuning for model-based and model-free human mesh recovery.\n","authors":["Matthieu Armando","Salma Galaaoui","Fabien Baradel","Thomas Lucas","Vincent Leroy","Romain Brégier","Philippe Weinzaepfel","Grégory Rogez"],"pdf_url":"https://arxiv.org/pdf/2311.09104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09103v1","updated":"2023-11-15T16:50:01Z","published":"2023-11-15T16:50:01Z","title":"Guided Scale Space Radon Transform for linear structures detection","summary":"  Using integral transforms to the end of lines detection in images with\ncomplex background, makes the detection a hard task needing additional\nprocessing to manage the detection. As an integral transform, the Scale Space\nRadon Transform (SSRT) suffers from such drawbacks, even with its great\nabilities for thick lines detection. In this work, we propose a method to\naddress this issue for automatic detection of thick linear structures in gray\nscale and binary images using the SSRT, whatever the image background content.\nThis method involves the calculated Hessian orientations of the investigated\nimage while computing its SSRT, in such a way that linear structures are\nemphasized in the SSRT space. As a consequence, the subsequent maxima detection\nin the SSRT space is done on a modified transform space freed from unwanted\nparts and, consequently, from irrelevant peaks that usually drown the peaks\nrepresenting lines. Besides, highlighting the linear structure in the SSRT\nspace permitting, thus, to efficiently detect lines of different thickness in\nsynthetic and real images, the experiments show also the method robustness\nagainst noise and complex background.\n","authors":["Aicha Baya Goumeidane","Djemel Ziou","Nafaa Nacereddine"],"pdf_url":"https://arxiv.org/pdf/2311.09103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14516v2","updated":"2023-11-15T16:49:34Z","published":"2023-03-25T16:52:42Z","title":"OVeNet: Offset Vector Network for Semantic Segmentation","summary":"  Semantic segmentation is a fundamental task in visual scene understanding. We\nfocus on the supervised setting, where ground-truth semantic annotations are\navailable. Based on knowledge about the high regularity of real-world scenes,\nwe propose a method for improving class predictions by learning to selectively\nexploit information from neighboring pixels. In particular, our method is based\non the prior that for each pixel, there is a seed pixel in its close\nneighborhood sharing the same prediction with the former. Motivated by this\nprior, we design a novel two-head network, named Offset Vector Network\n(OVeNet), which generates both standard semantic predictions and a dense 2D\noffset vector field indicating the offset from each pixel to the respective\nseed pixel, which is used to compute an alternative, seed-based semantic\nprediction. The two predictions are adaptively fused at each pixel using a\nlearnt dense confidence map for the predicted offset vector field. We supervise\noffset vectors indirectly via optimizing the seed-based prediction and via a\nnovel loss on the confidence map. Compared to the baseline state-of-the-art\narchitectures HRNet and HRNet+OCR on which OVeNet is built, the latter achieves\nsignificant performance gains on three prominent benchmarks for semantic\nsegmentation, namely Cityscapes, ACDC and ADE20K. Code is available at\nhttps://github.com/stamatisalex/OVeNet\n","authors":["Stamatis Alexandropoulos","Christos Sakaridis","Petros Maragos"],"pdf_url":"https://arxiv.org/pdf/2303.14516v2.pdf","comment":"Accepted at WACV 2024"},{"id":"http://arxiv.org/abs/2311.09093v1","updated":"2023-11-15T16:41:18Z","published":"2023-11-15T16:41:18Z","title":"Applications of Computer Vision in Autonomous Vehicles: Methods,\n  Challenges and Future Directions","summary":"  Autonomous vehicle refers to a vehicle capable of perceiving its surrounding\nenvironment and driving with little or no human driver input. The perception\nsystem is a fundamental component which enables the autonomous vehicle to\ncollect data and extract relevant information from the environment to drive\nsafely. Benefit from the recent advances in computer vision, the perception\ntask can be achieved by using sensors, such as camera, LiDAR, radar, and\nultrasonic sensor. This paper reviews publications on computer vision and\nautonomous driving that are published during the last ten years. In particular,\nwe first investigate the development of autonomous driving systems and\nsummarize these systems that are developed by the major automotive\nmanufacturers from different countries. Second, we investigate the sensors and\nbenchmark data sets that are commonly utilized for autonomous driving. Then, a\ncomprehensive overview of computer vision applications for autonomous driving\nsuch as depth estimation, object detection, lane detection, and traffic sign\nrecognition are discussed. Additionally, we review public opinions and concerns\non autonomous vehicles. Based on the discussion, we analyze the current\ntechnological challenges that autonomous vehicles meet with. Finally, we\npresent our insights and point out some promising directions for future\nresearch. This paper will help the reader to understand autonomous vehicles\nfrom the perspectives of academia and industry.\n","authors":["Xingshuai Dong","Massimiliano L. Cappuccio"],"pdf_url":"https://arxiv.org/pdf/2311.09093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09084v1","updated":"2023-11-15T16:26:49Z","published":"2023-11-15T16:26:49Z","title":"Contrastive Transformer Learning with Proximity Data Generation for\n  Text-Based Person Search","summary":"  Given a descriptive text query, text-based person search (TBPS) aims to\nretrieve the best-matched target person from an image gallery. Such a\ncross-modal retrieval task is quite challenging due to significant modality\ngap, fine-grained differences and insufficiency of annotated data. To better\nalign the two modalities, most existing works focus on introducing\nsophisticated network structures and auxiliary tasks, which are complex and\nhard to implement. In this paper, we propose a simple yet effective dual\nTransformer model for text-based person search. By exploiting a hardness-aware\ncontrastive learning strategy, our model achieves state-of-the-art performance\nwithout any special design for local feature alignment or side information.\nMoreover, we propose a proximity data generation (PDG) module to automatically\nproduce more diverse data for cross-modal training. The PDG module first\nintroduces an automatic generation algorithm based on a text-to-image diffusion\nmodel, which generates new text-image pair samples in the proximity space of\noriginal ones. Then it combines approximate text generation and feature-level\nmixup during training to further strengthen the data diversity. The PDG module\ncan largely guarantee the reasonability of the generated samples that are\ndirectly used for training without any human inspection for noise rejection. It\nimproves the performance of our model significantly, providing a feasible\nsolution to the data insufficiency problem faced by such fine-grained\nvisual-linguistic tasks. Extensive experiments on two popular datasets of the\nTBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach\noutperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,\n4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be\navailable at https://github.com/HCPLab-SYSU/PersonSearch-CTLG\n","authors":["Hefeng Wu","Weifeng Chen","Zhibin Liu","Tianshui Chen","Zhiguang Chen","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2311.09084v1.pdf","comment":"Accepted by IEEE T-CSVT"},{"id":"http://arxiv.org/abs/2311.09077v1","updated":"2023-11-15T16:19:13Z","published":"2023-11-15T16:19:13Z","title":"Spiking NeRF: Representing the Real-World Geometry by a Discontinuous\n  Representation","summary":"  A crucial reason for the success of existing NeRF-based methods is to build a\nneural density field for the geometry representation via multiple perceptron\nlayers (MLPs). MLPs are continuous functions, however, real geometry or density\nfield is frequently discontinuous at the interface between the air and the\nsurface. Such a contrary brings the problem of unfaithful geometry\nrepresentation. To this end, this paper proposes spiking NeRF, which leverages\nspiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural\nNetwork (SNN) framework to build a discontinuous density field for faithful\ngeometry representation. Specifically, we first demonstrate the reason why\ncontinuous density fields will bring inaccuracy. Then, we propose to use the\nspiking neurons to build a discontinuous density field. We conduct\ncomprehensive analysis for the problem of existing spiking neuron models and\nthen provide the numerical relationship between the parameter of spiking neuron\nand the theoretical accuracy of geometry, Based on this, we propose a bounded\nspiking neuron to build the discontinuous density field. Our results achieve\nSOTA performance. Our code and data will be released to the public.\n","authors":["Zhanfeng Liao","Qian Zheng","Yan Liu","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2311.09077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14119v2","updated":"2023-11-15T16:15:19Z","published":"2023-08-27T14:25:07Z","title":"Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario","summary":"  Semi-Supervised Learning (SSL) is a framework that utilizes both labeled and\nunlabeled data to enhance model performance. Conventional SSL methods operate\nunder the assumption that labeled and unlabeled data share the same label\nspace. However, in practical real-world scenarios, especially when the labeled\ntraining dataset is limited in size, some classes may be totally absent from\nthe labeled set. To address this broader context, we propose a general approach\nto augment existing SSL methods, enabling them to effectively handle situations\nwhere certain classes are missing. This is achieved by introducing an\nadditional term into their objective function, which penalizes the\nKL-divergence between the probability vectors of the true class frequencies and\nthe inferred class frequencies. Our experimental results reveal significant\nimprovements in accuracy when compared to state-of-the-art SSL, open-set SSL,\nand open-world SSL methods. We conducted these experiments on two benchmark\nimage classification datasets, CIFAR-100 and STL-10, with the most remarkable\nimprovements observed when the labeled data is severely limited, with only a\nfew labeled examples per class\n","authors":["Noam Fluss","Guy Hacohen","Daphna Weinshall"],"pdf_url":"https://arxiv.org/pdf/2308.14119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09064v1","updated":"2023-11-15T16:02:13Z","published":"2023-11-15T16:02:13Z","title":"Imagine the Unseen World: A Benchmark for Systematic Generalization in\n  Visual World Models","summary":"  Systematic compositionality, or the ability to adapt to novel situations by\ncreating a mental model of the world using reusable pieces of knowledge,\nremains a significant challenge in machine learning. While there has been\nconsiderable progress in the language domain, efforts towards systematic visual\nimagination, or envisioning the dynamical implications of a visual observation,\nare in their infancy. We introduce the Systematic Visual Imagination Benchmark\n(SVIB), the first benchmark designed to address this problem head-on. SVIB\noffers a novel framework for a minimal world modeling problem, where models are\nevaluated based on their ability to generate one-step image-to-image\ntransformations under a latent world dynamics. The framework provides benefits\nsuch as the possibility to jointly optimize for systematic perception and\nimagination, a range of difficulty levels, and the ability to control the\nfraction of possible factor combinations used during training. We provide a\ncomprehensive evaluation of various baseline models on SVIB, offering insight\ninto the current state-of-the-art in systematic visual imagination. We hope\nthat this benchmark will help advance visual systematic compositionality.\n","authors":["Yeongbin Kim","Gautam Singh","Junyeong Park","Caglar Gulcehre","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2311.09064v1.pdf","comment":"Published as a conference paper at NeurIPS 2023. The first two\n  authors contributed equally. To download the benchmark, visit\n  https://systematic-visual-imagination.github.io"},{"id":"http://arxiv.org/abs/2311.09050v1","updated":"2023-11-15T15:40:46Z","published":"2023-11-15T15:40:46Z","title":"Improving Zero-shot Visual Question Answering via Large Language Models\n  with Reasoning Question Prompts","summary":"  Zero-shot Visual Question Answering (VQA) is a prominent vision-language task\nthat examines both the visual and textual understanding capability of systems\nin the absence of training data. Recently, by converting the images into\ncaptions, information across multi-modalities is bridged and Large Language\nModels (LLMs) can apply their strong zero-shot generalization capability to\nunseen questions. To design ideal prompts for solving VQA via LLMs, several\nstudies have explored different strategies to select or generate\nquestion-answer pairs as the exemplar prompts, which guide LLMs to answer the\ncurrent questions effectively. However, they totally ignore the role of\nquestion prompts. The original questions in VQA tasks usually encounter\nellipses and ambiguity which require intermediate reasoning. To this end, we\npresent Reasoning Question Prompts for VQA tasks, which can further activate\nthe potential of LLMs in zero-shot scenarios. Specifically, for each question,\nwe first generate self-contained questions as reasoning question prompts via an\nunsupervised question edition module considering sentence fluency, semantic\nintegrity and syntactic invariance. Each reasoning question prompt clearly\nindicates the intent of the original question. This results in a set of\ncandidate answers. Then, the candidate answers associated with their confidence\nscores acting as answer heuristics are fed into LLMs and produce the final\nanswer. We evaluate reasoning question prompts on three VQA challenges,\nexperimental results demonstrate that they can significantly improve the\nresults of LLMs on zero-shot setting and outperform existing state-of-the-art\nzero-shot methods on three out of four data sets. Our source code is publicly\nreleased at \\url{https://github.com/ECNU-DASE-NLP/RQP}.\n","authors":["Yunshi Lan","Xiang Li","Xin Liu","Yang Li","Wei Qin","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2311.09050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09029v1","updated":"2023-11-15T15:20:24Z","published":"2023-11-15T15:20:24Z","title":"Self-Annotated 3D Geometric Learning for Smeared Points Removal","summary":"  There has been significant progress in improving the accuracy and quality of\nconsumer-level dense depth sensors. Nevertheless, there remains a common depth\npixel artifact which we call smeared points. These are points not on any 3D\nsurface and typically occur as interpolations between foreground and background\nobjects. As they cause fictitious surfaces, these points have the potential to\nharm applications dependent on the depth maps. Statistical outlier removal\nmethods fare poorly in removing these points as they tend also to remove actual\nsurface points. Trained network-based point removal faces difficulty in\nobtaining sufficient annotated data. To address this, we propose a fully\nself-annotated method to train a smeared point removal classifier. Our approach\nrelies on gathering 3D geometric evidence from multiple perspectives to\nautomatically detect and annotate smeared points and valid points. To validate\nthe effectiveness of our method, we present a new benchmark dataset: the Real\nAzure-Kinect dataset. Experimental results and ablation studies show that our\nmethod outperforms traditional filters and other self-annotated methods. Our\nwork is publicly available at\nhttps://github.com/wangmiaowei/wacv2024_smearedremover.git.\n","authors":["Miaowei Wang","Daniel Morris"],"pdf_url":"https://arxiv.org/pdf/2311.09029v1.pdf","comment":"The paper is accepted at WACV2024(https://wacv2024.thecvf.com/)"},{"id":"http://arxiv.org/abs/2208.07853v2","updated":"2023-11-15T15:18:11Z","published":"2022-08-16T17:21:00Z","title":"Estimating Appearance Models for Image Segmentation via Tensor\n  Factorization","summary":"  Image Segmentation is one of the core tasks in Computer Vision and solving it\noften depends on modeling the image appearance data via the color distributions\nof each it its constituent regions. Whereas many segmentation algorithms handle\nthe appearance models dependence using alternation or implicit methods, we\npropose here a new approach to directly estimate them from the image without\nprior information on the underlying segmentation. Our method uses local high\norder color statistics from the image as an input to tensor factorization-based\nestimator for latent variable models. This approach is able to estimate models\nin multiregion images and automatically output the regions proportions without\nprior user interaction, overcoming the drawbacks from a prior attempt to this\nproblem. We also demonstrate the performance of our proposed method in many\nchallenging synthetic and real imaging scenarios and show that it leads to an\nefficient segmentation algorithm.\n","authors":["Jeova Farias Sales Rocha Neto"],"pdf_url":"https://arxiv.org/pdf/2208.07853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09024v1","updated":"2023-11-15T15:14:16Z","published":"2023-11-15T15:14:16Z","title":"Fast Certification of Vision-Language Models Using Incremental\n  Randomized Smoothing","summary":"  A key benefit of deep vision-language models such as CLIP is that they enable\nzero-shot open vocabulary classification; the user has the ability to define\nnovel class labels via natural language prompts at inference time. However,\nwhile CLIP-based zero-shot classifiers have demonstrated competitive\nperformance across a range of domain shifts, they remain highly vulnerable to\nadversarial attacks. Therefore, ensuring the robustness of such models is\ncrucial for their reliable deployment in the wild.\n  In this work, we introduce Open Vocabulary Certification (OVC), a fast\ncertification method designed for open-vocabulary models like CLIP via\nrandomized smoothing techniques. Given a base \"training\" set of prompts and\ntheir corresponding certified CLIP classifiers, OVC relies on the observation\nthat a classifier with a novel prompt can be viewed as a perturbed version of\nnearby classifiers in the base training set. Therefore, OVC can rapidly certify\nthe novel classifier using a variation of incremental randomized smoothing. By\nusing a caching trick, we achieve approximately two orders of magnitude\nacceleration in the certification process for novel prompts. To achieve further\n(heuristic) speedups, OVC approximates the embedding space at a given input\nusing a multivariate normal distribution bypassing the need for sampling via\nforward passes through the vision backbone. We demonstrate the effectiveness of\nOVC on through experimental evaluation using multiple vision-language backbones\non the CIFAR-10 and ImageNet test datasets.\n","authors":["A K Nirala","A Joshi","C Hegde","S Sarkar"],"pdf_url":"https://arxiv.org/pdf/2311.09024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09004v1","updated":"2023-11-15T14:46:20Z","published":"2023-11-15T14:46:20Z","title":"Incremental Object-Based Novelty Detection with Feedback Loop","summary":"  Object-based Novelty Detection (ND) aims to identify unknown objects that do\nnot belong to classes seen during training by an object detection model. The\ntask is particularly crucial in real-world applications, as it allows to avoid\npotentially harmful behaviours, e.g. as in the case of object detection models\nadopted in a self-driving car or in an autonomous robot. Traditional approaches\nto ND focus on one time offline post processing of the pretrained object\ndetection output, leaving no possibility to improve the model robustness after\ntraining and discarding the abundant amount of out-of-distribution data\nencountered during deployment.\n  In this work, we propose a novel framework for object-based ND, assuming that\nhuman feedback can be requested on the predicted output and later incorporated\nto refine the ND model without negatively affecting the main object detection\nperformance. This refinement operation is repeated whenever new feedback is\navailable. To tackle this new formulation of the problem for object detection,\nwe propose a lightweight ND module attached on top of a pre-trained object\ndetection model, which is incrementally updated through a feedback loop. We\nalso propose a new benchmark to evaluate methods on this new setting and test\nextensively our ND approach against baselines, showing increased robustness and\na successful incorporation of the received feedback.\n","authors":["Simone Caldarella","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2311.09004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03217v2","updated":"2023-11-15T14:37:24Z","published":"2023-11-06T16:01:42Z","title":"Leveraging Transformers to Improve Breast Cancer Classification and Risk\n  Assessment with Multi-modal and Longitudinal Data","summary":"  Breast cancer screening, primarily conducted through mammography, is often\nsupplemented with ultrasound for women with dense breast tissue. However,\nexisting deep learning models analyze each modality independently, missing\nopportunities to integrate information across imaging modalities and time. In\nthis study, we present Multi-modal Transformer (MMT), a neural network that\nutilizes mammography and ultrasound synergistically, to identify patients who\ncurrently have cancer and estimate the risk of future cancer for patients who\nare currently cancer-free. MMT aggregates multi-modal data through\nself-attention and tracks temporal tissue changes by comparing current exams to\nprior imaging. Trained on 1.3 million exams, MMT achieves an AUROC of 0.943 in\ndetecting existing cancers, surpassing strong uni-modal baselines. For 5-year\nrisk prediction, MMT attains an AUROC of 0.826, outperforming prior\nmammography-based risk models. Our research highlights the value of multi-modal\nand longitudinal imaging in cancer diagnosis and risk stratification.\n","authors":["Yiqiu Shen","Jungkyu Park","Frank Yeung","Eliana Goldberg","Laura Heacock","Farah Shamout","Krzysztof J. Geras"],"pdf_url":"https://arxiv.org/pdf/2311.03217v2.pdf","comment":"ML4H 2023 Findings Track"},{"id":"http://arxiv.org/abs/2311.05836v2","updated":"2023-11-15T14:37:16Z","published":"2023-11-10T02:47:15Z","title":"Uncertainty-aware Single View Volumetric Rendering for Medical Neural\n  Radiance Fields","summary":"  In the field of clinical medicine, computed tomography (CT) is an effective\nmedical imaging modality for the diagnosis of various pathologies. Compared\nwith X-ray images, CT images can provide more information, including\nmulti-planar slices and three-dimensional structures for clinical diagnosis.\nHowever, CT imaging requires patients to be exposed to large doses of ionizing\nradiation for a long time, which may cause irreversible physical harm. In this\npaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on\ngenerated radiation fields. The network can learn a continuous representation\nof CT projections from 2D X-ray images by obtaining the internal structure and\ndepth information and using adaptive loss weights to ensure the quality of the\ngenerated images. Our model is trained on publicly available knee and chest\ndatasets, and we show the results of CT projection rendering with a single\nX-ray and compare our method with other methods based on generated radiation\nfields.\n","authors":["Jing Hu","Qinrui Fan","Shu Hu","Siwei Lyu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08995v1","updated":"2023-11-15T14:33:22Z","published":"2023-11-15T14:33:22Z","title":"Simple but Effective Unsupervised Classification for Specified Domain\n  Images: A Case Study on Fungi Images","summary":"  High-quality labeled datasets are essential for deep learning. Traditional\nmanual annotation methods are not only costly and inefficient but also pose\nchallenges in specialized domains where expert knowledge is needed.\nSelf-supervised methods, despite leveraging unlabeled data for feature\nextraction, still require hundreds or thousands of labeled instances to guide\nthe model for effective specialized image classification. Current unsupervised\nlearning methods offer automatic classification without prior annotation but\noften compromise on accuracy. As a result, efficiently procuring high-quality\nlabeled datasets remains a pressing challenge for specialized domain images\ndevoid of annotated data. Addressing this, an unsupervised classification\nmethod with three key ideas is introduced: 1) dual-step feature dimensionality\nreduction using a pre-trained model and manifold learning, 2) a voting\nmechanism from multiple clustering algorithms, and 3) post-hoc instead of prior\nmanual annotation. This approach outperforms supervised methods in\nclassification accuracy, as demonstrated with fungal image data, achieving\n94.1% and 96.7% on public and private datasets respectively. The proposed\nunsupervised classification method reduces dependency on pre-annotated\ndatasets, enabling a closed-loop for data classification. The simplicity and\nease of use of this method will also bring convenience to researchers in\nvarious fields in building datasets, promoting AI applications for images in\nspecialized domains.\n","authors":["Zhaocong liu","Fa Zhang","Lin Cheng","Huanxi Deng","Xiaoyan Yang","Zhenyu Zhang","Chichun Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08972v1","updated":"2023-11-15T14:04:37Z","published":"2023-11-15T14:04:37Z","title":"Unsupervised approaches based on optimal transport and convex analysis\n  for inverse problems in imaging","summary":"  Unsupervised deep learning approaches have recently become one of the crucial\nresearch areas in imaging owing to their ability to learn expressive and\npowerful reconstruction operators even when paired high-quality training data\nis scarcely available. In this chapter, we review theoretically principled\nunsupervised learning schemes for solving imaging inverse problems, with a\nparticular focus on methods rooted in optimal transport and convex analysis. We\nbegin by reviewing the optimal transport-based unsupervised approaches such as\nthe cycle-consistency-based models and learned adversarial regularization\nmethods, which have clear probabilistic interpretations. Subsequently, we give\nan overview of a recent line of works on provably convergent learned\noptimization algorithms applied to accelerate the solution of imaging inverse\nproblems, alongside their dedicated unsupervised training schemes. We also\nsurvey a number of provably convergent plug-and-play algorithms (based on\ngradient-step deep denoisers), which are among the most important and widely\napplied unsupervised approaches for imaging problems. At the end of this\nsurvey, we provide an overview of a few related unsupervised learning\nframeworks that complement our focused schemes. Together with a detailed\nsurvey, we provide an overview of the key mathematical results that underlie\nthe methods reviewed in the chapter to keep our discussion self-contained.\n","authors":["Marcello Carioni","Subhadip Mukherjee","Hong Ye Tan","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2311.08972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14136v3","updated":"2023-11-15T13:47:49Z","published":"2023-09-25T13:45:28Z","title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers","summary":"  Deeper Vision Transformers (ViTs) are more challenging to train. We expose a\ndegradation problem in deeper layers of ViT when using masked image modeling\n(MIM) for pre-training. To ease the training of deeper ViTs, we introduce a\nself-supervised learning framework called Masked Image Residual Learning\n(MIRL), which significantly alleviates the degradation problem, making scaling\nViT along depth a promising direction for performance upgrade. We reformulate\nthe pre-training objective for deeper layers of ViT as learning to recover the\nresidual of the masked image. We provide extensive empirical evidence showing\nthat deeper ViTs can be effectively optimized using MIRL and easily gain\naccuracy from increased depth. With the same level of computational complexity\nas ViT-Base and ViT-Large, we instantiate 4.5$\\times$ and 2$\\times$ deeper\nViTs, dubbed ViT-S-54 and ViT-B-48. The deeper ViT-S-54, costing 3$\\times$ less\nthan ViT-Large, achieves performance on par with ViT-Large. ViT-B-48 achieves\n86.2% top-1 accuracy on ImageNet. On one hand, deeper ViTs pre-trained with\nMIRL exhibit excellent generalization capabilities on downstream tasks, such as\nobject detection and semantic segmentation. On the other hand, MIRL\ndemonstrates high pre-training efficiency. With less pre-training time, MIRL\nyields competitive performance compared to other approaches.\n","authors":["Guoxi Huang","Hongtao Fu","Adrian G. Bors"],"pdf_url":"https://arxiv.org/pdf/2309.14136v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07944v2","updated":"2023-11-15T13:46:36Z","published":"2023-09-14T09:03:52Z","title":"Text-to-Image Models for Counterfactual Explanations: a Black-Box\n  Approach","summary":"  This paper addresses the challenge of generating Counterfactual Explanations\n(CEs), involving the identification and modification of the fewest necessary\nfeatures to alter a classifier's prediction for a given image. Our proposed\nmethod, Text-to-Image Models for Counterfactual Explanations (TIME), is a\nblack-box counterfactual technique based on distillation. Unlike previous\nmethods, this approach requires solely the image and its prediction, omitting\nthe need for the classifier's structure, parameters, or gradients. Before\ngenerating the counterfactuals, TIME introduces two distinct biases into Stable\nDiffusion in the form of textual embeddings: the context bias, associated with\nthe image's structure, and the class bias, linked to class-specific features\nlearned by the target classifier. After learning these biases, we find the\noptimal latent code applying the classifier's predicted class token and\nregenerate the image using the target embedding as conditioning, producing the\ncounterfactual explanation. Extensive empirical studies validate that TIME can\ngenerate explanations of comparable effectiveness even when operating within a\nblack-box setting.\n","authors":["Guillaume Jeanneret","Loïc Simon","Frédéric Jurie"],"pdf_url":"https://arxiv.org/pdf/2309.07944v2.pdf","comment":"WACV 2024 Camera ready + supplementary material"},{"id":"http://arxiv.org/abs/2211.06891v3","updated":"2023-11-15T13:41:43Z","published":"2022-11-13T12:31:49Z","title":"Residual Degradation Learning Unfolding Framework with Mixing Priors\n  across Spectral and Spatial for Compressive Spectral Imaging","summary":"  To acquire a snapshot spectral image, coded aperture snapshot spectral\nimaging (CASSI) is proposed. A core problem of the CASSI system is to recover\nthe reliable and fine underlying 3D spectral cube from the 2D measurement. By\nalternately solving a data subproblem and a prior subproblem, deep unfolding\nmethods achieve good performance. However, in the data subproblem, the used\nsensing matrix is ill-suited for the real degradation process due to the device\nerrors caused by phase aberration, distortion; in the prior subproblem, it is\nimportant to design a suitable model to jointly exploit both spatial and\nspectral priors. In this paper, we propose a Residual Degradation Learning\nUnfolding Framework (RDLUF), which bridges the gap between the sensing matrix\nand the degradation process. Moreover, a Mix$S^2$ Transformer is designed via\nmixing priors across spectral and spatial to strengthen the spectral-spatial\nrepresentation capability. Finally, plugging the Mix$S^2$ Transformer into the\nRDLUF leads to an end-to-end trainable neural network RDLUF-Mix$S^2$.\nExperimental results establish the superior performance of the proposed method\nover existing ones.\n","authors":["Yubo Dong","Dahua Gao","Tian Qiu","Yuyan Li","Minxi Yang","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2211.06891v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2311.08955v1","updated":"2023-11-15T13:40:58Z","published":"2023-11-15T13:40:58Z","title":"A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution","summary":"  Fusion-based hyperspectral image (HSI) super-resolution aims to produce a\nhigh-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a\nhigh-spatial-resolution multispectral image. Such a HSI super-resolution\nprocess can be modeled as an inverse problem, where the prior knowledge is\nessential for obtaining the desired solution. Motivated by the success of\ndiffusion models, we propose a novel spectral diffusion prior for fusion-based\nHSI super-resolution. Specifically, we first investigate the spectrum\ngeneration problem and design a spectral diffusion model to model the spectral\ndata distribution. Then, in the framework of maximum a posteriori, we keep the\ntransition information between every two neighboring states during the reverse\ngenerative process, and thereby embed the knowledge of trained spectral\ndiffusion model into the fusion problem in the form of a regularization term.\nAt last, we treat each generation step of the final optimization problem as its\nsubproblem, and employ the Adam to solve these subproblems in a reverse\nsequence. Experimental results conducted on both synthetic and real datasets\ndemonstrate the effectiveness of the proposed approach. The code of the\nproposed approach will be available on https://github.com/liuofficial/SDP.\n","authors":["Jianjun Liu","Zebin Wu","Liang Xiao"],"pdf_url":"https://arxiv.org/pdf/2311.08955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08949v1","updated":"2023-11-15T13:35:40Z","published":"2023-11-15T13:35:40Z","title":"Automated Volume Corrected Mitotic Index Calculation Through\n  Annotation-Free Deep Learning using Immunohistochemistry as Reference\n  Standard","summary":"  The volume-corrected mitotic index (M/V-Index) was shown to provide\nprognostic value in invasive breast carcinomas. However, despite its prognostic\nsignificance, it is not established as the standard method for assessing\naggressive biological behaviour, due to the high additional workload associated\nwith determining the epithelial proportion. In this work, we show that using a\ndeep learning pipeline solely trained with an annotation-free,\nimmunohistochemistry-based approach, provides accurate estimations of\nepithelial segmentation in canine breast carcinomas. We compare our automatic\nframework with the manually annotated M/V-Index in a study with three\nboard-certified pathologists. Our results indicate that the deep learning-based\npipeline shows expert-level performance, while providing time efficiency and\nreproducibility.\n","authors":["Jonas Ammeling","Moritz Hecker","Jonathan Ganz","Taryn A. Donovan","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2311.08949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02998v2","updated":"2023-11-15T13:30:39Z","published":"2022-10-06T15:38:02Z","title":"ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical\n  Prior Probability Maps for Thoracic Disease Classification","summary":"  Objective: Computer-aided disease diagnosis and prognosis based on medical\nimages is a rapidly emerging field. Many Convolutional Neural Network (CNN)\narchitectures have been developed by researchers for disease classification and\nlocalization from chest X-ray images. It is known that different thoracic\ndisease lesions are more likely to occur in specific anatomical regions\ncompared to others. This article aims to incorporate this disease and\nregion-dependent prior probability distribution within a deep learning\nframework. Methods: We present the ThoraX-PriorNet, a novel attention-based CNN\nmodel for thoracic disease classification. We first estimate a\ndisease-dependent spatial probability, i.e., an anatomical prior, that\nindicates the probability of occurrence of a disease in a specific region in a\nchest X-ray image. Next, we develop a novel attention-based classification\nmodel that combines information from the estimated anatomical prior and\nautomatically extracted chest region of interest (ROI) masks to provide\nattention to the feature maps generated from a deep convolution network. Unlike\nprevious works that utilize various self-attention mechanisms, the proposed\nmethod leverages the extracted chest ROI masks along with the probabilistic\nanatomical prior information, which selects the region of interest for\ndifferent diseases to provide attention. Results: The proposed method shows\nsuperior performance in disease classification on the NIH ChestX-ray14 dataset\ncompared to existing state-of-the-art methods while reaching an area under the\nROC curve (%AUC) of 84.67. Regarding disease localization, the anatomy prior\nattention method shows competitive performance compared to state-of-the-art\nmethods, achieving an accuracy of 0.80, 0.63, 0.49, 0.33, 0.28, 0.21, and 0.04\nwith an Intersection over Union (IoU) threshold of 0.1, 0.2, 0.3, 0.4, 0.5,\n0.6, and 0.7, respectively.\n","authors":["Md. Iqbal Hossain","Mohammad Zunaed","Md. Kawsar Ahmed","S. M. Jawwad Hossain","Anwarul Hasan","Taufiq Hasan"],"pdf_url":"https://arxiv.org/pdf/2210.02998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08936v1","updated":"2023-11-15T13:19:02Z","published":"2023-11-15T13:19:02Z","title":"Confident Naturalness Explanation (CNE): A Framework to Explain and\n  Assess Patterns Forming Naturalness","summary":"  Protected natural areas are regions that have been minimally affected by\nhuman activities such as urbanization, agriculture, and other human\ninterventions. To better understand and map the naturalness of these areas,\nmachine learning models can be used to analyze satellite imagery. Specifically,\nexplainable machine learning methods show promise in uncovering patterns that\ncontribute to the concept of naturalness within these protected environments.\nAdditionally, addressing the uncertainty inherent in machine learning models is\ncrucial for a comprehensive understanding of this concept. However, existing\napproaches have limitations. They either fail to provide explanations that are\nboth valid and objective or struggle to offer a quantitative metric that\naccurately measures the contribution of specific patterns to naturalness, along\nwith the associated confidence. In this paper, we propose a novel framework\ncalled the Confident Naturalness Explanation (CNE) framework. This framework\ncombines explainable machine learning and uncertainty quantification to assess\nand explain naturalness. We introduce a new quantitative metric that describes\nthe confident contribution of patterns to the concept of naturalness.\nFurthermore, we generate an uncertainty-aware segmentation mask for each input\nsample, highlighting areas where the model lacks knowledge. To demonstrate the\neffectiveness of our framework, we apply it to a study site in Fennoscandia\nusing two open-source satellite datasets.\n","authors":["Ahmed Emam","Mohamed Farag","Ribana Roscher"],"pdf_url":"https://arxiv.org/pdf/2311.08936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08931v1","updated":"2023-11-15T13:04:57Z","published":"2023-11-15T13:04:57Z","title":"Structural-Based Uncertainty in Deep Learning Across Anatomical Scales:\n  Analysis in White Matter Lesion Segmentation","summary":"  This paper explores uncertainty quantification (UQ) as an indicator of the\ntrustworthiness of automated deep-learning (DL) tools in the context of white\nmatter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of\nmultiple sclerosis (MS) patients. Our study focuses on two principal aspects of\nuncertainty in structured output segmentation tasks. Firstly, we postulate that\na good uncertainty measure should indicate predictions likely to be incorrect\nwith high uncertainty values. Second, we investigate the merit of quantifying\nuncertainty at different anatomical scales (voxel, lesion, or patient). We\nhypothesize that uncertainty at each scale is related to specific types of\nerrors. Our study aims to confirm this relationship by conducting separate\nanalyses for in-domain and out-of-domain settings. Our primary methodological\ncontributions are (i) the development of novel measures for quantifying\nuncertainty at lesion and patient scales, derived from structural prediction\ndiscrepancies, and (ii) the extension of an error retention curve analysis\nframework to facilitate the evaluation of UQ performance at both lesion and\npatient scales. The results from a multi-centric MRI dataset of 172 patients\ndemonstrate that our proposed measures more effectively capture model errors at\nthe lesion and patient scales compared to measures that average voxel-scale\nuncertainty values. We provide the UQ protocols code at\nhttps://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.\n","authors":["Nataliia Molchanova","Vatsal Raina","Andrey Malinin","Francesco La Rosa","Adrien Depeursinge","Mark Gales","Cristina Granziera","Henning Muller","Mara Graziani","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2311.08931v1.pdf","comment":"Preprint submitted to the journal"},{"id":"http://arxiv.org/abs/2311.08923v1","updated":"2023-11-15T12:55:19Z","published":"2023-11-15T12:55:19Z","title":"Leveraging Activation Maximization and Generative Adversarial Training\n  to Recognize and Explain Patterns in Natural Areas in Satellite Imagery","summary":"  Natural protected areas are vital for biodiversity, climate change\nmitigation, and supporting ecological processes. Despite their significance,\ncomprehensive mapping is hindered by a lack of understanding of their\ncharacteristics and a missing land cover class definition. This paper aims to\nadvance the explanation of the designating patterns forming protected and wild\nareas. To this end, we propose a novel framework that uses activation\nmaximization and a generative adversarial model. With this, we aim to generate\nsatellite images that, in combination with domain knowledge, are capable of\noffering complete and valid explanations for the spatial and spectral patterns\nthat define the natural authenticity of these regions. Our proposed framework\nproduces more precise attribution maps pinpointing the designating patterns\nforming the natural authenticity of protected areas. Our approach fosters our\nunderstanding of the ecological integrity of the protected natural areas and\nmay contribute to future monitoring and preservation efforts.\n","authors":["Ahmed Emam","Timo T. Stomberg","Ribana Roscher"],"pdf_url":"https://arxiv.org/pdf/2311.08923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08910v1","updated":"2023-11-15T12:31:43Z","published":"2023-11-15T12:31:43Z","title":"Progressive Feedback-Enhanced Transformer for Image Forgery Localization","summary":"  Blind detection of the forged regions in digital images is an effective\nauthentication means to counter the malicious use of local image editing\ntechniques. Existing encoder-decoder forensic networks overlook the fact that\ndetecting complex and subtle tampered regions typically requires more feedback\ninformation. In this paper, we propose a Progressive FeedbACk-enhanced\nTransformer (ProFact) network to achieve coarse-to-fine image forgery\nlocalization. Specifically, the coarse localization map generated by an initial\nbranch network is adaptively fed back to the early transformer encoder layers\nfor enhancing the representation of positive features while suppressing\ninterference factors. The cascaded transformer network, combined with a\ncontextual spatial pyramid module, is designed to refine discriminative\nforensic features for improving the forgery localization accuracy and\nreliability. Furthermore, we present an effective strategy to automatically\ngenerate large-scale forged image samples close to real-world forensic\nscenarios, especially in realistic and coherent processing. Leveraging on such\nsamples, a progressive and cost-effective two-stage training protocol is\napplied to the ProFact network. The extensive experimental results on nine\npublic forensic datasets show that our proposed localizer greatly outperforms\nthe state-of-the-art on the generalization ability and robustness of image\nforgery localization. Code will be publicly available at\nhttps://github.com/multimediaFor/ProFact.\n","authors":["Haochen Zhu","Gang Cao","Xianglin Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08909v1","updated":"2023-11-15T12:26:31Z","published":"2023-11-15T12:26:31Z","title":"DLAS: An Exploration and Assessment of the Deep Learning Acceleration\n  Stack","summary":"  Deep Neural Networks (DNNs) are extremely computationally demanding, which\npresents a large barrier to their deployment on resource-constrained devices.\nSince such devices are where many emerging deep learning applications lie\n(e.g., drones, vision-based medical technology), significant bodies of work\nfrom both the machine learning and systems communities have attempted to\nprovide optimizations to accelerate DNNs. To help unify these two perspectives,\nin this paper we combine machine learning and systems techniques within the\nDeep Learning Acceleration Stack (DLAS), and demonstrate how these layers can\nbe tightly dependent on each other with an across-stack perturbation study. We\nevaluate the impact on accuracy and inference time when varying different\nparameters of DLAS across two datasets, seven popular DNN architectures, four\nDNN compression techniques, three algorithmic primitives with sparse and dense\nvariants, untuned and auto-scheduled code generation, and four hardware\nplatforms. Our evaluation highlights how perturbations across DLAS parameters\ncan cause significant variation and across-stack interactions. The highest\nlevel observation from our evaluation is that the model size, accuracy, and\ninference time are not guaranteed to be correlated. Overall we make 13 key\nobservations, including that speedups provided by compression techniques are\nvery hardware dependent, and that compiler auto-tuning can significantly alter\nwhat the best algorithm to use for a given configuration is. With DLAS, we aim\nto provide a reference framework to aid machine learning and systems\npractitioners in reasoning about the context in which their respective DNN\nacceleration solutions exist in. With our evaluation strongly motivating the\nneed for co-design, we believe that DLAS can be a valuable concept for\nexploring the next generation of co-designed accelerated deep learning\nsolutions.\n","authors":["Perry Gibson","José Cano","Elliot J. Crowley","Amos Storkey","Michael O'Boyle"],"pdf_url":"https://arxiv.org/pdf/2311.08909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08908v1","updated":"2023-11-15T12:26:24Z","published":"2023-11-15T12:26:24Z","title":"Robust Brain MRI Image Classification with SIBOW-SVM","summary":"  The majority of primary Central Nervous System (CNS) tumors in the brain are\namong the most aggressive diseases affecting humans. Early detection of brain\ntumor types, whether benign or malignant, glial or non-glial, is critical for\ncancer prevention and treatment, ultimately improving human life expectancy.\nMagnetic Resonance Imaging (MRI) stands as the most effective technique to\ndetect brain tumors by generating comprehensive brain images through scans.\nHowever, human examination can be error-prone and inefficient due to the\ncomplexity, size, and location variability of brain tumors. Recently, automated\nclassification techniques using machine learning (ML) methods, such as\nConvolutional Neural Network (CNN), have demonstrated significantly higher\naccuracy than manual screening, while maintaining low computational costs.\nNonetheless, deep learning-based image classification methods, including CNN,\nface challenges in estimating class probabilities without proper model\ncalibration. In this paper, we propose a novel brain tumor image classification\nmethod, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with\nSIFT feature extraction and weighted Support Vector Machines (wSVMs). This new\napproach effectively captures hidden image features, enabling the\ndifferentiation of various tumor types and accurate label predictions.\nAdditionally, the SIBOW-SVM is able to estimate the probabilities of images\nbelonging to each class, thereby providing high-confidence classification\ndecisions. We have also developed scalable and parallelable algorithms to\nfacilitate the practical implementation of SIBOW-SVM for massive images. As a\nbenchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI\nimages containing four classes: glioma, meningioma, pituitary, and normal. Our\nresults show that the new method outperforms state-of-the-art methods,\nincluding CNN.\n","authors":["Liyun Zeng","Hao Helen Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08891v1","updated":"2023-11-15T11:51:10Z","published":"2023-11-15T11:51:10Z","title":"AdapterShadow: Adapting Segment Anything Model for Shadow Detection","summary":"  Segment anything model (SAM) has shown its spectacular performance in\nsegmenting universal objects, especially when elaborate prompts are provided.\nHowever, the drawback of SAM is twofold. On the first hand, it fails to segment\nspecific targets, e.g., shadow images or lesions in medical images. On the\nother hand, manually specifying prompts is extremely time-consuming. To\novercome the problems, we propose AdapterShadow, which adapts SAM model for\nshadow detection. To adapt SAM for shadow images, trainable adapters are\ninserted into the frozen image encoder of SAM, since the training of the full\nSAM model is both time and memory consuming. Moreover, we introduce a novel\ngrid sampling method to generate dense point prompts, which helps to\nautomatically segment shadows without any manual interventions. Extensive\nexperiments are conducted on four widely used benchmark datasets to demonstrate\nthe superior performance of our proposed method. Codes will are publicly\navailable at https://github.com/LeipingJie/AdapterShadow.\n","authors":["Leiping Jie","Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12554v3","updated":"2023-11-15T11:23:32Z","published":"2023-01-29T22:05:28Z","title":"Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive\n  Smoothing","summary":"  While prior research has proposed a plethora of methods that build neural\nclassifiers robust against adversarial robustness, practitioners are still\nreluctant to adopt them due to their unacceptably severe clean accuracy\npenalties. This paper significantly alleviates this accuracy-robustness\ntrade-off by mixing the output probabilities of a standard classifier and a\nrobust classifier, where the standard network is optimized for clean accuracy\nand is not robust in general. We show that the robust base classifier's\nconfidence difference for correct and incorrect examples is the key to this\nimprovement. In addition to providing intuitions and empirical evidence, we\ntheoretically certify the robustness of the mixed classifier under realistic\nassumptions. Furthermore, we adapt an adversarial input detector into a mixing\nnetwork that adaptively adjusts the mixture of the two base models, further\nreducing the accuracy penalty of achieving robustness. The proposed flexible\nmethod, termed \"adaptive smoothing\", can work in conjunction with existing or\neven future methods that improve clean accuracy, robustness, or adversary\ndetection. Our empirical evaluation considers strong attack methods, including\nAutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves\nan 85.21% clean accuracy while maintaining a 38.72% $\\ell_\\infty$-AutoAttacked\n($\\epsilon = 8/255$) accuracy, becoming the second most robust method on the\nRobustBench CIFAR-100 benchmark as of submission, while improving the clean\naccuracy by ten percentage points compared with all listed models. The code\nthat implements our method is available at\nhttps://github.com/Bai-YT/AdaptiveSmoothing.\n","authors":["Yatong Bai","Brendon G. Anderson","Aerin Kim","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2301.12554v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01258v3","updated":"2023-11-15T11:22:05Z","published":"2023-10-02T14:50:14Z","title":"MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device","summary":"  Neural video codecs have recently become competitive with standard codecs\nsuch as HEVC in the low-delay setting. However, most neural codecs are large\nfloating-point networks that use pixel-dense warping operations for temporal\nmodeling, making them too computationally expensive for deployment on mobile\ndevices. Recent work has demonstrated that running a neural decoder in real\ntime on mobile is feasible, but shows this only for 720p RGB video. This work\npresents the first neural video codec that decodes 1080p YUV420 video in real\ntime on a mobile device. Our codec relies on two major contributions. First, we\ndesign an efficient codec that uses a block-based motion compensation algorithm\navailable on the warping core of the mobile accelerator, and we show how to\nquantize this model to integer precision. Second, we implement a fast decoder\npipeline that concurrently runs neural network components on the neural signal\nprocessor, parallel entropy coding on the mobile GPU, and warping on the\nwarping core. Our codec outperforms the previous on-device codec by a large\nmargin with up to 48% BD-rate savings, while reducing the MAC count on the\nreceiver side by $10 \\times$. We perform a careful ablation to demonstrate the\neffect of the introduced motion compensation scheme, and ablate the effect of\nmodel quantization.\n","authors":["Ties van Rozendaal","Tushar Singhal","Hoang Le","Guillaume Sautiere","Amir Said","Krishna Buska","Anjuman Raha","Dimitris Kalatzis","Hitarth Mehta","Frank Mayer","Liang Zhang","Markus Nagel","Auke Wiggers"],"pdf_url":"https://arxiv.org/pdf/2310.01258v3.pdf","comment":"Matches version published at WACV 2024"},{"id":"http://arxiv.org/abs/2311.08870v1","updated":"2023-11-15T11:11:25Z","published":"2023-11-15T11:11:25Z","title":"One-Shot Federated Learning with Classifier-Guided Diffusion Models","summary":"  One-shot federated learning (OSFL) has gained attention in recent years due\nto its low communication cost. However, most of the existing methods require\nauxiliary datasets or training generators, which hinders their practicality in\nreal-world scenarios. In this paper, we explore the novel opportunities that\ndiffusion models bring to OSFL and propose FedCADO, utilizing guidance from\nclient classifiers to generate data that complies with clients' distributions\nand subsequently training the aggregated model on the server. Specifically, our\nmethod involves targeted optimizations in two aspects. On one hand, we\nconditionally edit the randomly sampled initial noises, embedding them with\nspecified semantics and distributions, resulting in a significant improvement\nin both the quality and stability of generation. On the other hand, we employ\nthe BN statistics from the classifiers to provide detailed guidance during\ngeneration. These tailored optimizations enable us to limitlessly generate\ndatasets, which closely resemble the distribution and quality of the original\nclient dataset. Our method effectively handles the heterogeneous client models\nand the problems of non-IID features or labels. In terms of privacy protection,\nour method avoids training any generator or transferring any auxiliary\ninformation on clients, eliminating any additional privacy leakage risks.\nLeveraging the extensive knowledge stored in the pre-trained diffusion model,\nthe synthetic datasets can assist us in surpassing the knowledge limitations of\nthe client samples, resulting in aggregation models that even outperform the\nperformance ceiling of centralized training in some cases, which is\nconvincingly demonstrated in the sufficient quantification and visualization\nexperiments conducted on three large-scale multi-domain image datasets.\n","authors":["Mingzhao Yang","Shangchao Su","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2311.08870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08863v1","updated":"2023-11-15T10:49:15Z","published":"2023-11-15T10:49:15Z","title":"Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques","summary":"  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment the\nself-supervised task of Masked Autoencoders and establish a baseline for\npixel-wise classification based on a conventional autoencoder combined with a\nRandom Forest classifier achieving 82% overall accuracy and 74% F1 score. The\nToulouse Hyperspectral Data Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n","authors":["Romain Thoreau","Laurent Risser","Véronique Achard","Béatrice Berthelot","Xavier Briottet"],"pdf_url":"https://arxiv.org/pdf/2311.08863v1.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2311.08851v1","updated":"2023-11-15T10:43:13Z","published":"2023-11-15T10:43:13Z","title":"Data Augmentations in Deep Weight Spaces","summary":"  Learning in weight spaces, where neural networks process the weights of other\ndeep neural networks, has emerged as a promising research direction with\napplications in various fields, from analyzing and editing neural fields and\nimplicit neural representations, to network pruning and quantization. Recent\nworks designed architectures for effective learning in that space, which takes\ninto account its unique, permutation-equivariant, structure. Unfortunately, so\nfar these architectures suffer from severe overfitting and were shown to\nbenefit from large datasets. This poses a significant challenge because\ngenerating data for this learning setup is laborious and time-consuming since\neach data sample is a full set of network weights that has to be trained. In\nthis paper, we address this difficulty by investigating data augmentations for\nweight spaces, a set of techniques that enable generating new data examples on\nthe fly without having to train additional input weight space elements. We\nfirst review several recently proposed data augmentation schemes %that were\nproposed recently and divide them into categories. We then introduce a novel\naugmentation scheme based on the Mixup method. We evaluate the performance of\nthese techniques on existing benchmarks as well as new benchmarks we generate,\nwhich can be valuable for future studies.\n","authors":["Aviv Shamsian","David W. Zhang","Aviv Navon","Yan Zhang","Miltiadis Kofinas","Idan Achituve","Riccardo Valperga","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","Ethan Fetaya","Gal Chechik","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2311.08851v1.pdf","comment":"Accepted to NeurIPS 2023 Workshop on Symmetry and Geometry in Neural\n  Representations"},{"id":"http://arxiv.org/abs/2311.08850v1","updated":"2023-11-15T10:42:06Z","published":"2023-11-15T10:42:06Z","title":"Controlling the Output of a Generative Model by Latent Feature Vector\n  Shifting","summary":"  State-of-the-art generative models (e.g. StyleGAN3 \\cite{karras2021alias})\noften generate photorealistic images based on vectors sampled from their latent\nspace. However, the ability to control the output is limited. Here we present\nour novel method for latent vector shifting for controlled output image\nmodification utilizing semantic features of the generated images. In our\napproach we use a pre-trained model of StyleGAN3 that generates images of\nrealistic human faces in relatively high resolution. We complement the\ngenerative model with a convolutional neural network classifier, namely\nResNet34, trained to classify the generated images with binary facial features\nfrom the CelebA dataset. Our latent feature shifter is a neural network model\nwith a task to shift the latent vectors of a generative model into a specified\nfeature direction. We have trained latent feature shifter for multiple facial\nfeatures, and outperformed our baseline method in the number of generated\nimages with the desired feature. To train our latent feature shifter neural\nnetwork, we have designed a dataset of pairs of latent vectors with and without\na certain feature. Based on the evaluation, we conclude that our latent feature\nshifter approach was successful in the controlled generation of the StyleGAN3\ngenerator.\n","authors":["Róbert Belanec","Peter Lacko","Kristína Malinovská"],"pdf_url":"https://arxiv.org/pdf/2311.08850v1.pdf","comment":"7 pages, presented on DISA2023 conference in Ko\\v{s}ice"},{"id":"http://arxiv.org/abs/2310.07250v3","updated":"2023-11-15T10:35:07Z","published":"2023-10-11T07:27:28Z","title":"Synthesizing Missing MRI Sequences from Available Modalities using\n  Generative Adversarial Networks in BraTS Dataset","summary":"  Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic\nresonance imaging (MRI) plays a significant role in the diagnosis, treatment\nplanning, and follow-up of glioblastoma patients due to its non-invasive and\nradiation-free nature. The International Brain Tumor Segmentation (BraTS)\nchallenge has contributed to generating numerous AI algorithms to accurately\nand efficiently segment glioblastoma sub-compartments using four structural\n(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not\nalways be available. To address this issue, Generative Adversarial Networks\n(GANs) can be used to synthesize the missing MRI sequences. In this paper, we\nimplement and utilize an open-source GAN approach that takes any three MRI\nsequences as input to generate the missing fourth structural sequence. Our\nproposed approach is contributed to the community-driven generally nuanced deep\nlearning framework (GaNDLF) and demonstrates promising results in synthesizing\nhigh-quality and realistic MRI sequences, enabling clinicians to improve their\ndiagnostic capabilities and support the application of AI methods to brain\ntumor MRI quantification.\n","authors":["Ibrahim Ethem Hamamci"],"pdf_url":"https://arxiv.org/pdf/2310.07250v3.pdf","comment":"Wrong paper submission"},{"id":"http://arxiv.org/abs/2311.08844v1","updated":"2023-11-15T10:34:14Z","published":"2023-11-15T10:34:14Z","title":"Violet: A Vision-Language Model for Arabic Image Captioning with Gemini\n  Decoder","summary":"  Although image captioning has a vast array of applications, it has not\nreached its full potential in languages other than English. Arabic, for\ninstance, although the native language of more than 400 million people, remains\nlargely underrepresented in this area. This is due to the lack of labeled data\nand powerful Arabic generative models. We alleviate this issue by presenting a\nnovel vision-language model dedicated to Arabic, dubbed \\textit{Violet}. Our\nmodel is based on a vision encoder and a Gemini text decoder that maintains\ngeneration fluency while allowing fusion between the vision and language\ncomponents. To train our model, we introduce a new method for automatically\nacquiring data from available English datasets. We also manually prepare a new\ndataset for evaluation. \\textit{Violet} performs sizeably better than our\nbaselines on all of our evaluation datasets. For example, it reaches a CIDEr\nscore of $61.2$ on our manually annotated dataset and achieves an improvement\nof $13$ points on Flickr8k.\n","authors":["Abdelrahman Mohamed","Fakhraddin Alwajih","El Moatez Billah Nagoudi","Alcides Alcoba Inciarte","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2311.08844v1.pdf","comment":"Accepted in ArabicNLP Conference"},{"id":"http://arxiv.org/abs/2311.08843v1","updated":"2023-11-15T10:33:20Z","published":"2023-11-15T10:33:20Z","title":"Personalized Video Relighting Using Casual Light Stage","summary":"  In this paper, we develop a personalized video relighting algorithm that\nproduces high-quality and temporally consistent relit video under any pose,\nexpression, and lighting conditions in real-time. Existing relighting\nalgorithms typically rely either on publicly available synthetic data, which\nyields poor relighting results, or instead on Light Stage data which is\ninaccessible and is not publicly available. We show that by casually capturing\nvideo of a user watching YouTube videos on a monitor we can train a\npersonalized algorithm capable of producing high-quality relighting under any\ncondition. Our key contribution is a novel neural relighting architecture that\neffectively separates the intrinsic appearance features, geometry and\nreflectance, from the source lighting and then combines it with the target\nlighting to generate a relit image. This neural architecture enables smoothing\nof intrinsic appearance features leading to temporally stable video relighting.\nBoth qualitative and quantitative evaluations show that our relighting\narchitecture improves portrait image relighting quality and temporal\nconsistency over state-of-the-art approaches on both casually captured Light\nStage at Your Desk (LSYD) data and Light Stage captured One Light At a Time\n(OLAT) datasets.\n","authors":["Jun Myeong Choi","Max Christman","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2311.08843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08835v1","updated":"2023-11-15T10:22:35Z","published":"2023-11-15T10:22:35Z","title":"Correlation-guided Query-Dependency Calibration in Video Representation\n  Learning for Temporal Grounding","summary":"  Recent endeavors in video temporal grounding enforce strong cross-modal\ninteractions through attention mechanisms to overcome the modality gap between\nvideo and text query. However, previous works treat all video clips equally\nregardless of their semantic relevance with the text query in attention\nmodules. In this paper, our goal is to provide clues for query-associated video\nclips within the crossmodal encoding process. With our Correlation-Guided\nDetection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of\ncross-modal interactions and how to exploit such degrees for prediction. First,\nwe design an adaptive cross-attention layer with dummy tokens. Dummy tokens\nconditioned by text query take a portion of the attention weights, preventing\nirrelevant video clips from being represented by the text query. Yet, not all\nword tokens equally inherit the text query's correlation to video clips. Thus,\nwe further guide the cross-attention map by inferring the fine-grained\ncorrelation between video clips and words. We enable this by learning a joint\nembedding space for high-level concepts, i.e., moment and sentence level, and\ninferring the clip-word correlation. Lastly, we use a moment-adaptive saliency\ndetector to exploit each video clip's degrees of text engagement. We validate\nthe superiority of CG-DETR with the state-of-the-art results on various\nbenchmarks for both moment retrieval and highlight detection. Codes are\navailable at https://github.com/wjun0830/CGDETR.\n","authors":["WonJun Moon","Sangeek Hyun","SuBeen Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2311.08835v1.pdf","comment":"20 pages, 14 figures, 14 tables, Code is available at\n  https://github.com/wjun0830/CGDETR"},{"id":"http://arxiv.org/abs/2306.09012v2","updated":"2023-11-15T10:16:55Z","published":"2023-06-15T10:12:10Z","title":"Yes, we CANN: Constrained Approximate Nearest Neighbors for local\n  feature-based visual localization","summary":"  Large-scale visual localization systems continue to rely on 3D point clouds\nbuilt from image collections using structure-from-motion. While the 3D points\nin these models are represented using local image features, directly matching a\nquery image's local features against the point cloud is challenging due to the\nscale of the nearest-neighbor search problem. Many recent approaches to visual\nlocalization have thus proposed a hybrid method, where first a global (per\nimage) embedding is used to retrieve a small subset of database images, and\nlocal features of the query are matched only against those. It seems to have\nbecome common belief that global embeddings are critical for said\nimage-retrieval in visual localization, despite the significant downside of\nhaving to compute two feature types for each query image. In this paper, we\ntake a step back from this assumption and propose Constrained Approximate\nNearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both\nthe geometry and appearance space using only local features. We first derive\nthe theoretical foundation for k-nearest-neighbor retrieval across multiple\nmetrics and then showcase how CANN improves visual localization. Our\nexperiments on public localization benchmarks demonstrate that our method\nsignificantly outperforms both state-of-the-art global feature-based retrieval\nand approaches using local feature aggregation schemes. Moreover, it is an\norder of magnitude faster in both index and query time than feature aggregation\nschemes for these datasets. Code will be released.\n","authors":["Dror Aiger","André Araujo","Simon Lynen"],"pdf_url":"https://arxiv.org/pdf/2306.09012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08819v1","updated":"2023-11-15T09:46:30Z","published":"2023-11-15T09:46:30Z","title":"Frequency Domain-based Dataset Distillation","summary":"  This paper presents FreD, a novel parameterization method for dataset\ndistillation, which utilizes the frequency domain to distill a small-sized\nsynthetic dataset from a large-sized original dataset. Unlike conventional\napproaches that focus on the spatial domain, FreD employs frequency-based\ntransforms to optimize the frequency representations of each data instance. By\nleveraging the concentration of spatial domain information on specific\nfrequency components, FreD intelligently selects a subset of frequency\ndimensions for optimization, leading to a significant reduction in the required\nbudget for synthesizing an instance. Through the selection of frequency\ndimensions based on the explained variance, FreD demonstrates both theoretical\nand empirical evidence of its ability to operate efficiently within a limited\nbudget, while better preserving the information of the original dataset\ncompared to conventional parameterization methods. Furthermore, based on the\northogonal compatibility of FreD with existing methods, we confirm that FreD\nconsistently improves the performances of existing distillation methods over\nthe evaluation scenarios with different benchmark datasets. We release the code\nat https://github.com/sdh0818/FreD.\n","authors":["Donghyeok Shin","Seungjae Shin","Il-Chul Moon"],"pdf_url":"https://arxiv.org/pdf/2311.08819v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08816v1","updated":"2023-11-15T09:35:07Z","published":"2023-11-15T09:35:07Z","title":"Target-oriented Domain Adaptation for Infrared Image Super-Resolution","summary":"  Recent efforts have explored leveraging visible light images to enrich\ntexture details in infrared (IR) super-resolution. However, this direct\nadaptation approach often becomes a double-edged sword, as it improves texture\nat the cost of introducing noise and blurring artifacts. To address these\nchallenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN),\nan innovative framework specifically engineered for robust IR super-resolution\nmodel adaptation. DASRGAN operates on the synergy of two key components: 1)\nTexture-Oriented Adaptation (TOA) to refine texture details meticulously, and\n2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer.\nSpecifically, TOA uniquely integrates a specialized discriminator,\nincorporating a prior extraction branch, and employs a Sobel-guided adversarial\nloss to align texture distributions effectively. Concurrently, NOA utilizes a\nnoise adversarial loss to distinctly separate the generative and Gaussian noise\npattern distributions during adversarial training. Our extensive experiments\nconfirm DASRGAN's superiority. Comparative analyses against leading methods\nacross multiple benchmarks and upsampling factors reveal that DASRGAN sets new\nstate-of-the-art performance standards. Code are available at\n\\url{https://github.com/yongsongH/DASRGAN}.\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Yafei Dong","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2311.08816v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.08815v1","updated":"2023-11-15T09:34:08Z","published":"2023-11-15T09:34:08Z","title":"Self-Supervised Disentanglement by Leveraging Structure in Data\n  Augmentations","summary":"  Self-supervised representation learning often uses data augmentations to\ninduce some invariance to \"style\" attributes of the data. However, with\ndownstream tasks generally unknown at training time, it is difficult to deduce\na priori which attributes of the data are indeed \"style\" and can be safely\ndiscarded. To address this, we introduce a more principled approach that seeks\nto disentangle style features rather than discard them. The key idea is to add\nmultiple style embedding spaces where: (i) each is invariant to all-but-one\naugmentation; and (ii) joint entropy is maximized. We formalize our structured\ndata-augmentation procedure from a causal latent-variable-model perspective,\nand prove identifiability of both content and (multiple blocks of) style\nvariables. We empirically demonstrate the benefits of our approach on synthetic\ndatasets and then present promising but limited results on ImageNet.\n","authors":["Cian Eastwood","Julius von Kügelgen","Linus Ericsson","Diane Bouchacourt","Pascal Vincent","Bernhard Schölkopf","Mark Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2311.08815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08811v1","updated":"2023-11-15T09:30:52Z","published":"2023-11-15T09:30:52Z","title":"Correlation-aware active learning for surgery video segmentation","summary":"  Semantic segmentation is a complex task that relies heavily on large amounts\nof annotated image data. However, annotating such data can be time-consuming\nand resource-intensive, especially in the medical domain. Active Learning (AL)\nis a popular approach that can help to reduce this burden by iteratively\nselecting images for annotation to improve the model performance. In the case\nof video data, it is important to consider the model uncertainty and the\ntemporal nature of the sequences when selecting images for annotation. This\nwork proposes a novel AL strategy for surgery video segmentation, \\COALSamp{},\nCOrrelation-aWare Active Learning. Our approach involves projecting images into\na latent space that has been fine-tuned using contrastive learning and then\nselecting a fixed number of representative images from local clusters of video\nframes. We demonstrate the effectiveness of this approach on two video datasets\nof surgical instruments and three real-world video datasets. The datasets and\ncode will be made publicly available upon receiving necessary approvals.\n","authors":["Fei Wu","Pablo Marquez-Neila","Mingyi Zheng","Hedyeh Rafii-Tari","Raphael Sznitman"],"pdf_url":"https://arxiv.org/pdf/2311.08811v1.pdf","comment":"WACV 2024, 8 pages"},{"id":"http://arxiv.org/abs/2311.08806v1","updated":"2023-11-15T09:22:52Z","published":"2023-11-15T09:22:52Z","title":"SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in\n  Spiking Transformer","summary":"  As the third-generation neural network, the Spiking Neural Network (SNN) has\nthe advantages of low power consumption and high energy efficiency, making it\nsuitable for implementation on edge devices. More recently, the most advanced\nSNN, Spikformer, combines the self-attention module from Transformer with SNN\nto achieve remarkable performance. However, it adopts larger channel dimensions\nin MLP layers, leading to an increased number of redundant model parameters. To\neffectively decrease the computational complexity and weight parameters of the\nmodel, we explore the Lottery Ticket Hypothesis (LTH) and discover a very\nsparse ($\\ge$90%) subnetwork that achieves comparable performance to the\noriginal network. Furthermore, we also design a lightweight token selector\nmodule, which can remove unimportant background information from images based\non the average spike firing rate of neurons, selecting only essential\nforeground image tokens to participate in attention calculation. Based on that,\nwe present SparseSpikformer, a co-design framework aimed at achieving sparsity\nin Spikformer through token and weight pruning techniques. Experimental results\ndemonstrate that our framework can significantly reduce 90% model parameters\nand cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining\nthe accuracy of the original model.\n","authors":["Yue Liu","Shanlin Xiao","Bo Li","Zhiyi Yu"],"pdf_url":"https://arxiv.org/pdf/2311.08806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08799v1","updated":"2023-11-15T09:11:37Z","published":"2023-11-15T09:11:37Z","title":"EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target\n  Approaching in Robotic Eye Surgery","summary":"  Robotic ophthalmic surgery is an emerging technology to facilitate\nhigh-precision interventions such as retina penetration in subretinal injection\nand removal of floating tissues in retinal detachment depending on the input\nimaging modalities such as microscopy and intraoperative OCT (iOCT). Although\niOCT is explored to locate the needle tip within its range-limited ROI, it is\nstill difficult to coordinate iOCT's motion with the needle, especially at the\ninitial target-approaching stage. Meanwhile, due to 2D perspective projection\nand thus the loss of depth information, current image-based methods cannot\neffectively estimate the needle tip's trajectory towards both retinal and\nfloating targets. To address this limitation, we propose to use the shadow\npositions of the target and the instrument tip to estimate their relative depth\nposition and accordingly optimize the instrument tip's insertion trajectory\nuntil the tip approaches targets within iOCT's scanning area. Our method\nsucceeds target approaching on a retina model, and achieves an average depth\nerror of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively\nin the surgical simulator without damaging the retina.\n","authors":["Junjie Yang","Zhihao Zhao","Siyuan Shen","Daniel Zapp","Mathias Maier","Kai Huang","Nassir Navab","M. Ali Nasseri"],"pdf_url":"https://arxiv.org/pdf/2311.08799v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2305.12704v3","updated":"2023-11-15T09:02:23Z","published":"2023-05-22T04:29:34Z","title":"Rotation-Constrained Cross-View Feature Fusion for Multi-View\n  Appearance-based Gaze Estimation","summary":"  Appearance-based gaze estimation has been actively studied in recent years.\nHowever, its generalization performance for unseen head poses is still a\nsignificant limitation for existing methods. This work proposes a generalizable\nmulti-view gaze estimation task and a cross-view feature fusion method to\naddress this issue. In addition to paired images, our method takes the relative\nrotation matrix between two cameras as additional input. The proposed network\nlearns to extract rotatable feature representation by using relative rotation\nas a constraint and adaptively fuses the rotatable features via stacked fusion\nmodules. This simple yet efficient approach significantly improves\ngeneralization performance under unseen head poses without significantly\nincreasing computational cost. The model can be trained with random\ncombinations of cameras without fixing the positioning and can generalize to\nunseen camera pairs during inference. Through experiments using multiple\ndatasets, we demonstrate the advantage of the proposed method over baseline\nmethods, including state-of-the-art domain generalization approaches. The code\nwill be available at https://github.com/ut-vision/Rot-MVGaze.\n","authors":["Yoichiro Hisadome","Tianyi Wu","Jiawei Qin","Yusuke Sugano"],"pdf_url":"https://arxiv.org/pdf/2305.12704v3.pdf","comment":"Accepted by WACV2024. The code will be available at\n  https://github.com/ut-vision/Rot-MVGaze"},{"id":"http://arxiv.org/abs/2311.08786v1","updated":"2023-11-15T08:59:02Z","published":"2023-11-15T08:59:02Z","title":"HFORD: High-Fidelity and Occlusion-Robust De-identification for Face\n  Privacy Protection","summary":"  With the popularity of smart devices and the development of computer vision\ntechnology, concerns about face privacy protection are growing. The face\nde-identification technique is a practical way to solve the identity protection\nproblem. The existing facial de-identification methods have revealed several\nproblems, including the impact on the realism of anonymized results when faced\nwith occlusions and the inability to maintain identity-irrelevant details in\nanonymized results. We present a High-Fidelity and Occlusion-Robust\nDe-identification (HFORD) method to deal with these issues. This approach can\ndisentangle identities and attributes while preserving image-specific details\nsuch as background, facial features (e.g., wrinkles), and lighting, even in\noccluded scenes. To disentangle the latent codes in the GAN inversion space, we\nintroduce an Identity Disentanglement Module (IDM). This module selects the\nlatent codes that are closely related to the identity. It further separates the\nlatent codes into identity-related codes and attribute-related codes, enabling\nthe network to preserve attributes while only modifying the identity. To ensure\nthe preservation of image details and enhance the network's robustness to\nocclusions, we propose an Attribute Retention Module (ARM). This module\nadaptively preserves identity-irrelevant details and facial occlusions and\nblends them into the generated results in a modulated manner. Extensive\nexperiments show that our method has higher quality, better detail fidelity,\nand stronger occlusion robustness than other face de-identification methods.\n","authors":["Dongxin Chen","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2311.08786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08782v1","updated":"2023-11-15T08:54:57Z","published":"2023-11-15T08:54:57Z","title":"Language Semantic Graph Guided Data-Efficient Learning","summary":"  Developing generalizable models that can effectively learn from limited data\nand with minimal reliance on human supervision is a significant objective\nwithin the machine learning community, particularly in the era of deep neural\nnetworks. Therefore, to achieve data-efficient learning, researchers typically\nexplore approaches that can leverage more related or unlabeled data without\nnecessitating additional manual labeling efforts, such as Semi-Supervised\nLearning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL\nleverages unlabeled data in the training process, while TL enables the transfer\nof expertise from related data distributions. DA broadens the dataset by\nsynthesizing new data from existing examples. However, the significance of\nadditional knowledge contained within labels has been largely overlooked in\nresearch. In this paper, we propose a novel perspective on data efficiency that\ninvolves exploiting the semantic information contained in the labels of the\navailable data. Specifically, we introduce a Language Semantic Graph (LSG)\nwhich is constructed from labels manifest as natural language descriptions.\nUpon this graph, an auxiliary graph neural network is trained to extract\nhigh-level semantic relations and then used to guide the training of the\nprimary model, enabling more adequate utilization of label knowledge. Across\nimage, video, and audio modalities, we utilize the LSG method in both TL and\nSSL scenarios and illustrate its versatility in significantly enhancing\nperformance compared to other data-efficient learning approaches. Additionally,\nour in-depth analysis shows that the LSG method also expedites the training\nprocess.\n","authors":["Wenxuan Ma","Shuang Li","Lincan Cai","Jingxuan Kang"],"pdf_url":"https://arxiv.org/pdf/2311.08782v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2212.12322v2","updated":"2023-11-15T08:53:29Z","published":"2022-12-22T08:33:32Z","title":"Infrared Image Super-Resolution: Systematic Review, and Future Trends","summary":"  Image Super-Resolution (SR) is essential for a wide range of computer vision\nand image processing tasks. Investigating infrared (IR) image (or thermal\nimages) super-resolution is a continuing concern within the development of deep\nlearning. This survey aims to provide a comprehensive perspective of IR image\nsuper-resolution, including its applications, hardware imaging system dilemmas,\nand taxonomy of image processing methodologies. In addition, the datasets and\nevaluation metrics in IR image super-resolution tasks are also discussed.\nFurthermore, the deficiencies in current technologies and possible promising\ndirections for the community to explore are highlighted. To cope with the rapid\ndevelopment in this field, we intend to regularly update the relevant excellent\nwork at \\url{https://github.com/yongsongH/Infrared_Image_SR_Survey\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2212.12322v2.pdf","comment":"Submitted to IEEE TNNLS"},{"id":"http://arxiv.org/abs/2308.00755v2","updated":"2023-11-15T08:51:11Z","published":"2023-08-01T18:00:08Z","title":"The Bias Amplification Paradox in Text-to-Image Generation","summary":"  Bias amplification is a phenomenon in which models exacerbate biases or\nstereotypes present in the training data. In this paper, we study bias\namplification in the text-to-image domain using Stable Diffusion by comparing\ngender ratios in training vs. generated images. We find that the model appears\nto amplify gender-occupation biases found in the training data (LAION)\nconsiderably. However, we discover that amplification can be largely attributed\nto discrepancies between training captions and model prompts. For example, an\ninherent difference is that captions from the training data often contain\nexplicit gender information while our prompts do not, which leads to a\ndistribution shift and consequently inflates bias measures. Once we account for\ndistributional differences between texts used for training and generation when\nevaluating amplification, we observe that amplification decreases drastically.\nOur findings illustrate the challenges of comparing biases in models and their\ntraining data, and highlight confounding factors that impact analyses.\n","authors":["Preethi Seshadri","Sameer Singh","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2308.00755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08774v1","updated":"2023-11-15T08:37:11Z","published":"2023-11-15T08:37:11Z","title":"Two-stage Joint Transductive and Inductive learning for Nuclei\n  Segmentation","summary":"  AI-assisted nuclei segmentation in histopathological images is a crucial task\nin the diagnosis and treatment of cancer diseases. It decreases the time\nrequired to manually screen microscopic tissue images and can resolve the\nconflict between pathologists during diagnosis. Deep Learning has proven useful\nin such a task. However, lack of labeled data is a significant barrier for deep\nlearning-based approaches. In this study, we propose a novel approach to nuclei\nsegmentation that leverages the available labelled and unlabelled data. The\nproposed method combines the strengths of both transductive and inductive\nlearning, which have been previously attempted separately, into a single\nframework. Inductive learning aims at approximating the general function and\ngeneralizing to unseen test data, while transductive learning has the potential\nof leveraging the unlabelled test data to improve the classification. To the\nbest of our knowledge, this is the first study to propose such a hybrid\napproach for medical image segmentation. Moreover, we propose a novel two-stage\ntransductive inference scheme. We evaluate our approach on MoNuSeg benchmark to\ndemonstrate the efficacy and potential of our method.\n","authors":["Hesham Ali","Idriss Tondji","Mennatullah Siam"],"pdf_url":"https://arxiv.org/pdf/2311.08774v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2208.02474v3","updated":"2023-11-15T08:35:24Z","published":"2022-08-04T05:54:36Z","title":"CFARnet: deep learning for target detection with constant false alarm\n  rate","summary":"  We consider the problem of target detection with a constant false alarm rate\n(CFAR). This constraint is crucial in many practical applications and is a\nstandard requirement in classical composite hypothesis testing. In settings\nwhere classical approaches are computationally expensive or where only data\nsamples are given, machine learning methodologies are advantageous. CFAR is\nless understood in these settings. To close this gap, we introduce a framework\nof CFAR constrained detectors. Theoretically, we prove that a CFAR constrained\nBayes optimal detector is asymptotically equivalent to the classical\ngeneralized likelihood ratio test (GLRT). Practically, we develop a deep\nlearning framework for fitting neural networks that approximate it. Experiments\nof target detection in different setting demonstrate that the proposed CFARnet\nallows a flexible tradeoff between CFAR and accuracy.\n","authors":["Tzvi Diskin","Yiftach Beer","Uri Okun","Ami Wiesel"],"pdf_url":"https://arxiv.org/pdf/2208.02474v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2206.05747"},{"id":"http://arxiv.org/abs/2311.08764v1","updated":"2023-11-15T08:13:52Z","published":"2023-11-15T08:13:52Z","title":"Combining Past, Present and Future: A Self-Supervised Approach for Class\n  Incremental Learning","summary":"  Class Incremental Learning (CIL) aims to handle the scenario where data of\nnovel classes occur continuously and sequentially. The model should recognize\nthe sequential novel classes while alleviating the catastrophic forgetting. In\nthe self-supervised manner, it becomes more challenging to avoid the conflict\nbetween the feature embedding spaces of novel classes and old ones without any\nclass labels. To address the problem, we propose a self-supervised CIL\nframework CPPF, meaning Combining Past, Present and Future. In detail, CPPF\nconsists of a prototype clustering module (PC), an embedding space reserving\nmodule (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the\nESR modules reserve embedding space for subsequent phases at the prototype\nlevel and the feature level respectively to prepare for knowledge learned in\nthe future. 2) The MTD module maintains the representations of the current\nphase without the interference of past knowledge. One of the teacher networks\nretains the representations of the past phases, and the other teacher network\ndistills relation information of the current phase to the student network.\nExtensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our\nproposed method boosts the performance of self-supervised class incremental\nlearning. We will release code in the near future.\n","authors":["Xiaoshuang Chen","Zhongyi Sun","Ke Yan","Shouhong Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2311.08764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08759v1","updated":"2023-11-15T08:01:12Z","published":"2023-11-15T08:01:12Z","title":"4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters","summary":"  The illumination of improperly exposed photographs has been widely corrected\nusing deep convolutional neural networks or Transformers. Despite with\npromising performance, these methods usually suffer from large parameter\namounts and heavy computational FLOPs on high-resolution photographs. In this\npaper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale\nLinear Transformation (MSLT) networks under the multi-layer perception\narchitecture, which can process 4K-resolution sRGB images at 125\nFrame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT\nnetworks first decompose an input image into high and low frequency layers by\nLaplacian pyramid techniques, and then sequentially correct different layers by\npixel-adaptive linear transformation, which is implemented by efficient\nbilateral grid learning or 1x1 convolutions. Experiments on two benchmark\ndatasets demonstrate the efficiency of our MSLTs against the state-of-the-arts\non photo exposure correction. Extensive ablation studies validate the\neffectiveness of our contributions. The code is available at\nhttps://github.com/Zhou-Yijie/MSLTNet.\n","authors":["Yijie Zhou","Chao Li","Jin Liang","Tianyi Xu","Xin Liu","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2311.08759v1.pdf","comment":"WACV2024"},{"id":"http://arxiv.org/abs/2305.10722v2","updated":"2023-11-15T07:53:57Z","published":"2023-05-18T05:41:36Z","title":"Discriminative Diffusion Models as Few-shot Vision and Language Learners","summary":"  Diffusion models, such as Stable Diffusion, have shown incredible performance\non text-to-image generation. Since text-to-image generation often requires\nmodels to generate visual concepts with fine-grained details and attributes\nspecified in text prompts, can we leverage the powerful representations learned\nby pre-trained diffusion models for discriminative tasks such as image-text\nmatching? To answer this question, we propose a novel approach, Discriminative\nStable Diffusion (DSD), which turns pre-trained text-to-image diffusion models\ninto few-shot discriminative learners. Our approach mainly uses the\ncross-attention score of a Stable Diffusion model to capture the mutual\ninfluence between visual and textual information and fine-tune the model via\nefficient attention-based prompt learning to perform image-text matching. By\ncomparing DSD with state-of-the-art methods on several benchmark datasets, we\ndemonstrate the potential of using pre-trained diffusion models for\ndiscriminative tasks with superior results on few-shot image-text matching.\n","authors":["Xuehai He","Weixi Feng","Tsu-Jui Fu","Varun Jampani","Arjun Akula","Pradyumna Narayana","Sugato Basu","William Yang Wang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2305.10722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08747v1","updated":"2023-11-15T07:29:24Z","published":"2023-11-15T07:29:24Z","title":"Improved Dense Nested Attention Network Based on Transformer for\n  Infrared Small Target Detection","summary":"  Infrared small target detection based on deep learning offers unique\nadvantages in separating small targets from complex and dynamic backgrounds.\nHowever, the features of infrared small targets gradually weaken as the depth\nof convolutional neural network (CNN) increases. To address this issue, we\npropose a novel method for detecting infrared small targets called improved\ndense nested attention network (IDNANet), which is based on the transformer\narchitecture. We preserve the dense nested structure of dense nested attention\nnetwork (DNANet) and introduce the Swin-transformer during feature extraction\nstage to enhance the continuity of features. Furthermore, we integrate the\nACmix attention structure into the dense nested structure to enhance the\nfeatures of intermediate layers. Additionally, we design a weighted dice binary\ncross-entropy (WD-BCE) loss function to mitigate the negative impact of\nforeground-background imbalance in the samples. Moreover, we develop a dataset\nspecifically for infrared small targets, called BIT-SIRST. The dataset\ncomprises a significant amount of real-world targets and manually annotated\nlabels, as well as synthetic data and corresponding labels. We have evaluated\nthe effectiveness of our method through experiments conducted on public\ndatasets. In comparison to other state-of-the-art methods, our approach\noutperforms in terms of probability of detection (P_d), false-alarm rate (F_a),\nand mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the\nNUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.\n","authors":["Chun Bao","Jie Cao","Yaqian Ning","Tianhua Zhao","Zhijun Li","Zechen Wang","Li Zhang","Qun Hao"],"pdf_url":"https://arxiv.org/pdf/2311.08747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08746v1","updated":"2023-11-15T07:29:23Z","published":"2023-11-15T07:29:23Z","title":"A Diffusion Model Based Quality Enhancement Method for HEVC Compressed\n  Video","summary":"  Video post-processing methods can improve the quality of compressed videos at\nthe decoder side. Most of the existing methods need to train corresponding\nmodels for compressed videos with different quantization parameters to improve\nthe quality of compressed videos. However, in most cases, the quantization\nparameters of the decoded video are unknown. This makes existing methods have\ntheir limitations in improving video quality. To tackle this problem, this work\nproposes a diffusion model based post-processing method for compressed videos.\nThe proposed method first estimates the feature vectors of the compressed video\nand then uses the estimated feature vectors as the prior information for the\nquality enhancement model to adaptively enhance the quality of compressed video\nwith different quantization parameters. Experimental results show that the\nquality enhancement results of our proposed method on mixed datasets are\nsuperior to existing methods.\n","authors":["Zheng Liu","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2311.08746v1.pdf","comment":"10 pages, conference"},{"id":"http://arxiv.org/abs/2311.00735v2","updated":"2023-11-15T07:28:10Z","published":"2023-11-01T12:04:33Z","title":"PET Tracer Conversion among Brain PET via Variable Augmented Invertible\n  Network","summary":"  Positron emission tomography (PET) serves as an essential tool for diagnosis\nof encephalopathy and brain science research. However, it suffers from the\nlimited choice of tracers. Nowadays, with the wide application of PET imaging\nin neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine\n(DOPA) has been found to be more effective than 18F-labeled\nfluorine-2-deoxyglucose (FDG) in the field. Nevertheless, due to the complexity\nof its preparation and other limitations, DOPA is far less widely used than\nFDG. To address this issue, a tracer conversion invertible neural network\n(TC-INN) for image projection is developed to map FDG images to DOPA images\nthrough deep learning. More diagnostic information is obtained by generating\nPET images from FDG to DOPA. Specifically, the proposed TC-INN consists of two\nseparate phases, one for training traceable data, the other for rebuilding new\ndata. The reference DOPA PET image is used as a learning target for the\ncorresponding network during the training process of tracer conversion.\nMeanwhile, the invertible network iteratively estimates the resultant DOPA PET\ndata and compares it to the reference DOPA PET data. Notably, the reversible\nmodel employs variable enhancement technique to achieve better power\ngeneration. Moreover, image registration needs to be performed before training\ndue to the angular deviation of the acquired FDG and DOPA data information.\nExperimental results exhibited excellent generation capability in mapping\nbetween FDG and DOPA, suggesting that PET tracer conversion has great potential\nin the case of limited tracer applications.\n","authors":["Bohui Shen","Wei Zhang","Xubiao Liu","Pengfei Yu","Shirui Jiang","Xinchong Shi","Xiangsong Zhang","Xiaoyu Zhou","Weirui Zhang","Bingxuan Li","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2311.00735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10894v3","updated":"2023-11-15T06:26:21Z","published":"2023-07-20T14:15:20Z","title":"Human Motion Generation: A Survey","summary":"  Human motion generation aims to generate natural human pose sequences and\nshows immense potential for real-world applications. Substantial progress has\nbeen made recently in motion data collection technologies and generation\nmethods, laying the foundation for increasing interest in human motion\ngeneration. Most research within this field focuses on generating human motions\nbased on conditional signals, such as text, audio, and scene contexts. While\nsignificant advancements have been made in recent years, the task continues to\npose challenges due to the intricate nature of human motion and its implicit\nrelationship with conditional signals. In this survey, we present a\ncomprehensive literature review of human motion generation, which, to the best\nof our knowledge, is the first of its kind in this field. We begin by\nintroducing the background of human motion and generative models, followed by\nan examination of representative methods for three mainstream sub-tasks:\ntext-conditioned, audio-conditioned, and scene-conditioned human motion\ngeneration. Additionally, we provide an overview of common datasets and\nevaluation metrics. Lastly, we discuss open problems and outline potential\nfuture research directions. We hope that this survey could provide the\ncommunity with a comprehensive glimpse of this rapidly evolving field and\ninspire novel ideas that address the outstanding challenges.\n","authors":["Wentao Zhu","Xiaoxuan Ma","Dongwoo Ro","Hai Ci","Jinlu Zhang","Jiaxin Shi","Feng Gao","Qi Tian","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2307.10894v3.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2311.08716v1","updated":"2023-11-15T05:43:14Z","published":"2023-11-15T05:43:14Z","title":"Scalable Federated Learning for Clients with Different Input Image Sizes\n  and Numbers of Output Categories","summary":"  Federated learning is a privacy-preserving training method which consists of\ntraining from a plurality of clients but without sharing their confidential\ndata. However, previous work on federated learning do not explore suitable\nneural network architectures for clients with different input images sizes and\ndifferent numbers of output categories. In this paper, we propose an effective\nfederated learning method named ScalableFL, where the depths and widths of the\nlocal models for each client are adjusted according to the clients' input image\nsize and the numbers of output categories. In addition, we provide a new bound\nfor the generalization gap of federated learning. In particular, this bound\nhelps to explain the effectiveness of our scalable neural network approach. We\ndemonstrate the effectiveness of ScalableFL in several heterogeneous client\nsettings for both image classification and object detection tasks.\n","authors":["Shuhei Nitta","Taiji Suzuki","Albert Rodríguez Mulet","Atsushi Yaguchi","Ryusuke Hirai"],"pdf_url":"https://arxiv.org/pdf/2311.08716v1.pdf","comment":"15 pages, 1 figure, 2023 22nd International Conference on Machine\n  Learning and Applications (ICMLA)"},{"id":"http://arxiv.org/abs/2311.08695v1","updated":"2023-11-15T04:50:30Z","published":"2023-11-15T04:50:30Z","title":"Attribute Diversity Determines the Systematicity Gap in VQA","summary":"  The degree to which neural networks can generalize to new combinations of\nfamiliar concepts, and the conditions under which they are able to do so, has\nlong been an open question. In this work, we study the systematicity gap in\nvisual question answering: the performance difference between reasoning on\npreviously seen and unseen combinations of object attributes. To test, we\nintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\nquantity of training data does not reduce the systematicity gap, increased\ntraining data diversity of the attributes in the unseen combination does. In\nall, our experiments suggest that the more distinct attribute type combinations\nare seen during training, the more systematic we can expect the resulting model\nto be.\n","authors":["Ian Berlot-Attwell","A. Michael Carrell","Kumar Krishna Agrawal","Yash Sharma","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2311.08695v1.pdf","comment":"18 pages, 20 figures"},{"id":"http://arxiv.org/abs/2311.02762v2","updated":"2023-11-15T04:38:09Z","published":"2023-11-05T20:43:46Z","title":"Fast Sparse 3D Convolution Network with VDB","summary":"  We proposed a new Convolution Neural Network implementation optimized for\nsparse 3D data inference. This implementation uses NanoVDB as the data\nstructure to store the sparse tensor. It leaves a relatively small memory\nfootprint while maintaining high performance. We demonstrate that this\narchitecture is around 20 times faster than the state-of-the-art dense CNN\nmodel on a high-resolution 3D object classification network.\n","authors":["Fangjun Zhou","Anyong Mao","Eftychios Sifakis"],"pdf_url":"https://arxiv.org/pdf/2311.02762v2.pdf","comment":"Unauthorized publication"},{"id":"http://arxiv.org/abs/2311.08673v1","updated":"2023-11-15T03:37:41Z","published":"2023-11-15T03:37:41Z","title":"CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking\n  Embedding","summary":"  This paper proposes a talking face generation method named \"CP-EB\" that takes\nan audio signal as input and a person image as reference, to synthesize a\nphoto-realistic people talking video with head poses controlled by a short\nvideo clip and proper eye blinking embedding. It's noted that not only the head\npose but also eye blinking are both important aspects for deep fake detection.\nThe implicit control of poses by video has already achieved by the state-of-art\nwork. According to recent research, eye blinking has weak correlation with\ninput audio which means eye blinks extraction from audio and generation are\npossible. Hence, we propose a GAN-based architecture to extract eye blink\nfeature from input audio and reference video respectively and employ\ncontrastive training between them, then embed it into the concatenated features\nof identity and poses to generate talking face images. Experimental results\nshow that the proposed method can generate photo-realistic talking face with\nsynchronous lips motions, natural head poses and blinking eyes.\n","authors":["Jianzong Wang","Yimin Deng","Ziqi Liang","Xulong Zhang","Ning Cheng","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2311.08673v1.pdf","comment":"Accepted by the 21st IEEE International Symposium on Parallel and\n  Distributed Processing with Applications (IEEE ISPA 2023)"},{"id":"http://arxiv.org/abs/2305.12437v2","updated":"2023-11-15T02:59:32Z","published":"2023-05-21T11:51:09Z","title":"PLAR: Prompt Learning for Action Recognition","summary":"  We present a new general learning approach, Prompt Learning for Action\nRecognition (PLAR), which leverages the strengths of prompt learning to guide\nthe learning process. Our approach is designed to predict the action label by\nhelping the models focus on the descriptions or instructions associated with\nactions in the input videos. Our formulation uses various prompts, including\nlearnable prompts, auxiliary visual information, and large vision models to\nimprove the recognition performance. In particular, we design a learnable\nprompt method that learns to dynamically generate prompts from a pool of prompt\nexperts under different inputs. By sharing the same objective with the task,\nour proposed PLAR can optimize prompts that guide the model's predictions while\nexplicitly learning input-invariant (prompt experts pool) and input-specific\n(data-dependent) prompt knowledge. We evaluate our approach on datasets\nconsisting of both ground camera videos and aerial videos, and scenes with\nsingle-agent and multi-agent actions. In practice, we observe a 3.17-10.2%\naccuracy improvement on the aerial multi-agent dataset Okutamam and a 1.0-3.6%\nimprovement on the ground camera single-agent dataset Something Something V2.\nWe plan to release our code on the WWW.\n","authors":["Xijun Wang","Ruiqi Xian","Tianrui Guan","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2305.12437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08661v1","updated":"2023-11-15T02:57:59Z","published":"2023-11-15T02:57:59Z","title":"Deep Neural Network Identification of Limnonectes Species and New Class\n  Detection Using Image Data","summary":"  As is true of many complex tasks, the work of discovering, describing, and\nunderstanding the diversity of life on Earth (viz., biological systematics and\ntaxonomy) requires many tools. Some of this work can be accomplished as it has\nbeen done in the past, but some aspects present us with challenges which\ntraditional knowledge and tools cannot adequately resolve. One such challenge\nis presented by species complexes in which the morphological similarities among\nthe group members make it difficult to reliably identify known species and\ndetect new ones. We address this challenge by developing new tools using the\nprinciples of machine learning to resolve two specific questions related to\nspecies complexes. The first question is formulated as a classification problem\nin statistics and machine learning and the second question is an\nout-of-distribution (OOD) detection problem. We apply these tools to a species\ncomplex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex)\nand employ a morphological character (hind limb skin texture) traditionally\ntreated qualitatively in a quantitative and objective manner. We demonstrate\nthat deep neural networks can successfully automate the classification of an\nimage into a known species group for which it has been trained. We further\ndemonstrate that the algorithm can successfully classify an image into a new\nclass if the image does not belong to the existing classes. Additionally, we\nuse the larger MNIST dataset to test the performance of our OOD detection\nalgorithm. We finish our paper with some concluding remarks regarding the\napplication of these methods to species complexes and our efforts to document\ntrue biodiversity. This paper has online supplementary materials.\n","authors":["Li Xu","Yili Hong","Eric P. Smith","David S. McLeod","Xinwei Deng","Laura J. Freeman"],"pdf_url":"https://arxiv.org/pdf/2311.08661v1.pdf","comment":"26 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2311.07955v2","updated":"2023-11-15T02:38:37Z","published":"2023-11-14T07:20:38Z","title":"Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle\n  Imagery: Review and Experimental Comparisons","summary":"  With the advancement of maritime unmanned aerial vehicles (UAVs) and deep\nlearning technologies, the application of UAV-based object detection has become\nincreasingly significant in the fields of maritime industry and ocean\nengineering. Endowed with intelligent sensing capabilities, the maritime UAVs\nenable effective and efficient maritime surveillance. To further promote the\ndevelopment of maritime UAV-based object detection, this paper provides a\ncomprehensive review of challenges, relative methods, and UAV aerial datasets.\nSpecifically, in this work, we first briefly summarize four challenges for\nobject detection on maritime UAVs, i.e., object feature diversity, device\nlimitation, maritime environment variability, and dataset scarcity. We then\nfocus on computational methods to improve maritime UAV-based object detection\nperformance in terms of scale-aware, small object detection, view-aware,\nrotated object detection, lightweight methods, and others. Next, we review the\nUAV aerial image/video datasets and propose a maritime UAV aerial dataset named\nMS2ship for ship detection. Furthermore, we conduct a series of experiments to\npresent the performance evaluation and robustness analysis of object detection\nmethods on maritime datasets. Eventually, we give the discussion and outlook on\nfuture works for maritime UAV-based object detection. The MS2ship dataset is\navailable at\n\\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.\n","authors":["Chenjie Zhao","Ryan Wen Liu","Jingxiang Qu","Ruobin Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07955v2.pdf","comment":"32 pages, 18 figures"},{"id":"http://arxiv.org/abs/2311.08657v1","updated":"2023-11-15T02:33:08Z","published":"2023-11-15T02:33:08Z","title":"ConeQuest: A Benchmark for Cone Segmentation on Mars","summary":"  Over the years, space scientists have collected terabytes of Mars data from\nsatellites and rovers. One important set of features identified in Mars orbital\nimages is pitted cones, which are interpreted to be mud volcanoes believed to\nform in regions that were once saturated in water (i.e., a lake or ocean).\nIdentifying pitted cones globally on Mars would be of great importance, but\nexpert geologists are unable to sort through the massive orbital image archives\nto identify all examples. However, this task is well suited for computer\nvision. Although several computer vision datasets exist for various\nMars-related tasks, there is currently no open-source dataset available for\ncone detection/segmentation. Furthermore, previous studies trained models using\ndata from a single region, which limits their applicability for global\ndetection and mapping. Motivated by this, we introduce ConeQuest, the first\nexpert-annotated public dataset to identify cones on Mars. ConeQuest consists\nof >13k samples from 3 different regions of Mars. We propose two benchmark\ntasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size\nGeneralization. We finetune and evaluate widely-used segmentation models on\nboth benchmark tasks. Results indicate that cone segmentation is a challenging\nopen problem not solved by existing segmentation models, which achieve an\naverage IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and\n(ii), respectively. We believe this new benchmark dataset will facilitate the\ndevelopment of more accurate and robust models for cone segmentation. Data and\ncode are available at https://github.com/kerner-lab/ConeQuest.\n","authors":["Mirali Purohit","Jacob Adler","Hannah Kerner"],"pdf_url":"https://arxiv.org/pdf/2311.08657v1.pdf","comment":"Accepted at WACV 2024"},{"id":"http://arxiv.org/abs/2311.08655v1","updated":"2023-11-15T02:28:52Z","published":"2023-11-15T02:28:52Z","title":"Review of AlexNet for Medical Image Classification","summary":"  In recent years, the rapid development of deep learning has led to a wide\nrange of applications in the field of medical image classification. The\nvariants of neural network models with ever-increasing performance share some\ncommonalities: to try to mitigate overfitting, improve generalization, avoid\ngradient vanishing and exploding, etc. AlexNet first utilizes the dropout\ntechnique to mitigate overfitting and the ReLU activation function to avoid\ngradient vanishing. Therefore, we focus our discussion on AlexNet, which has\ncontributed greatly to the development of CNNs in 2012. After reviewing over 40\npapers, including journal papers and conference papers, we give a narrative on\nthe technical details, advantages, and application areas of AlexNet.\n","authors":["Wenhao Tang","Junding Sun","Shuihua Wang","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08652v1","updated":"2023-11-15T02:26:41Z","published":"2023-11-15T02:26:41Z","title":"Refining Perception Contracts: Case Studies in Vision-based Safe\n  Auto-landing","summary":"  Perception contracts provide a method for evaluating safety of control\nsystems that use machine learning for perception. A perception contract is a\nspecification for testing the ML components, and it gives a method for proving\nend-to-end system-level safety requirements. The feasibility of contract-based\ntesting and assurance was established earlier in the context of straight lane\nkeeping: a 3-dimensional system with relatively simple dynamics. This paper\npresents the analysis of two 6 and 12-dimensional flight control systems that\nuse multi-stage, heterogeneous, ML-enabled perception. The paper advances\nmethodology by introducing an algorithm for constructing data and requirement\nguided refinement of perception contracts (DaRePC). The resulting analysis\nprovides testable contracts which establish the state and environment\nconditions under which an aircraft can safety touchdown on the runway and a\ndrone can safely pass through a sequence of gates. It can also discover\nconditions (e.g., low-horizon sun) that can possibly violate the safety of the\nvision-based control system.\n","authors":["Yangge Li","Benjamin C Yang","Yixuan Jia","Daniel Zhuang","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2311.08652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06507v2","updated":"2023-11-15T02:24:21Z","published":"2023-07-13T01:14:08Z","title":"Improving Nonalcoholic Fatty Liver Disease Classification Performance\n  With Latent Diffusion Models","summary":"  Integrating deep learning with clinical expertise holds great potential for\naddressing healthcare challenges and empowering medical professionals with\nimproved diagnostic tools. However, the need for annotated medical images is\noften an obstacle to leveraging the full power of machine learning models. Our\nresearch demonstrates that by combining synthetic images, generated using\ndiffusion models, with real images, we can enhance nonalcoholic fatty liver\ndisease (NAFLD) classification performance even in low-data regime settings. We\nevaluate the quality of the synthetic images by comparing two metrics:\nInception Score (IS) and Fr\\'{e}chet Inception Distance (FID), computed on\ndiffusion- and generative adversarial network (GAN)-generated images. Our\nresults show superior performance for the diffusion-generated images, with a\nmaximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score\nof $69.45$ compared to $100.05$ for GANs. Utilizing a partially frozen CNN\nbackbone (EfficientNet v1), our synthetic augmentation method achieves a\nmaximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.\n","authors":["Romain Hardy","Joe Klepich","Ryan Mitchell","Steve Hall","Jericho Villareal","Cornelia Ilin"],"pdf_url":"https://arxiv.org/pdf/2307.06507v2.pdf","comment":"36 pages, 13 figures"},{"id":"http://arxiv.org/abs/2311.07042v2","updated":"2023-11-15T02:17:52Z","published":"2023-11-13T02:54:17Z","title":"Open-Vocabulary Video Anomaly Detection","summary":"  Video anomaly detection (VAD) with weak supervision has achieved remarkable\nperformance in utilizing video-level labels to discriminate whether a video\nframe is normal or abnormal. However, current approaches are inherently limited\nto a closed-set setting and may struggle in open-world applications where there\ncan be anomaly categories in the test data unseen during training. A few recent\nstudies attempt to tackle a more realistic setting, open-set VAD, which aims to\ndetect unseen anomalies given seen anomalies and normal videos. However, such a\nsetting focuses on predicting frame anomaly scores, having no ability to\nrecognize the specific categories of anomalies, despite the fact that this\nability is essential for building more informed video surveillance systems.\nThis paper takes a step further and explores open-vocabulary video anomaly\ndetection (OVVAD), in which we aim to leverage pre-trained large models to\ndetect and categorize seen and unseen anomalies. To this end, we propose a\nmodel that decouples OVVAD into two mutually complementary tasks --\nclass-agnostic detection and class-specific classification -- and jointly\noptimizes both tasks. Particularly, we devise a semantic knowledge injection\nmodule to introduce semantic knowledge from large language models for the\ndetection task, and design a novel anomaly synthesis module to generate pseudo\nunseen anomaly videos with the help of large vision generation models for the\nclassification task. These semantic knowledge and synthesis anomalies\nsubstantially extend our model's capability in detecting and categorizing a\nvariety of seen and unseen anomalies. Extensive experiments on three\nwidely-used benchmarks demonstrate our model achieves state-of-the-art\nperformance on OVVAD task.\n","authors":["Peng Wu","Xuerong Zhou","Guansong Pang","Yujia Sun","Jing Liu","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07042v2.pdf","comment":"Submitted"},{"id":"http://arxiv.org/abs/2311.08646v1","updated":"2023-11-15T01:53:46Z","published":"2023-11-15T01:53:46Z","title":"Painterly Image Harmonization via Adversarial Residual Learning","summary":"  Image compositing plays a vital role in photo editing. After inserting a\nforeground object into another background image, the composite image may look\nunnatural and inharmonious. When the foreground is photorealistic and the\nbackground is an artistic painting, painterly image harmonization aims to\ntransfer the style of background painting to the foreground object, which is a\nchallenging task due to the large domain gap between foreground and background.\nIn this work, we employ adversarial learning to bridge the domain gap between\nforeground feature map and background feature map. Specifically, we design a\ndual-encoder generator, in which the residual encoder produces the residual\nfeatures added to the foreground feature map from main encoder. Then, a\npixel-wise discriminator plays against the generator, encouraging the refined\nforeground feature map to be indistinguishable from background feature map.\nExtensive experiments demonstrate that our method could achieve more harmonious\nand visually appealing results than previous methods.\n","authors":["Xudong Wang","Li Niu","Junyan Cao","Yan Hong","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08646v1.pdf","comment":"Accepted by WACV2024"},{"id":"http://arxiv.org/abs/2311.08623v1","updated":"2023-11-15T01:01:02Z","published":"2023-11-15T01:01:02Z","title":"DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder\n  Transformer Models","summary":"  Encoder-decoder transformer models have achieved great success on various\nvision-language (VL) tasks, but they suffer from high inference latency.\nTypically, the decoder takes up most of the latency because of the\nauto-regressive decoding. To accelerate the inference, we propose an approach\nof performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit\nencoder-decoder transformer model which is trained with deep supervision so\nthat each of its decoder layers is capable of generating plausible predictions.\nIn addition, we leverage simple yet practical techniques, including shared\ngeneration head and adaptation modules, to keep accuracy when exiting at\nshallow decoder layers. Based on the multi-exit model, we perform step-level\ndynamic early exit during inference, where the model may decide to use fewer\ndecoder layers based on its confidence of the current layer at each individual\ndecoding step. Considering different number of decoder layers may be used at\ndifferent decoding steps, we compute deeper-layer decoder features of previous\ndecoding steps just-in-time, which ensures the features from different decoding\nsteps are semantically aligned. We evaluate our approach with two\nstate-of-the-art encoder-decoder transformer models on various VL tasks. We\nshow our approach can reduce overall inference latency by 30%-60% with\ncomparable or even higher accuracy compared to baselines.\n","authors":["Peng Tang","Pengkai Zhu","Tian Li","Srikar Appalaraju","Vijay Mahadevan","R. Manmatha"],"pdf_url":"https://arxiv.org/pdf/2311.08623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08622v1","updated":"2023-11-15T01:00:02Z","published":"2023-11-15T01:00:02Z","title":"Multiple-Question Multiple-Answer Text-VQA","summary":"  We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do\ntext-VQA in encoder-decoder transformer models. The text-VQA task requires a\nmodel to answer a question by understanding multi-modal content: text\n(typically from OCR) and an associated image. To the best of our knowledge,\nalmost all previous approaches for text-VQA process a single question and its\nassociated content to predict a single answer. In order to answer multiple\nquestions from the same image, each question and content are fed into the model\nmultiple times. In contrast, our proposed MQMA approach takes multiple\nquestions and content as input at the encoder and predicts multiple answers at\nthe decoder in an auto-regressive manner at the same time. We make several\nnovel architectural modifications to standard encoder-decoder transformers to\nsupport MQMA. We also propose a novel MQMA denoising pre-training task which is\ndesigned to teach the model to align and delineate multiple questions and\ncontent with associated answers. MQMA pre-trained model achieves\nstate-of-the-art results on multiple text-VQA datasets, each with strong\nbaselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),\nDocVQA (+1.1%) absolute improvements over the previous state-of-the-art\napproaches.\n","authors":["Peng Tang","Srikar Appalaraju","R. Manmatha","Yusheng Xie","Vijay Mahadevan"],"pdf_url":"https://arxiv.org/pdf/2311.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10103v2","updated":"2023-11-15T00:14:08Z","published":"2023-08-19T20:18:15Z","title":"ASPIRE: Language-Guided Augmentation for Robust Image Classification","summary":"  Neural image classifiers can often learn to make predictions by overly\nrelying on non-predictive features that are spuriously correlated with the\nclass labels in the training data. This leads to poor performance in real-world\natypical scenarios where such features are absent. Supplementing the training\ndataset with images without such spurious features can aid robust learning\nagainst spurious correlations via better generalization. This paper presents\nASPIRE (Language-guided data Augmentation for SPurIous correlation REmoval), a\nsimple yet effective solution for expanding the training dataset with synthetic\nimages without spurious features. ASPIRE, guided by language, generates these\nimages without requiring any form of additional supervision or existing\nexamples. Precisely, we employ LLMs to first extract foreground and background\nfeatures from textual descriptions of an image, followed by advanced\nlanguage-guided image editing to discover the features that are spuriously\ncorrelated with the class label. Finally, we personalize a text-to-image\ngeneration model to generate diverse in-domain images without spurious\nfeatures. We demonstrate the effectiveness of ASPIRE on 4 datasets, including\nthe very challenging Hard ImageNet dataset, and 9 baselines and show that\nASPIRE improves the classification accuracy of prior methods by 1% - 38%. Code\nsoon at: https://github.com/Sreyan88/ASPIRE.\n","authors":["Sreyan Ghosh","Chandra Kiran Reddy Evuru","Sonal Kumar","Utkarsh Tyagi","Sakshi Singh","Sanjoy Chowdhury","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2308.10103v2.pdf","comment":"Pre-print Under Review"},{"id":"http://arxiv.org/abs/2303.02575v2","updated":"2023-11-15T23:42:49Z","published":"2023-03-05T04:05:17Z","title":"MITFAS: Mutual Information based Temporal Feature Alignment and Sampling\n  for Aerial Video Action Recognition","summary":"  We present a novel approach for action recognition in UAV videos. Our\nformulation is designed to handle occlusion and viewpoint changes caused by the\nmovement of a UAV. We use the concept of mutual information to compute and\nalign the regions corresponding to human action or motion in the temporal\ndomain. This enables our recognition model to learn from the key features\nassociated with the motion. We also propose a novel frame sampling method that\nuses joint mutual information to acquire the most informative frame sequence in\nUAV videos. We have integrated our approach with X3D and evaluated the\nperformance on multiple datasets. In practice, we achieve 18.9% improvement in\nTop-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,\n2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%\nimprovement on NEC Drones(Choi et al., 2020).\n","authors":["Ruiqi Xian","Xijun Wang","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2303.02575v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.10115v5","updated":"2023-11-15T22:57:49Z","published":"2022-02-21T10:57:16Z","title":"An Efficient Smoothing and Thresholding Image Segmentation Framework\n  with Weighted Anisotropic-Isotropic Total Variation","summary":"  In this paper, we design an efficient, multi-stage image segmentation\nframework that incorporates a weighted difference of anisotropic and isotropic\ntotal variation (AITV). The segmentation framework generally consists of two\nstages: smoothing and thresholding, thus referred to as SaT. In the first\nstage, a smoothed image is obtained by an AITV-regularized Mumford-Shah (MS)\nmodel, which can be solved efficiently by the alternating direction method of\nmultipliers (ADMM) with a closed-form solution of a proximal operator of the\n$\\ell_1 -\\alpha \\ell_2$ regularizer. Convergence of the ADMM algorithm is\nanalyzed. In the second stage, we threshold the smoothed image by $K$-means\nclustering to obtain the final segmentation result. Numerical experiments\ndemonstrate that the proposed segmentation framework is versatile for both\ngrayscale and color images, efficient in producing high-quality segmentation\nresults within a few seconds, and robust to input images that are corrupted\nwith noise, blur, or both. We compare the AITV method with its original convex\nTV and nonconvex TV$^p (0<p<1)$ counterparts, showcasing the qualitative and\nquantitative advantages of our proposed method.\n","authors":["Kevin Bui","Yifei Lou","Fredrick Park","Jack Xin"],"pdf_url":"https://arxiv.org/pdf/2202.10115v5.pdf","comment":"final version sent to Springer CAMC"},{"id":"http://arxiv.org/abs/2302.05425v2","updated":"2023-11-15T22:50:11Z","published":"2023-01-27T16:40:54Z","title":"Deep Learning Based Object Tracking in Walking Droplet and Granular\n  Intruder Experiments","summary":"  We present a deep-learning based tracking objects of interest in walking\ndroplet and granular intruder experiments. In a typical walking droplet\nexperiment, a liquid droplet, known as \\textit{walker}, propels itself\nlaterally on the free surface of a vibrating bath of the same liquid. This\nmotion is the result of the interaction between the droplets and the surface\nwaves generated by the droplet itself after each successive bounce. A walker\ncan exhibit a highly irregular trajectory over the course of its motion,\nincluding rapid acceleration and complex interactions with the other walkers\npresent in the same bath. In analogy with the hydrodynamic experiments, the\ngranular matter experiments consist of a vibrating bath of very small solid\nparticles and a larger solid \\textit{intruder}. Like the fluid droplets, the\nintruder interacts with and travels the domain due to the waves of the bath but\ntends to move much slower and much less smoothly than the droplets. When\nmultiple intruders are introduced, they also exhibit complex interactions with\neach other. We leverage the state-of-art object detection model YOLO and the\nHungarian Algorithm to accurately extract the trajectory of a walker or\nintruder in real-time. Our proposed methodology is capable of tracking\nindividual walker(s) or intruder(s) in digital images acquired from a broad\nspectrum of experimental settings and does not suffer from any identity-switch\nissues. Thus, the deep learning approach developed in this work could be used\nto automatize the efficient, fast and accurate extraction of observables of\ninterests in walking droplet and granular flow experiments. Such extraction\ncapabilities are critically enabling for downstream tasks such as building\ndata-driven dynamical models for the coarse-grained dynamics and interactions\nof the objects of interest.\n","authors":["Erdi Kara","George Zhang","Joseph J. Williams","Gonzalo Ferrandez-Quinto","Leviticus J. Rhoden","Maximilian Kim","J. Nathan Kutz","Aminur Rahman"],"pdf_url":"https://arxiv.org/pdf/2302.05425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09424v1","updated":"2023-11-15T22:49:08Z","published":"2023-11-15T22:49:08Z","title":"Predicting Spine Geometry and Scoliosis from DXA Scans","summary":"  Our objective in this paper is to estimate spine curvature in DXA scans. To\nthis end we first train a neural network to predict the middle spine curve in\nthe scan, and then use an integral-based method to determine the curvature\nalong the spine curve. We use the curvature to compare to the standard angle\nscoliosis measure obtained using the DXA Scoliosis Method (DSM). The\nperformance improves over the prior work of Jamaludin et al. 2018. We show that\nthe maximum curvature can be used as a scoring function for ordering the\nseverity of spinal deformation.\n","authors":["Amir Jamaludin","Timor Kadir","Emma Clark","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2311.09424v1.pdf","comment":"CSI@MICCAI 2019 Submission"},{"id":"http://arxiv.org/abs/2311.09402v1","updated":"2023-11-15T21:58:01Z","published":"2023-11-15T21:58:01Z","title":"Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical\n  Imaging Research","summary":"  Chest X-rays (CXR) are the most common medical imaging study and are used to\ndiagnose multiple medical conditions. This study examines the impact of\nsynthetic data supplementation, using diffusion models, on the performance of\ndeep learning (DL) classifiers for CXR analysis. We employed three datasets:\nCheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising\ndiffusion probabilistic models (DDPMs) to generate synthetic frontal\nradiographs. Our approach ensured that synthetic images mirrored the\ndemographic and pathological traits of the original data. Evaluating the\nclassifiers' performance on internal and external datasets revealed that\nsynthetic data supplementation enhances model accuracy, particularly in\ndetecting less prevalent pathologies. Furthermore, models trained on synthetic\ndata alone approached the performance of those trained on real data. This\nsuggests that synthetic data can potentially compensate for real data shortages\nin training robust DL models. However, despite promising outcomes, the\nsuperiority of real data persists.\n","authors":["Bardia Khosravi","Frank Li","Theo Dapamede","Pouria Rouzrokh","Cooper U. Gamble","Hari M. Trivedi","Cody C. Wyles","Andrew B. Sellergren","Saptarshi Purkayastha","Bradley J. Erickson","Judy W. Gichoya"],"pdf_url":"https://arxiv.org/pdf/2311.09402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09401v1","updated":"2023-11-15T21:56:47Z","published":"2023-11-15T21:56:47Z","title":"MoCo-Transfer: Investigating out-of-distribution contrastive learning\n  for limited-data domains","summary":"  Medical imaging data is often siloed within hospitals, limiting the amount of\ndata available for specialized model development. With limited in-domain data,\none might hope to leverage larger datasets from related domains. In this paper,\nwe analyze the benefit of transferring self-supervised contrastive\nrepresentations from moment contrast (MoCo) pretraining on out-of-distribution\ndata to settings with limited data. We consider two X-ray datasets which image\ndifferent parts of the body, and compare transferring from each other to\ntransferring from ImageNet. We find that depending on quantity of labeled and\nunlabeled data, contrastive pretraining on larger out-of-distribution datasets\ncan perform nearly as well or better than MoCo pretraining in-domain, and\npretraining on related domains leads to higher performance than if one were to\nuse the ImageNet pretrained weights. Finally, we provide a preliminary way of\nquantifying similarity between datasets.\n","authors":["Yuwen Chen","Helen Zhou","Zachary C. Lipton"],"pdf_url":"https://arxiv.org/pdf/2311.09401v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 4 pages"},{"id":"http://arxiv.org/abs/2305.18651v4","updated":"2023-11-15T21:51:23Z","published":"2023-05-29T23:06:05Z","title":"UMD: Unsupervised Model Detection for X2X Backdoor Attacks","summary":"  Backdoor (Trojan) attack is a common threat to deep neural networks, where\nsamples from one or more source classes embedded with a backdoor trigger will\nbe misclassified to adversarial target classes. Existing methods for detecting\nwhether a classifier is backdoor attacked are mostly designed for attacks with\na single adversarial target (e.g., all-to-one attack). To the best of our\nknowledge, without supervision, no existing methods can effectively address the\nmore general X2X attack with an arbitrary number of source classes, each paired\nwith an arbitrary target class. In this paper, we propose UMD, the first\nUnsupervised Model Detection method that effectively detects X2X backdoor\nattacks via a joint inference of the adversarial (source, target) class pairs.\nIn particular, we first define a novel transferability statistic to measure and\nselect a subset of putative backdoor class pairs based on a proposed clustering\napproach. Then, these selected class pairs are jointly assessed based on an\naggregation of their reverse-engineered trigger size for detection inference,\nusing a robust and unsupervised anomaly detector we proposed. We conduct\ncomprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show\nthat our unsupervised UMD outperforms SOTA detectors (even with supervision) by\n17%, 4%, and 8%, respectively, in terms of the detection accuracy against\ndiverse X2X attacks. We also show the strong detection performance of UMD\nagainst several strong adaptive attacks.\n","authors":["Zhen Xiang","Zidi Xiong","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2305.18651v4.pdf","comment":"Proceedings of the 40th International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2310.09444v2","updated":"2023-11-15T21:46:50Z","published":"2023-10-13T23:36:48Z","title":"Tackling Heterogeneity in Medical Federated learning via Vision\n  Transformers","summary":"  Optimization-based regularization methods have been effective in addressing\nthe challenges posed by data heterogeneity in medical federated learning,\nparticularly in improving the performance of underrepresented clients. However,\nthese methods often lead to lower overall model accuracy and slower convergence\nrates. In this paper, we demonstrate that using Vision Transformers can\nsubstantially improve the performance of underrepresented clients without a\nsignificant trade-off in overall accuracy. This improvement is attributed to\nthe Vision transformer's ability to capture long-range dependencies within the\ninput data.\n","authors":["Erfan Darzi","Yiqing Shen","Yangming Ou","Nanna M. Sijtsema","P. M. A van Ooijen"],"pdf_url":"https://arxiv.org/pdf/2310.09444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09361v1","updated":"2023-11-15T20:48:26Z","published":"2023-11-15T20:48:26Z","title":"RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination\n  Prior","summary":"  Inverse rendering is an ill-posed problem. Previous work has sought to\nresolve this by focussing on priors for object or scene shape or appearance. In\nthis work, we instead focus on a prior for natural illuminations. Current\nmethods rely on spherical harmonic lighting or other generic representations\nand, at best, a simplistic prior on the parameters. This results in limitations\nfor the inverse setting in terms of the expressivity of the illumination\nconditions, especially when taking specular reflections into account. We\npropose a conditional neural field representation based on a variational\nauto-decoder and a transformer decoder. We extend Vector Neurons to build\nequivariance directly into our architecture, and leveraging insights from depth\nestimation through a scale-invariant loss function, we enable the accurate\nrepresentation of High Dynamic Range (HDR) images. The result is a compact,\nrotation-equivariant HDR neural illumination model capable of capturing\ncomplex, high-frequency features in natural environment maps. Training our\nmodel on a curated dataset of 1.6K HDR environment maps of natural scenes, we\ncompare it against traditional representations, demonstrate its applicability\nfor an inverse rendering task and show environment map completion from partial\nobservations. We share our PyTorch implementation, dataset and trained models\nat https://github.com/JADGardner/ns_reni\n","authors":["James A. D. Gardner","Bernhard Egger","William A. P. Smith"],"pdf_url":"https://arxiv.org/pdf/2311.09361v1.pdf","comment":"Project Repo - https://github.com/JADGardner/ns_reni. arXiv admin\n  note: substantial text overlap with arXiv:2206.03858"},{"id":"http://arxiv.org/abs/2311.09355v1","updated":"2023-11-15T20:31:40Z","published":"2023-11-15T20:31:40Z","title":"Privacy Threats in Stable Diffusion Models","summary":"  This paper introduces a novel approach to membership inference attacks (MIA)\ntargeting stable diffusion computer vision models, specifically focusing on the\nhighly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract\nsensitive information about a model's training data, posing significant privacy\nconcerns. Despite its advancements in image synthesis, our research reveals\nprivacy vulnerabilities in the stable diffusion models' outputs. Exploiting\nthis information, we devise a black-box MIA that only needs to query the victim\nmodel repeatedly. Our methodology involves observing the output of a stable\ndiffusion model at different generative epochs and training a classification\nmodel to distinguish when a series of intermediates originated from a training\nsample or not. We propose numerous ways to measure the membership features and\ndiscuss what works best. The attack's efficacy is assessed using the ROC AUC\nmethod, demonstrating a 60\\% success rate in inferring membership information.\nThis paper contributes to the growing body of research on privacy and security\nin machine learning, highlighting the need for robust defenses against MIAs.\nOur findings prompt a reevaluation of the privacy implications of stable\ndiffusion models, urging practitioners and developers to implement enhanced\nsecurity measures to safeguard against such attacks.\n","authors":["Thomas Cilloni","Charles Fleming","Charles Walter"],"pdf_url":"https://arxiv.org/pdf/2311.09355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09346v1","updated":"2023-11-15T20:09:29Z","published":"2023-11-15T20:09:29Z","title":"Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud\n  Registration Under Large Geometric and Temporal Change","summary":"  Building 3D geometric maps of man-made spaces is a well-established and\nactive field that is fundamental to computer vision and robotics. However,\nconsidering the evolving nature of built environments, it is essential to\nquestion the capabilities of current mapping efforts in handling temporal\nchanges. In addition, spatiotemporal mapping holds significant potential for\nachieving sustainability and circularity goals. Existing mapping approaches\nfocus on small changes, such as object relocation or self-driving car\noperation; in all cases where the main structure of the scene remains fixed.\nConsequently, these approaches fail to address more radical changes in the\nstructure of the built environment, such as geometry and topology. To this end,\nwe introduce the Nothing Stands Still (NSS) benchmark, which focuses on the\nspatiotemporal registration of 3D scenes undergoing large spatial and temporal\nchange, ultimately creating one coherent spatiotemporal map. Specifically, the\nbenchmark involves registering two or more partial 3D point clouds (fragments)\nfrom the same scene but captured from different spatiotemporal views. In\naddition to the standard pairwise registration, we assess the multi-way\nregistration of multiple fragments that belong to any temporal stage. As part\nof NSS, we introduce a dataset of 3D point clouds recurrently captured in\nlarge-scale building indoor environments that are under construction or\nrenovation. The NSS benchmark presents three scenarios of increasing\ndifficulty, to quantify the generalization ability of point cloud registration\nmethods over space (within one building and across buildings) and time. We\nconduct extensive evaluations of state-of-the-art methods on NSS. The results\ndemonstrate the necessity for novel methods specifically designed to handle\nlarge spatiotemporal changes. The homepage of our benchmark is at\nhttp://nothing-stands-still.com.\n","authors":["Tao Sun","Yan Hao","Shengyu Huang","Silvio Savarese","Konrad Schindler","Marc Pollefeys","Iro Armeni"],"pdf_url":"https://arxiv.org/pdf/2311.09346v1.pdf","comment":"27 pages, 29 figures. For the project page, see\n  http://nothing-stands-still.com"},{"id":"http://arxiv.org/abs/2311.09276v1","updated":"2023-11-15T18:49:29Z","published":"2023-11-15T18:49:29Z","title":"Leveraging Citizen Science for Flood Extent Detection using Machine\n  Learning Benchmark Dataset","summary":"  Accurate detection of inundated water extents during flooding events is\ncrucial in emergency response decisions and aids in recovery efforts. Satellite\nRemote Sensing data provides a global framework for detecting flooding extents.\nSpecifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has\nproven to be useful in detecting water bodies due to low backscatter of water\nfeatures in both co-polarized and cross-polarized SAR imagery. However,\nincreased backscatter can be observed in certain flooded regions such as\npresence of infrastructure and trees - rendering simple methods such as pixel\nintensity thresholding and time-series differencing inadequate. Machine\nLearning techniques has been leveraged to precisely capture flood extents in\nflooded areas with bumps in backscatter but needs high amounts of labelled data\nto work desirably. Hence, we created a labeled known water body extent and\nflooded area extents during known flooding events covering about 36,000 sq.\nkilometers of regions within mainland U.S and Bangladesh. Further, We also\nleveraged citizen science by open-sourcing the dataset and hosting an open\ncompetition based on the dataset to rapidly prototype flood extent detection\nusing community generated models. In this paper we present the information\nabout the dataset, the data processing pipeline, a baseline model and the\ndetails about the competition, along with discussion on winning approaches. We\nbelieve the dataset adds to already existing datasets based on Sentinel-1C SAR\ndata and leads to more robust modeling of flood extents. We also hope the\nresults from the competition pushes the research in flood extent detection\nfurther.\n","authors":["Muthukumaran Ramasubramanian","Iksha Gurung","Shubhankar Gahlot","Ronny Hänsch","Andrew L. Molthan","Manil Maskey"],"pdf_url":"https://arxiv.org/pdf/2311.09276v1.pdf","comment":"13 pages in AGU format, 7 figures"},{"id":"http://arxiv.org/abs/2305.17718v2","updated":"2023-11-15T14:57:32Z","published":"2023-05-28T13:16:03Z","title":"FuseCap: Leveraging Large Language Models for Enriched Fused Image\n  Captions","summary":"  The advent of vision-language pre-training techniques enhanced substantial\nprogress in the development of models for image captioning. However, these\nmodels frequently produce generic captions and may omit semantically important\nimage details. This limitation can be traced back to the image-text datasets;\nwhile their captions typically offer a general description of image content,\nthey frequently omit salient details. Considering the magnitude of these\ndatasets, manual reannotation is impractical, emphasizing the need for an\nautomated approach. To address this challenge, we leverage existing captions\nand explore augmenting them with visual details using \"frozen\" vision experts\nincluding an object detector, an attribute recognizer, and an Optical Character\nRecognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such\nvision experts with the original captions using a large language model (LLM),\nyielding comprehensive image descriptions. We automatically curate a training\nset of 12M image-enriched caption pairs. These pairs undergo extensive\nevaluation through both quantitative and qualitative analyses. Subsequently,\nthis data is utilized to train a captioning generation BLIP-based model. This\nmodel outperforms current state-of-the-art approaches, producing more precise\nand detailed descriptions, demonstrating the effectiveness of the proposed\ndata-centric approach. We release this large-scale dataset of enriched\nimage-caption pairs for the community.\n","authors":["Noam Rotstein","David Bensaid","Shaked Brody","Roy Ganz","Ron Kimmel"],"pdf_url":"https://arxiv.org/pdf/2305.17718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05371v2","updated":"2023-11-15T13:57:53Z","published":"2023-11-09T13:55:45Z","title":"Training Robust Deep Physiological Measurement Models with Synthetic\n  Video-based Data","summary":"  Recent advances in supervised deep learning techniques have demonstrated the\npossibility to remotely measure human physiological vital signs (e.g.,\nphotoplethysmograph, heart rate) just from facial videos. However, the\nperformance of these methods heavily relies on the availability and diversity\nof real labeled data. Yet, collecting large-scale real-world data with\nhigh-quality labels is typically challenging and resource intensive, which also\nraises privacy concerns when storing personal bio-metric data. Synthetic\nvideo-based datasets (e.g., SCAMPS \\cite{mcduff2022scamps}) with\nphoto-realistic synthesized avatars are introduced to alleviate the issues\nwhile providing high-quality synthetic data. However, there exists a\nsignificant gap between synthetic and real-world data, which hinders the\ngeneralization of neural models trained on these synthetic datasets. In this\npaper, we proposed several measures to add real-world noise to synthetic\nphysiological signals and corresponding facial videos. We experimented with\nindividual and combined augmentation methods and evaluated our framework on\nthree public real-world datasets. Our results show that we were able to reduce\nthe average MAE from 6.9 to 2.0.\n","authors":["Yuxuan Ou","Yuzhe Zhang","Yuntang Wang","Shwetak Patel","Daniel McDuf","Yuzhe Yang","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.05371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09269v1","updated":"2023-11-15T12:02:57Z","published":"2023-11-15T12:02:57Z","title":"NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios","summary":"  Existing Object Pose Estimation (OPE) methods for stacked scenarios are not\nrobust to changes in object scale. This paper proposes a new 6DoF OPE network\n(NormNet) for different scale objects in stacked scenarios. Specifically, each\nobject's scale is first learned with point-wise regression. Then, all objects\nin the stacked scenario are normalized into the same scale through semantic\nsegmentation and affine transformation. Finally, they are fed into a shared\npose estimator to recover their 6D poses. In addition, we introduce a new\nSim-to-Real transfer pipeline, combining style transfer and domain\nrandomization. This improves the NormNet's performance on real data even if we\nonly train it on synthetic data. Extensive experiments demonstrate that the\nproposed method achieves state-of-the-art performance on public benchmarks and\nthe MultiScale dataset we constructed. The real-world experiments show that our\nmethod can robustly estimate the 6D pose of objects at different scales.\n","authors":["En-Te Lin","Wei-Jie Lv","Ding-Tao Huang","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2311.09269v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.09180v1","updated":"2023-11-15T18:19:58Z","published":"2023-11-15T18:19:58Z","title":"PEARL: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers","summary":"  Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style\nand specialized knowledge. In this paper, we address this challenge by\nproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\na generation-calibrated retriever. Our retriever is trained to select historic\nuser-authored documents for prompt augmentation, such that they are likely to\nbest personalize LLM generations for a user request. We propose two key\nnovelties for training our retriever: 1) A training data selection method that\nidentifies user requests likely to benefit from personalization and documents\nthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\nthat ensures that our retriever closely tracks the benefit of a document for\npersonalized generation. We demonstrate the effectiveness of PEARL in\ngenerating personalized workplace social media posts and Reddit comments.\nFinally, we showcase the potential of a generation-calibrated retriever to\ndouble as a performance predictor and further improve low-quality generations\nvia LLM chaining.\n","authors":["Sheshera Mysore","Zhuoran Lu","Mengting Wan","Longqi Yang","Steve Menezes","Tina Baghaee","Emmanuel Barajas Gonzalez","Jennifer Neville","Tara Safavi"],"pdf_url":"https://arxiv.org/pdf/2311.09180v1.pdf","comment":"Pre-print, work in progress"},{"id":"http://arxiv.org/abs/2311.09175v1","updated":"2023-11-15T18:11:41Z","published":"2023-11-15T18:11:41Z","title":"Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword\n  Generation for Zero-Shot Neural Rankers","summary":"  Query expansion has been proved to be effective in improving recall and\nprecision of first-stage retrievers, and yet its influence on a complicated,\nstate-of-the-art cross-encoder ranker remains under-explored. We first show\nthat directly applying the expansion techniques in the current literature to\nstate-of-the-art neural rankers can result in deteriorated zero-shot\nperformance. To this end, we propose GFF, a pipeline that includes a large\nlanguage model and a neural ranker, to Generate, Filter, and Fuse query\nexpansions more effectively in order to improve the zero-shot ranking metrics\nsuch as nDCG@10. Specifically, GFF first calls an instruction-following\nlanguage model to generate query-related keywords through a reasoning chain.\nLeveraging self-consistency and reciprocal rank weighting, GFF further filters\nand combines the ranking results of each expanded query dynamically. By\nutilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on\nBEIR and TREC DL 2019/2020. We also analyze different modelling choices in the\nGFF pipeline and shed light on the future directions in query expansion for\nzero-shot neural rankers.\n","authors":["Minghan Li","Honglei Zhuang","Kai Hui","Zhen Qin","Jimmy Lin","Rolf Jagerman","Xuanhui Wang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2311.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09134v1","updated":"2023-11-15T17:26:28Z","published":"2023-11-15T17:26:28Z","title":"Scalable and Effective Generative Information Retrieval","summary":"  Recent research has shown that transformer networks can be used as\ndifferentiable search indexes by representing each document as a sequences of\ndocument ID tokens. These generative retrieval models cast the retrieval\nproblem to a document ID generation problem for each given query. Despite their\nelegant design, existing generative retrieval models only perform well on\nartificially-constructed and small-scale collections. This has led to serious\nskepticism in the research community on their real-world impact. This paper\nrepresents an important milestone in generative retrieval research by showing,\nfor the first time, that generative retrieval models can be trained to perform\neffectively on large-scale standard retrieval benchmarks. For doing so, we\npropose RIPOR- an optimization framework for generative retrieval that can be\nadopted by any encoder-decoder architecture. RIPOR is designed based on two\noften-overlooked fundamental design considerations in generative retrieval.\nFirst, given the sequential decoding nature of document ID generation,\nassigning accurate relevance scores to documents based on the whole document ID\nsequence is not sufficient. To address this issue, RIPOR introduces a novel\nprefix-oriented ranking optimization algorithm. Second, initial document IDs\nshould be constructed based on relevance associations between queries and\ndocuments, instead of the syntactic and semantic information in the documents.\nRIPOR addresses this issue using a relevance-based document ID construction\napproach that quantizes relevance-based representations learned for documents.\nEvaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses\nstate-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR\nimprovements on MS MARCO Dev Set), and perform better on par with popular dense\nretrieval models.\n","authors":["Hansi Zeng","Chen Luo","Bowen Jin","Sheikh Muhammad Sarwar","Tianxin Wei","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2311.09134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09133v1","updated":"2023-11-15T17:24:56Z","published":"2023-11-15T17:24:56Z","title":"Explainable Text Classification Techniques in Legal Document Review:\n  Locating Rationales without Using Human Annotated Training Text Snippets","summary":"  US corporations regularly spend millions of dollars reviewing\nelectronically-stored documents in legal matters. Recently, attorneys apply\ntext classification to efficiently cull massive volumes of data to identify\nresponsive documents for use in these matters. While text classification is\nregularly used to reduce the discovery costs of legal matters, it also faces a\nperception challenge: amongst lawyers, this technology is sometimes looked upon\nas a \"black box\". Put simply, no extra information is provided for attorneys to\nunderstand why documents are classified as responsive. In recent years,\nexplainable machine learning has emerged as an active research area. In an\nexplainable machine learning system, predictions or decisions made by a machine\nlearning model are human understandable. In legal 'document review' scenarios,\na document is responsive, because one or more of its small text snippets are\ndeemed responsive. In these scenarios, if these responsive snippets can be\nlocated, then attorneys could easily evaluate the model's document\nclassification decisions - this is especially important in the field of\nresponsible AI. Our prior research identified that predictive models created\nusing annotated training text snippets improved the precision of a model when\ncompared to a model created using all of a set of documents' text as training.\nWhile interesting, manually annotating training text snippets is not generally\npractical during a legal document review. However, small increases in precision\ncan drastically decrease the cost of large document reviews. Automating the\nidentification of training text snippets without human review could then make\nthe application of training text snippet-based models a practical approach.\n","authors":["Christian Mahoney","Peter Gronvall","Nathaniel Huber-Fliflet","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09133v1.pdf","comment":"arXiv admin note: text overlap with arXiv:1912.09501"},{"id":"http://arxiv.org/abs/2311.09101v1","updated":"2023-11-15T16:47:57Z","published":"2023-11-15T16:47:57Z","title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning","summary":"  Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. Usually,\nanswer calibration strategies such as step-level or path-level calibration play\na vital role in multi-step reasoning. While effective, there remains a\nsignificant gap in our understanding of the key factors that drive their\nsuccess. In this paper, we break down the design of recent answer calibration\nstrategies and present a unified view which establishes connections between\nthem. We then conduct a thorough evaluation on these strategies from a unified\nview, systematically scrutinizing step-level and path-level answer calibration\nacross multiple paths. Our study holds the potential to illuminate key insights\nfor optimizing multi-step reasoning with answer calibration.\n","authors":["Shumin Deng","Ningyu Zhang","Nay Oo","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2311.09101v1.pdf","comment":"Working in Progress"},{"id":"http://arxiv.org/abs/2311.09049v1","updated":"2023-11-15T15:39:33Z","published":"2023-11-15T15:39:33Z","title":"Adapting Large Language Models by Integrating Collaborative Semantics\n  for Recommendation","summary":"  Recently, large language models (LLMs) have shown great potential in\nrecommender systems, either improving existing recommendation models or serving\nas the backbone. However, there exists a large semantic gap between LLMs and\nrecommender systems, since items to be recommended are often indexed by\ndiscrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs\ncapture language semantics while recommender systems imply collaborative\nsemantics, making it difficult to sufficiently leverage the model capacity of\nLLMs for recommendation. To address this challenge, in this paper, we propose a\nnew LLM-based recommendation model called LC-Rec, which can better integrate\nlanguage and collaborative semantics for recommender systems. Our approach can\ndirectly generate items from the entire item set for recommendation, without\nrelying on candidate items. Specifically, we make two major contributions in\nour approach. For item indexing, we design a learning-based vector quantization\nmethod with uniform semantic mapping, which can assign meaningful and\nnon-conflicting IDs (called item indices) for items. For alignment tuning, we\npropose a series of specially designed tuning tasks to enhance the integration\nof collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to\ndeeply integrate language and collaborative semantics (characterized by the\nlearned item indices), so as to achieve an effective adaptation to recommender\nsystems. Extensive experiments demonstrate the effectiveness of our method,\nshowing that our approach can outperform a number of competitive baselines\nincluding traditional recommenders and existing LLM-based recommenders. Our\ncode is available at https://github.com/RUCAIBox/LC-Rec/.\n","authors":["Bowen Zheng","Yupeng Hou","Hongyu Lu","Yu Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2311.09049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00423v3","updated":"2023-11-15T15:16:24Z","published":"2023-11-01T10:27:44Z","title":"LLMRec: Large Language Models with Graph Augmentation for Recommendation","summary":"  The problem of data sparsity has long been a challenge in recommendation\nsystems, and previous studies have attempted to address this issue by\nincorporating side information. However, this approach often introduces side\neffects such as noise, availability issues, and low data quality, which in turn\nhinder the accurate modeling of user preferences and adversely impact\nrecommendation performance. In light of the recent advancements in large\nlanguage models (LLMs), which possess extensive knowledge bases and strong\nreasoning capabilities, we propose a novel framework called LLMRec that\nenhances recommender systems by employing three simple yet effective LLM-based\ngraph augmentation strategies. Our approach leverages the rich content\navailable within online platforms (e.g., Netflix, MovieLens) to augment the\ninteraction graph in three ways: (i) reinforcing user-item interaction egde,\n(ii) enhancing the understanding of item node attributes, and (iii) conducting\nuser node profiling, intuitively from the natural language perspective. By\nemploying these strategies, we address the challenges posed by sparse implicit\nfeedback and low-quality side information in recommenders. Besides, to ensure\nthe quality of the augmentation, we develop a denoised data robustification\nmechanism that includes techniques of noisy implicit feedback pruning and\nMAE-based feature enhancement that help refine the augmented data and improve\nits reliability. Furthermore, we provide theoretical analysis to support the\neffectiveness of LLMRec and clarify the benefits of our method in facilitating\nmodel optimization. Experimental results on benchmark datasets demonstrate the\nsuperiority of our LLM-based augmentation approach over state-of-the-art\ntechniques. To ensure reproducibility, we have made our code and augmented data\npublicly available at: https://github.com/HKUDS/LLMRec.git\n","authors":["Wei Wei","Xubin Ren","Jiabin Tang","Qinyong Wang","Lixin Su","Suqi Cheng","Junfeng Wang","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2311.00423v3.pdf","comment":"WSDM 2024 Oral Presentation"},{"id":"http://arxiv.org/abs/2307.15053v2","updated":"2023-11-15T14:46:14Z","published":"2023-07-27T17:57:42Z","title":"On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation\n  Metric for Top-$n$ Recommendation","summary":"  Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.\n","authors":["Olivier Jeunen","Ivan Potapov","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2307.15053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10405v6","updated":"2023-11-15T14:00:17Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hyper network to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Ningyu Zhang","Bozhong Tian","Xi Chen","Qingbing Liu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v6.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2305.08703v4","updated":"2023-11-15T12:55:56Z","published":"2023-05-15T15:06:20Z","title":"Schema-adaptable Knowledge Graph Construction","summary":"  Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n","authors":["Hongbin Ye","Honghao Gui","Xin Xu","Xi Chen","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08703v4.pdf","comment":"EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2311.07861v2","updated":"2023-11-15T12:22:35Z","published":"2023-11-14T02:25:18Z","title":"Overview of the TREC 2023 Product Product Search Track","summary":"  This is the first year of the TREC Product search track. The focus this year\nwas the creation of a reusable collection and evaluation of the impact of the\nuse of metadata and multi-modal data on retrieval accuracy. This year we\nleverage the new product search corpus, which includes contextual metadata. Our\nanalysis shows that in the product search domain, traditional retrieval systems\nare highly effective and commonly outperform general-purpose pretrained\nembedding models. Our analysis also evaluates the impact of using simplified\nand metadata-enhanced collections, finding no clear trend in the impact of the\nexpanded collection. We also see some surprising outcomes; despite their\nwidespread adoption and competitive performance on other tasks, we find\nsingle-stage dense retrieval runs can commonly be noncompetitive or generate\nlow-quality results both in the zero-shot and fine-tuned domain.\n","authors":["Daniel Campos","Surya Kallumadi","Corby Rosset","Cheng Xiang Zhai","Alessandro Magnani"],"pdf_url":"https://arxiv.org/pdf/2311.07861v2.pdf","comment":"14 pages, 4 figures, 11 tables - TREC 2023"},{"id":"http://arxiv.org/abs/2305.13062v3","updated":"2023-11-15T12:18:39Z","published":"2023-05-22T14:23:46Z","title":"GPT4Table: Can Large Language Models Understand Structured Table Data? A\n  Benchmark and Empirical Study","summary":"  Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve Natural Language (NL)-related tasks. However, there is still much to\nlearn about how well LLMs understand structured data, such as tables. While it\nis true that tables can be used as inputs to LLMs with serialization, there is\na lack of comprehensive studies examining whether LLMs can truly comprehend\nsuch data. In this paper, we try to understand this by designing a benchmark to\nevaluate the structural understanding capabilities (SUC) of LLMs. The benchmark\nwe create includes seven tasks, each with its own unique challenges, \\eg, cell\nlookup, row retrieval, and size detection. We conduct a series of evaluations\non GPT-3.5 and GPT-4. We find that the performance varied depending on several\ninput choices, including table input format, content order, role prompting, and\npartition marks. Drawing from the insights gained through the benchmark\nevaluations, we propose \\textit{self-augmentation} for effective structural\nprompting, such as critical value / range identification using LLMs' internal\nknowledge. When combined with carefully chosen input choices, these structural\nprompting methods lead to promising improvements in LLM performance on a\nvariety of tabular tasks, \\eg, TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe that our benchmark and proposed\nprompting methods can serve as a simple yet generic selection for future\nresearch.\n","authors":["Yuan Sui","Mengyu Zhou","Mingjie Zhou","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13062v3.pdf","comment":"This paper has been accepted as a full paper at WSDM 2024"},{"id":"http://arxiv.org/abs/2105.14201v2","updated":"2023-11-15T09:52:12Z","published":"2021-05-29T03:47:10Z","title":"CNTLS: A Benchmark Dataset for Abstractive or Extractive Chinese\n  Timeline Summarization","summary":"  Timeline summarization (TLS) involves creating summaries of long-running\nevents using dated summaries from numerous news articles. However, limited data\navailability has significantly slowed down the development of timeline\nsummarization. In this paper, we introduce the CNTLS dataset, a versatile\nresource for Chinese timeline summarization. CNTLS encompasses 77 real-life\ntopics, each with 2524 documents and summarizes nearly 60\\% days duration\ncompression on average all topics.\n  We meticulously analyze the corpus using well-known metrics, focusing on the\nstyle of the summaries and the complexity of the summarization task.\nSpecifically, we evaluate the performance of various extractive and generative\nsummarization systems on the CNTLS corpus to provide benchmarks and support\nfurther research. To the best of our knowledge, CNTLS is the first Chinese\ntimeline summarization dataset. The dataset and source code are\nreleased\\footnote{Code and data available at:\n\\emph{\\url{https://github.com/OpenSUM/CNTLS}}.}.\n","authors":["Qianren Mao","Jiazheng Wang","Zheng Wang","Xi Li","Bo Li","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2105.14201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08744v1","updated":"2023-11-15T07:25:14Z","published":"2023-11-15T07:25:14Z","title":"Towards Graph-Aware Diffusion Modeling for Collaborative Filtering","summary":"  Recovering masked feedback with neural models is a popular paradigm in\nrecommender systems. Seeing the success of diffusion models in solving\nill-posed inverse problems, we introduce a conditional diffusion framework for\ncollaborative filtering that iteratively reconstructs a user's hidden\npreferences guided by its historical interactions. To better align with the\nintrinsic characteristics of implicit feedback data, we implement forward\ndiffusion by applying synthetic smoothing filters to interaction signals on an\nitem-item graph. The resulting reverse diffusion can be interpreted as a\npersonalized process that gradually refines preference scores. Through graph\nFourier transform, we equivalently characterize this model as an anisotropic\nGaussian diffusion in the graph spectral domain, establishing both forward and\nreverse formulations. Our model outperforms state-of-the-art methods by a large\nmargin on one dataset and yields competitive results on the others.\n","authors":["Yunqin Zhu","Chao Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08682v1","updated":"2023-11-15T04:15:58Z","published":"2023-11-15T04:15:58Z","title":"Enhancing Recommender System Performance by Histogram Equalization","summary":"  Recommender system has been researched for decades with millions of different\nversions of algorithms created in the industry. In spite of the huge amount of\nwork spent on the field, there are many basic questions to be answered in the\nfield. The most fundamental question to be answered is the accuracy problem,\nand in recent years, fairness becomes the new buzz word for researchers. In\nthis paper, we borrow an idea from image processing, namely, histogram\nequalization. As a preprocessing step to recommender system algorithms,\nhistogram equalization could enhance both the accuracy and fairness metrics of\nthe recommender system algorithms. In the experiment section, we prove that our\nnew approach could improve vanilla algorithms by a large margin in accuracy\nmetric and stay competitive on fairness metrics.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20091v2","updated":"2023-11-15T03:26:47Z","published":"2023-10-31T00:12:13Z","title":"Density-based User Representation through Gaussian Process Regression\n  for Multi-interest Personalized Retrieval","summary":"  Accurate modeling of the diverse and dynamic interests of users remains a\nsignificant challenge in the design of personalized recommender systems.\nExisting user modeling methods, like single-point and multi-point\nrepresentations, have limitations w.r.t. accuracy, diversity, computational\ncost, and adaptability. To overcome these deficiencies, we introduce\ndensity-based user representations (DURs), a novel model that leverages\nGaussian process regression for effective multi-interest recommendation and\nretrieval. Our approach, GPR4DUR, exploits DURs to capture user interest\nvariability without manual tuning, incorporates uncertainty-awareness, and\nscales well to large numbers of users. Experiments using real-world offline\ndatasets confirm the adaptability and efficiency of GPR4DUR, while online\nexperiments with simulated users demonstrate its ability to address the\nexploration-exploitation trade-off by effectively utilizing model uncertainty.\n","authors":["Haolun Wu","Ofer Meshi","Masrour Zoghi","Fernando Diaz","Xue Liu","Craig Boutilier","Maryam Karimzadehgan"],"pdf_url":"https://arxiv.org/pdf/2310.20091v2.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.08662v1","updated":"2023-11-15T02:59:10Z","published":"2023-11-15T02:59:10Z","title":"Multi-Set Inoculation: Assessing Model Robustness Across Multiple\n  Challenge Sets","summary":"  Language models, given their black-box nature, often exhibit sensitivity to\ninput perturbations, leading to trust issues due to hallucinations. To bolster\ntrust, it's essential to understand these models' failure modes and devise\nstrategies to enhance their performance. In this study, we propose a framework\nto study the effect of input perturbations on language models of different\nscales, from pre-trained models to large language models (LLMs). We use\nfine-tuning to train a robust model to perturbations, and we investigate\nwhether exposure to one perturbation improves or degrades the model's\nperformance on other perturbations. To address multi-perturbation robustness,\nwe suggest three distinct training strategies. We also extend the framework to\nLLMs via a chain of thought(COT) prompting with exemplars. We instantiate our\nframework for the Tabular-NLI task and show that the proposed strategies train\nthe model robust to different perturbations without losing accuracy on a given\ndataset.\n","authors":["Vatsal Gupta","Pranshu Pandya","Tushar Kataria","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2311.08662v1.pdf","comment":"13 pages, 2 Figure, 12 Tables"},{"id":"http://arxiv.org/abs/2311.07878v2","updated":"2023-11-15T01:55:29Z","published":"2023-11-14T03:15:48Z","title":"Evaluating LLMs on Document-Based QA: Exact Answer Selection and\n  Numerical Extraction using Cogtale dataset","summary":"  Document-based Question-Answering (QA) tasks are crucial for precise\ninformation retrieval. While some existing work focus on evaluating large\nlanguage model's performance on retrieving and answering questions from\ndocuments, assessing the LLMs' performance on QA types that require exact\nanswer selection from predefined options and numerical extraction is yet to be\nfully assessed. In this paper, we specifically focus on this underexplored\ncontext and conduct empirical analysis of LLMs (GPT-4 and GPT 3.5) on question\ntypes, including single-choice, yes-no, multiple-choice, and number extraction\nquestions from documents. We use the Cogtale dataset for evaluation, which\nprovide human expert-tagged responses, offering a robust benchmark for\nprecision and factual grounding. We found that LLMs, particularly GPT-4, can\nprecisely answer many single-choice and yes-no questions given relevant\ncontext, demonstrating their efficacy in information retrieval tasks. However,\ntheir performance diminishes when confronted with multiple-choice and number\nextraction formats, lowering the overall performance of the model on this task,\nindicating that these models may not be reliable for the task. This limits the\napplications of LLMs on applications demanding precise information extraction\nfrom documents, such as meta-analysis tasks. However, these findings hinge on\nthe assumption that the retrievers furnish pertinent context necessary for\naccurate responses, emphasizing the need for further research on the efficacy\nof retriever mechanisms in enhancing question-answering performance. Our work\noffers a framework for ongoing dataset evaluation, ensuring that LLM\napplications for information retrieval and document analysis continue to meet\nevolving standards.\n","authors":["Zafaryab Rasool","Scott Barnett","Stefanus Kurniawan","Sherwin Balugo","Rajesh Vasa","Courtney Chesser","Alex Bahar-Fuchs"],"pdf_url":"https://arxiv.org/pdf/2311.07878v2.pdf","comment":"14 pages, 1 figure, 8 tables"},{"id":"http://arxiv.org/abs/2305.12102v3","updated":"2023-11-15T00:22:44Z","published":"2023-05-20T05:35:40Z","title":"Unified Embedding: Battle-Tested Feature Representations for Web-Scale\n  ML Systems","summary":"  Learning high-quality feature embeddings efficiently and effectively is\ncritical for the performance of web-scale machine learning systems. A typical\nmodel ingests hundreds of features with vocabularies on the order of millions\nto billions of tokens. The standard approach is to represent each feature value\nas a d-dimensional embedding, introducing hundreds of billions of parameters\nfor extremely high-cardinality features. This bottleneck has led to substantial\nprogress in alternative embedding algorithms. Many of these methods, however,\nmake the assumption that each feature uses an independent embedding table. This\nwork introduces a simple yet highly effective framework, Feature Multiplexing,\nwhere one single representation space is used across many different categorical\nfeatures. Our theoretical and empirical analysis reveals that multiplexed\nembeddings can be decomposed into components from each constituent feature,\nallowing models to distinguish between features. We show that multiplexed\nrepresentations lead to Pareto-optimal parameter-accuracy tradeoffs for three\npublic benchmark datasets. Further, we propose a highly practical approach\ncalled Unified Embedding with three major benefits: simplified feature\nconfiguration, strong adaptation to dynamic data distributions, and\ncompatibility with modern hardware. Unified embedding gives significant\nimprovements in offline and online metrics compared to highly competitive\nbaselines across five web-scale search, ads, and recommender systems, where it\nserves billions of users across the world in industry-leading products.\n","authors":["Benjamin Coleman","Wang-Cheng Kang","Matthew Fahrbach","Ruoxi Wang","Lichan Hong","Ed H. Chi","Derek Zhiyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2305.12102v3.pdf","comment":"NeurIPS'23 Spotlight"},{"id":"http://arxiv.org/abs/2311.09438v1","updated":"2023-11-15T23:18:01Z","published":"2023-11-15T23:18:01Z","title":"Labeled Interactive Topic Models","summary":"  Topic models help users understand large document collections; however, topic\nmodels do not always find the ``right'' topics. While classical probabilistic\nand anchor-based topic models have interactive variants to guide models toward\nbetter topics, such interactions are not available for neural topic models such\nas the embedded topic model (\\abr{etm}). We correct this lacuna by adding an\nintuitive interaction to neural topic models: users can label a topic with a\nword, and topics are updated so that the topic words are close to the label.\nThis allows a user to refine topics based on their information need. While,\ninteractivity is intuitive for \\abr{etm}, we extend this framework to work with\nother neural topic models as well. We develop an interactive interface which\nallows users to interact and relabel topic models as they see fit. We evaluate\nour method through a human study, where users can relabel topics to find\nrelevant documents. Using our method, user labeling improves document rank\nscores, helping to find more relevant documents to a given query when compared\nto no user labeling.\n","authors":["Kyle Seelman","Mozhi Zhang","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2311.09438v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2210.00805v4","updated":"2023-11-15T18:56:59Z","published":"2022-10-03T10:34:38Z","title":"Limitations of neural network training due to numerical instability of\n  backpropagation","summary":"  We study the training of deep neural networks by gradient descent where\nfloating-point arithmetic is used to compute the gradients. In this framework\nand under realistic assumptions, we demonstrate that it is highly unlikely to\nfind ReLU neural networks that maintain, in the course of training with\ngradient descent, superlinearly many affine pieces with respect to their number\nof layers. In virtually all approximation theoretical arguments that yield\nhigh-order polynomial rates of approximation, sequences of ReLU neural networks\nwith exponentially many affine pieces compared to their numbers of layers are\nused. As a consequence, we conclude that approximating sequences of ReLU neural\nnetworks resulting from gradient descent in practice differ substantially from\ntheoretically constructed sequences. The assumptions and the theoretical\nresults are compared to a numerical study, which yields concurring results.\n","authors":["Clemens Karner","Vladimir Kazeev","Philipp Christian Petersen"],"pdf_url":"https://arxiv.org/pdf/2210.00805v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09215v1","updated":"2023-11-15T18:56:51Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09200v1","updated":"2023-11-15T18:43:29Z","published":"2023-11-15T18:43:29Z","title":"ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD","summary":"  In this pioneering work we formulate ExpM+NF, a method for training machine\nlearning (ML) on private data with pre-specified differentially privacy\nguarantee $\\varepsilon>0, \\delta=0$, by using the Exponential Mechanism (ExpM)\nand an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of\nExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the\nstate-of-the-art (SOTA) and de facto method for differentially private ML, and\nwe empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus\nwith PRV accounting) in multiple classification tasks on the Adult Dataset\n(census data) and MIMIC-III Dataset (electronic healthcare records) using\nLogistic Regression and GRU-D, a deep learning recurrent neural network with\n~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of\nthe non-private training accuracy (AUC) for $\\varepsilon \\in [1\\mathrm{e}{-3},\n1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\\varepsilon$\nwith $\\delta=0$) than DPSGD. Differentially private ML generally considers\n$\\varepsilon \\in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's\nability to provide strong accuracy for orders of magnitude better privacy\n(smaller $\\varepsilon$) substantially pushes what is currently possible in\ndifferentially private ML. Training time results are presented showing ExpM+NF\nis comparable to (slightly faster) than DPSGD. Code for these experiments will\nbe provided after review. Limitations and future directions are provided.\n","authors":["Robert A. Bridges","Vandy J. Tombs","Christopher B. Stanley"],"pdf_url":"https://arxiv.org/pdf/2311.09200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09197v1","updated":"2023-11-15T18:41:19Z","published":"2023-11-15T18:41:19Z","title":"A Unified Approach to Learning Ising Models: Beyond Independence and\n  Bounded Width","summary":"  We revisit the problem of efficiently learning the underlying parameters of\nIsing models from data. Current algorithmic approaches achieve essentially\noptimal sample complexity when given i.i.d. samples from the stationary measure\nand the underlying model satisfies \"width\" bounds on the total $\\ell_1$\ninteraction involving each node. We show that a simple existing approach based\non node-wise logistic regression provably succeeds at recovering the underlying\nmodel in several new settings where these assumptions are violated:\n  (1) Given dynamically generated data from a wide variety of local Markov\nchains, like block or round-robin dynamics, logistic regression recovers the\nparameters with optimal sample complexity up to $\\log\\log n$ factors. This\ngeneralizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEE\nTrans. Inf. Theory'18] for structure recovery in bounded degree graphs from\nGlauber dynamics.\n  (2) For the Sherrington-Kirkpatrick model of spin glasses, given\n$\\mathsf{poly}(n)$ independent samples, logistic regression recovers the\nparameters in most of the known high-temperature regime via a simple reduction\nto weaker structural properties of the measure. This improves on recent work of\nAnari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distribution\nlearning at higher temperature.\n  (3) As a simple byproduct of our techniques, logistic regression achieves an\nexponential improvement in learning from samples in the M-regime of data\nconsidered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novel\nguarantees for learning from the adversarial Glauber dynamics of Chin, Moitra,\nMossel, and Sandon [ArXiv'23].\n  Our approach thus significantly generalizes the elegant analysis of Wu,\nSanghavi, and Dimakis [Neurips'19] without any algorithmic modification.\n","authors":["Jason Gaitonde","Elchanan Mossel"],"pdf_url":"https://arxiv.org/pdf/2311.09197v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2311.09195v1","updated":"2023-11-15T18:40:10Z","published":"2023-11-15T18:40:10Z","title":"Self-Supervised Curriculum Generation for Autonomous Reinforcement\n  Learning without Task-Specific Knowledge","summary":"  A significant bottleneck in applying current reinforcement learning\nalgorithms to real-world scenarios is the need to reset the environment between\nevery episode. This reset process demands substantial human intervention,\nmaking it difficult for the agent to learn continuously and autonomously.\nSeveral recent works have introduced autonomous reinforcement learning (ARL)\nalgorithms that generate curricula for jointly training reset and forward\npolicies. While their curricula can reduce the number of required manual resets\nby taking into account the agent's learning progress, they rely on\ntask-specific knowledge, such as predefined initial states or reset reward\nfunctions. In this paper, we propose a novel ARL algorithm that can generate a\ncurriculum adaptive to the agent's learning progress without task-specific\nknowledge. Our curriculum empowers the agent to autonomously reset to diverse\nand informative initial states. To achieve this, we introduce a success\ndiscriminator that estimates the success probability from each initial state\nwhen the agent follows the forward policy. The success discriminator is trained\nwith relabeled transitions in a self-supervised manner. Our experimental\nresults demonstrate that our ARL algorithm can generate an adaptive curriculum\nand enable the agent to efficiently bootstrap to solve sparse-reward maze\nnavigation tasks, outperforming baselines with significantly fewer manual\nresets.\n","authors":["Sang-Hyun Lee","Seung-Woo Seo"],"pdf_url":"https://arxiv.org/pdf/2311.09195v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09190v1","updated":"2023-11-15T18:34:03Z","published":"2023-11-15T18:34:03Z","title":"On the Computation of the Gaussian Rate-Distortion-Perception Function","summary":"  In this paper, we study the computation of the rate-distortion-perception\nfunction (RDPF) for a multivariate Gaussian source under mean squared error\n(MSE) distortion and, respectively, Kullback-Leibler divergence, geometric\nJensen-Shannon divergence, squared Hellinger distance, and squared\nWasserstein-2 distance perception metrics. To this end, we first characterize\nthe analytical bounds of the scalar Gaussian RDPF for the aforementioned\ndivergence functions, also providing the RDPF-achieving forward \"test-channel\"\nrealization. Focusing on the multivariate case, we establish that, for\ntensorizable distortion and perception metrics, the optimal solution resides on\nthe vector space spanned by the eigenvector of the source covariance matrix.\nConsequently, the multivariate optimization problem can be expressed as a\nfunction of the scalar Gaussian RDPFs of the source marginals, constrained by\nglobal distortion and perception levels. Leveraging this characterization, we\ndesign an alternating minimization scheme based on the block nonlinear\nGauss-Seidel method, which optimally solves the problem while identifying the\nGaussian RDPF-achieving realization. Furthermore, the associated algorithmic\nembodiment is provided, as well as the convergence and the rate of convergence\ncharacterization. Lastly, for the \"perfect realism\" regime, the analytical\nsolution for the multivariate Gaussian RDPF is obtained. We corroborate our\nresults with numerical simulations and draw connections to existing results.\n","authors":["Giuseppe Serra","Photios A. Stavrou","Marios Kountouris"],"pdf_url":"https://arxiv.org/pdf/2311.09190v1.pdf","comment":"This paper has been submitted for journal publication"},{"id":"http://arxiv.org/abs/2311.08379v2","updated":"2023-11-15T18:30:24Z","published":"2023-11-14T18:42:40Z","title":"Scheming AIs: Will AIs fake alignment during training in order to get\n  power?","summary":"  This report examines whether advanced AIs that perform well in training will\nbe doing so in order to gain power later -- a behavior I call \"scheming\" (also\nsometimes called \"deceptive alignment\"). I conclude that scheming is a\ndisturbingly plausible outcome of using baseline machine learning methods to\ntrain goal-directed AIs sophisticated enough to scheme (my subjective\nprobability on such an outcome, given these conditions, is roughly 25%). In\nparticular: if performing well in training is a good strategy for gaining power\n(as I think it might well be), then a very wide variety of goals would motivate\nscheming -- and hence, good training performance. This makes it plausible that\ntraining might either land on such a goal naturally and then reinforce it, or\nactively push a model's motivations towards such a goal as an easy way of\nimproving performance. What's more, because schemers pretend to be aligned on\ntests designed to reveal their motivations, it may be quite difficult to tell\nwhether this has occurred. However, I also think there are reasons for comfort.\nIn particular: scheming may not actually be such a good strategy for gaining\npower; various selection pressures in training might work against schemer-like\ngoals (for example, relative to non-schemers, schemers need to engage in extra\ninstrumental reasoning, which might harm their training performance); and we\nmay be able to increase such pressures intentionally. The report discusses\nthese and a wide variety of other considerations in detail, and it suggests an\narray of empirical research directions for probing the topic further.\n","authors":["Joe Carlsmith"],"pdf_url":"https://arxiv.org/pdf/2311.08379v2.pdf","comment":"127 pages, 8 figures. Revised to correct typos"},{"id":"http://arxiv.org/abs/2311.09188v1","updated":"2023-11-15T18:28:29Z","published":"2023-11-15T18:28:29Z","title":"Towards Verifiable Text Generation with Symbolic References","summary":"  Large language models (LLMs) have demonstrated an impressive ability to\nsynthesize plausible and fluent text. However they remain vulnerable to\nhallucinations, and thus their outputs generally require manual human\nverification for high-stakes applications, which can be time-consuming and\ndifficult. This paper proposes symbolically grounded generation (SymGen) as a\nsimple approach for enabling easier validation of an LLM's output. SymGen\nprompts an LLM to interleave its regular output text with explicit symbolic\nreferences to fields present in some conditioning data (e.g., a table in JSON\nformat). The references can be used to display the provenance of different\nspans of text in the generation, reducing the effort required for manual\nverification. Across data-to-text and question answering experiments, we find\nthat LLMs are able to directly output text that makes use of symbolic\nreferences while maintaining fluency and accuracy.\n","authors":["Lucas Torroba Hennigen","Shannon Shen","Aniruddha Nrusimha","Bernhard Gapp","David Sontag","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.09188v1.pdf","comment":"46 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2311.09184v1","updated":"2023-11-15T18:25:26Z","published":"2023-11-15T18:25:26Z","title":"Benchmarking Generation and Evaluation Capabilities of Large Language\n  Models for Instruction Controllable Summarization","summary":"  While large language models (LLMs) already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for the desired\nsummary characteristics. To this end, we curate an evaluation-only dataset for\nthis task setting and conduct human evaluation on 5 LLM-based summarization\nsystems. We then benchmark LLM-based automatic evaluation for this task with 4\ndifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\nin total. Our study reveals that instruction controllable text summarization\nremains a challenging task for LLMs, since (1) all LLMs evaluated still make\nfactual and other types of errors in their summaries; (2) all LLM-based\nevaluation methods cannot achieve a strong alignment with human annotators when\njudging the quality of candidate summaries; (3) different LLMs show large\nperformance gaps in summary generation and evaluation. We make our collected\nbenchmark, InstruSum, publicly available to facilitate future research in this\ndirection.\n","authors":["Yixin Liu","Alexander R. Fabbri","Jiawen Chen","Yilun Zhao","Simeng Han","Shafiq Joty","Pengfei Liu","Dragomir Radev","Chien-Sheng Wu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09184v1.pdf","comment":"GitHub Repo: https://github.com/yale-nlp/InstruSum"},{"id":"http://arxiv.org/abs/2307.08423v2","updated":"2023-11-15T18:25:03Z","published":"2023-07-17T12:14:14Z","title":"Artificial Intelligence for Science in Quantum, Atomistic, and Continuum\n  Systems","summary":"  Advances in artificial intelligence (AI) are fueling a new paradigm of\ndiscoveries in natural sciences. Today, AI has started to advance natural\nsciences by improving, accelerating, and enabling our understanding of natural\nphenomena at a wide range of spatial and temporal scales, giving rise to a new\narea of research known as AI for science (AI4Science). Being an emerging\nresearch paradigm, AI4Science is unique in that it is an enormous and highly\ninterdisciplinary area. Thus, a unified and technical treatment of this field\nis needed yet challenging. This work aims to provide a technically thorough\naccount of a subarea of AI4Science; namely, AI for quantum, atomistic, and\ncontinuum systems. These areas aim at understanding the physical world from the\nsubatomic (wavefunctions and electron density), atomic (molecules, proteins,\nmaterials, and interactions), to macro (fluids, climate, and subsurface) scales\nand form an important subarea of AI4Science. A unique advantage of focusing on\nthese areas is that they largely share a common set of challenges, thereby\nallowing a unified and foundational treatment. A key common challenge is how to\ncapture physics first principles, especially symmetries, in natural systems by\ndeep learning methods. We provide an in-depth yet intuitive account of\ntechniques to achieve equivariance to symmetry transformations. We also discuss\nother common technical challenges, including explainability,\nout-of-distribution generalization, knowledge transfer with foundation and\nlarge language models, and uncertainty quantification. To facilitate learning\nand education, we provide categorized lists of resources that we found to be\nuseful. We strive to be thorough and unified and hope this initial effort may\ntrigger more community interests and efforts to further advance AI4Science.\n","authors":["Xuan Zhang","Limei Wang","Jacob Helwig","Youzhi Luo","Cong Fu","Yaochen Xie","Meng Liu","Yuchao Lin","Zhao Xu","Keqiang Yan","Keir Adams","Maurice Weiler","Xiner Li","Tianfan Fu","Yucheng Wang","Haiyang Yu","YuQing Xie","Xiang Fu","Alex Strasser","Shenglong Xu","Yi Liu","Yuanqi Du","Alexandra Saxton","Hongyi Ling","Hannah Lawrence","Hannes Stärk","Shurui Gui","Carl Edwards","Nicholas Gao","Adriana Ladera","Tailin Wu","Elyssa F. Hofgard","Aria Mansouri Tehrani","Rui Wang","Ameya Daigavane","Montgomery Bohde","Jerry Kurtin","Qian Huang","Tuong Phung","Minkai Xu","Chaitanya K. Joshi","Simon V. Mathis","Kamyar Azizzadenesheli","Ada Fang","Alán Aspuru-Guzik","Erik Bekkers","Michael Bronstein","Marinka Zitnik","Anima Anandkumar","Stefano Ermon","Pietro Liò","Rose Yu","Stephan Günnemann","Jure Leskovec","Heng Ji","Jimeng Sun","Regina Barzilay","Tommi Jaakkola","Connor W. Coley","Xiaoning Qian","Xiaofeng Qian","Tess Smidt","Shuiwang Ji"],"pdf_url":"https://arxiv.org/pdf/2307.08423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09165v1","updated":"2023-11-15T18:05:31Z","published":"2023-11-15T18:05:31Z","title":"Approaching adverse event detection utilizing transformers on clinical\n  time-series","summary":"  Patients being admitted to a hospital will most often be associated with a\ncertain clinical development during their stay. However, there is always a risk\nof patients being subject to the wrong diagnosis or to a certain treatment not\npertaining to the desired effect, potentially leading to adverse events. Our\nresearch aims to develop an anomaly detection system for identifying deviations\nfrom expected clinical trajectories. To address this goal we analyzed 16 months\nof vital sign recordings obtained from the Nordland Hospital Trust (NHT). We\nemployed an self-supervised framework based on the STraTS transformer\narchitecture to represent the time series data in a latent space. These\nrepresentations were then subjected to various clustering techniques to explore\npotential patient phenotypes based on their clinical progress. While our\npreliminary results from this ongoing research are promising, they underscore\nthe importance of enhancing the dataset with additional demographic information\nfrom patients. This additional data will be crucial for a more comprehensive\nevaluation of the method's performance.\n","authors":["Helge Fredriksen","Per Joel Burman","Ashenafi Woldaregay","Karl Øyvind Mikalsen","Ståle Nymo"],"pdf_url":"https://arxiv.org/pdf/2311.09165v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.09145v1","updated":"2023-11-15T17:40:48Z","published":"2023-11-15T17:40:48Z","title":"Model Agnostic Explainable Selective Regression via Uncertainty\n  Estimation","summary":"  With the wide adoption of machine learning techniques, requirements have\nevolved beyond sheer high performance, often requiring models to be\ntrustworthy. A common approach to increase the trustworthiness of such systems\nis to allow them to refrain from predicting. Such a framework is known as\nselective prediction. While selective prediction for classification tasks has\nbeen widely analyzed, the problem of selective regression is understudied. This\npaper presents a novel approach to selective regression that utilizes\nmodel-agnostic non-parametric uncertainty estimation. Our proposed framework\nshowcases superior performance compared to state-of-the-art selective\nregressors, as demonstrated through comprehensive benchmarking on 69 datasets.\nFinally, we use explainable AI techniques to gain an understanding of the\ndrivers behind selective regression. We implement our selective regression\nmethod in the open-source Python package doubt and release the code used to\nreproduce our experiments.\n","authors":["Andrea Pugnana","Carlos Mougan","Dan Saattrup Nielsen"],"pdf_url":"https://arxiv.org/pdf/2311.09145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09142v1","updated":"2023-11-15T17:39:25Z","published":"2023-11-15T17:39:25Z","title":"Machine-learning parameter tracking with partial state observation","summary":"  Complex and nonlinear dynamical systems often involve parameters that change\nwith time, accurate tracking of which is essential to tasks such as state\nestimation, prediction, and control. Existing machine-learning methods require\nfull state observation of the underlying system and tacitly assume adiabatic\nchanges in the parameter. Formulating an inverse problem and exploiting\nreservoir computing, we develop a model-free and fully data-driven framework to\naccurately track time-varying parameters from partial state observation in real\ntime. In particular, with training data from a subset of the dynamical\nvariables of the system for a small number of known parameter values, the\nframework is able to accurately predict the parameter variations in time. Low-\nand high-dimensional, Markovian and non-Markovian nonlinear dynamical systems\nare used to demonstrate the power of the machine-learning based\nparameter-tracking framework. Pertinent issues affecting the tracking\nperformance are addressed.\n","authors":["Zheng-Meng Zhai","Mohammadamin Moradi","Bryan Glaz","Mulugeta Haile","Ying-Cheng Lai"],"pdf_url":"https://arxiv.org/pdf/2311.09142v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.09137v1","updated":"2023-11-15T17:29:24Z","published":"2023-11-15T17:29:24Z","title":"Causal prediction models for medication safety monitoring: The diagnosis\n  of vancomycin-induced acute kidney injury","summary":"  The current best practice approach for the retrospective diagnosis of adverse\ndrug events (ADEs) in hospitalized patients relies on a full patient chart\nreview and a formal causality assessment by multiple medical experts. This\nevaluation serves to qualitatively estimate the probability of causation (PC);\nthe probability that a drug was a necessary cause of an adverse event. This\npractice is manual, resource intensive and prone to human biases, and may thus\nbenefit from data-driven decision support. Here, we pioneer a causal modeling\napproach using observational data to estimate a lower bound of the PC\n(PC$_{low}$). This method includes two key causal inference components: (1) the\ntarget trial emulation framework and (2) estimation of individualized treatment\neffects using machine learning. We apply our method to the clinically relevant\nuse-case of vancomycin-induced acute kidney injury in intensive care patients,\nand compare our causal model-based PC$_{low}$ estimates to qualitative\nestimates of the PC provided by a medical expert. Important limitations and\npotential improvements are discussed, and we conclude that future improved\ncausal models could provide essential data-driven support for medication safety\nmonitoring in hospitalized patients.\n","authors":["Izak Yasrebi-de Kom","Joanna Klopotowska","Dave Dongelmans","Nicolette De Keizer","Kitty Jager","Ameen Abu-Hanna","Giovanni Cinà"],"pdf_url":"https://arxiv.org/pdf/2311.09137v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 14 pages"},{"id":"http://arxiv.org/abs/2305.09863v2","updated":"2023-11-15T17:19:10Z","published":"2023-05-17T00:29:18Z","title":"Explaining black box text modules in natural language with language\n  models","summary":"  Large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their rapid proliferation\nand increasing opaqueness have created a growing need for interpretability.\nHere, we ask whether we can automatically obtain natural language explanations\nfor black box text modules. A \"text module\" is any function that maps text to a\nscalar continuous value, such as a submodule within an LLM or a fitted model of\na brain region. \"Black box\" indicates that we only have access to the module's\ninputs/outputs.\n  We introduce Summarize and Score (SASC), a method that takes in a text module\nand returns a natural language explanation of the module's selectivity along\nwith a score for how reliable the explanation is. We study SASC in 3 contexts.\nFirst, we evaluate SASC on synthetic modules and find that it often recovers\nground truth explanations. Second, we use SASC to explain modules found within\na pre-trained BERT model, enabling inspection of the model's internals.\nFinally, we show that SASC can generate explanations for the response of\nindividual fMRI voxels to language stimuli, with potential applications to\nfine-grained brain mapping. All code for using SASC and reproducing results is\nmade available on Github.\n","authors":["Chandan Singh","Aliyah R. Hsu","Richard Antonello","Shailee Jain","Alexander G. Huth","Bin Yu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2305.09863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09128v1","updated":"2023-11-15T17:17:49Z","published":"2023-11-15T17:17:49Z","title":"Fast Detection of Phase Transitions with Multi-Task\n  Learning-by-Confusion","summary":"  Machine learning has been successfully used to study phase transitions. One\nof the most popular approaches to identifying critical points from data without\nprior knowledge of the underlying phases is the learning-by-confusion scheme.\nAs input, it requires system samples drawn from a grid of the parameter whose\nchange is associated with potential phase transitions. Up to now, the scheme\nrequired training a distinct binary classifier for each possible splitting of\nthe grid into two sides, resulting in a computational cost that scales linearly\nwith the number of grid points. In this work, we propose and showcase an\nalternative implementation that only requires the training of a single\nmulti-class classifier. Ideally, such multi-task learning eliminates the\nscaling with respect to the number of grid points. In applications to the Ising\nmodel and an image dataset generated with Stable Diffusion, we find significant\nspeedups that closely correspond to the ideal case, with only minor deviations.\n","authors":["Julian Arnold","Frank Schäfer","Niels Lörch"],"pdf_url":"https://arxiv.org/pdf/2311.09128v1.pdf","comment":"7 pages, 3 figures, Machine Learning and the Physical Sciences\n  Workshop, NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.09127v1","updated":"2023-11-15T17:17:39Z","published":"2023-11-15T17:17:39Z","title":"Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts","summary":"  Existing work on jailbreak Multimodal Large Language Models (MLLMs) has\nfocused primarily on adversarial examples in model inputs, with less attention\nto vulnerabilities in model APIs. To fill the research gap, we carry out the\nfollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.\nThrough carefully designed dialogue, we successfully steal the internal system\nprompts of GPT-4V. This finding indicates potential exploitable security risks\nin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM\njailbreaking attack method termed SASP (Self-Adversarial Attack via System\nPrompt). By employing GPT-4 as a red teaming tool against itself, we aim to\nsearch for potential jailbreak prompts leveraging stolen system prompts.\nFurthermore, in pursuit of better performance, we also add human modification\nbased on GPT-4's analysis, which further improves the attack success rate to\n98.7\\%; 3) We evaluated the effect of modifying system prompts to defend\nagainst jailbreaking attacks. Results show that appropriately designed system\nprompts can significantly reduce jailbreak success rates. Overall, our work\nprovides new insights into enhancing MLLM security, demonstrating the important\nrole of system prompts in jailbreaking, which could be leveraged to greatly\nfacilitate jailbreak success rates while also holding the potential for\ndefending against jailbreaks.\n","authors":["Yuanwei Wu","Xiang Li","Yixin Liu","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12370v2","updated":"2023-11-15T17:06:57Z","published":"2023-06-21T16:26:14Z","title":"PriorBand: Practical Hyperparameter Optimization in the Age of Deep\n  Learning","summary":"  Hyperparameters of Deep Learning (DL) pipelines are crucial for their\ndownstream performance. While a large number of methods for Hyperparameter\nOptimization (HPO) have been developed, their incurred costs are often\nuntenable for modern DL. Consequently, manual experimentation is still the most\nprevalent approach to optimize hyperparameters, relying on the researcher's\nintuition, domain knowledge, and cheap preliminary explorations. To resolve\nthis misalignment between HPO algorithms and DL researchers, we propose\nPriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs\nand cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency\nacross a range of DL benchmarks and show its gains under informative expert\ninput and robustness against poor expert beliefs\n","authors":["Neeratyoy Mallik","Edward Bergman","Carl Hvarfner","Danny Stoll","Maciej Janowski","Marius Lindauer","Luigi Nardi","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2306.12370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09115v1","updated":"2023-11-15T17:06:26Z","published":"2023-11-15T17:06:26Z","title":"HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data","summary":"  Technological advances in medical data collection such as high-resolution\nhistopathology and high-throughput genomic sequencing have contributed to the\nrising requirement for multi-modal biomedical modelling, specifically for\nimage, tabular, and graph data. Most multi-modal deep learning approaches use\nmodality-specific architectures that are trained separately and cannot capture\nthe crucial cross-modal information that motivates the integration of different\ndata sources. This paper presents the Hybrid Early-fusion Attention Learning\nNetwork (HEALNet): a flexible multi-modal fusion architecture, which a)\npreserves modality-specific structural information, b) captures the cross-modal\ninteractions and structural information in a shared latent space, c) can\neffectively handle missing modalities during training and inference, and d)\nenables intuitive model inspection by learning on the raw data input instead of\nopaque embeddings. We conduct multi-modal survival analysis on Whole Slide\nImages and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas\n(TCGA). HEALNet achieves state-of-the-art performance, substantially improving\nover both uni-modal and recent multi-modal baselines, whilst being robust in\nscenarios with missing modalities.\n","authors":["Konstantin Hemker","Nikola Smidjievski","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2311.09115v1.pdf","comment":"7 pages body, 5 pages appendix"},{"id":"http://arxiv.org/abs/2311.09114v1","updated":"2023-11-15T17:04:56Z","published":"2023-11-15T17:04:56Z","title":"Ever: Mitigating Hallucination in Large Language Models through\n  Real-Time Verification and Rectification","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.\n","authors":["Haoqiang Kang","Juntong Ni","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2311.09114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09109v1","updated":"2023-11-15T16:56:49Z","published":"2023-11-15T16:56:49Z","title":"Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge\n  Graph Completion?","summary":"  Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.\n","authors":["Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2311.09109v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.14516v2","updated":"2023-11-15T16:49:34Z","published":"2023-03-25T16:52:42Z","title":"OVeNet: Offset Vector Network for Semantic Segmentation","summary":"  Semantic segmentation is a fundamental task in visual scene understanding. We\nfocus on the supervised setting, where ground-truth semantic annotations are\navailable. Based on knowledge about the high regularity of real-world scenes,\nwe propose a method for improving class predictions by learning to selectively\nexploit information from neighboring pixels. In particular, our method is based\non the prior that for each pixel, there is a seed pixel in its close\nneighborhood sharing the same prediction with the former. Motivated by this\nprior, we design a novel two-head network, named Offset Vector Network\n(OVeNet), which generates both standard semantic predictions and a dense 2D\noffset vector field indicating the offset from each pixel to the respective\nseed pixel, which is used to compute an alternative, seed-based semantic\nprediction. The two predictions are adaptively fused at each pixel using a\nlearnt dense confidence map for the predicted offset vector field. We supervise\noffset vectors indirectly via optimizing the seed-based prediction and via a\nnovel loss on the confidence map. Compared to the baseline state-of-the-art\narchitectures HRNet and HRNet+OCR on which OVeNet is built, the latter achieves\nsignificant performance gains on three prominent benchmarks for semantic\nsegmentation, namely Cityscapes, ACDC and ADE20K. Code is available at\nhttps://github.com/stamatisalex/OVeNet\n","authors":["Stamatis Alexandropoulos","Christos Sakaridis","Petros Maragos"],"pdf_url":"https://arxiv.org/pdf/2303.14516v2.pdf","comment":"Accepted at WACV 2024"},{"id":"http://arxiv.org/abs/2311.09101v1","updated":"2023-11-15T16:47:57Z","published":"2023-11-15T16:47:57Z","title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning","summary":"  Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. Usually,\nanswer calibration strategies such as step-level or path-level calibration play\na vital role in multi-step reasoning. While effective, there remains a\nsignificant gap in our understanding of the key factors that drive their\nsuccess. In this paper, we break down the design of recent answer calibration\nstrategies and present a unified view which establishes connections between\nthem. We then conduct a thorough evaluation on these strategies from a unified\nview, systematically scrutinizing step-level and path-level answer calibration\nacross multiple paths. Our study holds the potential to illuminate key insights\nfor optimizing multi-step reasoning with answer calibration.\n","authors":["Shumin Deng","Ningyu Zhang","Nay Oo","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2311.09101v1.pdf","comment":"Working in Progress"},{"id":"http://arxiv.org/abs/2211.14400v4","updated":"2023-11-15T16:26:40Z","published":"2022-11-25T23:32:26Z","title":"Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and\n  Besov Spaces","summary":"  Let $\\Omega = [0,1]^d$ be the unit cube in $\\mathbb{R}^d$. We study the\nproblem of how efficiently, in terms of the number of parameters, deep neural\nnetworks with the ReLU activation function can approximate functions in the\nSobolev spaces $W^s(L_q(\\Omega))$ and Besov spaces $B^s_r(L_q(\\Omega))$, with\nerror measured in the $L_p(\\Omega)$ norm. This problem is important when\nstudying the application of neural networks in a variety of fields, including\nscientific computing and signal processing, and has previously been solved only\nwhen $p=q=\\infty$. Our contribution is to provide a complete solution for all\n$1\\leq p,q\\leq \\infty$ and $s > 0$ for which the corresponding Sobolev or Besov\nspace compactly embeds into $L_p$. The key technical tool is a novel\nbit-extraction technique which gives an optimal encoding of sparse vectors.\nThis enables us to obtain sharp upper bounds in the non-linear regime where $p\n> q$. We also provide a novel method for deriving $L_p$-approximation lower\nbounds based upon VC-dimension when $p < \\infty$. Our results show that very\ndeep ReLU networks significantly outperform classical methods of approximation\nin terms of the number of parameters, but that this comes at the cost of\nparameters which are not encodable.\n","authors":["Jonathan W. Siegel"],"pdf_url":"https://arxiv.org/pdf/2211.14400v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14119v2","updated":"2023-11-15T16:15:19Z","published":"2023-08-27T14:25:07Z","title":"Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario","summary":"  Semi-Supervised Learning (SSL) is a framework that utilizes both labeled and\nunlabeled data to enhance model performance. Conventional SSL methods operate\nunder the assumption that labeled and unlabeled data share the same label\nspace. However, in practical real-world scenarios, especially when the labeled\ntraining dataset is limited in size, some classes may be totally absent from\nthe labeled set. To address this broader context, we propose a general approach\nto augment existing SSL methods, enabling them to effectively handle situations\nwhere certain classes are missing. This is achieved by introducing an\nadditional term into their objective function, which penalizes the\nKL-divergence between the probability vectors of the true class frequencies and\nthe inferred class frequencies. Our experimental results reveal significant\nimprovements in accuracy when compared to state-of-the-art SSL, open-set SSL,\nand open-world SSL methods. We conducted these experiments on two benchmark\nimage classification datasets, CIFAR-100 and STL-10, with the most remarkable\nimprovements observed when the labeled data is severely limited, with only a\nfew labeled examples per class\n","authors":["Noam Fluss","Guy Hacohen","Daphna Weinshall"],"pdf_url":"https://arxiv.org/pdf/2308.14119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09068v1","updated":"2023-11-15T16:10:34Z","published":"2023-11-15T16:10:34Z","title":"Learning Fair Division from Bandit Feedback","summary":"  This work addresses learning online fair division under uncertainty, where a\ncentral planner sequentially allocates items without precise knowledge of\nagents' values or utilities. Departing from conventional online algorithm, the\nplanner here relies on noisy, estimated values obtained after allocating items.\nWe introduce wrapper algorithms utilizing \\textit{dual averaging}, enabling\ngradual learning of both the type distribution of arriving items and agents'\nvalues through bandit feedback. This approach enables the algorithms to\nasymptotically achieve optimal Nash social welfare in linear Fisher markets\nwith agents having additive utilities. We establish regret bounds in Nash\nsocial welfare and empirically validate the superior performance of our\nproposed algorithms across synthetic and empirical datasets.\n","authors":["Hakuei Yamada","Junpei Komiyama","Kenshi Abe","Atsushi Iwasaki"],"pdf_url":"https://arxiv.org/pdf/2311.09068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16248v2","updated":"2023-11-15T16:07:23Z","published":"2023-06-28T14:18:52Z","title":"Latent SDEs on Homogeneous Spaces","summary":"  We consider the problem of variational Bayesian inference in a latent\nvariable model where a (possibly complex) observed stochastic process is\ngoverned by the solution of a latent stochastic differential equation (SDE).\nMotivated by the challenges that arise when trying to learn an (almost\narbitrary) latent neural SDE from large-scale data, such as efficient gradient\ncomputation, we take a step back and study a specific subclass instead. In our\ncase, the SDE evolves on a homogeneous latent space and is induced by\nstochastic dynamics of the corresponding (matrix) Lie group. In learning\nproblems, SDEs on the unit $n$-sphere are arguably the most relevant\nincarnation of this setup. Notably, for variational inference, the sphere not\nonly facilitates using a truly uninformative prior SDE, but we also obtain a\nparticularly simple and intuitive expression for the Kullback-Leibler\ndivergence between the approximate posterior and prior process in the evidence\nlower bound. Experiments demonstrate that a latent SDE of the proposed type can\nbe learned efficiently by means of an existing one-step geometric\nEuler-Maruyama scheme. Despite restricting ourselves to a less diverse class of\nSDEs, we achieve competitive or even state-of-the-art performance on various\ntime series interpolation and classification benchmarks.\n","authors":["Sebastian Zeng","Florian Graf","Roland Kwitt"],"pdf_url":"https://arxiv.org/pdf/2306.16248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07636v2","updated":"2023-11-15T16:06:36Z","published":"2023-11-13T15:16:24Z","title":"Attention-based Multi-task Learning for Base Editor Outcome Prediction","summary":"  Human genetic diseases often arise from point mutations, emphasizing the\ncritical need for precise genome editing techniques. Among these, base editing\nstands out as it allows targeted alterations at the single nucleotide level.\nHowever, its clinical application is hindered by low editing efficiency and\nunintended mutations, necessitating extensive trial-and-error experimentation\nin the laboratory. To speed up this process, we present an attention-based\ntwo-stage machine learning model that learns to predict the likelihood of all\npossible editing outcomes for a given genomic target sequence. We further\npropose a multi-task learning schema to jointly learn multiple base editors\n(i.e. variants) at once. Our model's predictions consistently demonstrated a\nstrong correlation with the actual experimental results on multiple datasets\nand base editor variants. These results provide further validation for the\nmodels' capacity to enhance and accelerate the process of refining base editing\ndesigns.\n","authors":["Amina Mollaysa","Ahmed Allam","Michael Krauthammer"],"pdf_url":"https://arxiv.org/pdf/2311.07636v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 15 pages.\n  arXiv admin note: substantial text overlap with arXiv:2310.02919"},{"id":"http://arxiv.org/abs/2311.09065v1","updated":"2023-11-15T16:05:43Z","published":"2023-11-15T16:05:43Z","title":"Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems\n  with Convex Constraints","summary":"  We give a damped proximal augmented Lagrangian method (DPALM) for solving\nproblems with a weakly-convex objective and convex linear/nonlinear\nconstraints. Instead of taking a full stepsize, DPALM adopts a damped dual\nstepsize to ensure the boundedness of dual iterates. We show that DPALM can\nproduce a (near) $\\vareps$-KKT point within $O(\\vareps^{-2})$ outer iterations\nif each DPALM subproblem is solved to a proper accuracy. In addition, we\nestablish overall iteration complexity of DPALM when the objective is either a\nregularized smooth function or in a regularized compositional form. For the\nformer case, DPALM achieves the complexity of\n$\\widetilde{\\mathcal{O}}\\left(\\varepsilon^{-2.5} \\right)$ to produce an\n$\\varepsilon$-KKT point by applying an accelerated proximal gradient (APG)\nmethod to each DPALM subproblem. For the latter case, the complexity of DPALM\nis $\\widetilde{\\mathcal{O}}\\left(\\varepsilon^{-3} \\right)$ to produce a near\n$\\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed\nversion of each subproblem. Our outer iteration complexity and the overall\ncomplexity either generalize existing best ones from unconstrained or\nlinear-constrained problems to convex-constrained ones, or improve over the\nbest-known results on solving the same-structured problems. Furthermore,\nnumerical experiments on linearly/quadratically constrained non-convex\nquadratic programs and linear-constrained robust nonlinear least squares are\nconducted to demonstrate the empirical efficiency of the proposed DPALM over\nseveral state-of-the art methods.\n","authors":["Hari Dahal","Wei Liu","Yangyang Xu"],"pdf_url":"https://arxiv.org/pdf/2311.09065v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2311.09064v1","updated":"2023-11-15T16:02:13Z","published":"2023-11-15T16:02:13Z","title":"Imagine the Unseen World: A Benchmark for Systematic Generalization in\n  Visual World Models","summary":"  Systematic compositionality, or the ability to adapt to novel situations by\ncreating a mental model of the world using reusable pieces of knowledge,\nremains a significant challenge in machine learning. While there has been\nconsiderable progress in the language domain, efforts towards systematic visual\nimagination, or envisioning the dynamical implications of a visual observation,\nare in their infancy. We introduce the Systematic Visual Imagination Benchmark\n(SVIB), the first benchmark designed to address this problem head-on. SVIB\noffers a novel framework for a minimal world modeling problem, where models are\nevaluated based on their ability to generate one-step image-to-image\ntransformations under a latent world dynamics. The framework provides benefits\nsuch as the possibility to jointly optimize for systematic perception and\nimagination, a range of difficulty levels, and the ability to control the\nfraction of possible factor combinations used during training. We provide a\ncomprehensive evaluation of various baseline models on SVIB, offering insight\ninto the current state-of-the-art in systematic visual imagination. We hope\nthat this benchmark will help advance visual systematic compositionality.\n","authors":["Yeongbin Kim","Gautam Singh","Junyeong Park","Caglar Gulcehre","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2311.09064v1.pdf","comment":"Published as a conference paper at NeurIPS 2023. The first two\n  authors contributed equally. To download the benchmark, visit\n  https://systematic-visual-imagination.github.io"},{"id":"http://arxiv.org/abs/2311.00684v2","updated":"2023-11-15T15:55:02Z","published":"2023-11-01T17:43:35Z","title":"Attention Alignment and Flexible Positional Embeddings Improve\n  Transformer Length Extrapolation","summary":"  An ideal length-extrapolatable Transformer language model can handle\nsequences longer than the training length without any fine-tuning. Such\nlong-context utilization capability relies heavily on a flexible positional\nembedding design. Upon investigating the flexibility of existing large\npre-trained Transformer language models, we find that the T5 family deserves a\ncloser look, as its positional embeddings capture rich and flexible attention\npatterns. However, T5 suffers from the dispersed attention issue: the longer\nthe input sequence, the flatter the attention distribution. To alleviate the\nissue, we propose two attention alignment strategies via temperature scaling.\nOur findings show improvement on the long-context utilization capability of T5\non language modeling, retrieval, multi-document question answering, and code\ncompletion tasks without any fine-tuning. This suggests that a flexible\npositional embedding design and attention alignment can go a long way toward\nTransformer length extrapolation.\n","authors":["Ta-Chung Chi","Ting-Han Fan","Alexander I. Rudnicky"],"pdf_url":"https://arxiv.org/pdf/2311.00684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09058v1","updated":"2023-11-15T15:50:34Z","published":"2023-11-15T15:50:34Z","title":"New Horizons in Parameter Regularization: A Constraint Approach","summary":"  This work presents constrained parameter regularization (CPR), an alternative\nto traditional weight decay. Instead of applying a constant penalty uniformly\nto all parameters, we enforce an upper bound on a statistical measure (e.g.,\nthe L$_2$-norm) of individual parameter groups. This reformulates learning as a\nconstrained optimization problem. To solve this, we utilize an adaptation of\nthe augmented Lagrangian method. Our approach allows for varying regularization\nstrengths across different parameter groups, removing the need for explicit\npenalty coefficients in the regularization terms. CPR only requires two\nhyperparameters and introduces no measurable runtime overhead. We offer\nempirical evidence of CPR's effectiveness through experiments in the \"grokking\"\nphenomenon, image classification, and language modeling. Our findings show that\nCPR can counteract the effects of grokking, and it consistently matches or\nsurpasses the performance of traditional weight decay.\n","authors":["Jörg K. H. Franke","Michael Hefenbrock","Gregor Koehler","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2311.09058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14463v2","updated":"2023-11-15T15:50:31Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a systematic study and comprehensive evaluation of large language\nmodels for automatic multilingual readability assessment. In particular, we\nconstruct ReadMe++, a multilingual multi-domain dataset with human annotations\nof 9757 sentences in Arabic, English, French, Hindi, and Russian collected from\n112 different data sources. ReadMe++ offers more domain and language diversity\nthan existing readability datasets, making it ideal for benchmarking\nmultilingual and non-English language models (including mBERT, XLM-R, mT5,\nLlama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot prompting\nsettings. Our experiments reveal that models fine-tuned on ReadMe++ outperform\nthose trained on single-domain datasets, showcasing superior performance on\nmulti-domain readability assessment and cross-lingual transfer capabilities. We\nalso compare to traditional readability metrics (such as Flesch-Kincaid Grade\nLevel and Open Source Metric for Measuring Arabic Narratives), as well as the\nstate-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will make\nour data and code publicly available at: https://github.com/tareknaous/readme.\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v2.pdf","comment":"We have added French and Russian as two new languages to the corpus"},{"id":"http://arxiv.org/abs/2302.05763v4","updated":"2023-11-15T15:41:01Z","published":"2023-02-11T19:27:07Z","title":"Towards Multi-User Activity Recognition through Facilitated Training\n  Data and Deep Learning for Human-Robot Collaboration Applications","summary":"  Human-robot interaction (HRI) research is progressively addressing\nmulti-party scenarios, where a robot interacts with more than one human user at\nthe same time. Conversely, research is still at an early stage for human-robot\ncollaboration. The use of machine learning techniques to handle such type of\ncollaboration requires data that are less feasible to produce than in a typical\nHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC\napplications. Based upon these concepts, this study also proposes an\nalternative way of gathering data regarding multi-user activity, by collecting\ndata related to single users and merging them in post-processing, to reduce the\neffort involved in producing recordings of pair settings. To validate this\nstatement, 3D skeleton poses of activity of single users were collected and\nmerged in pairs. After this, such datapoints were used to separately train a\nlong short-term memory (LSTM) network and a variational autoencoder (VAE)\ncomposed of spatio-temporal graph convolutional networks (STGCN) to recognise\nthe joint activities of the pairs of people. The results showed that it is\npossible to make use of data collected in this way for pair HRC settings and\nget similar performances compared to using training data regarding groups of\nusers recorded under the same settings, relieving from the technical\ndifficulties involved in producing these data.\n  The related code and collected data are publicly available.\n","authors":["Francesco Semeraro","Jon Carberry","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2302.05763v4.pdf","comment":"This is the accepted manuscript. Please see published version at\n  https://ieeexplore.ieee.org/document/10191782"},{"id":"http://arxiv.org/abs/2311.07772v2","updated":"2023-11-15T15:29:05Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n","authors":["Tomer Bar Natan","Gilad Deutch","Nadav Magar","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09027v1","updated":"2023-11-15T15:15:57Z","published":"2023-11-15T15:15:57Z","title":"Assessing the Robustness of Intelligence-Driven Reinforcement Learning","summary":"  Robustness to noise is of utmost importance in reinforcement learning\nsystems, particularly in military contexts where high stakes and uncertain\nenvironments prevail. Noise and uncertainty are inherent features of military\noperations, arising from factors such as incomplete information, adversarial\nactions, or unpredictable battlefield conditions. In RL, noise can critically\nimpact decision-making, mission success, and the safety of personnel. Reward\nmachines offer a powerful tool to express complex reward structures in RL\ntasks, enabling the design of tailored reinforcement signals that align with\nmission objectives. This paper considers the problem of the robustness of\nintelligence-driven reinforcement learning based on reward machines. The\npreliminary results presented suggest the need for further research in\nevidential reasoning and learning to harden current state-of-the-art\nreinforcement learning approaches before being mission-critical-ready.\n","authors":["Lorenzo Nodari","Federico Cerutti"],"pdf_url":"https://arxiv.org/pdf/2311.09027v1.pdf","comment":"Accepted for publication at IEEE TechDefense 2023"},{"id":"http://arxiv.org/abs/2305.06348v5","updated":"2023-11-15T15:12:42Z","published":"2023-05-10T17:54:21Z","title":"Supervised learning with probabilistic morphisms and kernel mean\n  embeddings","summary":"  In this paper I propose a generative model of supervised learning that\nunifies two approaches to supervised learning, using a concept of a correct\nloss function. Addressing two measurability problems, which have been ignored\nin statistical learning theory, I propose to use convergence in outer\nprobability to characterize the consistency of a learning algorithm. Building\nupon these results, I extend a result due to Cucker-Smale, which addresses the\nlearnability of a regression model, to the setting of a conditional probability\nestimation problem. Additionally, I present a variant of Vapnik-Stefanuyk's\nregularization method for solving stochastic ill-posed problems, and using it\nto prove the generalizability of overparameterized supervised learning models.\n","authors":["Hông Vân Lê"],"pdf_url":"https://arxiv.org/pdf/2305.06348v5.pdf","comment":"V5: 51 p., minor correction, Corollary 6.13(2) added"},{"id":"http://arxiv.org/abs/2311.09018v1","updated":"2023-11-15T15:02:23Z","published":"2023-11-15T15:02:23Z","title":"On the Foundation of Distributionally Robust Reinforcement Learning","summary":"  Motivated by the need for a robust policy in the face of environment shifts\nbetween training and the deployment, we contribute to the theoretical\nfoundation of distributionally robust reinforcement learning (DRRL). This is\naccomplished through a comprehensive modeling framework centered around\ndistributionally robust Markov decision processes (DRMDPs). This framework\nobliges the decision maker to choose an optimal policy under the worst-case\ndistributional shift orchestrated by an adversary. By unifying and extending\nexisting formulations, we rigorously construct DRMDPs that embraces various\nmodeling attributes for both the decision maker and the adversary. These\nattributes include adaptability granularity, exploring history-dependent,\nMarkov, and Markov time-homogeneous decision maker and adversary dynamics.\nAdditionally, we delve into the flexibility of shifts induced by the adversary,\nexamining SA and S-rectangularity. Within this DRMDP framework, we investigate\nconditions for the existence or absence of the dynamic programming principle\n(DPP). From an algorithmic standpoint, the existence of DPP holds significant\nimplications, as the vast majority of existing data and computationally\nefficiency RL algorithms are reliant on the DPP. To study its existence, we\ncomprehensively examine combinations of controller and adversary attributes,\nproviding streamlined proofs grounded in a unified methodology. We also offer\ncounterexamples for settings in which a DPP with full generality is absent.\n","authors":["Shengbo Wang","Nian Si","Jose Blanchet","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.09018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09017v1","updated":"2023-11-15T15:00:48Z","published":"2023-11-15T15:00:48Z","title":"Semidefinite programs simulate approximate message passing robustly","summary":"  Approximate message passing (AMP) is a family of iterative algorithms that\ngeneralize matrix power iteration. AMP algorithms are known to optimally solve\nmany average-case optimization problems. In this paper, we show that a large\nclass of AMP algorithms can be simulated in polynomial time by \\emph{local\nstatistics hierarchy} semidefinite programs (SDPs), even when an unknown\nprincipal minor of measure $1/\\mathrm{polylog}(\\mathrm{dimension})$ is\nadversarially corrupted. Ours are the first robust guarantees for many of these\nproblems. Further, our results offer an interesting counterpoint to strong\nlower bounds against less constrained SDP relaxations for average-case\nmax-cut-gain (a.k.a. \"optimizing the Sherrington-Kirkpatrick Hamiltonian\") and\nother problems.\n","authors":["Misha Ivkov","Tselil Schramm"],"pdf_url":"https://arxiv.org/pdf/2311.09017v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2311.09014v1","updated":"2023-11-15T14:56:49Z","published":"2023-11-15T14:56:49Z","title":"Adversarial Attacks to Reward Machine-based Reinforcement Learning","summary":"  In recent years, Reward Machines (RMs) have stood out as a simple yet\neffective automata-based formalism for exposing and exploiting task structure\nin reinforcement learning settings. Despite their relevance, little to no\nattention has been directed to the study of their security implications and\nrobustness to adversarial scenarios, likely due to their recent appearance in\nthe literature. With my thesis, I aim to provide the first analysis of the\nsecurity of RM-based reinforcement learning techniques, with the hope of\nmotivating further research in the field, and I propose and evaluate a novel\nclass of attacks on RM-based techniques: blinding attacks.\n","authors":["Lorenzo Nodari"],"pdf_url":"https://arxiv.org/pdf/2311.09014v1.pdf","comment":"Thesis Supervisor: Prof. Federico Cerutti (Universit\\`a degli Studi\n  di Brescia, IT)"},{"id":"http://arxiv.org/abs/2311.06281v2","updated":"2023-11-15T14:53:19Z","published":"2023-10-27T21:58:55Z","title":"Efficient Parallelization of an Ubiquitous Sequential Computation","summary":"  We find a succinct expression for computing the sequence $x_t = a_t x_{t-1} +\nb_t$ in parallel with two prefix sums, given $t = (1, 2, \\dots, n)$, $a_t \\in\n\\mathbb{R}^n$, $b_t \\in \\mathbb{R}^n$, and initial value $x_0 \\in \\mathbb{R}$.\nOn $n$ parallel processors, the computation of $n$ elements incurs\n$\\mathcal{O}(\\log n)$ time and $\\mathcal{O}(n)$ space. Sequences of this form\nare ubiquitous in science and engineering, making efficient parallelization\nuseful for a vast number of applications. We implement our expression in\nsoftware, test it on parallel hardware, and verify that it executes faster than\nsequential computation by a factor of $\\frac{n}{\\log n}$.\n","authors":["Franz A. Heinsen"],"pdf_url":"https://arxiv.org/pdf/2311.06281v2.pdf","comment":"Source code for replicating our results is available online at\n  https://github.com/glassroom/heinsen_sequence"},{"id":"http://arxiv.org/abs/2003.04103v4","updated":"2023-11-15T14:51:17Z","published":"2020-03-09T12:57:42Z","title":"Flexible numerical optimization with ensmallen","summary":"  This report provides an introduction to the ensmallen numerical optimization\nlibrary, as well as a deep dive into the technical details of how it works. The\nlibrary provides a fast and flexible C++ framework for mathematical\noptimization of arbitrary user-supplied functions. A large set of pre-built\noptimizers is provided, including many variants of Stochastic Gradient Descent\nand Quasi-Newton optimizers. Several types of objective functions are\nsupported, including differentiable, separable, constrained, and categorical\nobjective functions. Implementation of a new optimizer requires only one\nmethod, while a new objective function requires typically only one or two C++\nmethods. Through internal use of C++ template metaprogramming, ensmallen\nprovides support for arbitrary user-supplied callbacks and automatic inference\nof unsupplied methods without any runtime overhead. Empirical comparisons show\nthat ensmallen outperforms other optimization frameworks (such as Julia and\nSciPy), sometimes by large margins. The library is available at\nhttps://ensmallen.org and is distributed under the permissive BSD license.\n","authors":["Ryan R. Curtin","Marcus Edel","Rahul Ganesh Prabhu","Suryoday Basak","Zhihao Lou","Conrad Sanderson"],"pdf_url":"https://arxiv.org/pdf/2003.04103v4.pdf","comment":"https://ensmallen.org/"},{"id":"http://arxiv.org/abs/2311.09006v1","updated":"2023-11-15T14:48:08Z","published":"2023-11-15T14:48:08Z","title":"Data Similarity is Not Enough to Explain Language Model Performance","summary":"  Large language models achieve high performance on many but not all downstream\ntasks. The interaction between pretraining data and task data is commonly\nassumed to determine this variance: a task with data that is more similar to a\nmodel's pretraining data is assumed to be easier for that model. We test\nwhether distributional and example-specific similarity measures (embedding-,\ntoken- and model-based) correlate with language model performance through a\nlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\nbenchmarks. Similarity correlates with performance for multilingual datasets,\nbut in other benchmarks, we surprisingly find that similarity metrics are not\ncorrelated with accuracy or even each other. This suggests that the\nrelationship between pretraining data and downstream tasks is more complex than\noften assumed.\n","authors":["Gregory Yauney","Emily Reif","David Mimno"],"pdf_url":"https://arxiv.org/pdf/2311.09006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15053v2","updated":"2023-11-15T14:46:14Z","published":"2023-07-27T17:57:42Z","title":"On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation\n  Metric for Top-$n$ Recommendation","summary":"  Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.\n","authors":["Olivier Jeunen","Ivan Potapov","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2307.15053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03217v2","updated":"2023-11-15T14:37:24Z","published":"2023-11-06T16:01:42Z","title":"Leveraging Transformers to Improve Breast Cancer Classification and Risk\n  Assessment with Multi-modal and Longitudinal Data","summary":"  Breast cancer screening, primarily conducted through mammography, is often\nsupplemented with ultrasound for women with dense breast tissue. However,\nexisting deep learning models analyze each modality independently, missing\nopportunities to integrate information across imaging modalities and time. In\nthis study, we present Multi-modal Transformer (MMT), a neural network that\nutilizes mammography and ultrasound synergistically, to identify patients who\ncurrently have cancer and estimate the risk of future cancer for patients who\nare currently cancer-free. MMT aggregates multi-modal data through\nself-attention and tracks temporal tissue changes by comparing current exams to\nprior imaging. Trained on 1.3 million exams, MMT achieves an AUROC of 0.943 in\ndetecting existing cancers, surpassing strong uni-modal baselines. For 5-year\nrisk prediction, MMT attains an AUROC of 0.826, outperforming prior\nmammography-based risk models. Our research highlights the value of multi-modal\nand longitudinal imaging in cancer diagnosis and risk stratification.\n","authors":["Yiqiu Shen","Jungkyu Park","Frank Yeung","Eliana Goldberg","Laura Heacock","Farah Shamout","Krzysztof J. Geras"],"pdf_url":"https://arxiv.org/pdf/2311.03217v2.pdf","comment":"ML4H 2023 Findings Track"},{"id":"http://arxiv.org/abs/2311.05836v2","updated":"2023-11-15T14:37:16Z","published":"2023-11-10T02:47:15Z","title":"Uncertainty-aware Single View Volumetric Rendering for Medical Neural\n  Radiance Fields","summary":"  In the field of clinical medicine, computed tomography (CT) is an effective\nmedical imaging modality for the diagnosis of various pathologies. Compared\nwith X-ray images, CT images can provide more information, including\nmulti-planar slices and three-dimensional structures for clinical diagnosis.\nHowever, CT imaging requires patients to be exposed to large doses of ionizing\nradiation for a long time, which may cause irreversible physical harm. In this\npaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on\ngenerated radiation fields. The network can learn a continuous representation\nof CT projections from 2D X-ray images by obtaining the internal structure and\ndepth information and using adaptive loss weights to ensure the quality of the\ngenerated images. Our model is trained on publicly available knee and chest\ndatasets, and we show the results of CT projection rendering with a single\nX-ray and compare our method with other methods based on generated radiation\nfields.\n","authors":["Jing Hu","Qinrui Fan","Shu Hu","Siwei Lyu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08990v1","updated":"2023-11-15T14:22:53Z","published":"2023-11-15T14:22:53Z","title":"sQUlearn $\\unicode{x2013}$ A Python Library for Quantum Machine Learning","summary":"  sQUlearn introduces a user-friendly, NISQ-ready Python library for quantum\nmachine learning (QML), designed for seamless integration with classical\nmachine learning tools like scikit-learn. The library's dual-layer architecture\nserves both QML researchers and practitioners, enabling efficient prototyping,\nexperimentation, and pipelining. sQUlearn provides a comprehensive toolset that\nincludes both quantum kernel methods and quantum neural networks, along with\nfeatures like customizable data encoding strategies, automated execution\nhandling, and specialized kernel regularization techniques. By focusing on\nNISQ-compatibility and end-to-end automation, sQUlearn aims to bridge the gap\nbetween current quantum computing capabilities and practical machine learning\napplications.\n","authors":["David A. Kreplin","Moritz Willmann","Jan Schnabel","Frederic Rapp","Marco Roth"],"pdf_url":"https://arxiv.org/pdf/2311.08990v1.pdf","comment":"10+5 pages, 5+3 figures"},{"id":"http://arxiv.org/abs/2311.08979v1","updated":"2023-11-15T14:14:26Z","published":"2023-11-15T14:14:26Z","title":"A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory\n  Research","summary":"  This study introduces a novel, rich dataset obtained from home sleep apnea\ntests using the FDA-approved WatchPAT-300 device, collected from 7,077\nparticipants over 21,412 nights. The dataset comprises three levels of sleep\ndata: raw multi-channel time-series from sensors, annotated sleep events, and\ncomputed summary statistics, which include 447 features related to sleep\narchitecture, sleep apnea, and heart rate variability (HRV). We present\nreference values for Apnea/Hypopnea Index (AHI), sleep efficiency, Wake After\nSleep Onset (WASO), and HRV sample entropy, stratified by age and sex.\nMoreover, we demonstrate that the dataset improves the predictive capability\nfor various health related traits, including body composition, bone density,\nblood sugar levels and cardiovascular health. These results illustrate the\ndataset's potential to advance sleep research, personalized healthcare, and\nmachine learning applications in biomedicine.\n","authors":["Alon Diament","Maria Gorodetski","Adam Jankelow","Ayya Keshet","Tal Shor","Daphna Weissglas-Volkov","Hagai Rossman","Eran Segal"],"pdf_url":"https://arxiv.org/pdf/2311.08979v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 14 pages"},{"id":"http://arxiv.org/abs/2311.08978v1","updated":"2023-11-15T14:12:55Z","published":"2023-11-15T14:12:55Z","title":"Probability of Collision of satellites and space debris for short-term\n  encounters: Rederivation and fast-to-compute upper and lower bounds","summary":"  The proliferation of space debris in LEO has become a major concern for the\nspace industry. With the growing interest in space exploration, the prediction\nof potential collisions between objects in orbit has become a crucial issue. It\nis estimated that, in orbit, there are millions of fragments a few millimeters\nin size and thousands of inoperative satellites and discarded rocket stages.\nGiven the high speeds that these fragments can reach, even fragments a few\nmillimeters in size can cause fractures in a satellite's hull or put a serious\ncrack in the window of a space shuttle. The conventional method proposed by\nAkella and Alfriend in 2000 remains widely used to estimate the probability of\ncollision in short-term encounters. Given the small period of time, it is\nassumed that, during the encounter: (1) trajectories are represented by\nstraight lines with constant velocity; (2) there is no velocity uncertainty and\nthe position exhibits a stationary distribution throughout the encounter; and\n(3) position uncertainties are independent and represented by Gaussian\ndistributions. This study introduces a novel derivation based on first\nprinciples that naturally allows for tight and fast upper and lower bounds for\nthe probability of collision. We tested implementations of both probability and\nbound computations with the original and our formulation on a real CDM dataset\nused in ESA's Collision Avoidance Challenge. Our approach reduces the\ncalculation of the probability to two one-dimensional integrals and has the\npotential to significantly reduce the processing time compared to the\ntraditional method, from 80% to nearly real-time.\n","authors":["Ricardo Ferreira","Cláudia Soares","Marta Guimarães"],"pdf_url":"https://arxiv.org/pdf/2311.08978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15452v4","updated":"2023-11-15T14:06:30Z","published":"2023-08-29T17:22:39Z","title":"When Do Program-of-Thoughts Work for Reasoning?","summary":"  In the realm of embodied artificial intelligence, the reasoning capabilities\nof Large Language Models (LLMs) play a pivotal role. Although there are\neffective methods like program-of-thought prompting for LLMs which uses\nprogramming language to tackle complex reasoning tasks, the specific impact of\ncode data on the improvement of reasoning capabilities remains under-explored.\nTo address this gap, we propose complexity-impacted reasoning score (CIRS),\nwhich combines structural and logical attributes, to measure the correlation\nbetween code and reasoning abilities. Specifically, we use the abstract syntax\ntree to encode the structural information and calculate logical complexity by\nconsidering the difficulty and the cyclomatic complexity. Through an empirical\nanalysis, we find not all code data of complexity can be learned or understood\nby LLMs. Optimal level of complexity is critical to the improvement of\nreasoning abilities by program-aided prompting. Then we design an\nauto-synthesizing and stratifying algorithm, and apply it to instruction\ngeneration for mathematical reasoning and code data filtering for code\ngeneration tasks. Extensive results demonstrates the effectiveness of our\nproposed approach. Code will be integrated into the EasyInstruct framework at\nhttps://github.com/zjunlp/EasyInstruct.\n","authors":["Zhen Bi","Ningyu Zhang","Yinuo Jiang","Shumin Deng","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.15452v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.08972v1","updated":"2023-11-15T14:04:37Z","published":"2023-11-15T14:04:37Z","title":"Unsupervised approaches based on optimal transport and convex analysis\n  for inverse problems in imaging","summary":"  Unsupervised deep learning approaches have recently become one of the crucial\nresearch areas in imaging owing to their ability to learn expressive and\npowerful reconstruction operators even when paired high-quality training data\nis scarcely available. In this chapter, we review theoretically principled\nunsupervised learning schemes for solving imaging inverse problems, with a\nparticular focus on methods rooted in optimal transport and convex analysis. We\nbegin by reviewing the optimal transport-based unsupervised approaches such as\nthe cycle-consistency-based models and learned adversarial regularization\nmethods, which have clear probabilistic interpretations. Subsequently, we give\nan overview of a recent line of works on provably convergent learned\noptimization algorithms applied to accelerate the solution of imaging inverse\nproblems, alongside their dedicated unsupervised training schemes. We also\nsurvey a number of provably convergent plug-and-play algorithms (based on\ngradient-step deep denoisers), which are among the most important and widely\napplied unsupervised approaches for imaging problems. At the end of this\nsurvey, we provide an overview of a few related unsupervised learning\nframeworks that complement our focused schemes. Together with a detailed\nsurvey, we provide an overview of the key mathematical results that underlie\nthe methods reviewed in the chapter to keep our discussion self-contained.\n","authors":["Marcello Carioni","Subhadip Mukherjee","Hong Ye Tan","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2311.08972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10405v6","updated":"2023-11-15T14:00:17Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hyper network to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Ningyu Zhang","Bozhong Tian","Xi Chen","Qingbing Liu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v6.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2311.08949v1","updated":"2023-11-15T13:35:40Z","published":"2023-11-15T13:35:40Z","title":"Automated Volume Corrected Mitotic Index Calculation Through\n  Annotation-Free Deep Learning using Immunohistochemistry as Reference\n  Standard","summary":"  The volume-corrected mitotic index (M/V-Index) was shown to provide\nprognostic value in invasive breast carcinomas. However, despite its prognostic\nsignificance, it is not established as the standard method for assessing\naggressive biological behaviour, due to the high additional workload associated\nwith determining the epithelial proportion. In this work, we show that using a\ndeep learning pipeline solely trained with an annotation-free,\nimmunohistochemistry-based approach, provides accurate estimations of\nepithelial segmentation in canine breast carcinomas. We compare our automatic\nframework with the manually annotated M/V-Index in a study with three\nboard-certified pathologists. Our results indicate that the deep learning-based\npipeline shows expert-level performance, while providing time efficiency and\nreproducibility.\n","authors":["Jonas Ammeling","Moritz Hecker","Jonathan Ganz","Taryn A. Donovan","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2311.08949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02998v2","updated":"2023-11-15T13:30:39Z","published":"2022-10-06T15:38:02Z","title":"ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical\n  Prior Probability Maps for Thoracic Disease Classification","summary":"  Objective: Computer-aided disease diagnosis and prognosis based on medical\nimages is a rapidly emerging field. Many Convolutional Neural Network (CNN)\narchitectures have been developed by researchers for disease classification and\nlocalization from chest X-ray images. It is known that different thoracic\ndisease lesions are more likely to occur in specific anatomical regions\ncompared to others. This article aims to incorporate this disease and\nregion-dependent prior probability distribution within a deep learning\nframework. Methods: We present the ThoraX-PriorNet, a novel attention-based CNN\nmodel for thoracic disease classification. We first estimate a\ndisease-dependent spatial probability, i.e., an anatomical prior, that\nindicates the probability of occurrence of a disease in a specific region in a\nchest X-ray image. Next, we develop a novel attention-based classification\nmodel that combines information from the estimated anatomical prior and\nautomatically extracted chest region of interest (ROI) masks to provide\nattention to the feature maps generated from a deep convolution network. Unlike\nprevious works that utilize various self-attention mechanisms, the proposed\nmethod leverages the extracted chest ROI masks along with the probabilistic\nanatomical prior information, which selects the region of interest for\ndifferent diseases to provide attention. Results: The proposed method shows\nsuperior performance in disease classification on the NIH ChestX-ray14 dataset\ncompared to existing state-of-the-art methods while reaching an area under the\nROC curve (%AUC) of 84.67. Regarding disease localization, the anatomy prior\nattention method shows competitive performance compared to state-of-the-art\nmethods, achieving an accuracy of 0.80, 0.63, 0.49, 0.33, 0.28, 0.21, and 0.04\nwith an Intersection over Union (IoU) threshold of 0.1, 0.2, 0.3, 0.4, 0.5,\n0.6, and 0.7, respectively.\n","authors":["Md. Iqbal Hossain","Mohammad Zunaed","Md. Kawsar Ahmed","S. M. Jawwad Hossain","Anwarul Hasan","Taufiq Hasan"],"pdf_url":"https://arxiv.org/pdf/2210.02998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08945v1","updated":"2023-11-15T13:29:49Z","published":"2023-11-15T13:29:49Z","title":"A Single-Loop Algorithm for Decentralized Bilevel Optimization","summary":"  Bilevel optimization has received more and more attention recently due to its\nwide applications in machine learning. In this paper, we consider bilevel\noptimization in decentralized networks. In particular, we propose a novel\nsingle-loop algorithm for solving decentralized bilevel optimization with\nstrongly convex lower level problem. Our algorithm is fully single-loop and\ndoes not require heavy matrix-vector multiplications when approximating the\nhypergradient. Moreover, unlike existing methods for decentralized bilevel\noptimization and federated bilevel optimization, our algorithm does not require\nany gradient heterogeneity assumption. Our analysis shows that the proposed\nalgorithm achieves the best known convergence rate for bilevel optimization\nalgorithms.\n","authors":["Youran Dong","Shiqian Ma","Junfeng Yang","Chao Yin"],"pdf_url":"https://arxiv.org/pdf/2311.08945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08936v1","updated":"2023-11-15T13:19:02Z","published":"2023-11-15T13:19:02Z","title":"Confident Naturalness Explanation (CNE): A Framework to Explain and\n  Assess Patterns Forming Naturalness","summary":"  Protected natural areas are regions that have been minimally affected by\nhuman activities such as urbanization, agriculture, and other human\ninterventions. To better understand and map the naturalness of these areas,\nmachine learning models can be used to analyze satellite imagery. Specifically,\nexplainable machine learning methods show promise in uncovering patterns that\ncontribute to the concept of naturalness within these protected environments.\nAdditionally, addressing the uncertainty inherent in machine learning models is\ncrucial for a comprehensive understanding of this concept. However, existing\napproaches have limitations. They either fail to provide explanations that are\nboth valid and objective or struggle to offer a quantitative metric that\naccurately measures the contribution of specific patterns to naturalness, along\nwith the associated confidence. In this paper, we propose a novel framework\ncalled the Confident Naturalness Explanation (CNE) framework. This framework\ncombines explainable machine learning and uncertainty quantification to assess\nand explain naturalness. We introduce a new quantitative metric that describes\nthe confident contribution of patterns to the concept of naturalness.\nFurthermore, we generate an uncertainty-aware segmentation mask for each input\nsample, highlighting areas where the model lacks knowledge. To demonstrate the\neffectiveness of our framework, we apply it to a study site in Fennoscandia\nusing two open-source satellite datasets.\n","authors":["Ahmed Emam","Mohamed Farag","Ribana Roscher"],"pdf_url":"https://arxiv.org/pdf/2311.08936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08935v1","updated":"2023-11-15T13:16:16Z","published":"2023-11-15T13:16:16Z","title":"Supported Trust Region Optimization for Offline Reinforcement Learning","summary":"  Offline reinforcement learning suffers from the out-of-distribution issue and\nextrapolation error. Most policy constraint methods regularize the density of\nthe trained policy towards the behavior policy, which is too restrictive in\nmost cases. We propose Supported Trust Region optimization (STR) which performs\ntrust region policy optimization with the policy constrained within the support\nof the behavior policy, enjoying the less restrictive support constraint. We\nshow that, when assuming no approximation and sampling error, STR guarantees\nstrict policy improvement until convergence to the optimal support-constrained\npolicy in the dataset. Further with both errors incorporated, STR still\nguarantees safe policy improvement for each step. Empirical results validate\nthe theory of STR and demonstrate its state-of-the-art performance on MuJoCo\nlocomotion domains and much more challenging AntMaze domains.\n","authors":["Yixiu Mao","Hongchang Zhang","Chen Chen","Yi Xu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2311.08935v1.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2305.08703v4","updated":"2023-11-15T12:55:56Z","published":"2023-05-15T15:06:20Z","title":"Schema-adaptable Knowledge Graph Construction","summary":"  Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n","authors":["Hongbin Ye","Honghao Gui","Xin Xu","Xi Chen","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08703v4.pdf","comment":"EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2208.14407v3","updated":"2023-11-15T12:39:37Z","published":"2022-08-30T17:19:26Z","title":"An Analysis of Model-Based Reinforcement Learning From Abstracted\n  Observations","summary":"  Many methods for Model-based Reinforcement learning (MBRL) in Markov decision\nprocesses (MDPs) provide guarantees for both the accuracy of the model they can\ndeliver and the learning efficiency. At the same time, state abstraction\ntechniques allow for a reduction of the size of an MDP while maintaining a\nbounded loss with respect to the original problem. Therefore, it may come as a\nsurprise that no such guarantees are available when combining both techniques,\ni.e., where MBRL merely observes abstract states. Our theoretical analysis\nshows that abstraction can introduce a dependence between samples collected\nonline (e.g., in the real world). That means that, without taking this\ndependence into account, results for MBRL do not directly extend to this\nsetting. Our result shows that we can use concentration inequalities for\nmartingales to overcome this problem. This result makes it possible to extend\nthe guarantees of existing MBRL algorithms to the setting with abstraction. We\nillustrate this by combining R-MAX, a prototypical MBRL algorithm, with\nabstraction, thus producing the first performance guarantees for model-based\n'RL from Abstracted Observations': model-based reinforcement learning with an\nabstract model.\n","authors":["Rolf A. N. Starre","Marco Loog","Elena Congeduti","Frans A. Oliehoek"],"pdf_url":"https://arxiv.org/pdf/2208.14407v3.pdf","comment":"36 pages, 2 figures, published in Transactions on Machine Learning\n  Research (TMLR) 2023"},{"id":"http://arxiv.org/abs/2311.08914v1","updated":"2023-11-15T12:36:45Z","published":"2023-11-15T12:36:45Z","title":"Efficiently Escaping Saddle Points for Non-Convex Policy Optimization","summary":"  Policy gradient (PG) is widely used in reinforcement learning due to its\nscalability and good performance. In recent years, several variance-reduced PG\nmethods have been proposed with a theoretical guarantee of converging to an\napproximate first-order stationary point (FOSP) with the sample complexity of\n$O(\\epsilon^{-3})$. However, FOSPs could be bad local optima or saddle points.\nMoreover, these algorithms often use importance sampling (IS) weights which\ncould impair the statistical effectiveness of variance reduction. In this\npaper, we propose a variance-reduced second-order method that uses second-order\ninformation in the form of Hessian vector products (HVP) and converges to an\napproximate second-order stationary point (SOSP) with sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. This rate improves the best-known sample complexity\nfor achieving approximate SOSPs by a factor of $O(\\epsilon^{-0.5})$. Moreover,\nthe proposed variance reduction technique bypasses IS weights by using HVP\nterms. Our experimental results show that the proposed algorithm outperforms\nthe state of the art and is more robust to changes in random seeds.\n","authors":["Sadegh Khorasani","Saber Salehkaleybar","Negar Kiyavash","Niao He","Matthias Grossglauser"],"pdf_url":"https://arxiv.org/pdf/2311.08914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08909v1","updated":"2023-11-15T12:26:31Z","published":"2023-11-15T12:26:31Z","title":"DLAS: An Exploration and Assessment of the Deep Learning Acceleration\n  Stack","summary":"  Deep Neural Networks (DNNs) are extremely computationally demanding, which\npresents a large barrier to their deployment on resource-constrained devices.\nSince such devices are where many emerging deep learning applications lie\n(e.g., drones, vision-based medical technology), significant bodies of work\nfrom both the machine learning and systems communities have attempted to\nprovide optimizations to accelerate DNNs. To help unify these two perspectives,\nin this paper we combine machine learning and systems techniques within the\nDeep Learning Acceleration Stack (DLAS), and demonstrate how these layers can\nbe tightly dependent on each other with an across-stack perturbation study. We\nevaluate the impact on accuracy and inference time when varying different\nparameters of DLAS across two datasets, seven popular DNN architectures, four\nDNN compression techniques, three algorithmic primitives with sparse and dense\nvariants, untuned and auto-scheduled code generation, and four hardware\nplatforms. Our evaluation highlights how perturbations across DLAS parameters\ncan cause significant variation and across-stack interactions. The highest\nlevel observation from our evaluation is that the model size, accuracy, and\ninference time are not guaranteed to be correlated. Overall we make 13 key\nobservations, including that speedups provided by compression techniques are\nvery hardware dependent, and that compiler auto-tuning can significantly alter\nwhat the best algorithm to use for a given configuration is. With DLAS, we aim\nto provide a reference framework to aid machine learning and systems\npractitioners in reasoning about the context in which their respective DNN\nacceleration solutions exist in. With our evaluation strongly motivating the\nneed for co-design, we believe that DLAS can be a valuable concept for\nexploring the next generation of co-designed accelerated deep learning\nsolutions.\n","authors":["Perry Gibson","José Cano","Elliot J. Crowley","Amos Storkey","Michael O'Boyle"],"pdf_url":"https://arxiv.org/pdf/2311.08909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08902v1","updated":"2023-11-15T12:18:15Z","published":"2023-11-15T12:18:15Z","title":"On the Importance of Step-wise Embeddings for Heterogeneous Clinical\n  Time-Series","summary":"  Recent advances in deep learning architectures for sequence modeling have not\nfully transferred to tasks handling time-series from electronic health records.\nIn particular, in problems related to the Intensive Care Unit (ICU), the\nstate-of-the-art remains to tackle sequence classification in a tabular manner\nwith tree-based methods. Recent findings in deep learning for tabular data are\nnow surpassing these classical methods by better handling the severe\nheterogeneity of data input features. Given the similar level of feature\nheterogeneity exhibited by ICU time-series and motivated by these findings, we\nexplore these novel methods' impact on clinical sequence modeling tasks. By\njointly using such advances in deep learning for tabular data, our primary\nobjective is to underscore the importance of step-wise embeddings in\ntime-series modeling, which remain unexplored in machine learning methods for\nclinical data. On a variety of clinically relevant tasks from two large-scale\nICU datasets, MIMIC-III and HiRID, our work provides an exhaustive analysis of\nstate-of-the-art methods for tabular time-series as time-step embedding models,\nshowing overall performance improvement. In particular, we evidence the\nimportance of feature grouping in clinical time-series, with significant\nperformance gains when considering features within predefined semantic groups\nin the step-wise embedding module.\n","authors":["Rita Kuznetsova","Alizée Pace","Manuel Burger","Hugo Yèche","Gunnar Rätsch"],"pdf_url":"https://arxiv.org/pdf/2311.08902v1.pdf","comment":"Machine Learning for Health (ML4H) 2023 in Proceedings of Machine\n  Learning Research 225"},{"id":"http://arxiv.org/abs/2305.06295v3","updated":"2023-11-15T12:05:25Z","published":"2023-05-10T16:36:54Z","title":"Extracting Diagnosis Pathways from Electronic Health Records Using Deep\n  Reinforcement Learning","summary":"  Clinical diagnosis guidelines aim at specifying the steps that may lead to a\ndiagnosis. Inspired by guidelines, we aim to learn the optimal sequence of\nactions to perform in order to obtain a correct diagnosis from electronic\nhealth records. We apply various deep reinforcement learning algorithms to this\ntask and experiment on a synthetic but realistic dataset to differentially\ndiagnose anemia and its subtypes and particularly evaluate the robustness of\nvarious approaches to noise and missing data. Experimental results show that\nthe deep reinforcement learning algorithms show competitive performance\ncompared to the state-of-the-art methods with the added advantage that they\nenable the progressive generation of a pathway to the suggested diagnosis,\nwhich can both guide and explain the decision process.\n","authors":["Lillian Muyama","Antoine Neuraz","Adrien Coulet"],"pdf_url":"https://arxiv.org/pdf/2305.06295v3.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 17 pages"},{"id":"http://arxiv.org/abs/2207.09858v3","updated":"2023-11-15T11:47:19Z","published":"2022-07-20T12:46:26Z","title":"GenHPF: General Healthcare Predictive Framework with Multi-task\n  Multi-source Learning","summary":"  Despite the remarkable progress in the development of predictive models for\nhealthcare, applying these algorithms on a large scale has been challenging.\nAlgorithms trained on a particular task, based on specific data formats\navailable in a set of medical records, tend to not generalize well to other\ntasks or databases in which the data fields may differ. To address this\nchallenge, we propose General Healthcare Predictive Framework (GenHPF), which\nis applicable to any EHR with minimal preprocessing for multiple prediction\ntasks. GenHPF resolves heterogeneity in medical codes and schemas by converting\nEHRs into a hierarchical textual representation while incorporating as many\nfeatures as possible. To evaluate the efficacy of GenHPF, we conduct multi-task\nlearning experiments with single-source and multi-source settings, on three\npublicly available EHR datasets with different schemas for 12 clinically\nmeaningful prediction tasks. Our framework significantly outperforms baseline\nmodels that utilize domain knowledge in multi-source learning, improving\naverage AUROC by 1.2%P in pooled learning and 2.6%P in transfer learning while\nalso showing comparable results when trained on a single EHR dataset.\nFurthermore, we demonstrate that self-supervised pretraining using multi-source\ndatasets is effective when combined with GenHPF, resulting in a 0.6%P AUROC\nimprovement compared to models without pretraining. By eliminating the need for\npreprocessing and feature engineering, we believe that this work offers a solid\nframework for multi-task and multi-source learning that can be leveraged to\nspeed up the scaling and usage of predictive algorithms in healthcare.\n","authors":["Kyunghoon Hur","Jungwoo Oh","Junu Kim","Jiyoun Kim","Min Jae Lee","Eunbyeol Cho","Seong-Eun Moon","Young-Hak Kim","Louis Atallah","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2207.09858v3.pdf","comment":"Accepted by IEEE Journal of Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2205.01514v4","updated":"2023-11-15T11:42:47Z","published":"2022-05-03T14:10:15Z","title":"Tunable Quantum Neural Networks in the QPAC-Learning Framework","summary":"  In this paper, we investigate the performances of tunable quantum neural\nnetworks in the Quantum Probably Approximately Correct (QPAC) learning\nframework. Tunable neural networks are quantum circuits made of\nmulti-controlled X gates. By tuning the set of controls these circuits are able\nto approximate any Boolean functions. This architecture is particularly suited\nto be used in the QPAC-learning framework as it can handle the superposition\nproduced by the oracle. In order to tune the network so that it can approximate\na target concept, we have devised and implemented an algorithm based on\namplitude amplification. The numerical results show that this approach can\nefficiently learn concepts from a simple class.\n","authors":["Viet Pham Ngoc","David Tuckey","Herbert Wiklicky"],"pdf_url":"https://arxiv.org/pdf/2205.01514v4.pdf","comment":"In Proceedings QPL 2022, arXiv:2311.08375"},{"id":"http://arxiv.org/abs/2307.08433v3","updated":"2023-11-15T11:40:42Z","published":"2023-07-17T12:25:52Z","title":"From random-walks to graph-sprints: a low-latency node embedding\n  framework on continuous-time dynamic graphs","summary":"  Many real-world datasets have an underlying dynamic graph structure, where\nentities and their interactions evolve over time. Machine learning models\nshould consider these dynamics in order to harness their full potential in\ndownstream tasks. Previous approaches for graph representation learning have\nfocused on either sampling k-hop neighborhoods, akin to breadth-first search,\nor random walks, akin to depth-first search. However, these methods are\ncomputationally expensive and unsuitable for real-time, low-latency inference\non dynamic graphs. To overcome these limitations, we propose graph-sprints a\ngeneral purpose feature extraction framework for continuous-time-dynamic-graphs\n(CTDGs) that has low latency and is competitive with state-of-the-art, higher\nlatency models. To achieve this, a streaming, low latency approximation to the\nrandom-walk based features is proposed. In our framework, time-aware node\nembeddings summarizing multi-hop information are computed using only single-hop\noperations on the incoming edges. We evaluate our proposed approach on three\nopen-source datasets and two in-house datasets, and compare with three\nstate-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our\ngraph-sprints features, combined with a machine learning classifier, achieve\ncompetitive performance (outperforming all baselines for the node\nclassification tasks in five datasets). Simultaneously, graph-sprints\nsignificantly reduce inference latencies, achieving close to an order of\nmagnitude speed-up in our experimental setting.\n","authors":["Ahmad Naser Eddin","Jacopo Bono","David Aparício","Hugo Ferreira","João Ascensão","Pedro Ribeiro","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2307.08433v3.pdf","comment":"9 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2311.08877v1","updated":"2023-11-15T11:27:44Z","published":"2023-11-15T11:27:44Z","title":"Llamas Know What GPTs Don't Show: Surrogate Models for Confidence\n  Estimation","summary":"  To maintain user trust, large language models (LLMs) should signal low\nconfidence on examples where they are incorrect, instead of misleading the\nuser. The standard approach of estimating confidence is to use the softmax\nprobabilities of these models, but as of November 2023, state-of-the-art LLMs\nsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\nfirst study eliciting confidence linguistically -- asking an LLM for its\nconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\naveraged across 12 question-answering datasets -- 7% above a random baseline)\nbut leaves room for improvement. We then explore using a surrogate confidence\nmodel -- using a model where we do have probabilities to evaluate the original\nmodel's confidence in a given question. Surprisingly, even though these\nprobabilities come from a different and often weaker model, this method leads\nto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\nmethod composing linguistic confidences and surrogate model probabilities gives\nstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\nGPT-4).\n","authors":["Vaishnavi Shrivastava","Percy Liang","Ananya Kumar"],"pdf_url":"https://arxiv.org/pdf/2311.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12554v3","updated":"2023-11-15T11:23:32Z","published":"2023-01-29T22:05:28Z","title":"Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive\n  Smoothing","summary":"  While prior research has proposed a plethora of methods that build neural\nclassifiers robust against adversarial robustness, practitioners are still\nreluctant to adopt them due to their unacceptably severe clean accuracy\npenalties. This paper significantly alleviates this accuracy-robustness\ntrade-off by mixing the output probabilities of a standard classifier and a\nrobust classifier, where the standard network is optimized for clean accuracy\nand is not robust in general. We show that the robust base classifier's\nconfidence difference for correct and incorrect examples is the key to this\nimprovement. In addition to providing intuitions and empirical evidence, we\ntheoretically certify the robustness of the mixed classifier under realistic\nassumptions. Furthermore, we adapt an adversarial input detector into a mixing\nnetwork that adaptively adjusts the mixture of the two base models, further\nreducing the accuracy penalty of achieving robustness. The proposed flexible\nmethod, termed \"adaptive smoothing\", can work in conjunction with existing or\neven future methods that improve clean accuracy, robustness, or adversary\ndetection. Our empirical evaluation considers strong attack methods, including\nAutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves\nan 85.21% clean accuracy while maintaining a 38.72% $\\ell_\\infty$-AutoAttacked\n($\\epsilon = 8/255$) accuracy, becoming the second most robust method on the\nRobustBench CIFAR-100 benchmark as of submission, while improving the clean\naccuracy by ten percentage points compared with all listed models. The code\nthat implements our method is available at\nhttps://github.com/Bai-YT/AdaptiveSmoothing.\n","authors":["Yatong Bai","Brendon G. Anderson","Aerin Kim","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2301.12554v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08874v1","updated":"2023-11-15T11:23:15Z","published":"2023-11-15T11:23:15Z","title":"Towards Label Embedding -- Measuring classification difficulty","summary":"  Uncertainty quantification in machine learning is a timely and vast field of\nresearch. In supervised learning, uncertainty can already occur in the very\nfirst stage of the training process, the labelling step. In particular, this is\nthe case when not every instance can be unambiguously classified. The problem\noccurs for classifying instances, where classes may overlap or instances can\nnot be clearly categorised. In other words, there is inevitable ambiguity in\nthe annotation step and not necessarily a 'ground truth'. We look exemplary at\nthe classification of satellite images. Each image is annotated independently\nby multiple labellers and classified into local climate zones (LCZs). For each\ninstance we have multiple votes, leading to a distribution of labels rather\nthan a single value. The main idea of this work is that we do not assume a\nground truth label but embed the votes into a K-dimensional space, with K as\nthe number of possible categories. The embedding is derived from the voting\ndistribution in a Bayesian setup, modelled via a Dirichlet-Multinomial model.\nWe estimate the model and posteriors using a stochastic Expectation\nMaximisation algorithm with Markov Chain Monte Carlo steps. While we focus on\nthe particular example of LCZ classification, the methods developed in this\npaper readily extend to other situations where multiple annotators\nindependently label texts or images. We also apply our approach to two other\nbenchmark datasets for image classification to demonstrate this. Besides the\nembeddings themselves, we can investigate the resulting correlation matrices,\nwhich can be seen as generalised confusion matrices and reflect the semantic\nsimilarities of the original classes very well for all three exemplary\ndatasets. The insights gained are valuable and can serve as general label\nembedding if a single ground truth per observation cannot be guaranteed.\n","authors":["Katharina Hechinger","Christoph Koller","Xiao Xiang Zhu","Göran Kauermann"],"pdf_url":"https://arxiv.org/pdf/2311.08874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01258v3","updated":"2023-11-15T11:22:05Z","published":"2023-10-02T14:50:14Z","title":"MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device","summary":"  Neural video codecs have recently become competitive with standard codecs\nsuch as HEVC in the low-delay setting. However, most neural codecs are large\nfloating-point networks that use pixel-dense warping operations for temporal\nmodeling, making them too computationally expensive for deployment on mobile\ndevices. Recent work has demonstrated that running a neural decoder in real\ntime on mobile is feasible, but shows this only for 720p RGB video. This work\npresents the first neural video codec that decodes 1080p YUV420 video in real\ntime on a mobile device. Our codec relies on two major contributions. First, we\ndesign an efficient codec that uses a block-based motion compensation algorithm\navailable on the warping core of the mobile accelerator, and we show how to\nquantize this model to integer precision. Second, we implement a fast decoder\npipeline that concurrently runs neural network components on the neural signal\nprocessor, parallel entropy coding on the mobile GPU, and warping on the\nwarping core. Our codec outperforms the previous on-device codec by a large\nmargin with up to 48% BD-rate savings, while reducing the MAC count on the\nreceiver side by $10 \\times$. We perform a careful ablation to demonstrate the\neffect of the introduced motion compensation scheme, and ablate the effect of\nmodel quantization.\n","authors":["Ties van Rozendaal","Tushar Singhal","Hoang Le","Guillaume Sautiere","Amir Said","Krishna Buska","Anjuman Raha","Dimitris Kalatzis","Hitarth Mehta","Frank Mayer","Liang Zhang","Markus Nagel","Auke Wiggers"],"pdf_url":"https://arxiv.org/pdf/2310.01258v3.pdf","comment":"Matches version published at WACV 2024"},{"id":"http://arxiv.org/abs/2311.08870v1","updated":"2023-11-15T11:11:25Z","published":"2023-11-15T11:11:25Z","title":"One-Shot Federated Learning with Classifier-Guided Diffusion Models","summary":"  One-shot federated learning (OSFL) has gained attention in recent years due\nto its low communication cost. However, most of the existing methods require\nauxiliary datasets or training generators, which hinders their practicality in\nreal-world scenarios. In this paper, we explore the novel opportunities that\ndiffusion models bring to OSFL and propose FedCADO, utilizing guidance from\nclient classifiers to generate data that complies with clients' distributions\nand subsequently training the aggregated model on the server. Specifically, our\nmethod involves targeted optimizations in two aspects. On one hand, we\nconditionally edit the randomly sampled initial noises, embedding them with\nspecified semantics and distributions, resulting in a significant improvement\nin both the quality and stability of generation. On the other hand, we employ\nthe BN statistics from the classifiers to provide detailed guidance during\ngeneration. These tailored optimizations enable us to limitlessly generate\ndatasets, which closely resemble the distribution and quality of the original\nclient dataset. Our method effectively handles the heterogeneous client models\nand the problems of non-IID features or labels. In terms of privacy protection,\nour method avoids training any generator or transferring any auxiliary\ninformation on clients, eliminating any additional privacy leakage risks.\nLeveraging the extensive knowledge stored in the pre-trained diffusion model,\nthe synthetic datasets can assist us in surpassing the knowledge limitations of\nthe client samples, resulting in aggregation models that even outperform the\nperformance ceiling of centralized training in some cases, which is\nconvincingly demonstrated in the sufficient quantification and visualization\nexperiments conducted on three large-scale multi-domain image datasets.\n","authors":["Mingzhao Yang","Shangchao Su","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2311.08870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15681v2","updated":"2023-11-15T11:10:12Z","published":"2023-10-24T09:47:32Z","title":"Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed\n  Bandit","summary":"  We study the real-valued combinatorial pure exploration of the multi-armed\nbandit in the fixed-budget setting. We first introduce the Combinatorial\nSuccessive Asign (CSA) algorithm, which is the first algorithm that can\nidentify the best action even when the size of the action class is\nexponentially large with respect to the number of arms. We show that the upper\nbound of the probability of error of the CSA algorithm matches a lower bound up\nto a logarithmic factor in the exponent. Then, we introduce another algorithm\nnamed the Minimax Combinatorial Successive Accepts and Rejects\n(Minimax-CombSAR) algorithm for the case where the size of the action class is\npolynomial, and show that it is optimal, which matches a lower bound. Finally,\nwe experimentally compare the algorithms with previous methods and show that\nour algorithm performs better.\n","authors":["Shintaro Nakamura","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2310.15681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14421v4","updated":"2023-11-15T11:05:34Z","published":"2023-10-19T10:36:02Z","title":"On existence, uniqueness and scalability of adversarial robustness\n  measures for AI classifiers","summary":"  Simply-verifiable mathematical conditions for existence, uniqueness and\nexplicit analytical computation of minimal adversarial paths (MAP) and minimal\nadversarial distances (MAD) for (locally) uniquely-invertible classifiers, for\ngeneralized linear models (GLM), and for entropic AI (EAI) are formulated and\nproven. Practical computation of MAP and MAD, their comparison and\ninterpretations for various classes of AI tools (for neuronal networks, boosted\nrandom forests, GLM and EAI) are demonstrated on the common synthetic\nbenchmarks: on a double Swiss roll spiral and its extensions, as well as on the\ntwo biomedical data problems (for the health insurance claim predictions, and\nfor the heart attack lethality classification). On biomedical applications it\nis demonstrated how MAP provides unique minimal patient-specific\nrisk-mitigating interventions in the predefined subsets of accessible control\nvariables.\n","authors":["Illia Horenko"],"pdf_url":"https://arxiv.org/pdf/2310.14421v4.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.10238v3","updated":"2023-11-15T11:00:20Z","published":"2023-08-20T11:56:02Z","title":"Thompson Sampling for Real-Valued Combinatorial Pure Exploration of\n  Multi-Armed Bandit","summary":"  We study the real-valued combinatorial pure exploration of the multi-armed\nbandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic\narms, and the reward of each arm $s\\in\\{1, \\ldots, d\\}$ follows an unknown\ndistribution with mean $\\mu_s$. In each time step, a player pulls a single arm\nand observes its reward. The player's goal is to identify the optimal\n\\emph{action} $\\boldsymbol{\\pi}^{*} = \\argmax_{\\boldsymbol{\\pi} \\in\n\\mathcal{A}} \\boldsymbol{\\mu}^{\\top}\\boldsymbol{\\pi}$ from a finite-sized\nreal-valued \\emph{action set} $\\mathcal{A}\\subset \\mathbb{R}^{d}$ with as few\narm pulls as possible. Previous methods in the R-CPE-MAB assume that the size\nof the action set $\\mathcal{A}$ is polynomial in $d$. We introduce an algorithm\nnamed the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm,\nwhich is the first algorithm that can work even when the size of the action set\nis exponentially large in $d$. We also introduce a novel problem-dependent\nsample complexity lower bound of the R-CPE-MAB problem, and show that the\nGenTS-Explore algorithm achieves the optimal sample complexity up to a\nproblem-dependent constant factor.\n","authors":["Shintaro Nakamura","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2308.10238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04037v2","updated":"2023-11-15T10:50:13Z","published":"2023-11-07T14:44:27Z","title":"Causal Discovery Under Local Privacy","summary":"  Differential privacy is a widely adopted framework designed to safeguard the\nsensitive information of data providers within a data set. It is based on the\napplication of controlled noise at the interface between the server that stores\nand processes the data, and the data consumers. Local differential privacy is a\nvariant that allows data providers to apply the privatization mechanism\nthemselves on their data individually. Therefore it provides protection also in\ncontexts in which the server, or even the data collector, cannot be trusted.\nThe introduction of noise, however, inevitably affects the utility of the data,\nparticularly by distorting the correlations between individual data components.\nThis distortion can prove detrimental to tasks such as causal discovery. In\nthis paper, we consider various well-known locally differentially private\nmechanisms and compare the trade-off between the privacy they provide, and the\naccuracy of the causal structure produced by algorithms for causal learning\nwhen applied to data obfuscated by these mechanisms. Our analysis yields\nvaluable insights for selecting appropriate local differentially private\nprotocols for causal discovery tasks. We foresee that our findings will aid\nresearchers and practitioners in conducting locally private causal discovery.\n","authors":["Rūta Binkytė","Carlos Pinzón","Szilvia Lestyán","Kangsoo Jung","Héber H. Arcolezi","Catuscia Palamidessi"],"pdf_url":"https://arxiv.org/pdf/2311.04037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03571v2","updated":"2023-11-15T10:47:17Z","published":"2023-04-07T10:11:32Z","title":"$β$-Variational autoencoders and transformers for reduced-order\n  modelling of fluid flows","summary":"  Variational autoencoder (VAE) architectures have the potential to develop\nreduced-order models (ROMs) for chaotic fluid flows. We propose a method for\nlearning compact and near-orthogonal ROMs using a combination of a $\\beta$-VAE\nand a transformer, tested on numerical data from a two-dimensional viscous flow\nin both periodic and chaotic regimes. The $\\beta$-VAE is trained to learn a\ncompact latent representation of the flow velocity, and the transformer is\ntrained to predict the temporal dynamics in latent space. Using the $\\beta$-VAE\nto learn disentangled representations in latent-space, we obtain a more\ninterpretable flow model with features that resemble those observed in the\nproper orthogonal decomposition, but with a more efficient representation.\nUsing Poincar\\'e maps, the results show that our method can capture the\nunderlying dynamics of the flow outperforming other prediction models. The\nproposed method has potential applications in other fields such as weather\nforecasting, structural dynamics or biomedical engineering.\n","authors":["Alberto Solera-Rico","Carlos Sanmiguel Vila","M. A. Gómez","Yuning Wang","Abdulrahman Almashjary","Scott T. M. Dawson","Ricardo Vinuesa"],"pdf_url":"https://arxiv.org/pdf/2304.03571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.03279v5","updated":"2023-11-15T10:43:40Z","published":"2022-05-06T15:04:12Z","title":"Probabilistic Control and Majorization of Optimal Control","summary":"  Probabilistic control design is founded on the principle that a rational\nagent attempts to match modelled with an arbitrary desired closed-loop system\ntrajectory density. The framework was originally proposed as a tractable\nalternative to traditional optimal control design, parametrizing desired\nbehaviour through fictitious transition and policy densities and using the\ninformation projection as a proximity measure. In this work we introduce an\nalternative parametrization of desired closed-loop behaviour and explore\nalternative proximity measures between densities. It is then illustrated how\nthe associated probabilistic control problems solve into uncertain or\nprobabilistic policies. Our main result is to show that the probabilistic\ncontrol objectives majorize conventional, stochastic and risk sensitive,\noptimal control objectives. This observation allows us to identify two\nprobabilistic fixed point iterations that converge to the deterministic optimal\ncontrol policies establishing an explicit connection between either\nformulations. Further we demonstrate that the risk sensitive optimal control\nformulation is also technically equivalent to a Maximum Likelihood estimation\nproblem on a probabilistic graph model where the notion of costs is directly\nencoded into the model. The associated treatment of the estimation problem is\nthen shown to coincide with the moment projected probabilistic control\nformulation. That way optimal decision making can be reformulated as an\niterative inference problem. Based on these insights we discuss directions for\nalgorithmic development.\n","authors":["Tom Lefebvre"],"pdf_url":"https://arxiv.org/pdf/2205.03279v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08851v1","updated":"2023-11-15T10:43:13Z","published":"2023-11-15T10:43:13Z","title":"Data Augmentations in Deep Weight Spaces","summary":"  Learning in weight spaces, where neural networks process the weights of other\ndeep neural networks, has emerged as a promising research direction with\napplications in various fields, from analyzing and editing neural fields and\nimplicit neural representations, to network pruning and quantization. Recent\nworks designed architectures for effective learning in that space, which takes\ninto account its unique, permutation-equivariant, structure. Unfortunately, so\nfar these architectures suffer from severe overfitting and were shown to\nbenefit from large datasets. This poses a significant challenge because\ngenerating data for this learning setup is laborious and time-consuming since\neach data sample is a full set of network weights that has to be trained. In\nthis paper, we address this difficulty by investigating data augmentations for\nweight spaces, a set of techniques that enable generating new data examples on\nthe fly without having to train additional input weight space elements. We\nfirst review several recently proposed data augmentation schemes %that were\nproposed recently and divide them into categories. We then introduce a novel\naugmentation scheme based on the Mixup method. We evaluate the performance of\nthese techniques on existing benchmarks as well as new benchmarks we generate,\nwhich can be valuable for future studies.\n","authors":["Aviv Shamsian","David W. Zhang","Aviv Navon","Yan Zhang","Miltiadis Kofinas","Idan Achituve","Riccardo Valperga","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","Ethan Fetaya","Gal Chechik","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2311.08851v1.pdf","comment":"Accepted to NeurIPS 2023 Workshop on Symmetry and Geometry in Neural\n  Representations"},{"id":"http://arxiv.org/abs/2207.04053v3","updated":"2023-11-15T10:37:30Z","published":"2022-07-08T10:37:22Z","title":"On the Need and Applicability of Causality for Fair Machine Learning","summary":"  Besides its common use cases in epidemiology, political, and social sciences,\ncausality turns out to be crucial in evaluating the fairness of automated\ndecisions, both in a legal and everyday sense. We provide arguments and\nexamples, of why causality is particularly important for fairness evaluation.\nIn particular, we point out the social impact of non-causal predictions and the\nlegal anti-discrimination process that relies on causal claims. We conclude\nwith a discussion about the challenges and limitations of applying causality in\npractical scenarios as well as possible solutions.\n","authors":["Rūta Binkytė","Ljupcho Grozdanovski","Sami Zhioua"],"pdf_url":"https://arxiv.org/pdf/2207.04053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08845v1","updated":"2023-11-15T10:35:23Z","published":"2023-11-15T10:35:23Z","title":"Statistical learning by sparse deep neural networks","summary":"  We consider a deep neural network estimator based on empirical risk\nminimization with l_1-regularization. We derive a general bound for its excess\nrisk in regression and classification (including multiclass), and prove that it\nis adaptively nearly-minimax (up to log-factors) simultaneously across the\nentire range of various function classes.\n","authors":["Felix Abramovich"],"pdf_url":"https://arxiv.org/pdf/2311.08845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07250v3","updated":"2023-11-15T10:35:07Z","published":"2023-10-11T07:27:28Z","title":"Synthesizing Missing MRI Sequences from Available Modalities using\n  Generative Adversarial Networks in BraTS Dataset","summary":"  Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic\nresonance imaging (MRI) plays a significant role in the diagnosis, treatment\nplanning, and follow-up of glioblastoma patients due to its non-invasive and\nradiation-free nature. The International Brain Tumor Segmentation (BraTS)\nchallenge has contributed to generating numerous AI algorithms to accurately\nand efficiently segment glioblastoma sub-compartments using four structural\n(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not\nalways be available. To address this issue, Generative Adversarial Networks\n(GANs) can be used to synthesize the missing MRI sequences. In this paper, we\nimplement and utilize an open-source GAN approach that takes any three MRI\nsequences as input to generate the missing fourth structural sequence. Our\nproposed approach is contributed to the community-driven generally nuanced deep\nlearning framework (GaNDLF) and demonstrates promising results in synthesizing\nhigh-quality and realistic MRI sequences, enabling clinicians to improve their\ndiagnostic capabilities and support the application of AI methods to brain\ntumor MRI quantification.\n","authors":["Ibrahim Ethem Hamamci"],"pdf_url":"https://arxiv.org/pdf/2310.07250v3.pdf","comment":"Wrong paper submission"},{"id":"http://arxiv.org/abs/2307.13390v2","updated":"2023-11-15T10:26:57Z","published":"2023-07-25T10:21:26Z","title":"Counterfactual Explanation via Search in Gaussian Mixture Distributed\n  Latent Space","summary":"  Counterfactual Explanations (CEs) are an important tool in Algorithmic\nRecourse for addressing two questions: 1. What are the crucial factors that led\nto an automated prediction/decision? 2. How can these factors be changed to\nachieve a more favorable outcome from a user's perspective? Thus, guiding the\nuser's interaction with AI systems by proposing easy-to-understand explanations\nand easy-to-attain feasible changes is essential for the trustworthy adoption\nand long-term acceptance of AI systems. In the literature, various methods have\nbeen proposed to generate CEs, and different quality measures have been\nsuggested to evaluate these methods. However, the generation of CEs is usually\ncomputationally expensive, and the resulting suggestions are unrealistic and\nthus non-actionable. In this paper, we introduce a new method to generate CEs\nfor a pre-trained binary classifier by first shaping the latent space of an\nautoencoder to be a mixture of Gaussian distributions. CEs are then generated\nin latent space by linear interpolation between the query sample and the\ncentroid of the target class. We show that our method maintains the\ncharacteristics of the input sample during the counterfactual search. In\nvarious experiments, we show that the proposed method is competitive based on\ndifferent quality measures on image and tabular datasets -- efficiently returns\nresults that are closer to the original data manifold compared to three\nstate-of-the-art methods, which are essential for realistic high-dimensional\nmachine learning applications.\n","authors":["Xuan Zhao","Klaus Broelemann","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2307.13390v2.pdf","comment":"XAI workshop of IJCAI 2023"},{"id":"http://arxiv.org/abs/2310.17658v2","updated":"2023-11-15T10:12:14Z","published":"2023-10-18T15:24:34Z","title":"Is Channel Independent strategy optimal for Time Series Forecasting?","summary":"  There has been an emergence of various models for long-term time series\nforecasting. Recent studies have demonstrated that a single linear layer, using\nChannel Dependent (CD) or Channel Independent (CI) modeling, can even\noutperform a large number of sophisticated models. However, current research\nprimarily considers CD and CI as two complementary yet mutually exclusive\napproaches, unable to harness these two extremes simultaneously. And it is also\na challenging issue that both CD and CI are static strategies that cannot be\ndetermined to be optimal for a specific dataset without extensive experiments.\nIn this paper, we reconsider whether the current CI strategy is the best\nsolution for time series forecasting. First, we propose a simple yet effective\nstrategy called CSC, which stands for $\\mathbf{C}$hannel\n$\\mathbf{S}$elf-$\\mathbf{C}$lustering strategy, for linear models. Our Channel\nSelf-Clustering (CSC) enhances CI strategy's performance improvements while\nreducing parameter size, for exmpale by over 10 times on electricity dataset,\nand significantly cutting training time. Second, we further propose Channel\nRearrangement (CR), a method for deep models inspired by the self-clustering.\nCR attains competitive performance against baselines. Finally, we also discuss\nwhether it is best to forecast the future values using the historical values of\nthe same channel as inputs. We hope our findings and methods could inspire new\nsolutions beyond CD/CI.\n","authors":["Yuan Peiwen","Zhu Changsheng"],"pdf_url":"https://arxiv.org/pdf/2310.17658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.14201v2","updated":"2023-11-15T09:52:12Z","published":"2021-05-29T03:47:10Z","title":"CNTLS: A Benchmark Dataset for Abstractive or Extractive Chinese\n  Timeline Summarization","summary":"  Timeline summarization (TLS) involves creating summaries of long-running\nevents using dated summaries from numerous news articles. However, limited data\navailability has significantly slowed down the development of timeline\nsummarization. In this paper, we introduce the CNTLS dataset, a versatile\nresource for Chinese timeline summarization. CNTLS encompasses 77 real-life\ntopics, each with 2524 documents and summarizes nearly 60\\% days duration\ncompression on average all topics.\n  We meticulously analyze the corpus using well-known metrics, focusing on the\nstyle of the summaries and the complexity of the summarization task.\nSpecifically, we evaluate the performance of various extractive and generative\nsummarization systems on the CNTLS corpus to provide benchmarks and support\nfurther research. To the best of our knowledge, CNTLS is the first Chinese\ntimeline summarization dataset. The dataset and source code are\nreleased\\footnote{Code and data available at:\n\\emph{\\url{https://github.com/OpenSUM/CNTLS}}.}.\n","authors":["Qianren Mao","Jiazheng Wang","Zheng Wang","Xi Li","Bo Li","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2105.14201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08819v1","updated":"2023-11-15T09:46:30Z","published":"2023-11-15T09:46:30Z","title":"Frequency Domain-based Dataset Distillation","summary":"  This paper presents FreD, a novel parameterization method for dataset\ndistillation, which utilizes the frequency domain to distill a small-sized\nsynthetic dataset from a large-sized original dataset. Unlike conventional\napproaches that focus on the spatial domain, FreD employs frequency-based\ntransforms to optimize the frequency representations of each data instance. By\nleveraging the concentration of spatial domain information on specific\nfrequency components, FreD intelligently selects a subset of frequency\ndimensions for optimization, leading to a significant reduction in the required\nbudget for synthesizing an instance. Through the selection of frequency\ndimensions based on the explained variance, FreD demonstrates both theoretical\nand empirical evidence of its ability to operate efficiently within a limited\nbudget, while better preserving the information of the original dataset\ncompared to conventional parameterization methods. Furthermore, based on the\northogonal compatibility of FreD with existing methods, we confirm that FreD\nconsistently improves the performances of existing distillation methods over\nthe evaluation scenarios with different benchmark datasets. We release the code\nat https://github.com/sdh0818/FreD.\n","authors":["Donghyeok Shin","Seungjae Shin","Il-Chul Moon"],"pdf_url":"https://arxiv.org/pdf/2311.08819v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08817v1","updated":"2023-11-15T09:38:53Z","published":"2023-11-15T09:38:53Z","title":"MAP's not dead yet: Uncovering true language model modes by conditioning\n  away degeneracy","summary":"  It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has\ngenerally been attributed to either a fundamental inadequacy of modes in models\nor weaknesses in language modeling. Contrastingly in this work, we emphasize\nthat degenerate modes can even occur in the absence of any model error, due to\ncontamination of the training data. Specifically, we show that mixing even a\ntiny amount of low-entropy noise with a population text distribution can cause\nthe data distribution's mode to become degenerate, implying that any models\ntrained on it will be as well. As the unconditional mode of NLG models will\noften be degenerate, we therefore propose to apply MAP decoding to the model's\ndistribution conditional on avoiding specific degeneracies. Using exact-search,\nwe empirically verify that the length-conditional modes of machine translation\nmodels and language models are indeed more fluent and topical than their\nunconditional modes. For the first time, we also share many examples of exact\nmodal sequences from these models, and from several variants of the LLaMA-7B\nmodel. Notably, the modes of the LLaMA models are still degenerate, showing\nthat improvements in modeling have not fixed this issue. Because of the cost of\nexact mode finding algorithms, we develop an approximate mode finding approach,\nACBS, which finds sequences that are both high-likelihood and high-quality. We\napply this approach to LLaMA-7B, a model which was not trained for instruction\nfollowing, and find that we are able to elicit reasonable outputs without any\nfinetuning.\n","authors":["Davis Yoshida","Kartik Goyal","Kevin Gimpel"],"pdf_url":"https://arxiv.org/pdf/2311.08817v1.pdf","comment":"49 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.08815v1","updated":"2023-11-15T09:34:08Z","published":"2023-11-15T09:34:08Z","title":"Self-Supervised Disentanglement by Leveraging Structure in Data\n  Augmentations","summary":"  Self-supervised representation learning often uses data augmentations to\ninduce some invariance to \"style\" attributes of the data. However, with\ndownstream tasks generally unknown at training time, it is difficult to deduce\na priori which attributes of the data are indeed \"style\" and can be safely\ndiscarded. To address this, we introduce a more principled approach that seeks\nto disentangle style features rather than discard them. The key idea is to add\nmultiple style embedding spaces where: (i) each is invariant to all-but-one\naugmentation; and (ii) joint entropy is maximized. We formalize our structured\ndata-augmentation procedure from a causal latent-variable-model perspective,\nand prove identifiability of both content and (multiple blocks of) style\nvariables. We empirically demonstrate the benefits of our approach on synthetic\ndatasets and then present promising but limited results on ImageNet.\n","authors":["Cian Eastwood","Julius von Kügelgen","Linus Ericsson","Diane Bouchacourt","Pascal Vincent","Bernhard Schölkopf","Mark Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2311.08815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06025v3","updated":"2023-11-15T09:02:18Z","published":"2022-07-13T08:14:18Z","title":"URANUS: Radio Frequency Tracking, Classification and Identification of\n  Unmanned Aircraft Vehicles","summary":"  Safety and security issues for Critical Infrastructures are growing as\nattackers adopt drones as an attack vector flying in sensitive airspaces, such\nas airports, military bases, city centers, and crowded places. Despite the use\nof UAVs for logistics, shipping recreation activities, and commercial\napplications, their usage poses severe concerns to operators due to the\nviolations and the invasions of the restricted airspaces. A cost-effective and\nreal-time framework is needed to detect the presence of drones in such cases.\nIn this contribution, we propose an efficient radio frequency-based detection\nframework called URANUS. We leverage real-time data provided by the Radio\nFrequency/Direction Finding system, and radars in order to detect, classify and\nidentify drones (multi-copter and fixed-wings) invading no-drone zones. We\nadopt a Multilayer Perceptron neural network to identify and classify UAVs in\nreal-time, with $90$% accuracy. For the tracking task, we use a Random Forest\nmodel to predict the position of a drone with an MSE $\\approx0.29$, MAE\n$\\approx0.04$, and $R^2\\approx 0.93$. Furthermore, coordinate regression is\nperformed using Universal Transverse Mercator coordinates to ensure high\naccuracy. Our analysis shows that URANUS is an ideal framework for identifying,\nclassifying, and tracking UAVs that most Critical Infrastructure operators can\nadopt.\n","authors":["Domenico Lofù","Pietro Di Gennaro","Pietro Tedeschi","Tommaso Di Noia","Eugenio Di Sciascio"],"pdf_url":"https://arxiv.org/pdf/2207.06025v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08788v1","updated":"2023-11-15T09:01:55Z","published":"2023-11-15T09:01:55Z","title":"X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented\n  Instruction Tuning with Auxiliary Evaluation Aspects","summary":"  Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n","authors":["Minqian Liu","Ying Shen","Zhiyang Xu","Yixin Cao","Eunah Cho","Vaibhav Kumar","Reza Ghanadan","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08788v1.pdf","comment":"17 pages, 5 figures, 14 tables"},{"id":"http://arxiv.org/abs/2212.12322v2","updated":"2023-11-15T08:53:29Z","published":"2022-12-22T08:33:32Z","title":"Infrared Image Super-Resolution: Systematic Review, and Future Trends","summary":"  Image Super-Resolution (SR) is essential for a wide range of computer vision\nand image processing tasks. Investigating infrared (IR) image (or thermal\nimages) super-resolution is a continuing concern within the development of deep\nlearning. This survey aims to provide a comprehensive perspective of IR image\nsuper-resolution, including its applications, hardware imaging system dilemmas,\nand taxonomy of image processing methodologies. In addition, the datasets and\nevaluation metrics in IR image super-resolution tasks are also discussed.\nFurthermore, the deficiencies in current technologies and possible promising\ndirections for the community to explore are highlighted. To cope with the rapid\ndevelopment in this field, we intend to regularly update the relevant excellent\nwork at \\url{https://github.com/yongsongH/Infrared_Image_SR_Survey\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2212.12322v2.pdf","comment":"Submitted to IEEE TNNLS"},{"id":"http://arxiv.org/abs/2308.00755v2","updated":"2023-11-15T08:51:11Z","published":"2023-08-01T18:00:08Z","title":"The Bias Amplification Paradox in Text-to-Image Generation","summary":"  Bias amplification is a phenomenon in which models exacerbate biases or\nstereotypes present in the training data. In this paper, we study bias\namplification in the text-to-image domain using Stable Diffusion by comparing\ngender ratios in training vs. generated images. We find that the model appears\nto amplify gender-occupation biases found in the training data (LAION)\nconsiderably. However, we discover that amplification can be largely attributed\nto discrepancies between training captions and model prompts. For example, an\ninherent difference is that captions from the training data often contain\nexplicit gender information while our prompts do not, which leads to a\ndistribution shift and consequently inflates bias measures. Once we account for\ndistributional differences between texts used for training and generation when\nevaluating amplification, we observe that amplification decreases drastically.\nOur findings illustrate the challenges of comparing biases in models and their\ntraining data, and highlight confounding factors that impact analyses.\n","authors":["Preethi Seshadri","Sameer Singh","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2308.00755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15612v3","updated":"2023-11-15T08:47:28Z","published":"2023-10-24T08:27:56Z","title":"Machine Translation for Nko: Tools, Corpora and Baseline Results","summary":"  Currently, there is no usable machine translation system for Nko, a language\nspoken by tens of millions of people across multiple West African countries,\nwhich holds significant cultural and educational value.\n  To address this issue, we present a set of tools, resources, and baseline\nresults aimed towards the development of usable machine translation systems for\nNko and other languages that do not currently have sufficiently large parallel\ntext corpora available.\n  (1) Fria$\\parallel$el: A novel collaborative parallel text curation software\nthat incorporates quality control through copyedit-based workflows.\n  (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193\nhigh-quality Nko translations in parallel with 204 and 40 other languages.\n  (3) nicolingua-0005: A collection of trilingual and bilingual corpora with\n130,850 parallel segments and monolingual corpora containing over 3 million Nko\nwords.\n  (4) Baseline bilingual and multilingual neural machine translation results\nwith the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.\n","authors":["Moussa Koulako Bala Doumbouya","Baba Mamadi Diané","Solo Farabado Cissé","Djibrila Diané","Abdoulaye Sow","Séré Moussa Doumbouya","Daouda Bangoura","Fodé Moriba Bayo","Ibrahima Sory 2. Condé","Kalo Mory Diané","Chris Piech","Christopher Manning"],"pdf_url":"https://arxiv.org/pdf/2310.15612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08774v1","updated":"2023-11-15T08:37:11Z","published":"2023-11-15T08:37:11Z","title":"Two-stage Joint Transductive and Inductive learning for Nuclei\n  Segmentation","summary":"  AI-assisted nuclei segmentation in histopathological images is a crucial task\nin the diagnosis and treatment of cancer diseases. It decreases the time\nrequired to manually screen microscopic tissue images and can resolve the\nconflict between pathologists during diagnosis. Deep Learning has proven useful\nin such a task. However, lack of labeled data is a significant barrier for deep\nlearning-based approaches. In this study, we propose a novel approach to nuclei\nsegmentation that leverages the available labelled and unlabelled data. The\nproposed method combines the strengths of both transductive and inductive\nlearning, which have been previously attempted separately, into a single\nframework. Inductive learning aims at approximating the general function and\ngeneralizing to unseen test data, while transductive learning has the potential\nof leveraging the unlabelled test data to improve the classification. To the\nbest of our knowledge, this is the first study to propose such a hybrid\napproach for medical image segmentation. Moreover, we propose a novel two-stage\ntransductive inference scheme. We evaluate our approach on MoNuSeg benchmark to\ndemonstrate the efficacy and potential of our method.\n","authors":["Hesham Ali","Idriss Tondji","Mennatullah Siam"],"pdf_url":"https://arxiv.org/pdf/2311.08774v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2208.02474v3","updated":"2023-11-15T08:35:24Z","published":"2022-08-04T05:54:36Z","title":"CFARnet: deep learning for target detection with constant false alarm\n  rate","summary":"  We consider the problem of target detection with a constant false alarm rate\n(CFAR). This constraint is crucial in many practical applications and is a\nstandard requirement in classical composite hypothesis testing. In settings\nwhere classical approaches are computationally expensive or where only data\nsamples are given, machine learning methodologies are advantageous. CFAR is\nless understood in these settings. To close this gap, we introduce a framework\nof CFAR constrained detectors. Theoretically, we prove that a CFAR constrained\nBayes optimal detector is asymptotically equivalent to the classical\ngeneralized likelihood ratio test (GLRT). Practically, we develop a deep\nlearning framework for fitting neural networks that approximate it. Experiments\nof target detection in different setting demonstrate that the proposed CFARnet\nallows a flexible tradeoff between CFAR and accuracy.\n","authors":["Tzvi Diskin","Yiftach Beer","Uri Okun","Ami Wiesel"],"pdf_url":"https://arxiv.org/pdf/2208.02474v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2206.05747"},{"id":"http://arxiv.org/abs/2311.08755v1","updated":"2023-11-15T07:49:46Z","published":"2023-11-15T07:49:46Z","title":"Environment-independent mmWave Fall Detection with Interacting Multiple\n  Model","summary":"  The ageing society brings attention to daily elderly care through sensing\ntechnologies. The future smart home is expected to enable in-home daily\nmonitoring, such as fall detection, for seniors in a non-invasive,\nnon-cooperative, and non-contact manner. The mmWave radar is a promising\ncandidate technology for its privacy-preserving and non-contact manner.\nHowever, existing solutions suffer from low accuracy and robustness due to\nenvironment dependent features. In this paper, we present FADE\n(\\underline{FA}ll \\underline{DE}tection), a practical fall detection radar\nsystem with enhanced accuracy and robustness in real-world scenarios. The key\nenabler underlying FADE is an interacting multiple model (IMM) state estimator\nthat can extract environment-independent features for highly accurate and\ninstantaneous fall detection. Furthermore, we proposed a robust multiple-user\ntracking system to deal with noises from the environment and other human\nbodies. We deployed our algorithm on low computing power and low power\nconsumption system-on-chip (SoC) composed of data front end, DSP, and ARM\nprocessor, and tested its performance in real-world. The experiment shows that\nthe accuracy of fall detection is up to 95\\%.\n","authors":["Xuyao Yu","Jiazhao Wang","Wenchao Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.08755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00735v2","updated":"2023-11-15T07:28:10Z","published":"2023-11-01T12:04:33Z","title":"PET Tracer Conversion among Brain PET via Variable Augmented Invertible\n  Network","summary":"  Positron emission tomography (PET) serves as an essential tool for diagnosis\nof encephalopathy and brain science research. However, it suffers from the\nlimited choice of tracers. Nowadays, with the wide application of PET imaging\nin neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine\n(DOPA) has been found to be more effective than 18F-labeled\nfluorine-2-deoxyglucose (FDG) in the field. Nevertheless, due to the complexity\nof its preparation and other limitations, DOPA is far less widely used than\nFDG. To address this issue, a tracer conversion invertible neural network\n(TC-INN) for image projection is developed to map FDG images to DOPA images\nthrough deep learning. More diagnostic information is obtained by generating\nPET images from FDG to DOPA. Specifically, the proposed TC-INN consists of two\nseparate phases, one for training traceable data, the other for rebuilding new\ndata. The reference DOPA PET image is used as a learning target for the\ncorresponding network during the training process of tracer conversion.\nMeanwhile, the invertible network iteratively estimates the resultant DOPA PET\ndata and compares it to the reference DOPA PET data. Notably, the reversible\nmodel employs variable enhancement technique to achieve better power\ngeneration. Moreover, image registration needs to be performed before training\ndue to the angular deviation of the acquired FDG and DOPA data information.\nExperimental results exhibited excellent generation capability in mapping\nbetween FDG and DOPA, suggesting that PET tracer conversion has great potential\nin the case of limited tracer applications.\n","authors":["Bohui Shen","Wei Zhang","Xubiao Liu","Pengfei Yu","Shirui Jiang","Xinchong Shi","Xiangsong Zhang","Xiaoyu Zhou","Weirui Zhang","Bingxuan Li","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2311.00735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08745v1","updated":"2023-11-15T07:27:40Z","published":"2023-11-15T07:27:40Z","title":"Using Stochastic Gradient Descent to Smooth Nonconvex Functions:\n  Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling","summary":"  The graduated optimization approach is a heuristic method for finding\nglobally optimal solutions for nonconvex functions and has been theoretically\nanalyzed in several studies. This paper defines a new family of nonconvex\nfunctions for graduated optimization, discusses their sufficient conditions,\nand provides a convergence analysis of the graduated optimization algorithm for\nthem. It shows that stochastic gradient descent (SGD) with mini-batch\nstochastic gradients has the effect of smoothing the function, the degree of\nwhich is determined by the learning rate and batch size. This finding provides\ntheoretical insights from a graduated optimization perspective on why large\nbatch sizes fall into sharp local minima, why decaying learning rates and\nincreasing batch sizes are superior to fixed learning rates and batch sizes,\nand what the optimal learning rate scheduling is. To the best of our knowledge,\nthis is the first paper to provide a theoretical explanation for these aspects.\nMoreover, a new graduated optimization framework that uses a decaying learning\nrate and increasing batch size is analyzed and experimental results of image\nclassification that support our theoretical findings are reported.\n","authors":["Naoki Sato","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2311.08745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08744v1","updated":"2023-11-15T07:25:14Z","published":"2023-11-15T07:25:14Z","title":"Towards Graph-Aware Diffusion Modeling for Collaborative Filtering","summary":"  Recovering masked feedback with neural models is a popular paradigm in\nrecommender systems. Seeing the success of diffusion models in solving\nill-posed inverse problems, we introduce a conditional diffusion framework for\ncollaborative filtering that iteratively reconstructs a user's hidden\npreferences guided by its historical interactions. To better align with the\nintrinsic characteristics of implicit feedback data, we implement forward\ndiffusion by applying synthetic smoothing filters to interaction signals on an\nitem-item graph. The resulting reverse diffusion can be interpreted as a\npersonalized process that gradually refines preference scores. Through graph\nFourier transform, we equivalently characterize this model as an anisotropic\nGaussian diffusion in the graph spectral domain, establishing both forward and\nreverse formulations. Our model outperforms state-of-the-art methods by a large\nmargin on one dataset and yields competitive results on the others.\n","authors":["Yunqin Zhu","Chao Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08724v1","updated":"2023-11-15T06:35:01Z","published":"2023-11-15T06:35:01Z","title":"Method for Text Entity Linking in Power Distribution Scheduling Oriented\n  to Power Distribution Network Knowledge Graph","summary":"  The proposed method for linking entities in power distribution dispatch texts\nto a power distribution network knowledge graph is based on a deep\nunderstanding of these networks. This method leverages the unique features of\nentities in both the power distribution network's knowledge graph and the\ndispatch texts, focusing on their semantic, phonetic, and syntactic\ncharacteristics. An enhanced model, the Lexical Semantic Feature-based Skip\nConvolutional Neural Network (LSF-SCNN), is utilized for effectively matching\ndispatch text entities with those in the knowledge graph. The efficacy of this\nmodel, compared to a control model, is evaluated through cross-validation\nmethods in real-world power distribution dispatch scenarios. The results\nindicate that the LSF-SCNN model excels in accurately linking a variety of\nentity types, demonstrating high overall accuracy in entity linking when the\nprocess is conducted in English.\n","authors":["Xiang Li","Che Wang","Bing Li","Hao Chen","Sizhe Li"],"pdf_url":"https://arxiv.org/pdf/2311.08724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11305v2","updated":"2023-11-15T05:49:17Z","published":"2023-10-17T14:29:25Z","title":"MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello,\n  and Atari Games","summary":"  This paper presents MiniZero, a zero-knowledge learning framework that\nsupports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel\nAlphaZero, and Gumbel MuZero. While these algorithms have demonstrated\nsuper-human performance in many games, it remains unclear which among them is\nmost suitable or efficient for specific tasks. Through MiniZero, we\nsystematically evaluate the performance of each algorithm in two board games,\n9x9 Go and 8x8 Othello, as well as 57 Atari games. For two board games, using\nmore simulations generally results in higher performance. However, the choice\nof AlphaZero and MuZero may differ based on game properties. For Atari games,\nboth MuZero and Gumbel MuZero are worth considering. Since each game has unique\ncharacteristics, different algorithms and simulations yield varying results. In\naddition, we introduce an approach, called progressive simulation, which\nprogressively increases the simulation budget during training to allocate\ncomputation more efficiently. Our empirical results demonstrate that\nprogressive simulation achieves significantly superior performance in two board\ngames. By making our framework and trained models publicly available, this\npaper contributes a benchmark for future research on zero-knowledge learning\nalgorithms, assisting researchers in algorithm selection and comparison against\nthese zero-knowledge learning baselines. Our code and data are available at\nhttps://rlg.iis.sinica.edu.tw/papers/minizero.\n","authors":["Ti-Rong Wu","Hung Guei","Po-Wei Huang","Pei-Chiun Peng","Ting Han Wei","Chung-Chin Shih","Yun-Jui Tsai"],"pdf_url":"https://arxiv.org/pdf/2310.11305v2.pdf","comment":"Submitted to IEEE Transactions on Games, under review"},{"id":"http://arxiv.org/abs/2311.09441v1","updated":"2023-11-15T23:23:42Z","published":"2023-11-15T23:23:42Z","title":"Exploring the Privacy-Energy Consumption Tradeoff for Split Federated\n  Learning","summary":"  Split Federated Learning (SFL) has recently emerged as a promising\ndistributed learning technology, leveraging the strengths of both federated\nlearning and split learning. It emphasizes the advantages of rapid convergence\nwhile addressing privacy concerns. As a result, this innovation has received\nsignificant attention from both industry and academia. However, since the model\nis split at a specific layer, known as a cut layer, into both client-side and\nserver-side models for the SFL, the choice of the cut layer in SFL can have a\nsubstantial impact on the energy consumption of clients and their privacy, as\nit influences the training burden and the output of the client-side models.\nMoreover, the design challenge of determining the cut layer is highly\nintricate, primarily due to the inherent heterogeneity in the computing and\nnetworking capabilities of clients. In this article, we provide a comprehensive\noverview of the SFL process and conduct a thorough analysis of energy\nconsumption and privacy. This analysis takes into account the influence of\nvarious system parameters on the cut layer selection strategy. Additionally, we\nprovide an illustrative example of the cut layer selection, aiming to minimize\nthe risk of clients from reconstructing the raw data at the server while\nsustaining energy consumption within the required energy budget, which involve\ntrade-offs. Finally, we address open challenges in this field including their\napplications to 6G technology. These directions represent promising avenues for\nfuture research and development.\n","authors":["Joohyung Lee","Mohamed Seif","Jungchan Cho","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2311.09441v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09438v1","updated":"2023-11-15T23:18:01Z","published":"2023-11-15T23:18:01Z","title":"Labeled Interactive Topic Models","summary":"  Topic models help users understand large document collections; however, topic\nmodels do not always find the ``right'' topics. While classical probabilistic\nand anchor-based topic models have interactive variants to guide models toward\nbetter topics, such interactions are not available for neural topic models such\nas the embedded topic model (\\abr{etm}). We correct this lacuna by adding an\nintuitive interaction to neural topic models: users can label a topic with a\nword, and topics are updated so that the topic words are close to the label.\nThis allows a user to refine topics based on their information need. While,\ninteractivity is intuitive for \\abr{etm}, we extend this framework to work with\nother neural topic models as well. We develop an interactive interface which\nallows users to interact and relabel topic models as they see fit. We evaluate\nour method through a human study, where users can relabel topics to find\nrelevant documents. Using our method, user labeling improves document rank\nscores, helping to find more relevant documents to a given query when compared\nto no user labeling.\n","authors":["Kyle Seelman","Mozhi Zhang","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2311.09438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08176v3","updated":"2023-11-15T23:17:58Z","published":"2023-10-12T10:01:39Z","title":"Infinite Width Graph Neural Networks for Node Regression/ Classification","summary":"  This work analyzes Graph Neural Networks, a generalization of Fully-Connected\nDeep Neural Nets on Graph structured data, when their width, that is the number\nof nodes in each fullyconnected layer is increasing to infinity. Infinite Width\nNeural Networks are connecting Deep Learning to Gaussian Processes and Kernels,\nboth Machine Learning Frameworks with long traditions and extensive theoretical\nfoundations. Gaussian Processes and Kernels have much less hyperparameters then\nNeural Networks and can be used for uncertainty estimation, making them more\nuser friendly for applications. This works extends the increasing amount of\nresearch connecting Gaussian Processes and Kernels to Neural Networks. The\nKernel and Gaussian Process closed forms are derived for a variety of\narchitectures, namely the standard Graph Neural Network, the Graph Neural\nNetwork with Skip-Concatenate Connections and the Graph Attention Neural\nNetwork. All architectures are evaluated on a variety of datasets on the task\nof transductive Node Regression and Classification. Additionally, a Spectral\nSparsification method known as Effective Resistance is used to improve runtime\nand memory requirements. Extending the setting to inductive graph learning\ntasks (Graph Regression/ Classification) is straightforward and is briefly\ndiscussed in 3.5.\n","authors":["Yunus Cobanoglu"],"pdf_url":"https://arxiv.org/pdf/2310.08176v3.pdf","comment":"49 Pages, 2 Figures (with subfigures), multiple tables, v2: made\n  table of contents fit to one page and added derivatives on GAT*NTK and GAT*GP\n  in A.4, v3: shorten parts of introduction and fixed typos, added numberings\n  to equations and discussion section"},{"id":"http://arxiv.org/abs/2308.09543v2","updated":"2023-11-15T23:08:40Z","published":"2023-08-18T13:20:08Z","title":"Latent State Models of Training Dynamics","summary":"  The impact of randomness on model training is poorly understood. How do\ndifferences in data order and initialization actually manifest in the model,\nsuch that some training runs outperform others or converge faster? Furthermore,\nhow can we interpret the resulting training dynamics and the phase transitions\nthat characterize different trajectories? To understand the effect of\nrandomness on the dynamics and outcomes of neural network training, we train\nmodels multiple times with different random seeds and compute a variety of\nmetrics throughout training, such as the $L_2$ norm, mean, and variance of the\nneural network's weights. We then fit a hidden Markov model (HMM) over the\nresulting sequences of metrics. The HMM represents training as a stochastic\nprocess of transitions between latent states, providing an intuitive overview\nof significant changes during training. Using our method, we produce a\nlow-dimensional, discrete representation of training dynamics on grokking\ntasks, image classification, and masked language modeling. We use the HMM\nrepresentation to study phase transitions and identify latent \"detour\" states\nthat slow down convergence.\n","authors":["Michael Y. Hu","Angelica Chen","Naomi Saphra","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2308.09543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09431v1","updated":"2023-11-15T23:01:02Z","published":"2023-11-15T23:01:02Z","title":"Striped Attention: Faster Ring Attention for Causal Transformers","summary":"  To help address the growing demand for ever-longer sequence lengths in\ntransformer models, Liu et al. recently proposed Ring Attention, an exact\nattention algorithm capable of overcoming per-device memory bottle- necks by\ndistributing self-attention across multiple devices. In this paper, we study\nthe performance characteristics of Ring Attention in the important special case\nof causal transformer models, and identify a key workload imbal- ance due to\ntriangular structure of causal attention computations. We propose a simple\nextension to Ring Attention, which we call Striped Attention to fix this\nimbalance. Instead of devices having contiguous subsequences, each device has a\nsubset of tokens distributed uniformly throughout the sequence, which we\ndemonstrate leads to more even workloads. In experiments running Striped\nAttention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x\nend-to-end throughput improvements over the original Ring Attention algorithm\non causal transformer training at a sequence length of 256k. Furthermore, on 16\nTPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of\n786k. We release the code for our experiments as open source\n","authors":["William Brandon","Aniruddha Nrusimha","Kevin Qian","Zachary Ankner","Tian Jin","Zhiye Song","Jonathan Ragan-Kelley"],"pdf_url":"https://arxiv.org/pdf/2311.09431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09428v1","updated":"2023-11-15T22:57:13Z","published":"2023-11-15T22:57:13Z","title":"Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language\n  Models","summary":"  This work investigates the potential of undermining both fairness and\ndetection performance in abusive language detection. In a dynamic and complex\ndigital world, it is crucial to investigate the vulnerabilities of these\ndetection models to adversarial fairness attacks to improve their fairness\nrobustness. We propose a simple yet effective framework FABLE that leverages\nbackdoor attacks as they allow targeted control over the fairness and detection\nperformance. FABLE explores three types of trigger designs (i.e., rare,\nartificial, and natural triggers) and novel sampling strategies. Specifically,\nthe adversary can inject triggers into samples in the minority group with the\nfavored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored\noutcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the\neffectiveness of FABLE attacking fairness and utility in abusive language\ndetection.\n","authors":["Yueqing Liang","Lu Cheng","Ali Payani","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2311.09428v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2201.00292v3","updated":"2023-11-15T22:37:30Z","published":"2022-01-02T05:05:26Z","title":"Fair Data Representation for Machine Learning at the Pareto Frontier","summary":"  As machine learning powered decision-making becomes increasingly important in\nour daily lives, it is imperative to strive for fairness in the underlying data\nprocessing. We propose a pre-processing algorithm for fair data representation\nvia which supervised learning results in estimations of the Pareto frontier\nbetween prediction error and statistical disparity. Particularly, the present\nwork applies the optimal affine transport to approach the post-processing\nWasserstein-2 barycenter characterization of the optimal fair $L^2$-objective\nsupervised learning via a pre-processing data deformation. Furthermore, we show\nthat the Wasserstein-2 geodesics from the conditional (on sensitive\ninformation) distributions of the learning outcome to their barycenter\ncharacterizes the Pareto frontier between $L^2$-loss and the average pairwise\nWasserstein-2 distance among sensitive groups on the learning outcome.\nNumerical simulations underscore the advantages: (1) the pre-processing step is\ncompositive with arbitrary conditional expectation estimation supervised\nlearning methods and unseen data; (2) the fair representation protects the\nsensitive information by limiting the inference capability of the remaining\ndata with respect to the sensitive data; (3) the optimal affine maps are\ncomputationally efficient even for high-dimensional data.\n","authors":["Shizhou Xu","Thomas Strohmer"],"pdf_url":"https://arxiv.org/pdf/2201.00292v3.pdf","comment":"62 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.05417v2","updated":"2023-11-15T22:13:11Z","published":"2023-11-09T14:54:08Z","title":"Predicting the Position Uncertainty at the Time of Closest Approach with\n  Diffusion Models","summary":"  The risk of collision between resident space objects has significantly\nincreased in recent years. As a result, spacecraft collision avoidance\nprocedures have become an essential part of satellite operations. To ensure\nsafe and effective space activities, satellite owners and operators rely on\nconstantly updated estimates of encounters. These estimates include the\nuncertainty associated with the position of each object at the expected TCA.\nThese estimates are crucial in planning risk mitigation measures, such as\ncollision avoidance manoeuvres. As the TCA approaches, the accuracy of these\nestimates improves, as both objects' orbit determination and propagation\nprocedures are made for increasingly shorter time intervals. However, this\nimprovement comes at the cost of taking place close to the critical decision\nmoment. This means that safe avoidance manoeuvres might not be possible or\ncould incur significant costs. Therefore, knowing the evolution of this\nvariable in advance can be crucial for operators. This work proposes a machine\nlearning model based on diffusion models to forecast the position uncertainty\nof objects involved in a close encounter, particularly for the secondary object\n(usually debris), which tends to be more unpredictable. We compare the\nperformance of our model with other state-of-the-art solutions and a na\\\"ive\nbaseline approach, showing that the proposed solution has the potential to\nsignificantly improve the safety and effectiveness of spacecraft operations.\n","authors":["Marta Guimarães","Cláudia Soares","Chiara Manfletti"],"pdf_url":"https://arxiv.org/pdf/2311.05417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05426v2","updated":"2023-11-15T22:13:06Z","published":"2023-11-09T15:04:14Z","title":"Statistical Learning of Conjunction Data Messages Through a Bayesian\n  Non-Homogeneous Poisson Process","summary":"  Current approaches for collision avoidance and space traffic management face\nmany challenges, mainly due to the continuous increase in the number of objects\nin orbit and the lack of scalable and automated solutions. To avoid\ncatastrophic incidents, satellite owners/operators must be aware of their\nassets' collision risk to decide whether a collision avoidance manoeuvre needs\nto be performed. This process is typically executed through the use of warnings\nissued in the form of CDMs which contain information about the event, such as\nthe expected TCA and the probability of collision. Our previous work presented\na statistical learning model that allowed us to answer two important questions:\n(1) Will any new conjunctions be issued in the next specified time interval?\n(2) When and with what uncertainty will the next CDM arrive? However, the model\nwas based on an empirical Bayes homogeneous Poisson process, which assumes that\nthe arrival rates of CDMs are constant over time. In fact, the rate at which\nthe CDMs are issued depends on the behaviour of the objects as well as on the\nscreening process performed by third parties. Thus, in this work, we extend the\nprevious study and propose a Bayesian non-homogeneous Poisson process\nimplemented with high precision using a Probabilistic Programming Language to\nfully describe the underlying phenomena. We compare the proposed solution with\na baseline model to demonstrate the added value of our approach. The results\nshow that this problem can be successfully modelled by our Bayesian\nnon-homogeneous Poisson Process with greater accuracy, contributing to the\ndevelopment of automated collision avoidance systems and helping operators\nreact timely but sparingly with satellite manoeuvres.\n","authors":["Marta Guimarães","Cláudia Soares","Chiara Manfletti"],"pdf_url":"https://arxiv.org/pdf/2311.05426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05430v2","updated":"2023-11-15T22:12:59Z","published":"2023-11-09T15:14:08Z","title":"Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach","summary":"  The increasing number of RSOs has raised concerns about the risk of\ncollisions and catastrophic incidents for all direct and indirect users of\nspace. To mitigate this issue, it is essential to have a good understanding of\nthe various RSOs in orbit and their behaviour. A well-established taxonomy\ndefining several classes of RSOs is a critical step in achieving this\nunderstanding. This taxonomy helps assign objects to specific categories based\non their main characteristics, leading to better tracking services.\nFurthermore, a well-established taxonomy can facilitate research and analysis\nprocesses by providing a common language and framework for better understanding\nthe factors that influence RSO behaviour in space. These factors, in turn, help\ndesign more efficient and effective strategies for space traffic management.\nOur work proposes a new taxonomy for RSOs focusing on the low Earth orbit\nregime to enhance space traffic management. In addition, we present a deep\nlearning-based model that uses an autoencoder architecture to reduce the\nfeatures representing the characteristics of the RSOs. The autoencoder\ngenerates a lower-dimensional space representation that is then explored using\ntechniques such as Uniform Manifold Approximation and Projection to identify\nfundamental clusters of RSOs based on their unique characteristics. This\napproach captures the complex and non-linear relationships between the features\nand the RSOs' classes identified. Our proposed taxonomy and model offer a\nsignificant contribution to the ongoing efforts to mitigate the overall risks\nposed by the increasing number of RSOs in orbit.\n","authors":["Marta Guimarães","Cláudia Soares","Chiara Manfletti"],"pdf_url":"https://arxiv.org/pdf/2311.05430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09406v1","updated":"2023-11-15T22:10:42Z","published":"2023-11-15T22:10:42Z","title":"Alternatives to the Scaled Dot Product for Attention in the Transformer\n  Neural Network Architecture","summary":"  The transformer neural network architecture uses a form of attention in which\nthe dot product of query and key is divided by the square root of the key\ndimension before applying softmax. This scaling of the dot product is designed\nto avoid the absolute value of the dot products becoming so large that applying\nsoftmax leads to vanishing gradients. In this paper, we propose some\nalternative scalings, including dividing the dot product instead by the sum of\nthe key lengths before applying softmax. We use simulated keys and queries to\nshow that in many situations this appears to be more effective at avoiding\nregions where applying softmax leads to vanishing gradients.\n","authors":["James Bernhard"],"pdf_url":"https://arxiv.org/pdf/2311.09406v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.19537v2","updated":"2023-11-15T22:09:08Z","published":"2023-10-30T13:43:50Z","title":"On consequences of finetuning on data with highly discriminative\n  features","summary":"  In the era of transfer learning, training neural networks from scratch is\nbecoming obsolete. Transfer learning leverages prior knowledge for new tasks,\nconserving computational resources. While its advantages are well-documented,\nwe uncover a notable drawback: networks tend to prioritize basic data patterns,\nforsaking valuable pre-learned features. We term this behavior \"feature\nerosion\" and analyze its impact on network performance and internal\nrepresentations.\n","authors":["Wojciech Masarczyk","Tomasz Trzciński","Mateusz Ostaszewski"],"pdf_url":"https://arxiv.org/pdf/2310.19537v2.pdf","comment":"NeurIPS 2023 -- UniReps Workshop"},{"id":"http://arxiv.org/abs/2311.09402v1","updated":"2023-11-15T21:58:01Z","published":"2023-11-15T21:58:01Z","title":"Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical\n  Imaging Research","summary":"  Chest X-rays (CXR) are the most common medical imaging study and are used to\ndiagnose multiple medical conditions. This study examines the impact of\nsynthetic data supplementation, using diffusion models, on the performance of\ndeep learning (DL) classifiers for CXR analysis. We employed three datasets:\nCheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising\ndiffusion probabilistic models (DDPMs) to generate synthetic frontal\nradiographs. Our approach ensured that synthetic images mirrored the\ndemographic and pathological traits of the original data. Evaluating the\nclassifiers' performance on internal and external datasets revealed that\nsynthetic data supplementation enhances model accuracy, particularly in\ndetecting less prevalent pathologies. Furthermore, models trained on synthetic\ndata alone approached the performance of those trained on real data. This\nsuggests that synthetic data can potentially compensate for real data shortages\nin training robust DL models. However, despite promising outcomes, the\nsuperiority of real data persists.\n","authors":["Bardia Khosravi","Frank Li","Theo Dapamede","Pouria Rouzrokh","Cooper U. Gamble","Hari M. Trivedi","Cody C. Wyles","Andrew B. Sellergren","Saptarshi Purkayastha","Bradley J. Erickson","Judy W. Gichoya"],"pdf_url":"https://arxiv.org/pdf/2311.09402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09401v1","updated":"2023-11-15T21:56:47Z","published":"2023-11-15T21:56:47Z","title":"MoCo-Transfer: Investigating out-of-distribution contrastive learning\n  for limited-data domains","summary":"  Medical imaging data is often siloed within hospitals, limiting the amount of\ndata available for specialized model development. With limited in-domain data,\none might hope to leverage larger datasets from related domains. In this paper,\nwe analyze the benefit of transferring self-supervised contrastive\nrepresentations from moment contrast (MoCo) pretraining on out-of-distribution\ndata to settings with limited data. We consider two X-ray datasets which image\ndifferent parts of the body, and compare transferring from each other to\ntransferring from ImageNet. We find that depending on quantity of labeled and\nunlabeled data, contrastive pretraining on larger out-of-distribution datasets\ncan perform nearly as well or better than MoCo pretraining in-domain, and\npretraining on related domains leads to higher performance than if one were to\nuse the ImageNet pretrained weights. Finally, we provide a preliminary way of\nquantifying similarity between datasets.\n","authors":["Yuwen Chen","Helen Zhou","Zachary C. Lipton"],"pdf_url":"https://arxiv.org/pdf/2311.09401v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 4 pages"},{"id":"http://arxiv.org/abs/2305.18651v4","updated":"2023-11-15T21:51:23Z","published":"2023-05-29T23:06:05Z","title":"UMD: Unsupervised Model Detection for X2X Backdoor Attacks","summary":"  Backdoor (Trojan) attack is a common threat to deep neural networks, where\nsamples from one or more source classes embedded with a backdoor trigger will\nbe misclassified to adversarial target classes. Existing methods for detecting\nwhether a classifier is backdoor attacked are mostly designed for attacks with\na single adversarial target (e.g., all-to-one attack). To the best of our\nknowledge, without supervision, no existing methods can effectively address the\nmore general X2X attack with an arbitrary number of source classes, each paired\nwith an arbitrary target class. In this paper, we propose UMD, the first\nUnsupervised Model Detection method that effectively detects X2X backdoor\nattacks via a joint inference of the adversarial (source, target) class pairs.\nIn particular, we first define a novel transferability statistic to measure and\nselect a subset of putative backdoor class pairs based on a proposed clustering\napproach. Then, these selected class pairs are jointly assessed based on an\naggregation of their reverse-engineered trigger size for detection inference,\nusing a robust and unsupervised anomaly detector we proposed. We conduct\ncomprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show\nthat our unsupervised UMD outperforms SOTA detectors (even with supervision) by\n17%, 4%, and 8%, respectively, in terms of the detection accuracy against\ndiverse X2X attacks. We also show the strong detection performance of UMD\nagainst several strong adaptive attacks.\n","authors":["Zhen Xiang","Zidi Xiong","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2305.18651v4.pdf","comment":"Proceedings of the 40th International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2309.07383v3","updated":"2023-11-15T21:43:08Z","published":"2023-09-14T02:02:08Z","title":"Rates of Convergence in Certain Native Spaces of Approximations used in\n  Reinforcement Learning","summary":"  This paper studies convergence rates for some value function approximations\nthat arise in a collection of reproducing kernel Hilbert spaces (RKHS)\n$H(\\Omega)$. By casting an optimal control problem in a specific class of\nnative spaces, strong rates of convergence are derived for the operator\nequation that enables offline approximations that appear in policy iteration.\nExplicit upper bounds on error in value function and controller approximations\nare derived in terms of power function $\\Pwr_{H,N}$ for the space of finite\ndimensional approximants $H_N$ in the native space $H(\\Omega)$. These bounds\nare geometric in nature and refine some well-known, now classical results\nconcerning convergence of approximations of value functions.\n","authors":["Ali Bouland","Shengyuan Niu","Sai Tej Paruchuri","Andrew Kurdila","John Burns","Eugenio Schuster"],"pdf_url":"https://arxiv.org/pdf/2309.07383v3.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.15015v3","updated":"2023-11-15T21:33:46Z","published":"2023-10-23T15:10:37Z","title":"Leveraging Deep Learning for Abstractive Code Summarization of\n  Unofficial Documentation","summary":"  Usually, programming languages have official documentation to guide\ndevelopers with APIs, methods, and classes. However, researchers identified\ninsufficient or inadequate documentation examples and flaws with the API's\ncomplex structure as barriers to learning an API. As a result, developers may\nconsult other sources (StackOverflow, GitHub, etc.) to learn more about an API.\nRecent research studies have shown that unofficial documentation is a valuable\nsource of information for generating code summaries. We, therefore, have been\nmotivated to leverage such a type of documentation along with deep learning\ntechniques towards generating high-quality summaries for APIs discussed in\ninformal documentation. This paper proposes an automatic approach using the\nBART algorithm, a state-of-the-art transformer model, to generate summaries for\nAPIs discussed in StackOverflow. We built an oracle of human-generated\nsummaries to evaluate our approach against it using ROUGE and BLEU metrics\nwhich are the most widely used evaluation metrics in text summarization.\nFurthermore, we evaluated our summaries empirically against a previous work in\nterms of quality. Our findings demonstrate that using deep learning algorithms\ncan improve summaries' quality and outperform the previous work by an average\nof %57 for Precision, %66 for Recall, and %61 for F-measure, and it runs 4.4\ntimes faster.\n","authors":["AmirHossein Naghshzan","Latifa Guerrouj","Olga Baysal"],"pdf_url":"https://arxiv.org/pdf/2310.15015v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09389v1","updated":"2023-11-15T21:32:44Z","published":"2023-11-15T21:32:44Z","title":"Neural machine translation for automated feedback on children's\n  early-stage writing","summary":"  In this work, we address the problem of assessing and constructing feedback\nfor early-stage writing automatically using machine learning. Early-stage\nwriting is typically vastly different from conventional writing due to phonetic\nspelling and lack of proper grammar, punctuation, spacing etc. Consequently,\nearly-stage writing is highly non-trivial to analyze using common linguistic\nmetrics. We propose to use sequence-to-sequence models for \"translating\"\nearly-stage writing by students into \"conventional\" writing, which allows the\ntranslated text to be analyzed using linguistic metrics. Furthermore, we\npropose a novel robust likelihood to mitigate the effect of noise in the\ndataset. We investigate the proposed methods using a set of numerical\nexperiments and demonstrate that the conventional text can be predicted with\nhigh accuracy.\n","authors":["Jonas Vestergaard Jensen","Mikkel Jordahn","Michael Riis Andersen"],"pdf_url":"https://arxiv.org/pdf/2311.09389v1.pdf","comment":"9 pages, 1 figure, 1 table, to be published in the proceedings of the\n  Northern Lights Deep Learning Conference 2024"},{"id":"http://arxiv.org/abs/2311.09387v1","updated":"2023-11-15T21:30:26Z","published":"2023-11-15T21:30:26Z","title":"Banach-Tarski Embeddings and Transformers","summary":"  We introduce a new construction of embeddings of arbitrary recursive data\nstructures into high dimensional vectors. These embeddings provide an\ninterpretable model for the latent state vectors of transformers. We\ndemonstrate that these embeddings can be decoded to the original data structure\nwhen the embedding dimension is sufficiently large. This decoding algorithm has\na natural implementation as a transformer. We also show that these embedding\nvectors can be manipulated directly to perform computations on the underlying\ndata without decoding. As an example we present an algorithm that constructs\nthe embedded parse tree of an embedded token sequence using only vector\noperations in embedding space.\n","authors":["Joshua Maher"],"pdf_url":"https://arxiv.org/pdf/2311.09387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09386v1","updated":"2023-11-15T21:29:57Z","published":"2023-11-15T21:29:57Z","title":"Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction","summary":"  Linear feature extraction at the presence of nonlinear dependencies among the\ndata is a fundamental challenge in unsupervised learning. We propose using a\nProbabilistic Gram-Schmidt (PGS) type orthogonalization process in order to\ndetect and map out redundant dimensions. Specifically, by applying the PGS\nprocess over any family of functions which presumably captures the nonlinear\ndependencies in the data, we construct a series of covariance matrices that can\neither be used to remove those dependencies from the principal components, or\nto identify new large-variance directions. In the former case, we prove that\nunder certain assumptions the resulting algorithms detect and remove nonlinear\ndependencies whenever those dependencies lie in the linear span of the chosen\nfunction family. In the latter, we provide information-theoretic guarantees in\nterms of entropy reduction. Both proposed methods extract linear features from\nthe data while removing nonlinear redundancies. We provide simulation results\non synthetic and real-world datasets which show improved performance over PCA\nand state-of-the-art linear feature extraction algorithms, both in terms of\nvariance maximization of the extracted features, and in terms of improved\nperformance of classification algorithms.\n","authors":["Bahram Yaghooti","Netanel Raviv","Bruno Sinopoli"],"pdf_url":"https://arxiv.org/pdf/2311.09386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00310v2","updated":"2023-11-15T21:26:20Z","published":"2023-07-01T11:51:56Z","title":"Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD","summary":"  Differentially private stochastic gradient descent (DP-SGD) is the canonical\napproach to private deep learning. While the current privacy analysis of DP-SGD\nis known to be tight in some settings, several empirical results suggest that\nmodels trained on common benchmark datasets leak significantly less privacy for\nmany datapoints. Yet, despite past attempts, a rigorous explanation for why\nthis is the case has not been reached. Is it because there exist tighter\nprivacy upper bounds when restricted to these dataset settings, or are our\nattacks not strong enough for certain datapoints? In this paper, we provide the\nfirst per-instance (i.e., ``data-dependent\") DP analysis of DP-SGD. Our\nanalysis captures the intuition that points with similar neighbors in the\ndataset enjoy better data-dependent privacy than outliers. Formally, this is\ndone by modifying the per-step privacy analysis of DP-SGD to introduce a\ndependence on the distribution of model updates computed from a training\ndataset. We further develop a new composition theorem to effectively use this\nnew per-step analysis to reason about an entire training run. Put all together,\nour evaluation shows that this novel DP-SGD analysis allows us to now formally\nshow that DP-SGD leaks significantly less privacy for many datapoints (when\ntrained on common benchmarks) than the current data-independent guarantee. This\nimplies privacy attacks will necessarily fail against many datapoints if the\nadversary does not have sufficient control over the possible training datasets.\n","authors":["Anvith Thudi","Hengrui Jia","Casey Meehan","Ilia Shumailov","Nicolas Papernot"],"pdf_url":"https://arxiv.org/pdf/2307.00310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09383v1","updated":"2023-11-15T21:22:27Z","published":"2023-11-15T21:22:27Z","title":"Long-form Question Answering: An Iterative Planning-Retrieval-Generation\n  Approach","summary":"  Long-form question answering (LFQA) poses a challenge as it involves\ngenerating detailed answers in the form of paragraphs, which go beyond simple\nyes/no responses or short factual answers. While existing QA models excel in\nquestions with concise answers, LFQA requires handling multiple topics and\ntheir intricate relationships, demanding comprehensive explanations. Previous\nattempts at LFQA focused on generating long-form answers by utilizing relevant\ncontexts from a corpus, relying solely on the question itself. However, they\noverlooked the possibility that the question alone might not provide sufficient\ninformation to identify the relevant contexts. Additionally, generating\ndetailed long-form answers often entails aggregating knowledge from diverse\nsources. To address these limitations, we propose an LFQA model with iterative\nPlanning, Retrieval, and Generation. This iterative process continues until a\ncomplete answer is generated for the given question. From an extensive\nexperiment on both an open domain and a technical domain QA dataset, we find\nthat our model outperforms the state-of-the-art models on various textual and\nfactual metrics for the LFQA task.\n","authors":["Pritom Saha Akash","Kashob Kumar Roy","Lucian Popa","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2311.09383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09369v1","updated":"2023-11-15T21:00:00Z","published":"2023-11-15T21:00:00Z","title":"Time-dependent Probabilistic Generative Models for Disease Progression","summary":"  Electronic health records contain valuable information for monitoring\npatients' health trajectories over time. Disease progression models have been\ndeveloped to understand the underlying patterns and dynamics of diseases using\nthese data as sequences. However, analyzing temporal data from EHRs is\nchallenging due to the variability and irregularities present in medical\nrecords. We propose a Markovian generative model of treatments developed to (i)\nmodel the irregular time intervals between medical events; (ii) classify\ntreatments into subtypes based on the patient sequence of medical events and\nthe time intervals between them; and (iii) segment treatments into subsequences\nof disease progression patterns. We assume that sequences have an associated\nstructure of latent variables: a latent class representing the different\nsubtypes of treatments; and a set of latent stages indicating the phase of\nprogression of the treatments. We use the Expectation-Maximization algorithm to\nlearn the model, which is efficiently solved with a dynamic programming-based\nmethod. Various parametric models have been employed to model the time\nintervals between medical events during the learning process, including the\ngeometric, exponential, and Weibull distributions. The results demonstrate the\neffectiveness of our model in recovering the underlying model from data and\naccurately modeling the irregular time intervals between medical actions.\n","authors":["Onintze Zaballa","Aritz Pérez","Elisa Gómez-Inhiesto","Teresa Acaiturri-Ayesta","Jose A. Lozano"],"pdf_url":"https://arxiv.org/pdf/2311.09369v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 17 pages"},{"id":"http://arxiv.org/abs/2311.09355v1","updated":"2023-11-15T20:31:40Z","published":"2023-11-15T20:31:40Z","title":"Privacy Threats in Stable Diffusion Models","summary":"  This paper introduces a novel approach to membership inference attacks (MIA)\ntargeting stable diffusion computer vision models, specifically focusing on the\nhighly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract\nsensitive information about a model's training data, posing significant privacy\nconcerns. Despite its advancements in image synthesis, our research reveals\nprivacy vulnerabilities in the stable diffusion models' outputs. Exploiting\nthis information, we devise a black-box MIA that only needs to query the victim\nmodel repeatedly. Our methodology involves observing the output of a stable\ndiffusion model at different generative epochs and training a classification\nmodel to distinguish when a series of intermediates originated from a training\nsample or not. We propose numerous ways to measure the membership features and\ndiscuss what works best. The attack's efficacy is assessed using the ROC AUC\nmethod, demonstrating a 60\\% success rate in inferring membership information.\nThis paper contributes to the growing body of research on privacy and security\nin machine learning, highlighting the need for robust defenses against MIAs.\nOur findings prompt a reevaluation of the privacy implications of stable\ndiffusion models, urging practitioners and developers to implement enhanced\nsecurity measures to safeguard against such attacks.\n","authors":["Thomas Cilloni","Charles Fleming","Charles Walter"],"pdf_url":"https://arxiv.org/pdf/2311.09355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09354v1","updated":"2023-11-15T20:28:31Z","published":"2023-11-15T20:28:31Z","title":"Nondestructive, quantitative viability analysis of 3D tissue cultures\n  using machine learning image segmentation","summary":"  Ascertaining the collective viability of cells in different cell culture\nconditions has typically relied on averaging colorimetric indicators and is\noften reported out in simple binary readouts. Recent research has combined\nviability assessment techniques with image-based deep-learning models to\nautomate the characterization of cellular properties. However, further\ndevelopment of viability measurements to assess the continuity of possible\ncellular states and responses to perturbation across cell culture conditions is\nneeded. In this work, we demonstrate an image processing algorithm for\nquantifying cellular viability in 3D cultures without the need for assay-based\nindicators. We show that our algorithm performs similarly to a pair of human\nexperts in whole-well images over a range of days and culture matrix\ncompositions. To demonstrate potential utility, we perform a longitudinal study\ninvestigating the impact of a known therapeutic on pancreatic cancer spheroids.\nUsing images taken with a high content imaging system, the algorithm\nsuccessfully tracks viability at the individual spheroid and whole-well level.\nThe method we propose reduces analysis time by 97% in comparison to the\nexperts. Because the method is independent of the microscope or imaging system\nused, this approach lays the foundation for accelerating progress in and for\nimproving the robustness and reproducibility of 3D culture analysis across\nbiological and clinical research.\n","authors":["Kylie J. Trettner","Jeremy Hsieh","Weikun Xiao","Jerry S. H. Lee","Andrea M. Armani"],"pdf_url":"https://arxiv.org/pdf/2311.09354v1.pdf","comment":"44 total pages, Main text and SI included, 11 figures, 6 tables, 5\n  datasets (provided on linked GitHub), linked image files on Zenodo"},{"id":"http://arxiv.org/abs/2311.09349v1","updated":"2023-11-15T20:14:21Z","published":"2023-11-15T20:14:21Z","title":"Generative AI-Based Probabilistic Constellation Shaping With Diffusion\n  Models","summary":"  Diffusion models are at the vanguard of generative AI research with renowned\nsolutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI.\nNevertheless, the potential merits of diffusion models for communication\nengineering applications are not fully understood yet. In this paper, we aim to\nunleash the power of generative AI for PHY design of constellation symbols in\ncommunication systems. Although the geometry of constellations is predetermined\naccording to networking standards, e.g., quadrature amplitude modulation (QAM),\nprobabilistic shaping can design the probability of occurrence (generation) of\nconstellation symbols. This can help improve the information rate and decoding\nperformance of communication systems. We exploit the ``denoise-and-generate''\ncharacteristics of denoising diffusion probabilistic models (DDPM) for\nprobabilistic constellation shaping. The key idea is to learn generating\nconstellation symbols out of noise, ``mimicking'' the way the receiver performs\nsymbol reconstruction. This way, we make the constellation symbols sent by the\ntransmitter, and what is inferred (reconstructed) at the receiver become as\nsimilar as possible, resulting in as few mismatches as possible. Our results\nshow that the generative AI-based scheme outperforms deep neural network\n(DNN)-based benchmark and uniform shaping, while providing network resilience\nas well as robust out-of-distribution performance under low-SNR regimes and\nnon-Gaussian assumptions. Numerical evaluations highlight 30% improvement in\nterms of cosine similarity and a threefold improvement in terms of mutual\ninformation compared to DNN-based approach for 64-QAM geometry.\n","authors":["Mehdi Letafati","Samad Ali","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2311.09349v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.08688"},{"id":"http://arxiv.org/abs/2305.12025v2","updated":"2023-11-15T20:11:39Z","published":"2023-05-19T22:36:20Z","title":"Biomembrane-based Memcapacitive Reservoir Computing System for Energy\n  Efficient Temporal Data Processing","summary":"  Reservoir computing is a highly efficient machine learning framework for\nprocessing temporal data by extracting features from the input signal and\nmapping them into higher dimensional spaces. Physical reservoir layers have\nbeen realized using spintronic oscillators, atomic switch networks, silicon\nphotonic modules, ferroelectric transistors, and volatile memristors. However,\nthese devices are intrinsically energy-dissipative due to their resistive\nnature, which leads to increased power consumption. Therefore, capacitive\nmemory devices can provide a more energy-efficient approach. Here, we leverage\nvolatile biomembrane-based memcapacitors that closely mimic certain short-term\nsynaptic plasticity functions as reservoirs to solve classification tasks and\nanalyze time-series data in simulation and experimentally. Our system achieves\na 99.6% accuracy rate for spoken digit classification and a normalized mean\nsquare error of 7.81*10^{-4} in a second-order non-linear regression task.\nFurthermore, to showcase the device's real-time temporal data processing\ncapability, we achieve 100% accuracy for a real-time epilepsy detection problem\nfrom an inputted electroencephalography (EEG) signal. Most importantly, we\ndemonstrate that each memcapacitor consumes an average of 41.5 fJ of energy per\nspike, regardless of the selected input voltage pulse width, while maintaining\nan average power of 415 fW for a pulse width of 100 ms. These values are orders\nof magnitude lower than those achieved by state-of-the-art memristors used as\nreservoirs. Lastly, we believe the biocompatible, soft nature of our\nmemcapacitor makes it highly suitable for computing and signal-processing\napplications in biological environments.\n","authors":["Md Razuan Hossain","Ahmed Salah Mohamed","Nicholas Xavier Armendarez","Joseph S. Najem","Md Sakib Hasan"],"pdf_url":"https://arxiv.org/pdf/2305.12025v2.pdf","comment":"Supplementary information is attached under the main text"}],"Multimedia":[{"id":"http://arxiv.org/abs/2311.08884v1","updated":"2023-11-15T11:43:48Z","published":"2023-11-15T11:43:48Z","title":"CREPE Notes: A new method for segmenting pitch contours into discrete\n  notes","summary":"  Tracking the fundamental frequency (f0) of a monophonic instrumental\nperformance is effectively a solved problem with several solutions achieving\n99% accuracy. However, the related task of automatic music transcription\nrequires a further processing step to segment an f0 contour into discrete\nnotes. This sub-task of note segmentation is necessary to enable a range of\napplications including musicological analysis and symbolic music generation.\nBuilding on CREPE, a state-of-the-art monophonic pitch tracking solution based\non a simple neural network, we propose a simple and effective method for\npost-processing CREPE's output to achieve monophonic note segmentation. The\nproposed method demonstrates state-of-the-art results on two challenging\ndatasets of monophonic instrumental music. Our approach also gives a 97%\nreduction in the total number of parameters used when compared with other deep\nlearning based methods.\n","authors":["Xavier Riley","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2311.08884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08782v1","updated":"2023-11-15T08:54:57Z","published":"2023-11-15T08:54:57Z","title":"Language Semantic Graph Guided Data-Efficient Learning","summary":"  Developing generalizable models that can effectively learn from limited data\nand with minimal reliance on human supervision is a significant objective\nwithin the machine learning community, particularly in the era of deep neural\nnetworks. Therefore, to achieve data-efficient learning, researchers typically\nexplore approaches that can leverage more related or unlabeled data without\nnecessitating additional manual labeling efforts, such as Semi-Supervised\nLearning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL\nleverages unlabeled data in the training process, while TL enables the transfer\nof expertise from related data distributions. DA broadens the dataset by\nsynthesizing new data from existing examples. However, the significance of\nadditional knowledge contained within labels has been largely overlooked in\nresearch. In this paper, we propose a novel perspective on data efficiency that\ninvolves exploiting the semantic information contained in the labels of the\navailable data. Specifically, we introduce a Language Semantic Graph (LSG)\nwhich is constructed from labels manifest as natural language descriptions.\nUpon this graph, an auxiliary graph neural network is trained to extract\nhigh-level semantic relations and then used to guide the training of the\nprimary model, enabling more adequate utilization of label knowledge. Across\nimage, video, and audio modalities, we utilize the LSG method in both TL and\nSSL scenarios and illustrate its versatility in significantly enhancing\nperformance compared to other data-efficient learning approaches. Additionally,\nour in-depth analysis shows that the LSG method also expedites the training\nprocess.\n","authors":["Wenxuan Ma","Shuang Li","Lincan Cai","Jingxuan Kang"],"pdf_url":"https://arxiv.org/pdf/2311.08782v1.pdf","comment":"Accepted by NeurIPS 2023"}]},"2023-11-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.10085v1","updated":"2023-11-16T18:44:22Z","published":"2023-11-16T18:44:22Z","title":"A Computationally Efficient Sparsified Online Newton Method","summary":"  Second-order methods hold significant promise for enhancing the convergence\nof deep neural network training; however, their large memory and computational\ndemands have limited their practicality. Thus there is a need for scalable\nsecond-order methods that can efficiently train large models. In this paper, we\nintroduce the Sparsified Online Newton (SONew) method, a memory-efficient\nsecond-order algorithm that yields a sparsified yet effective preconditioner.\nThe algorithm emerges from a novel use of the LogDet matrix divergence measure;\nwe combine it with sparsity constraints to minimize regret in the online convex\noptimization framework. Empirically, we test our method on large scale\nbenchmarks of up to 1B parameters. We achieve up to 30% faster convergence,\n3.4% relative improvement in validation performance, and 80% relative\nimprovement in training loss, in comparison to memory efficient optimizers\nincluding first order methods. Powering the method is a surprising fact --\nimposing structured sparsity patterns, like tridiagonal and banded structure,\nrequires little to no overhead, making it as efficient and parallelizable as\nfirst-order methods. In wall-clock time, tridiagonal SONew is only about 3%\nslower per step than first-order methods but gives overall gains due to much\nfaster convergence. In contrast, one of the state-of-the-art (SOTA)\nmemory-intensive second-order methods, Shampoo, is unable to scale to large\nbenchmarks. Additionally, while Shampoo necessitates significant engineering\nefforts to scale to large benchmarks, SONew offers a more straightforward\nimplementation, increasing its practical appeal. SONew code is available at:\nhttps://github.com/devvrit/SONew\n","authors":["Fnu Devvrit","Sai Surya Duvvuri","Rohan Anil","Vineet Gupta","Cho-Jui Hsieh","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2311.10085v1.pdf","comment":"30 pages. First two authors contributed equally. Accepted at NeurIPS\n  2023"},{"id":"http://arxiv.org/abs/2311.10083v1","updated":"2023-11-16T18:38:25Z","published":"2023-11-16T18:38:25Z","title":"Characterizing Tradeoffs in Language Model Decoding with Informational\n  Interpretations","summary":"  We propose a theoretical framework for formulating language model decoder\nalgorithms with dynamic programming and information theory. With dynamic\nprogramming, we lift the design of decoder algorithms from the logit space to\nthe action-state value function space, and show that the decoding algorithms\nare consequences of optimizing the action-state value functions. Each component\nin the action-state value function space has an information theoretical\ninterpretation. With the lifting and interpretation, it becomes evident what\nthe decoder algorithm is optimized for, and hence facilitating the arbitration\nof the tradeoffs in sensibleness, diversity, and attribution.\n","authors":["Chung-Ching Chang","William W. Cohen","Yun-Hsuan Sung"],"pdf_url":"https://arxiv.org/pdf/2311.10083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10081v1","updated":"2023-11-16T18:37:29Z","published":"2023-11-16T18:37:29Z","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback","summary":"  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2311.10081v1.pdf","comment":"The feedback datasets will be released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF"},{"id":"http://arxiv.org/abs/2311.10075v1","updated":"2023-11-16T18:30:14Z","published":"2023-11-16T18:30:14Z","title":"ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve\n  Health Literacy and Communication in Pediatric Populations and Beyond","summary":"  Purpose: Enhanced health literacy has been linked to better health outcomes;\nhowever, few interventions have been studied. We investigate whether large\nlanguage models (LLMs) can serve as a medium to improve health literacy in\nchildren and other populations.\n  Methods: We ran 288 conditions using 26 different prompts through\nChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate\nlimits, we tested a subset of 150 conditions through ChatGPT-4. The primary\noutcome measurements were the reading grade level (RGL) and word counts of\noutput.\n  Results: Across all models, output for basic prompts such as \"Explain\" and\n\"What is (are)\" were at, or exceeded, a 10th-grade RGL. When prompts were\nspecified to explain conditions from the 1st to 12th RGL, we found that LLMs\nhad varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided\nresponses that ranged from the 7th-grade to college freshmen RGL while\nChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL.\nMicrosoft Bing provided responses from the 9th to 11th RGL while Google Bard\nprovided responses from the 7th to 10th RGL.\n  Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade\nlevel outputs. Meanwhile Bard and Bing tended to consistently produce an RGL\nthat is at the high school level regardless of prompt. Additionally, Bard's\nhesitancy in providing certain outputs indicates a cautious approach towards\nhealth information. LLMs demonstrate promise in enhancing health communication,\nbut future research should verify the accuracy and effectiveness of such tools\nin this context.\n  Implications: LLMs face challenges in crafting outputs below a sixth-grade\nreading level. However, their capability to modify outputs above this threshold\nprovides a potential mechanism to improve health literacy and communication in\na pediatric population and beyond.\n","authors":["Kanhai S. Amin","Linda Mayes","Pavan Khosla","Rushabh Doshi"],"pdf_url":"https://arxiv.org/pdf/2311.10075v1.pdf","comment":"15 pages, 1 Table, 3 Figures, and 3 Supplemental Figures"},{"id":"http://arxiv.org/abs/2302.14838v3","updated":"2023-11-16T18:02:19Z","published":"2023-02-28T18:37:25Z","title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search","summary":"  Given the recent impressive accomplishments of language models (LMs) for code\ngeneration, we explore the use of LMs as adaptive mutation and crossover\noperators for an evolutionary neural architecture search (NAS) algorithm. While\nNAS still proves too difficult a task for LMs to succeed at solely through\nprompting, we find that the combination of evolutionary prompt engineering with\nsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverse\nand high performing models. We first demonstrate that EvoPrompting is effective\non the computationally efficient MNIST-1D dataset, where EvoPrompting produces\nconvolutional architecture variants that outperform both those designed by\nhuman experts and naive few-shot prompting in terms of accuracy and model size.\nWe then apply our method to searching for graph neural networks on the CLRS\nAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novel\narchitectures that outperform current state-of-the-art models on 21 out of 30\nalgorithmic reasoning tasks while maintaining similar model size. EvoPrompting\nis successful at designing accurate and efficient neural network architectures\nacross a variety of machine learning tasks, while also being general enough for\neasy adaptation to other tasks beyond neural network design.\n","authors":["Angelica Chen","David M. Dohan","David R. So"],"pdf_url":"https://arxiv.org/pdf/2302.14838v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.10057v1","updated":"2023-11-16T17:52:21Z","published":"2023-11-16T17:52:21Z","title":"The Song Describer Dataset: a Corpus of Audio Captions for\n  Music-and-Language Evaluation","summary":"  We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of\nhigh-quality audio-caption pairs, designed for the evaluation of\nmusic-and-language models. The dataset consists of 1.1k human-written natural\nlanguage descriptions of 706 music recordings, all publicly accessible and\nreleased under Creative Common licenses. To showcase the use of our dataset, we\nbenchmark popular models on three key music-and-language tasks (music\ncaptioning, text-to-music generation and music-language retrieval). Our\nexperiments highlight the importance of cross-dataset evaluation and offer\ninsights into how researchers can use SDD to gain a broader understanding of\nmodel performance.\n","authors":["Ilaria Manco","Benno Weck","SeungHeon Doh","Minz Won","Yixiao Zhang","Dmitry Bodganov","Yusong Wu","Ke Chen","Philip Tovstogan","Emmanouil Benetos","Elio Quinton","György Fazekas","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2311.10057v1.pdf","comment":"Accepted to NeurIPS 2023 Workshop on Machine Learning for Audio"},{"id":"http://arxiv.org/abs/2311.10054v1","updated":"2023-11-16T17:48:55Z","published":"2023-11-16T17:48:55Z","title":"Is \"A Helpful Assistant\" the Best Role for Large Language Models? A\n  Systematic Evaluation of Social Roles in System Prompts","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of the\ndefault system prompt. But is \"a helpful assistant\" the best role for LLMs? In\nthis study, we present a systematic evaluation of how social roles in system\nprompts affect model performance. We curate a list of 162 roles covering 6\ntypes of interpersonal relationships and 8 types of occupations. Through\nextensive analysis of 3 popular LLMs and 2457 questions, we show that adding\ninterpersonal roles in prompts consistently improves the models' performance\nover a range of questions. Moreover, while we find that using gender-neutral\nroles and specifying the role as the audience leads to better performances,\npredicting which role leads to the best performance remains a challenging task,\nand that frequency, similarity, and perplexity do not fully explain the effect\nof social roles on model performances. Our results can help inform the design\nof system prompts for AI systems. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16137v5","updated":"2023-11-16T17:26:20Z","published":"2023-08-30T16:47:51Z","title":"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language\n  Models","summary":"  In recent years, there have been remarkable advancements in the performance\nof Transformer-based Large Language Models (LLMs) across various domains. As\nthese LLMs are deployed for increasingly complex domains, they often face the\nneed to follow longer user prompts or generate longer texts. In these\nsituations, the $\\textit{length generalization failure}$ of LLMs on long\nsequences becomes more prominent. Most pre-training schemes truncate training\nsequences to a fixed length. LLMs often struggle to generate fluent and\ncoherent texts after longer contexts, even with relative positional encoding\nspecifically designed to cope with this problem. Common solutions such as\nfinetuning on longer corpora often involve daunting hardware and time costs and\nrequire careful training process design. To more efficiently extrapolate\nexisting LLMs' generation quality to longer texts, we theoretically and\nempirically investigate the main out-of-distribution (OOD) factors contributing\nto this problem. Inspired by this diagnosis, we propose a simple yet effective\nsolution for on-the-fly length generalization, LM-Infinite. It involves only a\n$\\mathbf{\\Lambda}$-shaped attention mask (to avoid excessive attended tokens)\nand a distance limit (to avoid unseen distances) while requiring no parameter\nupdates or learning. We find it applicable to a variety of LLMs using\nrelative-position encoding methods. LM-Infinite is computationally efficient\nwith $O(n)$ time and space, and demonstrates consistent text generation fluency\nand quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with\n2.72x decoding speedup. We will make the codes publicly available following\npublication.\n","authors":["Chi Han","Qifan Wang","Wenhan Xiong","Yu Chen","Heng Ji","Sinong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.16137v5.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.11860v2","updated":"2023-11-16T16:47:05Z","published":"2023-05-19T17:49:25Z","title":"Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning\n  and Coding with LLMs","summary":"  A popular approach for improving the correctness of output from large\nlanguage models (LLMs) is Self-Consistency - poll the LLM multiple times and\noutput the most frequent solution. Existing Self-Consistency techniques always\ngenerate a constant number of samples per question, where a better approach\nwill be to non-uniformly distribute the available budget based on the amount of\nagreement in the samples generated so far. In response, we introduce\nAdaptive-Consistency, a cost-efficient, model-agnostic technique that\ndynamically adjusts the number of samples per question using a lightweight\nstopping criterion. Our experiments over 17 reasoning and code generation\ndatasets and three LLMs demonstrate that Adaptive-Consistency reduces sample\nbudget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our\ncode and data are available at https://www.sample-step-by-step.info\n","authors":["Pranjal Aggarwal","Aman Madaan","Yiming Yang"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2305.11860v2.pdf","comment":"Published at EMNLP 2023"},{"id":"http://arxiv.org/abs/2206.08955v5","updated":"2023-11-16T16:13:27Z","published":"2022-06-17T18:11:34Z","title":"Making first order linear logic a generating grammar","summary":"  It is known that different categorial grammars have surface representation in\na fragment of first order multiplicative linear logic (MLL1). We show that the\nfragment of interest is equivalent to the recently introduced extended tensor\ntype calculus (ETTC). ETTC is a calculus of specific typed terms, which\nrepresent tuples of strings, more precisely bipartite graphs decorated with\nstrings. Types are derived from linear logic formulas, and rules correspond to\nconcrete operations on these string-labeled graphs, so that they can be\nconveniently visualized. This provides the above mentioned fragment of MLL1\nthat is relevant for language modeling not only with some alternative syntax\nand intuitive geometric representation, but also with an intrinsic deductive\nsystem, which has been absent.\n  In this work we consider a non-trivial notationally enriched variation of the\npreviously introduced ETTC, which allows more concise and transparent\ncomputations. We present both a cut-free sequent calculus and a natural\ndeduction formalism.\n","authors":["Sergey Slavnov"],"pdf_url":"https://arxiv.org/pdf/2206.08955v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09993v1","updated":"2023-11-16T16:09:43Z","published":"2023-11-16T16:09:43Z","title":"Generative AI for Hate Speech Detection: Evaluation and Findings","summary":"  Automatic hate speech detection using deep neural models is hampered by the\nscarcity of labeled datasets, leading to poor generalization. To mitigate this\nproblem, generative AI has been utilized to generate large amounts of synthetic\nhate speech sequences from available labeled examples, leveraging the generated\ndata in finetuning large pre-trained language models (LLMs). In this chapter,\nwe provide a review of relevant methods, experimental setups and evaluation of\nthis approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,\nwe apply and evaluate the impact of train set augmentation with generated data\nusing LLMs that have been already adapted for hate detection, including\nRoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical\nstudy corroborates our previous findings, showing that this approach improves\nhate speech generalization, boosting recall performance across data\ndistributions. In addition, we explore and compare the performance of the\nfinetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results\ndemonstrate that while better generalization is achieved using the GPT-3.5\nmodel, it achieves mediocre recall and low precision on most datasets. It is an\nopen question whether the sensitivity of models such as GPT-3.5, and onward,\ncan be improved using similar techniques of text generation.\n","authors":["Sagi Pendzel","Tomer Wullach","Amir Adler","Einat Minkov"],"pdf_url":"https://arxiv.org/pdf/2311.09993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03656v2","updated":"2023-11-16T16:04:22Z","published":"2023-08-07T15:18:30Z","title":"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench","summary":"  Recently, the community has witnessed the advancement of Large Language\nModels (LLMs), which have shown remarkable performance on various downstream\ntasks. Led by powerful models like ChatGPT and Claude, LLMs are revolutionizing\nhow users engage with software, assuming more than mere tools but intelligent\nassistants. Consequently, evaluating LLMs' anthropomorphic capabilities becomes\nincreasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes five LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4 and LLaMA 2. A conclusion can be\ndrawn from the results that, despite several misalignments, LLMs can generally\nrespond appropriately to certain situations. Nevertheless, they fall short in\nalignment with the emotional behaviors of human beings and cannot establish\nconnections between similar situations. Our collected dataset of situations,\nthe human evaluation results, and the code of our testing framework, dubbed\nEmotionBench, is made publicly in https://github.com/CUHK-ARISE/EmotionBench.\nWe aspire to contribute to the advancement of LLMs regarding better alignment\nwith the emotional behaviors of human beings, thereby enhancing their utility\nand applicability as intelligent assistants.\n","authors":["Jen-tse Huang","Man Ho Lam","Eric John Li","Shujie Ren","Wenxuan Wang","Wenxiang Jiao","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2308.03656v2.pdf","comment":"16 pages. Added demographic distribution of the user study. Added\n  ethics statements. Added more details. Fixed some typos"},{"id":"http://arxiv.org/abs/2311.09979v1","updated":"2023-11-16T15:52:24Z","published":"2023-11-16T15:52:24Z","title":"Unambiguity and Fewness for Nonuniform Families of Polynomial-Size\n  Nondeterministic Finite Automata","summary":"  Nonuniform families of polynomial-size finite automata, which are series of\nindexed finite automata having polynomially many inner states, are used in the\npast literature to solve nonuniform families of promise decision problems.\nAmong such nonuniform families of finite automata, we focus our attention, in\nparticular, on the variants of nondeterministic finite automata, which have at\nmost \"one\" (unambiguous), \"polynomially many\" (few) accepting computation\npaths, or unambiguous/few computation paths leading to each fixed\nconfiguration. When such machines are limited to make only one-way head moves,\nwe can prove with no unproven hardness assumptions that some of these variants\nare different in computational power from each other. As for two-way machines\nrestricted to instances of polynomially-bounded length, families of two-way\npolynomial-size nondeterministic finite automata are equivalent in power to\nfamilies of polynomial-size unambiguous finite automata.\n","authors":["Tomoyuki Yamakami"],"pdf_url":"https://arxiv.org/pdf/2311.09979v1.pdf","comment":"(A4, 10pt, 17 pages) This work corrects and also significantly alters\n  the preliminary report that appeared in the Proceedings of the 16th\n  International Conference on Reachability Problems (RP 2022), Kaiserslautern,\n  Germany, October 17--21, 2022, Lecture Notes in Computer Science, vol. 13608,\n  pp. 77--92, Springer Cham, 2022"},{"id":"http://arxiv.org/abs/2311.09948v1","updated":"2023-11-16T15:01:48Z","published":"2023-11-16T15:01:48Z","title":"Hijacking Large Language Models via Adversarial In-Context Learning","summary":"  In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific tasks by utilizing labeled examples as demonstrations in the\nprecondition prompts. Despite its promising performance, ICL suffers from\ninstability with the choice and arrangement of examples. Additionally, crafted\nadversarial attacks pose a notable threat to the robustness of ICL. However,\nexisting attacks are either easy to detect, rely on external models, or lack\nspecificity towards ICL. To address these issues, this work introduces a novel\ntransferable attack for ICL, aiming to hijack LLMs to generate the targeted\nresponse. The proposed LLM hijacking attack leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demonstrations. Extensive experimental results on various tasks and\ndatasets demonstrate the effectiveness of our LLM hijacking attack, resulting\nin a distracted attention towards adversarial tokens, consequently leading to\nthe targeted unwanted outputs.\n","authors":["Yao Qiang","Xiangyu Zhou","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.09948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09945v1","updated":"2023-11-16T14:56:09Z","published":"2023-11-16T14:56:09Z","title":"An Attention-Based Denoising Framework for Personality Detection in\n  Social Media Texts","summary":"  In social media networks, users produce a large amount of text content\nanytime, providing researchers with a valuable approach to digging for\npersonality-related information. Personality detection based on user-generated\ntexts is a universal method that can be used to build user portraits. The\npresence of noise in social media texts hinders personality detection. However,\nprevious studies have not fully addressed this challenge. Inspired by the\nscanning reading technique, we propose an attention-based information\nextraction mechanism (AIEM) for long texts, which is applied to quickly locate\nvaluable pieces of information, and focus more attention on the deep semantics\nof key pieces. Then, we provide a novel attention-based denoising framework\n(ADF) for personality detection tasks and achieve state-of-the-art performance\non two commonly used datasets. Notably, we obtain an average accuracy\nimprovement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator\n(Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed\nlight on how AIEM works to magnify personality-related signals.\n","authors":["Qirui Tang","Wenkang Jiang","Yihua Du","Lei Lin"],"pdf_url":"https://arxiv.org/pdf/2311.09945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12059v3","updated":"2023-11-16T14:04:15Z","published":"2023-10-18T15:48:07Z","title":"Evaluating the Symbol Binding Ability of Large Language Models for\n  Multiple-Choice Questions in Vietnamese General Education","summary":"  In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only.\n","authors":["Duc-Vu Nguyen","Quoc-Nam Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.12059v3.pdf","comment":"Accepted at SoICT 2023"},{"id":"http://arxiv.org/abs/2311.09889v1","updated":"2023-11-16T13:37:21Z","published":"2023-11-16T13:37:21Z","title":"Language Generation from Human Brain Activities","summary":"  Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.\n","authors":["Ziyi Ye","Qingyao Ai","Yiqun Liu","Min Zhang","Christina Lioma","Tuukka Ruotsalo"],"pdf_url":"https://arxiv.org/pdf/2311.09889v1.pdf","comment":"Preprint. Under Submission"},{"id":"http://arxiv.org/abs/2303.03290v2","updated":"2023-11-16T12:47:29Z","published":"2023-03-06T17:06:50Z","title":"AmQA: Amharic Question Answering Dataset","summary":"  Question Answering (QA) returns concise answers or answer lists from natural\nlanguage text given a context document. Many resources go into curating QA\ndatasets to advance robust models' development. There is a surge of QA datasets\nfor languages like English, however, this is not true for Amharic. Amharic, the\nofficial language of Ethiopia, is the second most spoken Semitic language in\nthe world. There is no published or publicly available Amharic QA dataset.\nHence, to foster the research in Amharic QA, we present the first Amharic QA\n(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia\narticles. Additionally, we run an XLMR Large-based baseline model to spark\nopen-domain QA research interest. The best-performing baseline achieves an\nF-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension\nsettings respectively.\n","authors":["Tilahun Abedissa","Ricardo Usbeck","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2303.03290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09862v1","updated":"2023-11-16T12:45:41Z","published":"2023-11-16T12:45:41Z","title":"Which Modality should I use -- Text, Motif, or Image? : Understanding\n  Graphs with Large Language Models","summary":"  Large language models (LLMs) are revolutionizing various fields by leveraging\nlarge text corpora for context-aware intelligence. Due to the context size,\nhowever, encoding an entire graph with LLMs is fundamentally limited. This\npaper explores how to better integrate graph data with LLMs and presents a\nnovel approach using various encoding modalities (e.g., text, image, and motif)\nand approximation of global connectivity of a graph using different prompting\nmethods to enhance LLMs' effectiveness in handling complex graph structures.\nThe study also introduces GraphTMI, a new benchmark for evaluating LLMs in\ngraph structure analysis, focusing on factors such as homophily, motif\npresence, and graph difficulty. Key findings reveal that image modality,\nsupported by advanced vision-language models like GPT-4V, is more effective\nthan text in managing token limits while retaining critical information. The\nresearch also examines the influence of different factors on each encoding\nmodality's performance. This study highlights the current limitations and\ncharts future directions for LLMs in graph understanding and reasoning tasks.\n","authors":["Debarati Das","Ishaan Gupta","Jaideep Srivastava","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2311.09862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09861v1","updated":"2023-11-16T12:43:18Z","published":"2023-11-16T12:43:18Z","title":"PsyBench: a balanced and in-depth Psychological Chinese Evaluation\n  Benchmark for Foundation Models","summary":"  As Large Language Models (LLMs) are becoming prevalent in various fields,\nthere is an urgent need for improved NLP benchmarks that encompass all the\nnecessary knowledge of individual discipline. Many contemporary benchmarks for\nfoundational models emphasize a broad range of subjects but often fall short in\npresenting all the critical subjects and encompassing necessary professional\nknowledge of them. This shortfall has led to skewed results, given that LLMs\nexhibit varying performance across different subjects and knowledge areas. To\naddress this issue, we present psybench, the first comprehensive Chinese\nevaluation suite that covers all the necessary knowledge required for graduate\nentrance exams. psybench offers a deep evaluation of a model's strengths and\nweaknesses in psychology through multiple-choice questions. Our findings show\nsignificant differences in performance across different sections of a subject,\nhighlighting the risk of skewed results when the knowledge in test sets is not\nbalanced. Notably, only the ChatGPT model reaches an average accuracy above\n$70\\%$, indicating that there is still plenty of room for improvement. We\nexpect that psybench will help to conduct thorough evaluations of base models'\nstrengths and weaknesses and assist in practical application in the field of\npsychology.\n","authors":["Junlei Zhang","Hongliang He","Nirui Song","Shuyuan He","\\\\Shuai Zhang","Huachuan Qiu","Anqi Li","Lizhi Ma","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2311.09861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09860v1","updated":"2023-11-16T12:43:02Z","published":"2023-11-16T12:43:02Z","title":"GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity\n  Extraction Focused on Machine Learning Models and Datasets","summary":"  Named Entity Recognition (NER) models play a crucial role in various NLP\ntasks, including information extraction (IE) and text understanding. In\nacademic writing, references to machine learning models and datasets are\nfundamental components of various computer science publications and necessitate\naccurate models for identification. Despite the advancements in NER, existing\nground truth datasets do not treat fine-grained types like ML model and model\narchitecture as separate entity types, and consequently, baseline models cannot\nrecognize them as such. In this paper, we release a corpus of 100 manually\nannotated full-text scientific publications and a first baseline model for 10\nentity types centered around ML models and datasets. In order to provide a\nnuanced understanding of how ML models and datasets are mentioned and utilized,\nour dataset also contains annotations for informal mentions like \"our\nBERT-based model\" or \"an image CNN\". You can find the ground truth dataset and\ncode to replicate model training at https://data.gesis.org/gsap/gsap-ner.\n","authors":["Wolfgang Otto","Matthäus Zloch","Lu Gan","Saurav Karmakar","Stefan Dietze"],"pdf_url":"https://arxiv.org/pdf/2311.09860v1.pdf","comment":"10 pages, 1 figure, Accepted at EMNLP2023-Findings"},{"id":"http://arxiv.org/abs/2308.02463v5","updated":"2023-11-16T12:38:46Z","published":"2023-08-04T17:00:38Z","title":"Towards Generalist Foundation Model for Radiology by Leveraging\n  Web-scale 2D&3D Medical Data","summary":"  In this study, we aim to initiate the development of Radiology Foundation\nModel, termed as RadFM. We consider the construction of foundational models\nfrom three perspectives, namely, dataset construction, model design, and\nthorough evaluation. Our contribution can be concluded as follows: (i), we\nconstruct a large-scale Medical Multi-modal Dataset, MedMD, which consists of\n16M 2D and 3D medical scans with high-quality text descriptions or reports\nacross various data formats, modalities, and tasks, covering over 5000 distinct\ndiseases. To the best of our knowledge, this is the first large-scale,\nhigh-quality, medical visual-language dataset, with both 2D and 3D scans; (ii),\nwe propose an architecture that enables visually conditioned generative\npre-training, i.e., allowing for integration of text input with 2D or 3D\nmedical scans, and generate responses for diverse radiologic tasks. The model\nwas initially pre-trained on MedMD and subsequently fine-tuned on the\ndomain-specific dataset, which is a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs, termed as RadMD; (iii), we\npropose a new evaluation benchmark, RadBench, that comprises five tasks,\nincluding modality recognition, disease diagnosis, visual question answering,\nreport generation and rationale diagnosis, aiming to comprehensively assess the\ncapability of foundation models in handling practical clinical problems. We\nconduct both automatic and human evaluation on RadBench, in both cases, RadFM\noutperforms existing multi-modal foundation models, that are publicaly\naccessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V.\nAdditionally, we also adapt RadFM for different public benchmarks, surpassing\nexisting SOTAs on diverse datasets. All codes, data, and model checkpoint will\nall be made publicly available to promote further research and development in\nthe field.\n","authors":["Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2308.02463v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00002v3","updated":"2023-11-16T12:33:12Z","published":"2023-07-28T01:30:15Z","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition","summary":"  Temporal commonsense reasoning refers to the ability to understand the\ntypical temporal context of phrases, actions, and events, and use it to reason\nover problems requiring such knowledge. This trait is essential in temporal\nnatural language processing tasks, with possible applications such as timeline\nsummarization, temporal question answering, and temporal natural language\ninference. Recent research on the performance of large language models suggests\nthat, although they are adept at generating syntactically correct sentences and\nsolving classification tasks, they often take shortcuts in their reasoning and\nfall prey to simple linguistic traps. This article provides an overview of\nresearch in the domain of temporal commonsense reasoning, particularly focusing\non enhancing language model performance through a variety of augmentations and\ntheir evaluation across a growing number of datasets. However, these augmented\nmodels still struggle to approach human performance on reasoning tasks over\ntemporal common sense properties, such as the typical occurrence times,\norderings, or durations of events. We further emphasize the need for careful\ninterpretation of research to guard against overpromising evaluation results in\nlight of the shallow reasoning present in transformers. This can be achieved by\nappropriately preparing datasets and suitable evaluation metrics.\n","authors":["Georg Wenzel","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2308.00002v3.pdf","comment":"27 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2311.09841v1","updated":"2023-11-16T12:13:49Z","published":"2023-11-16T12:13:49Z","title":"Leveraging LLMs in Scholarly Knowledge Graph Question Answering","summary":"  This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.\n","authors":["Tilahun Abedissa Taffa","Ricardo Usbeck"],"pdf_url":"https://arxiv.org/pdf/2311.09841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09836v1","updated":"2023-11-16T12:05:23Z","published":"2023-11-16T12:05:23Z","title":"PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization","summary":"  We investigate pre-training techniques for abstractive multi-document\nsummarization (MDS), which is much less studied than summarizing single\ndocuments. Though recent work has demonstrated the effectiveness of\nhighlighting information salience for pre-training strategy design, it\nstruggles to generate abstractive and reflective summaries, which are critical\nproperties for MDS. To this end, we present PELMS, a pre-trained model that\nuses objectives based on semantic coherence heuristics and faithfulness\nconstraints with un-labeled multi-document inputs, to promote the generation of\nconcise, fluent, and faithful summaries. To support the training of PELMS, we\ncompile MultiPT, a multi-document pre-training corpus containing over 93\nmillion documents to form more than 3 million unlabeled topic-centric document\nclusters, covering diverse genres such as product reviews, news, and general\nknowledge. We perform extensive evaluation of PELMS in low-shot settings on a\nwide range of MDS datasets. Our approach consistently outperforms competitive\ncomparisons with respect to overall informativeness, abstractiveness,\ncoherence, and faithfulness.\n","authors":["Joseph J. Peper","Wenzhao Qiu","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09835v1","updated":"2023-11-16T12:03:21Z","published":"2023-11-16T12:03:21Z","title":"ML-Bench: Large Language Models Leverage Open-source Libraries for\n  Machine Learning Tasks","summary":"  Large language models have shown promising performance in code generation\nbenchmarks. However, a considerable divide exists between these benchmark\nachievements and their practical applicability, primarily attributed to\nreal-world programming's reliance on pre-existing libraries. Instead of\nevaluating LLMs to code from scratch, this work aims to propose a new\nevaluation setup where LLMs use open-source libraries to finish machine\nlearning tasks. Therefore, we propose ML-Bench, an expansive benchmark\ndeveloped to assess the effectiveness of LLMs in leveraging existing functions\nin open-source libraries. Consisting of 10044 samples spanning 130 tasks over\n14 notable machine learning GitHub repositories. In this setting, given a\nspecific machine learning task instruction and the accompanying README in a\ncodebase, an LLM is tasked to generate code to accomplish the task. This\nnecessitates the comprehension of long and language-code interleaved documents,\nas well as the understanding of complex cross-file code structures, introducing\nnew challenges. Notably, while GPT-4 exhibits remarkable improvement over other\nLLMs, it manages to accomplish only 39.73\\% of the tasks, leaving a huge space\nfor improvement. We address these challenges by proposing ML-Agent, designed to\neffectively navigate the codebase, locate documentation, retrieve code, and\ngenerate executable code. Empirical results demonstrate that ML-Agent, built\nupon GPT-4, results in further improvements. Code, data, and models are\navailable at \\url{https://ml-bench.github.io/}.\n","authors":["Yuliang Liu","Xiangru Tang","Zefan Cai","Junjie Lu","Yichi Zhang","Yanjun Shao","Zexuan Deng","Helan Hu","Zengxian Yang","Kaikai An","Ruijun Huang","Shuzheng Si","Sheng Chen","Haozhe Zhao","Zhengliang Li","Liang Chen","Yiming Zong","Yan Wang","Tianyu Liu","Zhiwei Jiang","Baobao Chang","Yujia Qin","Wangchunshu Zhou","Yilun Zhao","Arman Cohan","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2311.09835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09834v1","updated":"2023-11-16T12:01:19Z","published":"2023-11-16T12:01:19Z","title":"Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens\n  Contributing to Explicit Hate in English by Span Detection","summary":"  As hate speech continues to proliferate on the web, it is becoming\nincreasingly important to develop computational methods to mitigate it.\nReactively, using black-box models to identify hateful content can perplex\nusers as to why their posts were automatically flagged as hateful. On the other\nhand, proactive mitigation can be achieved by suggesting rephrasing before a\npost is made public. However, both mitigation techniques require information\nabout which part of a post contains the hateful aspect, i.e., what spans within\na text are responsible for conveying hate. Better detection of such spans can\nsignificantly reduce explicitly hateful content on the web. To further\ncontribute to this research area, we organized HateNorm at HASOC-FIRE 2023,\nfocusing on explicit span detection in English Tweets. A total of 12 teams\nparticipated in the competition, with the highest macro-F1 observed at 0.58.\n","authors":["Sarah Masud","Mohammad Aflah Khan","Md. Shad Akhtar","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2311.09834v1.pdf","comment":"8 pages, 1 figure, 4 Tables"},{"id":"http://arxiv.org/abs/2311.09832v1","updated":"2023-11-16T11:58:31Z","published":"2023-11-16T11:58:31Z","title":"X-Mark: Towards Lossless Watermarking Through Lexical Redundancy","summary":"  Text watermarking has emerged as an important technique for detecting\nmachine-generated text. However, existing methods can severely degrade text\nquality due to arbitrary vocabulary partitioning, which disrupts the language\nmodel's expressiveness and impedes textual coherence. To mitigate this, we\nintroduce XMark, a novel approach that capitalizes on text redundancy within\nthe lexical space. Specifically, XMark incorporates a mutually exclusive rule\nfor synonyms during the language model decoding process, thereby integrating\nprior knowledge into vocabulary partitioning and preserving the capabilities of\nlanguage generation. We present theoretical analyses and empirical evidence\ndemonstrating that XMark substantially enhances text generation fluency while\nmaintaining watermark detectability. Furthermore, we investigate watermarking's\nimpact on the emergent abilities of large language models, including zero-shot\nand few-shot knowledge recall, logical reasoning, and instruction following.\nOur comprehensive experiments confirm that XMark consistently outperforms\nexisting methods in retaining these crucial capabilities of LLMs.\n","authors":["Liang Chen","Yatao Bian","Yang Deng","Shuaiyi Li","Bingzhe Wu","Peilin Zhao","Kam-fai Wong"],"pdf_url":"https://arxiv.org/pdf/2311.09832v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2311.07194v2","updated":"2023-11-16T11:56:12Z","published":"2023-11-13T09:32:12Z","title":"Exploring the Dialogue Comprehension Ability of Large Language Models","summary":"  LLMs may interact with users in the form of dialogue and generate responses\nfollowing their instructions, which naturally require dialogue comprehension\nabilities. However, dialogue comprehension is a general language ability which\nis hard to be evaluated directly. In this work, we propose to perform the\nevaluation with the help of the dialogue summarization task. Beside evaluating\nand analyzing the dialogue summarization performance (DIAC-Sum) of different\nLLMs, we also derive factual questions from the generated summaries and use\nthem as a more flexible measurement of dialogue comprehension (DIAC-FactQA).\nOur evaluation shows that, on average, 27% of the summaries generated by LLMs\ncontain factual inconsistency. Even ChatGPT, the strongest model evaluated, has\nsuch errors in 16% of its summaries. For answering the factual questions, which\nis more challenging, the average error rate of all evaluated LLMs is 37.2%.\nBoth results indicate serious deficiencies. Detailed analysis shows that the\nunderstanding of subject/object of the conversation is still the most\nchallenging problem for LLMs. Furthermore, to stimulate and enhance the\ndialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with\nauto-constructed multi-task data. The experimental results demonstrate that our\nmethod achieved an error rate improvement of 10.9% on DIAC-FactQA.\n","authors":["Shuaijie She","Shujian Huang","Xingyun Wang","Yanke Zhou","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09830v1","updated":"2023-11-16T11:55:27Z","published":"2023-11-16T11:55:27Z","title":"AutoPlanBench: : Automatically generating benchmarks for LLM planners\n  from PDDL","summary":"  LLMs are being increasingly used for planning-style tasks, but their\ncapabilities for planning and reasoning are poorly understood. We present a\nnovel method for automatically converting planning benchmarks written in PDDL\ninto textual descriptions and offer a benchmark dataset created with our\nmethod. We show that while the best LLM planners do well on many planning\ntasks, others remain out of reach of current methods.\n","authors":["Katharina Stein","Alexander Koller"],"pdf_url":"https://arxiv.org/pdf/2311.09830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09829v1","updated":"2023-11-16T11:53:31Z","published":"2023-11-16T11:53:31Z","title":"FollowEval: A Multi-Dimensional Benchmark for Assessing the\n  Instruction-Following Capability of Large Language Models","summary":"  The effective assessment of the instruction-following ability of large\nlanguage models (LLMs) is of paramount importance. A model that cannot adhere\nto human instructions might be not able to provide reliable and helpful\nresponses. In pursuit of this goal, various benchmarks have been constructed to\nevaluate the instruction-following capacity of these models. However, these\nbenchmarks are limited to a single language and are constructed using automated\napproaches, which restricts their applicability and the quality of the test\nexamples they contain. To bridge this gap, we introduce the FollowEval\nbenchmark in this paper. This benchmark is composed of instances in both\nEnglish and Chinese, and all test examples are crafted by human experts.\nFurthermore, the FollowEval benchmark is designed to assess LLMs across five\ncritical dimensions of instruction following: string manipulation, commonsense\nreasoning, logical reasoning, spatial reasoning, and response constraints. To\nenhance the complexity and present a sufficient challenge, each test example is\ndesigned to evaluate more than one dimension. We have evaluated various LLMs\nusing the FollowEval benchmark and found that their performance significantly\nlags behind that of humans. This highlights the considerable room for\nimprovement in the instruction-following ability of these models.\n","authors":["Yimin Jing","Renren Jin","Jiahao Hu","Huishi Qiu","Xiaohua Wang","Peng Wang","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.09829v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09828v1","updated":"2023-11-16T11:52:52Z","published":"2023-11-16T11:52:52Z","title":"AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced\n  African Languages","summary":"  Despite the progress we have recorded in scaling multilingual machine\ntranslation (MT) models and evaluation data to several under-resourced African\nlanguages, it is difficult to measure accurately the progress we have made on\nthese languages because evaluation is often performed on n-gram matching\nmetrics like BLEU that often have worse correlation with human judgments.\nEmbedding-based metrics such as COMET correlate better; however, lack of\nevaluation data with human ratings for under-resourced languages, complexity of\nannotation guidelines like Multidimensional Quality Metrics (MQM), and limited\nlanguage coverage of multilingual encoders have hampered their applicability to\nAfrican languages. In this paper, we address these challenges by creating\nhigh-quality human evaluation data with a simplified MQM guideline for\nerror-span annotation and direct assessment (DA) scoring for 13 typologically\ndiverse African languages. Furthermore, we develop AfriCOMET, a COMET\nevaluation metric for African languages by leveraging DA training data from\nhigh-resource languages and African-centric multilingual encoder\n(AfroXLM-Roberta) to create the state-of-the-art evaluation metric for African\nlanguages MT with respect to Spearman-rank correlation with human judgments\n(+0.406).\n","authors":["Jiayi Wang","David Ifeoluwa Adelani","Sweta Agrawal","Ricardo Rei","Eleftheria Briakou","Marine Carpuat","Marek Masiak","Xuanli He","Sofia Bourhim","Andiswa Bukula","Muhidin Mohamed","Temitayo Olatoye","Hamam Mokayede","Christine Mwase","Wangui Kimotho","Foutse Yuehgoh","Anuoluwapo Aremu","Jessica Ojo","Shamsuddeen Hassan Muhammad","Salomey Osei","Abdul-Hakeem Omotayo","Chiamaka Chukwuneke","Perez Ogayo","Oumaima Hourrane","Salma El Anigri","Lolwethu Ndolela","Thabiso Mangwana","Shafie Abdi Mohamed","Ayinde Hassan","Oluwabusayo Olufunke Awoyomi","Lama Alkhaled","Sana Al-Azzawi","Naome A. Etori","Millicent Ochieng","Clemencia Siro","Samuel Njoroge","Eric Muchiri","Wangari Kimotho","Lyse Naomi Wamba Momo","Daud Abolade","Simbiat Ajao","Tosin Adewumi","Iyanuoluwa Shode","Ricky Macharm","Ruqayya Nasir Iro","Saheed S. Abdullahi","Stephen E. Moore","Bernard Opoku","Zainab Akinjobi","Abeeb Afolabi","Nnaemeka Obiefuna","Onyekachi Raphael Ogbu","Sam Brian","Verrah Akinyi Otiende","Chinedu Emmanuel Mbonu","Sakayo Toadoum Sari","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2311.09828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09827v1","updated":"2023-11-16T11:52:22Z","published":"2023-11-16T11:52:22Z","title":"Cognitive Overload: Jailbreaking Large Language Models with Overloaded\n  Logical Thinking","summary":"  While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.\n","authors":["Nan Xu","Fei Wang","Ben Zhou","Bang Zheng Li","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.09827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09825v1","updated":"2023-11-16T11:51:13Z","published":"2023-11-16T11:51:13Z","title":"Human Still Wins over LLM: An Empirical Study of Active Learning on\n  Domain-Specific Annotation Tasks","summary":"  Large Language Models (LLMs) have demonstrated considerable advances, and\nseveral claims have been made about their exceeding human performance. However,\nin real-world tasks, domain knowledge is often required. Low-resource learning\nmethods like Active Learning (AL) have been proposed to tackle the cost of\ndomain expert annotation, raising this question: Can LLMs surpass compact\nmodels trained with expert annotations in domain-specific tasks? In this work,\nwe conduct an empirical experiment on four datasets from three different\ndomains comparing SOTA LLMs with small models trained on expert annotations\nwith AL. We found that small models can outperform GPT-3.5 with a few hundreds\nof labeled data, and they achieve higher or similar performance with GPT-4\ndespite that they are hundreds time smaller. Based on these findings, we posit\nthat LLM predictions can be used as a warmup method in real-world applications\nand human experts remain indispensable in tasks involving data annotation\ndriven by domain-specific knowledge.\n","authors":["Yuxuan Lu","Bingsheng Yao","Shao Zhang","Yun Wang","Peng Zhang","Tun Lu","Toby Jia-Jun Li","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09821v1","updated":"2023-11-16T11:49:29Z","published":"2023-11-16T11:49:29Z","title":"Towards Robust Temporal Reasoning of Large Language Models via a\n  Multi-Hop QA Dataset and Pseudo-Instruction Tuning","summary":"  Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering did not emphasize multi-answer and\nmulti-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering (QA) dataset Complex-TR that focuses on\nmulti-answer and multi-hop temporal reasoning. Besides, we also propose a novel\ndata augmentation strategy to improve the complex temporal reasoning capability\nand robustness of LLMs. We conducted experiments on multiple temporal QA\ndatasets. Experimental results show that our method is able to improve LLMs'\nperformance on temporal QA benchmarks by significant margins.\n","authors":["Qingyu Tan","Hwee Tou Ng","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2311.09821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09818v1","updated":"2023-11-16T11:48:17Z","published":"2023-11-16T11:48:17Z","title":"SUQL: Conversational Search over Structured and Unstructured Data with\n  Large Language Models","summary":"  Many knowledge sources consist of both structured information such as\nrelational databases as well as unstructured free text. Building a\nconversational interface to such data sources is challenging.\n  This paper introduces SUQL, Structured and Unstructured Query Language, the\nfirst formal executable representation that naturally covers compositions of\nstructured and unstructured data queries. Specifically, it augments SQL with\nseveral free-text primitives to form a precise, succinct, and expressive\nrepresentation. This paper also presents a conversational search agent based on\nlarge language models, including a few-shot contextual semantic parser for\nSUQL.\n  To validate our approach, we introduce a dataset consisting of crowdsourced\nquestions and conversations about real restaurants. Over 51% of the questions\nin the dataset require both structured and unstructured data, suggesting that\nit is a common phenomenon. We show that our few-shot conversational agent based\non SUQL finds an entity satisfying all user requirements 89.3% of the time,\ncompared to just 65.0% for a strong and commonly used baseline.\n","authors":["Shicheng Liu","Jialiang Xu","Wesley Tjangnaka","Sina J. Semnani","Chen Jie Yu","Gui Dávid","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2311.09818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14806v2","updated":"2023-11-16T11:47:05Z","published":"2023-05-24T07:00:00Z","title":"AWESOME: GPU Memory-constrained Long Document Summarization using Memory\n  Mechanism and Global Salient Content","summary":"  Long document summarization systems are critical for domains with lengthy and\njargonladen text, yet they present significant challenges to researchers and\ndevelopers with limited computing resources. Existing solutions mainly focus on\nefficient attentions or divide-and-conquer strategies. The former reduces\ntheoretical time complexity, but is still memory-heavy. The latter methods\nsacrifice global context, leading to uninformative and incoherent summaries.\nThis work aims to leverage the memory-efficient nature of divide-and-conquer\nmethods while preserving global context. Concretely, our framework AWESOME uses\ntwo novel mechanisms: (1) External memory mechanisms track previously encoded\ndocument segments and their corresponding summaries, to enhance global document\nunderstanding and summary coherence. (2) Global salient content is further\nidentified beforehand to augment each document segment to support its\nsummarization. Extensive experiments on diverse genres of text, including\ngovernment reports, transcripts, scientific papers, and novels, show that\nAWESOME produces summaries with improved informativeness, faithfulness, and\ncoherence than competitive baselines on longer documents, while having a\nsmaller GPU memory footprint.\n","authors":["Shuyang Cao","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2305.14806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09816v1","updated":"2023-11-16T11:44:58Z","published":"2023-11-16T11:44:58Z","title":"Performance Trade-offs of Watermarking Large Language Models","summary":"  Amidst growing concerns of large language models (LLMs) being misused for\ngenerating misinformation or completing homework assignments, watermarking has\nemerged as an effective solution for distinguishing human-written and\nLLM-generated text. A prominent watermarking strategy is to embed a signal into\ngenerated text by upsampling a (pseudorandomly-chosen) subset of tokens at\nevery generation step. Although this signal is imperceptible to a human reader,\nit is detectable through statistical testing. However, implanting such signals\nalters the model's output distribution and can have unintended effects when\nwatermarked LLMs are used for downstream applications. In this work, we\nevaluate the performance of watermarked LLMs on a diverse suite of tasks,\nincluding text classification, textual entailment, reasoning, question\nanswering, translation, summarization, and language modeling. We find that\nwatermarking has negligible impact on the performance of tasks posed as k-class\nclassification problems in the average case. However, the accuracy can plummet\nto that of a random classifier for some scenarios (that occur with\nnon-negligible probability). Tasks that are cast as multiple-choice questions\nand short-form generation are surprisingly unaffected by watermarking. For\nlong-form generation tasks, including summarization and translation, we see a\ndrop of 15-20% in the performance due to watermarking. Our findings highlight\nthe trade-offs that users should be cognizant of when using watermarked models,\nand point to cases where future research could improve existing trade-offs.\n","authors":["Anirudh Ajith","Sameer Singh","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2311.09816v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.08562v2","updated":"2023-11-16T11:40:26Z","published":"2023-11-14T21:46:27Z","title":"MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration","summary":"  Large Language Models (LLMs) have marked a significant advancement in the\nfield of natural language processing, demonstrating exceptional capabilities in\nreasoning, tool usage, and memory. As their applications extend into\nmulti-agent environments, a need has arisen for a comprehensive evaluation\nframework that captures their abilities in reasoning, planning, collaboration,\nand more. This work introduces a novel benchmarking framework specifically\ntailored to assess LLMs within multi-agent settings, providing quantitative\nmetrics to evaluate their judgment, reasoning, deception, self-awareness,\ncooperation, coordination, and rationality. We utilize games such as Chameleon\nand Undercover, alongside game theory scenarios like Cost Sharing, Multi-player\nPrisoner's Dilemma, and Public Good, to create diverse testing environments.\nOur framework is fortified with the Probabilistic Graphical Modeling (PGM)\nmethod, enhancing the LLMs' capabilities in navigating complex social and\ncognitive dimensions. The benchmark evaluates seven multi-agent systems powered\nby different LLMs, quantitatively highlighting a significant capability gap\nover threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It\nalso confirms that our PGM enhancement boosts the inherent abilities of all\nselected models by 50% on average. Our codes are released here\nhttps://github.com/cathyxl/MAgIC.\n","authors":["Lin Xu","Zhiyuan Hu","Daquan Zhou","Hongyu Ren","Zhen Dong","Kurt Keutzer","See Kiong Ng","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2311.08562v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2311.09812v1","updated":"2023-11-16T11:37:54Z","published":"2023-11-16T11:37:54Z","title":"Large Language Models for Propaganda Span Annotation","summary":"  The use of propagandistic techniques in online communication has increased in\nrecent years, aiming to manipulate online audiences. Efforts to automatically\ndetect and debunk such content have been made, addressing various modeling\nscenarios. These include determining whether the content (text, image, or\nmultimodal) (i) is propagandistic, (ii) employs one or more techniques, and\n(iii) includes techniques with identifiable spans. Significant research efforts\nhave been devoted to the first two scenarios compared to the latter. Therefore,\nin this study, we focus on the task of detecting propagandistic textual spans.\nWe investigate whether large language models such as GPT-4 can be utilized to\nperform the task of an annotator. For the experiments, we used an in-house\ndeveloped dataset consisting of annotations from multiple annotators. Our\nresults suggest that providing more information to the model as prompts\nimproves the annotation agreement and performance compared to human\nannotations. We plan to make the annotated labels from multiple annotators,\nincluding GPT-4, available for the community.\n","authors":["Maram Hasanain","Fatema Ahmed","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2311.09812v1.pdf","comment":"propaganda, span detection, disinformation, misinformation, fake\n  news, LLMs, GPT-4"},{"id":"http://arxiv.org/abs/2311.09808v1","updated":"2023-11-16T11:32:47Z","published":"2023-11-16T11:32:47Z","title":"PixT3: Pixel-based Table To Text generation","summary":"  Table-to-Text has been traditionally approached as a linear language to text\nproblem. However, visually represented tables are rich in visual information\nand serve as a concise, effective form of representing data and its\nrelationships. When using text-based approaches, after the linearization\nprocess, this information is either lost or represented in a space inefficient\nmanner. This inefficiency has remained a constant challenge for text-based\napproaches making them struggle with large tables. In this paper, we\ndemonstrate that image representation of tables are more space-efficient than\nthe typical textual linearizations, and multi-modal approaches are competitive\nin Table-to-Text tasks. We present PixT3, a multimodal table-to-text model that\noutperforms the state-of-the-art (SotA) in the ToTTo benchmark in a pure\nTable-to-Text setting while remaining competitive in controlled Table-to-Text\nscenarios. It also generalizes better in unseen datasets, outperforming ToTTo\nSotA in all generation settings. Additionally, we introduce a new intermediate\ntraining curriculum to reinforce table structural awareness, leading to\nimproved generation and overall faithfulness of the models.\n","authors":["Iñigo Alonso","Eneko Agirre","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2311.09808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09807v1","updated":"2023-11-16T11:31:50Z","published":"2023-11-16T11:31:50Z","title":"The Curious Decline of Linguistic Diversity: Training Language Models on\n  Synthetic Text","summary":"  This study investigates the consequences of training large language models\n(LLMs) on synthetic data generated by their predecessors, an increasingly\nprevalent practice aimed at addressing the limited supply of human-generated\ntraining data. Diverging from the usual emphasis on performance metrics, we\nfocus on the impact of this training methodology on linguistic diversity,\nespecially when conducted recursively over time. To assess this, we developed a\nset of novel metrics targeting lexical, syntactic, and semantic diversity,\napplying them in recursive fine-tuning experiments across various natural\nlanguage generation tasks. Our findings reveal a marked decrease in the\ndiversity of the models' outputs through successive iterations. This trend\nunderscores the potential risks of training LLMs on predecessor-generated text,\nparticularly concerning the preservation of linguistic richness. Our study\nhighlights the need for careful consideration of the long-term effects of such\ntraining approaches on the linguistic capabilities of LLMs.\n","authors":["Yanzhu Guo","Guokan Shang","Michalis Vazirgiannis","Chloé Clavel"],"pdf_url":"https://arxiv.org/pdf/2311.09807v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09805v1","updated":"2023-11-16T11:30:53Z","published":"2023-11-16T11:30:53Z","title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in\n  Understanding Long Documents with Tabular Data","summary":"  Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning and problem-solving capabilities of LLMs in the context of\nunderstanding and analyzing financial documents containing both text and\ntables. We evaluate a wide spectrum of 19 LLMs, including those specialized in\ncoding and finance. We also incorporate different prompting strategies (i.e.,\nChain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the\ncapabilities and limitations of existing LLMs in DocMath-Eval. We found that,\nalthough the current best-performing system (i.e., GPT-4), can perform well on\nsimple problems such as calculating the rate of increase in a financial metric\nwithin a short document context, it significantly lags behind human experts in\nmore complex problems grounded in longer contexts. We believe DocMath-Eval can\nbe used as a valuable benchmark to evaluate LLMs' capabilities to solve\nchallenging numerical reasoning problems in expert domains. We will release the\nbenchmark and code at https://github.com/yale-nlp/DocMath-Eval.\n","authors":["Yilun Zhao","Yitao Long","Hongjun Liu","Linyong Nan","Lyuhao Chen","Ryo Kamoi","Yixin Liu","Xiangru Tang","Rui Zhang","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09805v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2311.09802v1","updated":"2023-11-16T11:26:21Z","published":"2023-11-16T11:26:21Z","title":"Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs","summary":"  Though prompting LLMs with various reasoning structures produces reasoning\nproofs along with answers, these proofs are not ensured to be causal and\nreliable due to the inherent defects of LLMs. Tracking such deficiencies, we\npresent a neuro-symbolic integration method, in which a neural LLM is used to\nrepresent the knowledge of the problem while an LLM-free symbolic solver is\nadopted to do deliberative reasoning using the knowledge. Specifically, our\ncustomized meta-interpreters allow the production of reasoning proofs and\nsupport flexible search strategies. These reasoning proofs are ensured to be\ncausal and reliable because of the deterministic executing nature of the\nsymbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT\nbaseline by nearly double in accuracy and more than triple in proof similarity.\nOn GSM8K, our method also shows accuracy improvements and nearly doubled proof\nsimilarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing\n","authors":["Sen Yang","Xin Li","Leyang Cui","Lidong Bing","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2311.09802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09800v1","updated":"2023-11-16T11:25:44Z","published":"2023-11-16T11:25:44Z","title":"$\\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of\n  Information-Seeking Dialogue via Behavioural Fine-Tuning","summary":"  Factuality is a crucial requirement in information seeking dialogue: the\nsystem should respond to the user's queries so that the responses are\nmeaningful and aligned with the knowledge provided to the system. However, most\nmodern large language models suffer from hallucinations, that is, they generate\nresponses not supported by or contradicting the knowledge source. To mitigate\nthe issue and increase faithfulness of information-seeking dialogue systems, we\nintroduce BeInfo, a simple yet effective method that applies behavioural tuning\nto aid information-seeking dialogue. Relying on three standard datasets, we\nshow that models tuned with BeInfo} become considerably more faithful to the\nknowledge source both for datasets and domains seen during BeInfo-tuning, as\nwell as on unseen domains, when applied in a zero-shot manner. In addition, we\nshow that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo\ndemonstrate strong performance on data from real `production' conversations and\noutperform GPT4 when tuned on a limited amount of such realistic in-domain\ndialogues.\n","authors":["Evgeniia Razumovskaia","Ivan Vulić","Pavle Marković","Tomasz Cichy","Qian Zheng","Tsung-Hsien Wen","Paweł Budzianowski"],"pdf_url":"https://arxiv.org/pdf/2311.09800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09799v1","updated":"2023-11-16T11:23:38Z","published":"2023-11-16T11:23:38Z","title":"How Far Can We Extract Diverse Perspectives from Large Language Models?\n  Criteria-Based Diversity Prompting!","summary":"  Collecting diverse human data on subjective NLP topics is costly and\nchallenging. As Large Language Models (LLMs) have developed human-like\ncapabilities, there is a recent trend in collaborative efforts between humans\nand LLMs for generating diverse data, offering potential scalable and efficient\nsolutions. However, the extent of LLMs' capability to generate diverse\nperspectives on subjective topics remains an unexplored question. In this\nstudy, we investigate LLMs' capacity for generating diverse perspectives and\nrationales on subjective topics, such as social norms and argumentative texts.\nWe formulate this problem as diversity extraction in LLMs and propose a\ncriteria-based prompting technique to ground diverse opinions and measure\nperspective diversity from the generated criteria words. Our results show that\nmeasuring semantic diversity through sentence embeddings and distance metrics\nis not enough to measure perspective diversity. To see how far we can extract\ndiverse perspectives from LLMs, or called diversity coverage, we employ a\nstep-by-step recall prompting for generating more outputs from the model in an\niterative manner. As we apply our prompting method to other tasks (hate speech\nlabeling and story continuation), indeed we find that LLMs are able to generate\ndiverse opinions according to the degree of task subjectivity.\n","authors":["Shirley Anugrah Hayati","Minhwa Lee","Dheeraj Rajagopal","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2311.09799v1.pdf","comment":"NLP"},{"id":"http://arxiv.org/abs/2311.09797v1","updated":"2023-11-16T11:22:08Z","published":"2023-11-16T11:22:08Z","title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance\n  Domains","summary":"  We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs'\ncapabilities in applying financial knowledge to solve complex math word\nproblems. Compared to prior works, this study features three core advancements.\nFirst, KnowledgeMath includes 1,259 problems with a hybrid of textual and\ntabular content and require college-level knowledge in the finance domain for\neffective resolution. Second, we provide expert-annotated, detailed solution\nreferences in Python program format, ensuring a high-quality benchmark for LLM\nassessment. Finally, we evaluate a wide spectrum of 14 LLMs with different\nprompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The\ncurrent best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves\nonly 45.4% accuracy, leaving substantial room for improvement. While\nknowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0%\nfor GPT-3.5), it is still significantly lower the estimated human expert\nperformance of 94%. We believe that KnowledgeMath can facilitate future\nresearch on domain-specific knowledge retrieval and augmentation into the math\nword problem-solving process. We will release the benchmark and code at\nhttps://github.com/yale-nlp/KnowledgeMath.\n","authors":["Yilun Zhao","Hongjun Liu","Yitao Long","Rui Zhang","Chen Zhao","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09797v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2311.09796v1","updated":"2023-11-16T11:19:26Z","published":"2023-11-16T11:19:26Z","title":"Interpreting User Requests in the Context of Natural Language Standing\n  Instructions","summary":"  Users of natural language interfaces, generally powered by Large Language\nModels (LLMs),often must repeat their preferences each time they make a similar\nrequest. To alleviate this, we propose including some of a user's preferences\nand instructions in natural language -- collectively termed standing\ninstructions -- as additional context for such interfaces. For example, when a\nuser states I'm hungry, their previously expressed preference for Persian food\nwill be automatically added to the LLM prompt, so as to influence the search\nfor relevant restaurants. We develop NLSI, a language-to-program dataset\nconsisting of over 2.4K dialogues spanning 17 domains, where each dialogue is\npaired with a user profile (a set of users specific standing instructions) and\ncorresponding structured representations (API calls). A key challenge in NLSI\nis to identify which subset of the standing instructions is applicable to a\ngiven dialogue. NLSI contains diverse phenomena, from simple preferences to\ninterdependent instructions such as triggering a hotel search whenever the user\nis booking tickets to an event. We conduct experiments on NLSI using prompting\nwith large language models and various retrieval approaches, achieving a\nmaximum of 44.7% exact match on API prediction. Our results demonstrate the\nchallenges in identifying the relevant standing instructions and their\ninterpretation into API calls.\n","authors":["Nikita Moghe","Patrick Xia","Jacob Andreas","Jason Eisner","Benjamin Van Durme","Harsh Jhamtani"],"pdf_url":"https://arxiv.org/pdf/2311.09796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11722v2","updated":"2023-11-16T11:08:51Z","published":"2023-10-18T05:42:22Z","title":"Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical\n  Foundation Model: A Computational Analysis","summary":"  Foundation Models (FMs) have the potential to revolutionize the way users\nself-diagnose through search engines by offering direct and efficient\nsuggestions. Recent studies primarily focused on the quality of FMs evaluated\nby GPT-4 or their ability to pass medical exams, no studies have quantified the\nextent of self-diagnostic atomic knowledge stored in FMs' memory, which is the\nbasis of foundation models to provide factual and reliable suggestions. In this\npaper, we first constructed a benchmark of Self-diagnostic Atomic Knowledge\n(SdAK), including the most common types of atomic knowledge involved in\nself-diagnostic queries, with 17 atomic types and a total of 14, 048 pieces of\natomic knowledge. Then, we evaluated both generic and open-source Chinese\nmedical FMs on the benchmark. The experimental results showcase that generic\nFMs perform better than medical FMs in terms of self-diagnostic atomic\nknowledge. Error analysis revealed that both generic and medical FMs are\nsycophantic, e.g., always catering to users' claims when it comes to unknown\nknowledge. We further explored different types of data commonly adopted for\nfine-tuning medical FMs, i.e., real-world, semi-distilled, and distilled data,\nand found that distilled data can benefit FMs most. The code and data are\navailable at \\url{https://github.com/FreedomIntelligence/SDAK}.\n","authors":["Yaxin Fan","Feng Jiang","Benyou Wang","Peifeng Li","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2310.11722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09783v1","updated":"2023-11-16T11:03:04Z","published":"2023-11-16T11:03:04Z","title":"Investigating Data Contamination in Modern Benchmarks for Large Language\n  Models","summary":"  Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.\n","authors":["Chunyuan Deng","Yilun Zhao","Xiangru Tang","Mark Gerstein","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09783v1.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2311.09782v1","updated":"2023-11-16T11:02:49Z","published":"2023-11-16T11:02:49Z","title":"More Samples or More Prompt Inputs? Exploring Effective In-Context\n  Sampling for LLM Few-Shot Prompt Engineering","summary":"  While most existing works on LLM prompt-engineering focus only on how to\nselect a better set of data samples inside one single prompt input (In-Context\nLearning or ICL), why can't we design and leverage multiple prompt inputs\ntogether to further improve the LLM performance? In this work, we propose\nIn-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to\nproduce the most confident prediction results by optimizing the construction of\nmultiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL\nand Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate\nthat ICS can consistently enhance LLM's prediction performance and confidence.\nAn ablation study suggests that a diversity-based ICS strategy may further\nimprove LLM's performance, which sheds light on a new yet promising future\nresearch direction.\n","authors":["Bingsheng Yao","Guiming Chen","Ruishi Zou","Yuxuan Lu","Jiachen Li","Shao Zhang","Sijia Liu","James Hendler","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09774v1","updated":"2023-11-16T10:56:24Z","published":"2023-11-16T10:56:24Z","title":"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs","summary":"  Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.\n","authors":["Junying Chen","Xidong Wang","Anningzhe Gao","Feng Jiang","Shunian Chen","Hongbo Zhang","Dingjie Song","Wenya Xie","Chuyi Kong","Jianquan Li","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09773v1","updated":"2023-11-16T10:55:29Z","published":"2023-11-16T10:55:29Z","title":"To be or not to be? an exploration of continuously controllable prompt\n  engineering","summary":"  As the use of large language models becomes more widespread, techniques like\nparameter-efficient fine-tuning and other methods for controlled generation are\ngaining traction for customizing models and managing their outputs. However,\nthe challenge of precisely controlling how prompts influence these models is an\narea ripe for further investigation. In response, we introduce ControlPE\n(Continuously Controllable Prompt Engineering). ControlPE enables finer\nadjustments to prompt effects, complementing existing prompt engineering, and\neffectively controls continuous targets. This approach harnesses the power of\nLoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,\nenabling fine-tuned adjustments to the impact of prompts. Our methodology\ninvolves generating specialized datasets for prompt distillation, incorporating\nthese prompts into the LoRA model, and carefully adjusting LoRA merging weight\nto regulate the influence of prompts. This provides a dynamic and adaptable\ntool for prompt control. Through our experiments, we have validated the\npracticality and efficacy of ControlPE. It proves to be a promising solution\nfor control a variety of prompts, ranging from generating short responses\nprompts, refusal prompts to chain-of-thought prompts.\n","authors":["Yuhan Sun","Mukai Li","Yixin Cao","Kun Wang","Wenxiao Wang","Xingyu Zeng","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.09773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09766v1","updated":"2023-11-16T10:43:26Z","published":"2023-11-16T10:43:26Z","title":"LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores","summary":"  Automatic evaluation of generated textual content presents an ongoing\nchallenge within the field of NLP. Given the impressive capabilities of modern\nlanguage models (LMs) across diverse NLP tasks, there is a growing trend to\nemploy these models in creating innovative evaluation metrics for automated\nassessment of generation tasks. This paper investigates a pivotal question: Do\nlanguage model-driven evaluation metrics inherently exhibit bias favoring texts\ngenerated by the same underlying language model? Specifically, we assess\nwhether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and\nGPTScore--demonstrate a favorable bias toward their respective underlying LMs\nin the context of summarization tasks. Our findings unveil a latent bias,\nparticularly pronounced when such evaluation metrics are used in an\nreference-free manner without leveraging gold summaries. These results\nunderscore that assessments provided by generative evaluation models can be\ninfluenced by factors beyond the inherent text quality, highlighting the\nnecessity of developing more dependable evaluation protocols in the future.\n","authors":["Yiqi Liu","Nafise Sadat Moosavi","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2311.09766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09763v1","updated":"2023-11-16T10:38:43Z","published":"2023-11-16T10:38:43Z","title":"Test-time Backdoor Mitigation for Black-Box Large Language Models with\n  Defensive Demonstrations","summary":"  Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes particularly pronounced in the context of Large Language Models\n(LLMs) deployed as Web Services, which typically offer only black-box access,\nrendering training-time defenses impractical. To bridge this gap, our work\nintroduces defensive demonstrations, an innovative backdoor defense strategy\nfor blackbox large language models. Our method involves identifying the task\nand retrieving task-relevant demonstrations from an uncontaminated pool. These\ndemonstrations are then combined with user queries and presented to the model\nduring testing, without requiring any modifications/tuning to the black-box\nmodel or insights into its internal mechanisms. Defensive demonstrations are\ndesigned to counteract the adverse effects of triggers, aiming to recalibrate\nand correct the behavior of poisoned models during test-time evaluations.\nExtensive experiments show that defensive demonstrations are effective in\ndefending both instance-level and instruction-level backdoor attacks, not only\nrectifying the behavior of poisoned models but also surpassing existing\nbaselines in most scenarios.\n","authors":["Wenjie Mo","Jiashu Xu","Qin Liu","Jiongxiao Wang","Jun Yan","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.09763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09762v1","updated":"2023-11-16T10:36:08Z","published":"2023-11-16T10:36:08Z","title":"Graph-Guided Reasoning for Multi-Hop Question Answering in Large\n  Language Models","summary":"  Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning\ncapabilities of Large Language Models (LLMs) by generating a series of\nrationales before the final answer. We analyze the reasoning paths generated by\nCoT and find two issues in multi-step reasoning: (i) Generating rationales\nirrelevant to the question, (ii) Unable to compose subquestions or queries for\ngenerating/retrieving all the relevant information. To address them, we propose\na graph-guided CoT prompting method, which guides the LLMs to reach the correct\nanswer with graph representation/verification steps. Specifically, we first\nleverage LLMs to construct a \"question/rationale graph\" by using knowledge\nextraction prompting given the initial question and the rationales generated in\nthe previous steps. Then, the graph verification step diagnoses the current\nrationale triplet by comparing it with the existing question/rationale graph to\nfilter out irrelevant rationales and generate follow-up questions to obtain\nrelevant information. Additionally, we generate CoT paths that exclude the\nextracted graph information to represent the context information missed from\nthe graph extraction. Our graph-guided reasoning method shows superior\nperformance compared to previous CoT prompting and the variants on multi-hop\nquestion answering benchmark datasets.\n","authors":["Jinyoung Park","Ameen Patel","Omar Zia Khan","Hyunwoo J. Kim","Joo-Kyung Kim"],"pdf_url":"https://arxiv.org/pdf/2311.09762v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09761v1","updated":"2023-11-16T10:35:11Z","published":"2023-11-16T10:35:11Z","title":"MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and\n  Classification","summary":"  Fallacies can be used to spread disinformation, fake news, and propaganda,\nunderlining the importance of their detection. Automated detection and\nclassification of fallacies, however, remain challenging, mainly because of the\ninnate subjectivity of the task and the need for a comprehensive, unified\napproach in existing research. Addressing these limitations, our study\nintroduces a novel taxonomy of fallacies that aligns and refines previous\nclassifications, a new annotation scheme tailored for subjective NLP tasks, and\na new evaluation method designed to handle subjectivity, adapted to precision,\nrecall, and F1-Score metrics. Using our annotation scheme, the paper introduces\nMAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset.\nMAFALDA is based on examples from various previously existing fallacy datasets\nunder our unified taxonomy across three levels of granularity. We then evaluate\nseveral language models under a zero-shot learning setting using MAFALDA to\nassess their fallacy detection and classification capability. Our comprehensive\nevaluation not only benchmarks the performance of these models but also\nprovides valuable insights into their strengths and limitations in addressing\nfallacious reasoning.\n","authors":["Chadi Helwe","Tom Calamai","Pierre-Henri Paris","Chloé Clavel","Fabian Suchanek"],"pdf_url":"https://arxiv.org/pdf/2311.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09758v1","updated":"2023-11-16T10:30:55Z","published":"2023-11-16T10:30:55Z","title":"OrchestraLLM: Efficient Orchestration of Language Models for Dialogue\n  State Tracking","summary":"  Large language models (LLMs) have revolutionized the landscape of Natural\nLanguage Processing systems, but are computationally expensive. To reduce the\ncost without sacrificing performance, previous studies have explored various\napproaches to harness the potential of Small Language Models (SLMs) as\ncost-effective alternatives to their larger counterparts. Driven by findings\nthat SLMs and LLMs exhibit complementary strengths in a structured knowledge\nextraction task, this work presents a novel SLM/LLM routing framework designed\nto improve computational efficiency and enhance task performance. First,\nexemplar pools are created to represent the types of contexts where each LM\nprovides a more reliable answer, leveraging a sentence embedding fine-tuned so\nthat context similarity is close to dialogue state similarity. Then, during\ninference, the k-nearest exemplars to the testing instance are retrieved, and\nthe instance is routed according to majority vote. In dialogue state tracking\ntasks, the proposed routing framework enhances performance substantially\ncompared to relying solely on LLMs, while reducing the computational costs by\nover 50%.\n","authors":["Chia-Hsuan Lee","Hao Cheng","Mari Ostendorf"],"pdf_url":"https://arxiv.org/pdf/2311.09758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09756v1","updated":"2023-11-16T10:30:26Z","published":"2023-11-16T10:30:26Z","title":"FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's\n  Storybook Narratives","summary":"  AI models (including LLM) often rely on narrative question-answering (QA)\ndatasets to provide customized QA functionalities to support downstream\nchildren education applications; however, existing datasets only include QA\npairs that are grounded within the given storybook content, but children can\nlearn more when teachers refer the storybook content to real-world knowledge\n(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is\nannotated by children education experts, to supplement 278 storybook narratives\nwith educationally appropriate commonsense knowledge. The dataset has 5,868 QA\npairs that not only originate from the storybook narrative but also contain the\ncommonsense knowledge grounded by an external knowledge graph (i.e.,\nConceptNet). A follow-up experiment shows that a smaller model (T5-large)\nfine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered\nLLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result\nsuggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)\nhuman experts' data annotation are still critical as they have much nuanced\nknowledge that LLMs do not know in the children educational domain.\n","authors":["Jiaju Chen","Yuxuan Lu","Shao Zhang","Bingsheng Yao","Yuanzhe Dong","Ying Xu","Yunyao Li","Qianwen Wang","Dakuo Wang","Yuling Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09755v1","updated":"2023-11-16T10:30:00Z","published":"2023-11-16T10:30:00Z","title":"How Does Calibration Data Affect the Post-training Pruning and\n  Quantization of Large Language Models?","summary":"  Pruning and quantization form the foundation of model compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nstate-of-the-art performance in a post-training setting. They rely upon\ncalibration data, a small set of unlabeled examples, to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of pruning and\nquantization methods, tasks, models, and datasets. Surprisingly, we find\nsubstantial variations in downstream task performance, contrasting existing\nwork that suggests a greater level of robustness to the calibration data.\nFinally, we make a series of recommendations for the effective use of\ncalibration data in LLM quantization and pruning.\n","authors":["Miles Williams","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2311.09755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09748v1","updated":"2023-11-16T10:25:22Z","published":"2023-11-16T10:25:22Z","title":"Translation Aligned Sentence Embeddings for Turkish Language","summary":"  Due to the limited availability of high quality datasets for training\nsentence embeddings in Turkish, we propose a training methodology and a regimen\nto develop a sentence embedding model. The central idea is simple but effective\n: is to fine-tune a pretrained encoder-decoder model in two consecutive stages,\nwhere the first stage involves aligning the embedding space with translation\npairs. Thanks to this alignment, the prowess of the main model can be better\nprojected onto the target language in a sentence embedding setting where it can\nbe fine-tuned with high accuracy in short duration with limited target language\ndataset.\n","authors":["Eren Unlu","Unver Ciftci"],"pdf_url":"https://arxiv.org/pdf/2311.09748v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.09743v1","updated":"2023-11-16T10:18:32Z","published":"2023-11-16T10:18:32Z","title":"Capturing Perspectives of Crowdsourced Annotators in Subjective Learning\n  Tasks","summary":"  In most classification models, it has been assumed to have a single ground\ntruth label for each data point. However, subjective tasks like toxicity\nclassification can lead to genuine disagreement among annotators. In these\ncases aggregating labels will result in biased labeling and, consequently,\nbiased models that can overlook minority opinions. Previous studies have shed\nlight on the pitfalls of label aggregation and have introduced a handful of\npractical approaches to tackle this issue. Recently proposed multi-annotator\nmodels, which predict labels individually per annotator, are vulnerable to\nunder-determination for annotators with small samples. This problem is\nespecially the case in crowd-sourced datasets. In this work, we propose\nAnnotator Aware Representations for Texts (AART) for subjective classification\ntasks. We will show the improvement of our method on metrics that assess the\nperformance on capturing annotators' perspectives. Additionally, our approach\ninvolves learning representations for annotators, allowing for an exploration\nof the captured annotation behaviors.\n","authors":["Negar Mokhberian","Myrl G. Marmarelis","Frederic R. Hopp","Valerio Basile","Fred Morstatter","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2311.09743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09741v1","updated":"2023-11-16T10:14:28Z","published":"2023-11-16T10:14:28Z","title":"What Constitutes a Faithful Summary? Preserving Author Perspectives in\n  News Summarization","summary":"  In this work, we take a first step towards designing summarization systems\nthat are faithful to the author's opinions and perspectives. Focusing on a case\nstudy of preserving political perspectives in news summarization, we find that\nexisting approaches alter the political opinions and stances of news articles\nin more than 50% of summaries, misrepresenting the intent and perspectives of\nthe news authors. We thus propose P^3Sum, a diffusion model-based summarization\napproach controlled by political perspective classifiers. In P^3Sum, the\npolitical leaning of a generated summary is iteratively evaluated at each\ndecoding step, and any drift from the article's original stance incurs a loss\nback-propagated to the embedding layers, steering the political stance of the\nsummary at inference time. Extensive experiments on three news summarization\ndatasets demonstrate that P^3Sum outperforms state-of-the-art summarization\nsystems and large language models by up to 11.4% in terms of the success rate\nof stance preservation, with on-par performance on standard summarization\nutility metrics. These findings highlight the lacunae that even for\nstate-of-the-art models it is still challenging to preserve author perspectives\nin news summarization, while P^3Sum presents an important first step towards\nevaluating and developing summarization systems that are faithful to author\nintent and perspectives.\n","authors":["Yuhan Liu","Shangbin Feng","Xiaochuang Han","Vidhisha Balachandran","Chan Young Park","Sachin Kumar","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2311.09741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09736v1","updated":"2023-11-16T10:06:19Z","published":"2023-11-16T10:06:19Z","title":"CARE: Extracting Experimental Findings From Clinical Literature","summary":"  Extracting fine-grained experimental findings from literature can provide\nmassive utility for scientific applications. Prior work has focused on\ndeveloping annotation schemas and datasets for limited aspects of this problem,\nleading to simpler information extraction datasets which do not capture the\nreal-world complexity and nuance required for this task. Focusing on\nbiomedicine, this work presents CARE (Clinical Aggregation-oriented Result\nExtraction) -- a new IE dataset for the task of extracting clinical findings.\nWe develop a new annotation schema capturing fine-grained findings as n-ary\nrelations between entities and attributes, which includes phenomena challenging\nfor current IE systems such as discontinuous entity spans, nested relations,\nand variable arity n-ary relations. Using this schema, we collect extensive\nannotations for 700 abstracts from two sources: clinical trials and case\nreports. We also benchmark the performance of various state-of-the-art IE\nsystems on our dataset, including extractive models and generative LLMs in\nfully supervised and limited data settings. Our results demonstrate the\ndifficulty of our dataset -- even SOTA models such as GPT4 struggle,\nparticularly on relation extraction. We release our annotation schema and CARE\nto encourage further research on extracting and aggregating scientific findings\nfrom literature.\n","authors":["Aakanksha Naik","Bailey Kuehl","Erin Bransom","Doug Downey","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2311.09736v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.09734v1","updated":"2023-11-16T10:05:26Z","published":"2023-11-16T10:05:26Z","title":"Tracking the Newsworthiness of Public Documents","summary":"  Journalists must find stories in huge amounts of textual data (e.g. leaks,\nbills, press releases) as part of their jobs: determining when and why text\nbecomes news can help us understand coverage patterns and help us build\nassistive tools. Yet, this is challenging because very few labelled links\nexist, language use between corpora is very different, and text may be covered\nfor a variety of reasons. In this work we focus on news coverage of local\npublic policy in the San Francisco Bay Area by the San Francisco Chronicle.\nFirst, we gather news articles, public policy documents and meeting recordings\nand link them using probabilistic relational modeling, which we show is a\nlow-annotation linking methodology that outperforms other retrieval-based\nbaselines. Second, we define a new task: newsworthiness prediction, to predict\nif a policy item will get covered. We show that different aspects of public\npolicy discussion yield different newsworthiness signals. Finally we perform\nhuman evaluation with expert journalists and show our systems identify policies\nthey consider newsworthy with 68% F1 and our coverage recommendations are\nhelpful with an 84% win-rate.\n","authors":["Alexander Spangher","Emilio Ferrara","Ben Welsh","Nanyun Peng","Serdar Tumgoren","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2311.09734v1.pdf","comment":"9 pages, 7 pages appendix"},{"id":"http://arxiv.org/abs/2311.09733v1","updated":"2023-11-16T10:04:49Z","published":"2023-11-16T10:04:49Z","title":"MOKA: Moral Knowledge Augmentation for Moral Event Extraction","summary":"  News media employ moral language to create memorable stories, and readers\noften engage with the content that align with their values. Moral theories have\nbeen applied to news analysis studying moral values in isolation, while the\nintricate dynamics among participating entities in shaping moral events have\nbeen overlooked. This is mainly due to the use of obscure language to conceal\nevident ideology and values, coupled with the insufficient moral reasoning\ncapability in most existing NLP systems, where LLMs are no exception. To study\nthis phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of\n5,494 structured annotations on 474 news articles by diverse US media across\nthe political spectrum. We further propose MOKA, a moral event extraction\nframework with MOral Knowledge Augmentation, that leverages knowledge derived\nfrom moral words and moral scenarios. Experimental results show that MOKA\noutperforms competitive baselines across three moral event understanding tasks.\nFurther analyses illuminate the selective reporting of moral events by media\noutlets of different ideological leanings, suggesting the significance of\nevent-level morality analysis in news. Our datasets and codebase are available\nat https://github.com/launchnlp/MOKA.\n","authors":["Xinliang Frederick Zhang","Winston Wu","Nick Beauchamp","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09732v1","updated":"2023-11-16T10:03:26Z","published":"2023-11-16T10:03:26Z","title":"Source Prompt: Coordinated Pre-training of Language Models on Diverse\n  Corpora from Multiple Sources","summary":"  Pre-trained language models (PLMs) have established the new paradigm in the\nfield of NLP. For more powerful PLMs, one of the most popular and successful\nway is to continuously scale up sizes of the models and the pre-training\ncorpora. These large corpora are generally obtained by converging smaller ones\nfrom multiple sources, they are thus growing increasingly diverse. However, the\nside-effects of these colossal converged corpora remain understudied. In this\npaper, we identify the disadvantage of heterogeneous corpora from multiple\nsources for pre-training PLMs. Towards coordinated pre-training on diverse\ncorpora, we further propose source prompts (SP), which explicitly prompt the\nmodel of the data source at the pre-training and fine-tuning stages. Results of\nextensive experiments demonstrate that PLMs pre-trained with SP on diverse\ncorpora gain significant improvement in various downstream tasks.\n","authors":["Yipei Xu","Dakuan Lu","Jiaqing Liang","Xintao Wang","Yipeng Geng","Yingsi Xin","Hengkui Wu","Ken Chen","ruiji zhang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2311.09732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09731v1","updated":"2023-11-16T10:02:40Z","published":"2023-11-16T10:02:40Z","title":"Prudent Silence or Foolish Babble? Examining Large Language Models'\n  Responses to the Unknown","summary":"  Large Language Models (LLMs) often struggle when faced with situations where\nthey lack the prerequisite knowledge to generate a sensical response. In these\ncases, models tend to fabricate and hallucinate, rather than appropriately\nsignaling uncertainty as humans would. This behavior misaligns with human\nconversational norms and presents challenges surrounding responsible and\nethical AI development. This work aims to systematically investigate LLMs'\nbehaviors in such situations. We curate an adversarial question-answering\nbenchmark containing unanswerable questions targeting information absent from\nthe LLM's training data. Concretely, these unanswerable questions contain\nnon-existent concepts or false premises. When presented with such unanswerable\nquestions, an LLM should appropriately convey uncertainty, and be able to\nchallenge the premise and refuse to generate a response. While facing\nanswerable valid questions, a model should demonstrate a positive correlation\nbetween accuracy and confidence. Using a model-agnostic unified confidence\nelicitation approach, we observe that LLMs that have gone through instruction\nfinetuning and reinforcement learning from human feedback (RLHF) perform\nsignificantly better than their counterparts that do not. Moreover, uncertainty\nexpression 1 through our elicitation method does not always stay consistent\nwith the perceived confidence of the direct response of an LLM. Our findings\ncall for further research into teaching LLMs to proactively and reliably\nexpress uncertainty.\n","authors":["Genglin Liu","Xingyao Wang","Lifan Yuan","Yangyi Chen","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2311.09731v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2311.09730v1","updated":"2023-11-16T10:02:24Z","published":"2023-11-16T10:02:24Z","title":"Aligning with Whom? Large Language Models Have Gender and Racial Biases\n  in Subjective NLP Tasks","summary":"  Human perception of language depends on personal backgrounds like gender and\nethnicity. While existing studies have shown that large language models (LLMs)\nhold values that are closer to certain societal groups, it is unclear whether\ntheir prediction behaviors on subjective NLP tasks also exhibit a similar bias.\nIn this study, leveraging the POPQUORN dataset which contains annotations of\ndiverse demographic backgrounds, we conduct a series of experiments on four\npopular LLMs to investigate their capability to understand group differences\nand potential biases in their predictions for politeness and offensiveness. We\nfind that for both tasks, model predictions are closer to the labels from White\nand female participants. We further explore prompting with the target\ndemographic labels and show that including the target demographic in the prompt\nactually worsens the model's performance. More specifically, when being\nprompted to respond from the perspective of \"Black\" and \"Asian\" individuals,\nmodels show lower performance in predicting both overall scores as well as the\nscores from corresponding groups. Our results suggest that LLMs hold gender and\nracial biases for subjective NLP tasks and that demographic-infused prompts\nalone may be insufficient to mitigate such effects. Code and data are available\nat https://github.com/Jiaxin-Pei/LLM-Group-Bias.\n","authors":["Huaman Sun","Jiaxin Pei","Minje Choi","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.09730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09724v1","updated":"2023-11-16T09:56:28Z","published":"2023-11-16T09:56:28Z","title":"Outcome-supervised Verifiers for Planning in Mathematical Reasoning","summary":"  Large language models (LLMs) often struggle with maintaining accuracy across\na sequence of intermediate reasoning steps in mathematical reasoning, leading\nto error propagation that undermines the final result. The current methodology\nto mitigate this issue primarily involves using a verifier model to assess the\ncorrectness of generated solution candidates, focusing either on the overall\nreasoning path or on an incomplete reasoning path. By rethinking this approach,\nwe argue that assessing potentials of incomplete reasoning paths could be more\nadvantageous as it guides towards correct final answers, transforming the task\ninto a \\textit{planning} problem. Our proposed verifier, the\nOutcome-supervision Value Model (OVM), employs outcome supervision for\ntraining, offering an efficient and intuitive method for \\textit{planning} by\nprioritizing steps that lead to accurate conclusions over mere per-step\ncorrectness. Furthermore, the OVM eschews the need for labor-intensive\nannotations on step-level correctness, enhancing its scalability. Our\nexperiments on two multi-step mathematical reasoning datasets, GSM8K and Game\nof 24, demonstrate the superior performance of the OVM model. Notably, in\nGSM8K, our \\textbf{OVM-7B model achieves state-of-the-art results among LLMs up\nto 13B parameters}; especially it does not utilize GPT-4 or code execution.\nThese findings offer a novel perspective on the role of outcome supervision in\ntraining verifiers for multi-step reasoning tasks and provide theoretical\njustification for its advantage in value estimation for planning.\n","authors":["Fei Yu","Anningzhe Gao","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09724v1.pdf","comment":"https://github.com/FreedomIntelligence/OVM"},{"id":"http://arxiv.org/abs/2311.09721v1","updated":"2023-11-16T09:55:07Z","published":"2023-11-16T09:55:07Z","title":"On Evaluating the Integration of Reasoning and Action in LLM Agents with\n  Database Question Answering","summary":"  This study introduces a new long-form database question answering dataset\ndesigned to evaluate how Large Language Models (LLMs) interact with a SQL\ninterpreter. The task necessitates LLMs to strategically generate multiple SQL\nqueries to retrieve sufficient data from a database, to reason with the\nacquired context, and to synthesize them into a comprehensive analytical\nnarrative. Our findings highlight that this task poses great challenges even\nfor the state-of-the-art GPT-4 model. We propose and evaluate two interaction\nstrategies, and provide a fine-grained analysis of the individual stages within\nthe interaction. A key discovery is the identification of two primary\nbottlenecks hindering effective interaction: the capacity for planning and the\nability to generate multiple SQL queries. To address the challenge of\naccurately assessing answer quality, we introduce a multi-agent evaluation\nframework that simulates the academic peer-review process, enhancing the\nprecision and reliability of our evaluations. This framework allows for a more\nnuanced understanding of the strengths and limitations of current LLMs in\ncomplex retrieval and reasoning tasks.\n","authors":["Linyong Nan","Ellen Zhang","Weijin Zou","Yilun Zhao","Wenfei Zhou","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09718v1","updated":"2023-11-16T09:50:53Z","published":"2023-11-16T09:50:53Z","title":"You don't need a personality test to know these models are unreliable:\n  Assessing the Reliability of Large Language Models on Psychometric\n  Instruments","summary":"  The versatility of Large Language Models (LLMs) on natural language\nunderstanding tasks has made them popular for research in social sciences. In\nparticular, to properly understand the properties and innate personas of LLMs,\nresearchers have performed studies that involve using prompts in the form of\nquestions that ask LLMs of particular opinions. In this study, we take a\ncautionary step back and examine whether the current format of prompting\nenables LLMs to provide responses in a consistent and robust manner. We first\nconstruct a dataset that contains 693 questions encompassing 39 different\ninstruments of persona measurement on 115 persona axes. Additionally, we design\na set of prompts containing minor variations and examine LLM's capabilities to\ngenerate accurate answers, as well as consistency variations to examine their\nconsistency towards simple perturbations such as switching the option order.\nOur experiments on 15 different open-source LLMs reveal that even simple\nperturbations are sufficient to significantly downgrade a model's\nquestion-answering ability, and that most LLMs have low negation consistency.\nOur results suggest that the currently widespread practice of prompting is\ninsufficient to accurately capture model perceptions, and we discuss potential\nalternatives to improve such issues.\n","authors":["Bangzhao Shu","Lechen Zhang","Minje Choi","Lavinia Dunagan","Dallas Card","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.09718v1.pdf","comment":"15 pages, 5 figures, 5 tables. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2311.09712v1","updated":"2023-11-16T09:42:36Z","published":"2023-11-16T09:42:36Z","title":"Regularized Conventions: Equilibrium Computation as a Model of Pragmatic\n  Reasoning","summary":"  We present a model of pragmatic language understanding, where utterances are\nproduced and understood by searching for regularized equilibria of signaling\ngames. In this model (which we call ReCo, for Regularized Conventions),\nspeakers and listeners search for contextually appropriate utterance--meaning\nmappings that are both close to game-theoretically optimal conventions and\nclose to a shared, ''default'' semantics. By characterizing pragmatic\ncommunication as equilibrium search, we obtain principled sampling algorithms\nand formal guarantees about the trade-off between communicative success and\nnaturalness. Across several datasets capturing real and idealized human\njudgments about pragmatic implicatures, ReCo matches or improves upon\npredictions made by best response and rational speech act models of language\nunderstanding.\n","authors":["Athul Paul Jacob","Gabriele Farina","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2311.09712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08398v2","updated":"2023-11-16T09:41:28Z","published":"2023-11-14T18:57:15Z","title":"Are Large Language Models Temporally Grounded?","summary":"  Are Large language models (LLMs) temporally grounded? Since LLMs cannot\nperceive and interact with the environment, it is impossible to answer this\nquestion directly. Instead, we provide LLMs with textual narratives and probe\nthem with respect to their common-sense knowledge of the structure and duration\nof events, their ability to order events along a timeline, and self-consistency\nwithin their temporal model (e.g., temporal relations such as after and before\nare mutually exclusive for any pair of events). We evaluate state-of-the-art\nLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\nGenerally, we find that LLMs lag significantly behind both human performance as\nwell as small-scale, specialised LMs. In-context learning, instruction tuning,\nand chain-of-thought prompting reduce this gap only to a limited degree.\nCrucially, LLMs struggle the most with self-consistency, displaying incoherent\nbehaviour in at least 27.23% of their predictions. Contrary to expectations, we\nalso find that scaling the model size does not guarantee positive gains in\nperformance. To explain these results, we study the sources from which LLMs may\ngather temporal information: we find that sentence ordering in unlabelled\ntexts, available during pre-training, is only weakly correlated with event\nordering. Moreover, public instruction tuning mixtures contain few temporal\ntasks. Hence, we conclude that current LLMs lack a consistent temporal model of\ntextual narratives. Code, datasets, and LLM outputs are available at\nhttps://github.com/yfqiu-nlp/temporal-llms.\n","authors":["Yifu Qiu","Zheng Zhao","Yftah Ziser","Anna Korhonen","Edoardo M. Ponti","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2311.08398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08329v3","updated":"2023-11-16T09:38:39Z","published":"2023-11-14T17:18:08Z","title":"KTRL+F: Knowledge-Augmented In-Document Search","summary":"  We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. This task addresses following unique challenges for in-document search:\n1) utilizing knowledge outside the document for extended use of additional\ninformation about targets to bridge the semantic gap between the query and the\ntargets, and 2) balancing between real-time applicability with the performance.\nWe analyze various baselines in KTRL+F and find there are limitations of\nexisting models, such as hallucinations, low latency, or difficulties in\nleveraging external knowledge. Therefore we propose a Knowledge-Augmented\nPhrase Retrieval model that shows a promising balance between speed and\nperformance by simply augmenting external knowledge embedding in phrase\nembedding. Additionally, we conduct a user study to verify whether solving\nKTRL+F can enhance search experience of users. It demonstrates that even with\nour simple model users can reduce the time for searching with less queries and\nreduced extra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.\n","authors":["Hanseok Oh","Haebin Shin","Miyoung Ko","Hyunji Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.08329v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09709v1","updated":"2023-11-16T09:35:50Z","published":"2023-11-16T09:35:50Z","title":"Large Language Model Inference with Lexical Shortlisting","summary":"  Large language model (LLM) inference is computation and memory intensive, so\nwe adapt lexical shortlisting to it hoping to improve both. While lexical\nshortlisting is well-explored in tasks like machine translation, it requires\nmodifications before being suitable for LLMs as the intended applications vary\nsignificantly. Our work studies two heuristics to shortlist sub-vocabulary at\nLLM inference time: Unicode-based script filtering and corpus-based selection.\nWe explore different LLM families and sizes, and we find that lexical\nshortlisting can reduce the memory usage of some models by nearly 50\\% and has\nan upper bound of 25\\% improvement in generation speed. In this pilot study, we\nalso identify the drawbacks of such vocabulary selection methods and propose\navenues for future research.\n","authors":["Nikolay Bogoychev","Pinzhen Chen","Barry Haddow","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2311.09709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09708v1","updated":"2023-11-16T09:35:24Z","published":"2023-11-16T09:35:24Z","title":"A Self-enhancement Multitask Framework for Unsupervised Aspect Category\n  Detection","summary":"  Our work addresses the problem of unsupervised Aspect Category Detection\nusing a small set of seed words. Recent works have focused on learning\nembedding spaces for seed words and sentences to establish similarities between\nsentences and aspects. However, aspect representations are limited by the\nquality of initial seed words, and model performances are compromised by noise.\nTo mitigate this limitation, we propose a simple framework that automatically\nenhances the quality of initial seed words and selects high-quality sentences\nfor training instead of using the entire dataset. Our main concepts are to add\na number of seed words to the initial set and to treat the task of noise\nresolution as a task of augmenting data for a low-resource task. In addition,\nwe jointly train Aspect Category Detection with Aspect Term Extraction and\nAspect Term Polarity to further enhance performance. This approach facilitates\nshared representation learning, allowing Aspect Category Detection to benefit\nfrom the additional guidance offered by other tasks. Extensive experiments\ndemonstrate that our framework surpasses strong baselines on standard datasets.\n","authors":["Thi-Nhung Nguyen","Hoang Ngo","Kiem-Hieu Nguyen","Tuan-Dung Cao"],"pdf_url":"https://arxiv.org/pdf/2311.09708v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09707v1","updated":"2023-11-16T09:35:00Z","published":"2023-11-16T09:35:00Z","title":"GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization\n  in Programming Language Understanding","summary":"  Language models can serve as a valuable tool for software developers to\nincrease productivity. Large generative models can be used for code generation\nand code completion, while smaller encoder-only models are capable of\nperforming code search tasks using natural language queries.These capabilities\nare heavily influenced by the quality and diversity of the available training\ndata. Source code datasets used for training usually focus on the most popular\nlanguages and testing is mostly conducted on the same distributions, often\noverlooking low-resource programming languages. Motivated by the NLP\ngeneralization taxonomy proposed by Hupkes et.\\,al., we propose a new benchmark\ndataset called GenCodeSearchNet (GeCS) which builds upon existing natural\nlanguage code search datasets to systemically evaluate the programming language\nunderstanding generalization capabilities of language models. As part of the\nfull dataset, we introduce a new, manually curated subset StatCodeSearch that\nfocuses on R, a popular but so far underrepresented programming language that\nis often used by researchers outside the field of computer science. For\nevaluation and comparison, we collect several baseline results using fine-tuned\nBERT-style models and GPT-style large language models in a zero-shot setting.\n","authors":["Andor Diera","Abdelhalim Dahou","Lukas Galke","Fabian Karl","Florian Sihler","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2311.09707v1.pdf","comment":"accepted at GenBench workshop, EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09702v1","updated":"2023-11-16T09:27:36Z","published":"2023-11-16T09:27:36Z","title":"Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go\n  without Hallucination?","summary":"  Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.\n","authors":["Bangzheng Li","Ben Zhou","Fei Wang","Xingyu Fu","Dan Roth","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.09702v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09696v1","updated":"2023-11-16T09:12:20Z","published":"2023-11-16T09:12:20Z","title":"Fumbling in Babel: An Investigation into ChatGPT's Language\n  Identification Ability","summary":"  Recently, ChatGPT has emerged as a powerful NLP tool that can carry out\nseveral tasks. However, the range of languages ChatGPT can handle remains\nlargely a mystery. In this work, we investigate ChatGPT's language\nidentification abilities. For this purpose, we compile Babel-670, a benchmark\ncomprising $670$ languages representing $23$ language families. Languages in\nBabel-670 run the gamut between the very high-resource to the very low-resource\nand are spoken in five continents. We then study ChatGPT's (both GPT-3.5 and\nGPT-4) ability to (i) identify both language names and language codes (ii)\nunder both zero- and few-shot conditions (iii) with and without provision of\nlabel set. When compared to smaller finetuned language identification tools, we\nfind that ChatGPT lags behind. Our empirical analysis shows the reality that\nChatGPT still resides in a state of potential enhancement before it can\nsufficiently serve diverse communities.\n","authors":["Wei-Rui Chen","Ife Adebara","Khai Duy Doan","Qisheng Liao","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2311.09696v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09694v1","updated":"2023-11-16T09:09:32Z","published":"2023-11-16T09:09:32Z","title":"Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness","summary":"  Are the longstanding robustness issues in NLP resolved by today's larger and\nmore performant models? To address this question, we conduct a thorough\ninvestigation using 19 models of different sizes spanning different\narchitectural choices and pretraining objectives. We conduct evaluations using\n(a) OOD and challenge test sets, (b) CheckLists, (c) contrast sets, and (d)\nadversarial inputs. Our analysis reveals that not all OOD tests provide further\ninsight into robustness. Evaluating with CheckLists and contrast sets shows\nsignificant gaps in model performance; merely scaling models does not make them\nsufficiently robust. Finally, we point out that current approaches for\nadversarial evaluations of models are themselves problematic: they can be\neasily thwarted, and in their current forms, do not represent a sufficiently\ndeep probe of model robustness. We conclude that not only is the question of\nrobustness in NLP as yet unresolved, but even some of the approaches to measure\nrobustness need to be reassessed.\n","authors":["Ashim Gupta","Rishanth Rajendhran","Nathan Stringham","Vivek Srikumar","Ana Marasović"],"pdf_url":"https://arxiv.org/pdf/2311.09694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09693v1","updated":"2023-11-16T09:09:22Z","published":"2023-11-16T09:09:22Z","title":"BLT: Can Large Language Models Handle Basic Legal Text?","summary":"  We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently\nperform poorly at basic text handling required of lawyers or paralegals, such\nas looking up the text at a line of a witness deposition or at a subsection of\na contract. We introduce a benchmark to quantify this poor performance, which\ncasts into doubt LLMs' current reliability as-is for legal practice. Finetuning\nfor these tasks brings an older LLM to near-perfect performance on our test set\nand also raises performance on a related legal task. This stark result\nhighlights the need for more domain expertise in LLM training.\n","authors":["Andrew Blair-Stanek","Nils Holzenberger","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2311.09693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09687v1","updated":"2023-11-16T08:57:53Z","published":"2023-11-16T08:57:53Z","title":"Inducing Political Bias Allows Language Models Anticipate Partisan\n  Reactions to Controversies","summary":"  Social media platforms are rife with politically charged discussions.\nTherefore, accurately deciphering and predicting partisan biases using Large\nLanguage Models (LLMs) is increasingly critical. In this study, we address the\nchallenge of understanding political bias in digitized discourse using LLMs.\nWhile traditional approaches often rely on finetuning separate models for each\npolitical faction, our work innovates by employing a singular,\ninstruction-tuned LLM to reflect a spectrum of political ideologies. We present\na comprehensive analytical framework, consisting of Partisan Bias Divergence\nAssessment and Partisan Class Tendency Prediction, to evaluate the model's\nalignment with real-world political ideologies in terms of stances, emotions,\nand moral foundations. Our findings reveal the model's effectiveness in\ncapturing emotional and moral nuances, albeit with some challenges in stance\ndetection, highlighting the intricacies and potential for refinement in NLP\ntools for politically sensitive contexts. This research contributes\nsignificantly to the field by demonstrating the feasibility and importance of\nnuanced political understanding in LLMs, particularly for applications\nrequiring acute awareness of political bias.\n","authors":["Zihao He","Siyi Guo","Ashwin Rao","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2311.09687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09684v1","updated":"2023-11-16T08:54:52Z","published":"2023-11-16T08:54:52Z","title":"Do Physicians Know How to Prompt? The Need for Automatic Prompt\n  Optimization Help in Clinical Note Generation","summary":"  This study examines the effect of prompt engineering on the performance of\nLarge Language Models (LLMs) in clinical note generation. We introduce an\nAutomatic Prompt Optimization (APO) framework to refine initial prompts and\ncompare the outputs of medical experts, non-medical experts, and APO-enhanced\nGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in\nstandardizing prompt quality across clinical note sections. A human-in-the-loop\napproach shows that experts maintain content quality post-APO, with a\npreference for their own modifications, suggesting the value of expert\ncustomization. We recommend a two-phase optimization process, leveraging\nAPO-GPT4 for consistency and expert input for personalization.\n","authors":["Zonghai Yao","Ahmed Jaafar","Beining Wang","Yue Zhu","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.09684v1.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2311.09682v1","updated":"2023-11-16T08:52:27Z","published":"2023-11-16T08:52:27Z","title":"MacGyver: Are Large Language Models Creative Problem Solvers?","summary":"  We explore the creative problem-solving capabilities of modern large language\nmodels (LLMs) in a constrained setting. The setting requires circumventing a\ncognitive bias known in psychology as ''functional fixedness'' to use familiar\nobjects in innovative or unconventional ways. To this end, we create MacGyver,\nan automatically generated dataset consisting of 1,600 real-world problems that\ndeliberately trigger functional fixedness and require thinking\n'out-of-the-box'. We then present our collection of problems to both LLMs and\nhumans to compare and contrast their problem-solving abilities. We show that\nMacGyver is challenging for both groups, but in unique and complementary ways.\nFor example, humans typically excel in solving problems that they are familiar\nwith but may struggle with tasks requiring domain-specific knowledge, leading\nto a higher variance. On the other hand, LLMs, being exposed to a variety of\nhighly specialized knowledge, attempt broader problems but are prone to\noverconfidence and propose actions that are physically infeasible or\ninefficient. We also provide a detailed error analysis of LLMs, and demonstrate\nthe potential of enhancing their problem-solving ability with novel prompting\ntechniques such as iterative step-wise reflection and divergent-convergent\nthinking. This work provides insight into the creative problem-solving\ncapabilities of humans and AI and illustrates how psychological paradigms can\nbe extended into large-scale tasks for comparing humans and machines.\n","authors":["Yufei Tian","Abhilasha Ravichander","Lianhui Qin","Ronan Le Bras","Raja Marjieh","Nanyun Peng","Yejin Choi","Thomas L. Griffiths","Faeze Brahman"],"pdf_url":"https://arxiv.org/pdf/2311.09682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09677v1","updated":"2023-11-16T08:45:44Z","published":"2023-11-16T08:45:44Z","title":"R-Tuning: Teaching Large Language Models to Refuse Unknown Questions","summary":"  Large language models (LLMs) have revolutionized numerous domains with their\nimpressive performance but still face their challenges. A predominant issue is\nthe propensity for these models to generate non-existent facts, a concern\ntermed hallucination. Our research is motivated by the observation that\nprevious instruction tuning methods force the model to complete a sentence no\nmatter whether the model knows the knowledge or not. When the question is out\nof the parametric knowledge, it will try to make up something and fail to\nindicate when it lacks knowledge. In this paper, we present a new approach\ncalled Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized\nby first identifying the knowledge gap between parametric knowledge and the\ninstruction tuning data. Then, we construct the refusal-aware data based on the\nknowledge intersection, to tune LLMs to refrain from responding to questions\nbeyond its parametric knowledge. Experimental results demonstrate this new\ninstruction tuning approach effectively improves a model's ability to answer\nknown questions and refrain from answering unknown questions. Furthermore, when\ntested on out-of-domain datasets, the refusal ability was found to be a\nmeta-skill that could be generalized to other tasks. Further analysis\nsurprisingly finds that learning the uncertainty during training displays a\nbetter ability to estimate uncertainty than uncertainty-based testing. Our code\nwill be released at https://github.com/shizhediao/R-Tuning.\n","authors":["Hanning Zhang","Shizhe Diao","Yong Lin","Yi R. Fung","Qing Lian","Xingyao Wang","Yangyi Chen","Heng Ji","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09677v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.09675v1","updated":"2023-11-16T08:42:26Z","published":"2023-11-16T08:42:26Z","title":"Where Do People Tell Stories Online? Story Detection Across Online\n  Communities","summary":"  People share stories online for a myriad of purposes, whether as a means of\nself-disclosure, processing difficult personal experiences, providing needed\ninformation or entertainment, or persuading others to share their beliefs.\nBetter understanding of online storytelling can illuminate the dynamics of\nsocial movements, sensemaking practices, persuasion strategies, and more.\nHowever, unlike other media such as books and visual content where the\nnarrative nature of the content is often overtly signaled at the document\nlevel, studying storytelling in online communities is challenging due to the\nmixture of storytelling and non-storytelling behavior, which can be\ninterspersed within documents and across diverse topics and settings. We\nintroduce a codebook and create the Storytelling in Online Communities Corpus,\nan expert-annotated dataset of 502 English-language posts and comments with\nlabeled story and event spans. Using our corpus, we train and evaluate an\nonline story detection model, which we use to investigate the role storytelling\nof in different social contexts. We identify distinctive features of online\nstorytelling, the prevalence of storytelling among different communities, and\nthe conversational patterns of storytelling.\n","authors":["Maria Antoniak","Joel Mire","Maarten Sap","Elliott Ash","Andrew Piper"],"pdf_url":"https://arxiv.org/pdf/2311.09675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09668v1","updated":"2023-11-16T08:36:00Z","published":"2023-11-16T08:36:00Z","title":"Improving the Generation Quality of Watermarked Large Language Models\n  via Word Importance Scoring","summary":"  The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.\n","authors":["Yuhang Li","Yihan Wang","Zhouxing Shi","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2311.09668v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.07468v2","updated":"2023-11-16T08:35:05Z","published":"2023-11-13T17:01:12Z","title":"Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation\n  of the Reversal Curse","summary":"  Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B as the answer. However, it may\nencounter confusion when presented with questions concerning B. We contend that\nthe reversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.\n","authors":["Ang Lv","Kaiyi Zhang","Shufang Xie","Quan Tu","Yuhan Chen","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2311.07468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09665v1","updated":"2023-11-16T08:30:15Z","published":"2023-11-16T08:30:15Z","title":"Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case\n  Study on Wisdom of Partisan Crowds","summary":"  This study investigates the potential of Large Language Models (LLMs) to\nsimulate human group dynamics, particularly within politically charged\ncontexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to\nrole-play as Democrat and Republican personas, engaging in a structured\ninteraction akin to human group study. Our approach evaluates how agents'\nresponses evolve through social influence. Our key findings indicate that LLM\nagents role-playing detailed personas and without Chain-of-Thought (CoT)\nreasoning closely align with human behaviors, while having CoT reasoning hurts\nthe alignment. However, incorporating explicit biases into agent prompts does\nnot necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning\nLLMs with human data shows promise in achieving human-like behavior but poses a\nrisk of overfitting certain behaviors. These findings show the potential and\nlimitations of using LLM agents in modeling human group phenomena.\n","authors":["Yun-Shiuan Chuang","Siddharth Suresh","Nikunj Harlalka","Agam Goyal","Robert Hawkins","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2311.09665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09661v1","updated":"2023-11-16T08:28:00Z","published":"2023-11-16T08:28:00Z","title":"Evolving Domain Adaptation of Pretrained Language Models for Text\n  Classification","summary":"  Adapting pre-trained language models (PLMs) for time-series text\nclassification amidst evolving domain shifts (EDS) is critical for maintaining\naccuracy in applications like stance detection. This study benchmarks the\neffectiveness of evolving domain adaptation (EDA) strategies, notably\nself-training, domain-adversarial training, and domain-adaptive pretraining,\nwith a focus on an incremental self-training method. Our analysis across\nvarious datasets reveals that this incremental method excels at adapting PLMs\nto EDS, outperforming traditional domain adaptation techniques. These findings\nhighlight the importance of continually updating PLMs to ensure their\neffectiveness in real-world applications, paving the way for future research\ninto PLM robustness against the natural temporal evolution of language.\n","authors":["Yun-Shiuan Chuang","Yi Wu","Dhruv Gupta","Rheeya Uppaal","Ananya Kumar","Luhang Sun","Makesh Narsimhan Sreedhar","Sijia Yang","Timothy T. Rogers","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2311.09661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18787v2","updated":"2023-11-16T08:26:59Z","published":"2023-05-30T06:47:07Z","title":"Universality and Limitations of Prompt Tuning","summary":"  Despite the demonstrated empirical efficacy of prompt tuning to adapt a\npretrained language model for a new task, the theoretical underpinnings of the\ndifference between \"tuning parameters before the input\" against \"the tuning of\nmodel weights\" are limited. We thus take one of the first steps to understand\nthe role of soft-prompt tuning for transformer-based architectures. By\nconsidering a general purpose architecture, we analyze prompt tuning from the\nlens of both: universal approximation and limitations with finite-depth\nfixed-weight pretrained transformers for continuous-valued functions. Our\nuniversality result guarantees the existence of a strong transformer with a\nprompt to approximate any sequence-to-sequence function in the set of Lipschitz\nfunctions. The limitations of prompt tuning for limited-depth transformers are\nfirst proved by constructing a set of datasets, that cannot be memorized by a\nprompt of any length for a given single encoder layer. We also provide a lower\nbound on the required number of tunable prompt parameters and compare the\nresult with the number of parameters required for a low-rank update (based on\nLoRA) for a single-layer setting. We finally extend our analysis to multi-layer\nsettings by providing sufficient conditions under which the transformer can at\nbest learn datasets from invertible functions only. Our theoretical claims are\nalso corroborated by empirical results.\n","authors":["Yihan Wang","Jatin Chauhan","Wei Wang","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2305.18787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09656v1","updated":"2023-11-16T08:20:36Z","published":"2023-11-16T08:20:36Z","title":"Structured Chemistry Reasoning with Large Language Models","summary":"  This paper studies the problem of solving complex chemistry problems with\nlarge language models (LLMs). Despite the extensive general knowledge in LLMs\n(such as GPT-4), they struggle with chemistry reasoning that requires faithful\ngrounded reasoning with diverse chemical knowledge and an integrative\nunderstanding of chemical interactions. We propose InstructChem, a new\nstructured reasoning approach that substantially boosts the LLMs' chemical\nreasoning capabilities. InstructChem explicitly decomposes the reasoning into\nthree critical phrases, including chemical formulae generation by LLMs that\noffers the basis for subsequent grounded reasoning, step-by-step reasoning that\nmakes multi-step derivations with the identified formulae for a preliminary\nanswer, and iterative review-and-refinement that steers LLMs to progressively\nrevise the previous phases for increasing confidence, leading to the final\nhigh-confidence answer. We conduct extensive experiments on four different\nchemistry challenges, including quantum chemistry, quantum mechanics, physical\nchemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on\nchemistry reasoning, yielding an 8% average absolute improvement and a 30% peak\nimprovement. We further use the generated reasoning by GPT-4 to fine-tune\nsmaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs.\nThis validates our approach and enables LLMs to generate high-quality\nreasoning.\n","authors":["Siru Ouyang","Zhuosheng Zhang","Bing Yan","Xuan Liu","Jiawei Han","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2311.09656v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09174v2","updated":"2023-11-16T08:20:19Z","published":"2023-11-15T18:11:23Z","title":"AbsPyramid: Benchmarking the Abstraction Ability of Language Models with\n  a Unified Entailment Graph","summary":"  Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.\n","authors":["Zhaowei Wang","Haochen Shi","Weiqi Wang","Tianqing Fang","Hongming Zhang","Sehyun Choi","Xin Liu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2311.09174v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09649v1","updated":"2023-11-16T08:01:17Z","published":"2023-11-16T08:01:17Z","title":"ICXML: An In-Context Learning Framework for Zero-Shot Extreme\n  Multi-Label Classification","summary":"  This paper focuses on the task of Extreme Multi-Label Classification (XMC)\nwhose goal is to predict multiple labels for each instance from an extremely\nlarge label space. While existing research has primarily focused on fully\nsupervised XMC, real-world scenarios often lack complete supervision signals,\nhighlighting the importance of zero-shot settings. Given the large label space,\nutilizing in-context learning approaches is not trivial. We address this issue\nby introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage\nframework that cuts down the search space by generating a set of candidate\nlabels through incontext learning and then reranks them. Extensive experiments\nsuggest that ICXML advances the state of the art on two diverse public\nbenchmarks.\n","authors":["Yaxin Zhu","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2311.09649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09648v1","updated":"2023-11-16T07:59:12Z","published":"2023-11-16T07:59:12Z","title":"Event Causality Is Key to Computational Story Understanding","summary":"  Psychological research suggests the central role of event causality in human\nstory understanding. Further, event causality has been heavily utilized in\nsymbolic story generation. However, few machine learning systems for story\nunderstanding employ event causality, partially due to the lack of reliable\nmethods for identifying open-world causal event relations. Leveraging recent\nprogress in large language models (LLMs), we present the first method for event\ncausality identification that leads to material improvements in computational\nstory understanding. We design specific prompts for extracting event causal\nrelations from GPT. Against human-annotated event causal relations in the\nGLUCOSE dataset, our technique performs on par with supervised models, while\nbeing easily generalizable to stories of different types and lengths. The\nextracted causal relations lead to 5.7\\% improvements on story quality\nevaluation and 8.7\\% on story video-text alignment. Our findings indicate\nenormous untapped potential for event causality in computational story\nunderstanding.\n","authors":["Yidan Sun","Qin Chao","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2311.09648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15518v2","updated":"2023-11-16T07:59:08Z","published":"2023-06-27T14:46:47Z","title":"Paradigm Shift in Sustainability Disclosure Analysis: Empowering\n  Stakeholders with CHATREPORT, a Language Model-Based Tool","summary":"  This paper introduces a novel approach to enhance Large Language Models\n(LLMs) with expert knowledge to automate the analysis of corporate\nsustainability reports by benchmarking them against the Task Force for\nClimate-Related Financial Disclosures (TCFD) recommendations. Corporate\nsustainability reports are crucial in assessing organizations' environmental\nand social risks and impacts. However, analyzing these reports' vast amounts of\ninformation makes human analysis often too costly. As a result, only a few\nentities worldwide have the resources to analyze these reports, which could\nlead to a lack of transparency. While AI-powered tools can automatically\nanalyze the data, they are prone to inaccuracies as they lack domain-specific\nexpertise. This paper introduces a novel approach to enhance LLMs with expert\nknowledge to automate the analysis of corporate sustainability reports. We\nchristen our tool CHATREPORT, and apply it in a first use case to assess\ncorporate climate risk disclosures following the TCFD recommendations.\nCHATREPORT results from collaborating with experts in climate science, finance,\neconomic policy, and computer science, demonstrating how domain experts can be\ninvolved in developing AI tools. We make our prompt templates, generated data,\nand scores available to the public to encourage transparency.\n","authors":["Jingwei Ni","Julia Bingler","Chiara Colesanti-Senni","Mathias Kraus","Glen Gostlow","Tobias Schimanski","Dominik Stammbach","Saeid Ashraf Vaghefi","Qian Wang","Nicolas Webersinke","Tobias Wekhof","Tingyu Yu","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2306.15518v2.pdf","comment":"A new version of the ChatReport paper: arXiv:2307.15770"},{"id":"http://arxiv.org/abs/2311.09641v1","updated":"2023-11-16T07:48:45Z","published":"2023-11-16T07:48:45Z","title":"On the Exploitability of Reinforcement Learning with Human Feedback for\n  Large Language Models","summary":"  Reinforcement Learning with Human Feedback (RLHF) is a methodology designed\nto align Large Language Models (LLMs) with human preferences, playing an\nimportant role in LLMs alignment. Despite its advantages, RLHF relies on human\nannotators to rank the text, which can introduce potential security\nvulnerabilities if any adversarial annotator (i.e., attackers) manipulates the\nranking score by up-ranking any malicious text to steer the LLM adversarially.\nTo assess the red-teaming of RLHF against human preference data poisoning, we\npropose RankPoison, a poisoning attack method on candidates' selection of\npreference rank flipping to reach certain malicious behaviors (e.g., generating\nlonger sequences, which can increase the computational cost). With poisoned\ndataset generated by RankPoison, we can perform poisoning attacks on LLMs to\ngenerate longer tokens without hurting the original safety alignment\nperformance. Moreover, applying RankPoison, we also successfully implement a\nbackdoor attack where LLMs can generate longer answers under questions with the\ntrigger word. Our findings highlight critical security challenges in RLHF,\nunderscoring the necessity for more robust alignment methods for LLMs.\n","authors":["Jiongxiao Wang","Junlin Wu","Muhao Chen","Yevgeniy Vorobeychik","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2311.09641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09635v1","updated":"2023-11-16T07:37:25Z","published":"2023-11-16T07:37:25Z","title":"Evaluating In-Context Learning of Libraries for Code Generation","summary":"  Contemporary Large Language Models (LLMs) exhibit a high degree of code\ngeneration and comprehension capability. A particularly promising area is their\nability to interpret code modules from unfamiliar libraries for solving\nuser-instructed tasks. Recent work has shown that large proprietary LLMs can\nlearn novel library usage in-context from demonstrations. These results raise\nseveral open questions: whether demonstrations of library usage is required,\nwhether smaller (and more open) models also possess such capabilities, etc. In\nthis work, we take a broader approach by systematically evaluating a diverse\narray of LLMs across three scenarios reflecting varying levels of domain\nspecialization to understand their abilities and limitations in generating code\nbased on libraries defined in-context. Our results show that even smaller\nopen-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding\nof novel code libraries based on specification presented in-context. Our\nfindings further reveal that LLMs exhibit a surprisingly high proficiency in\nlearning novel library modules even when provided with just natural language\ndescriptions or raw code implementations of the functions, which are often\ncheaper to obtain than demonstrations. Overall, our results pave the way for\nharnessing LLMs in more adaptable and dynamic coding environments.\n","authors":["Arkil Patel","Siva Reddy","Dzmitry Bahdanau","Pradeep Dasigi"],"pdf_url":"https://arxiv.org/pdf/2311.09635v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09632v1","updated":"2023-11-16T07:31:03Z","published":"2023-11-16T07:31:03Z","title":"Online Continual Knowledge Learning for Language Models","summary":"  Large Language Models (LLMs) serve as repositories of extensive world\nknowledge, enabling them to perform tasks such as question-answering and\nfact-checking. However, this knowledge can become obsolete as global contexts\nchange. In this paper, we introduce a novel problem in the realm of continual\nlearning: Online Continual Knowledge Learning (OCKL). This problem formulation\naims to manage the dynamic nature of world knowledge in LMs under real-time\nconstraints. We propose a new benchmark and evaluation metric designed to\nmeasure both the rate of new knowledge acquisition and the retention of\npreviously learned knowledge. Our empirical evaluation, conducted using a\nvariety of state-of-the-art methods, establishes robust base-lines for OCKL.\nOur results reveal that existing continual learning approaches are\nunfortunately insufficient for tackling the unique challenges posed by OCKL. We\nidentify key factors that influence the trade-off between knowledge acquisition\nand retention, thereby advancing our understanding of how to train LMs in a\ncontinually evolving environment.\n","authors":["Yuhao Wu","Tongjun Shi","Karthick Sharma","Chun Wei Seah","Shuhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10923v2","updated":"2023-11-16T07:23:03Z","published":"2023-09-19T20:53:13Z","title":"Semi-automatic staging area for high-quality structured data extraction\n  from scientific literature","summary":"  We propose a semi-automatic staging area for efficiently building an accurate\ndatabase of experimental physical properties of superconductors from\nliterature, called SuperCon2, to enrich the existing manually-built\nsuperconductor database SuperCon. Here we report our curation interface\n(SuperCon2 Interface) and a workflow managing the state transitions of each\nexamined record, to validate the dataset of superconductors from PDF documents\ncollected using Grobid-superconductors in a previous work. This curation\nworkflow allows both automatic and manual operations, the former contains\n``anomaly detection'' that scans new data identifying outliers, and a\n``training data collector'' mechanism that collects training data examples\nbased on manual corrections. Such training data collection policy is effective\nin improving the machine-learning models with a reduced number of examples. For\nmanual operations, the interface (SuperCon2 interface) is developed to increase\nefficiency during manual correction by providing a smart interface and an\nenhanced PDF document viewer. We show that our interface significantly improves\nthe curation quality by boosting precision and recall as compared with the\ntraditional ``manual correction''. Our semi-automatic approach would provide a\nsolution for achieving a reliable database with text-data mining of scientific\ndocuments.\n","authors":["Luca Foppiano","Tomoya Mato","Kensei Terashima","Pedro Ortiz Suarez","Taku Tou","Chikako Sakai","Wei-Sheng Wang","Toshiyuki Amagasa","Yoshihiko Takano","Masashi Ishii"],"pdf_url":"https://arxiv.org/pdf/2309.10923v2.pdf","comment":"5 tables, 6 figures, 18 pages"},{"id":"http://arxiv.org/abs/2311.09630v1","updated":"2023-11-16T07:22:56Z","published":"2023-11-16T07:22:56Z","title":"From Scroll to Misbelief: Modeling the Unobservable Susceptibility to\n  Misinformation on Social Media","summary":"  Susceptibility to misinformation describes the extent to believe unverifiable\nclaims, which is hidden in people's mental process and infeasible to observe.\nExisting susceptibility studies heavily rely on the self-reported beliefs,\nmaking any downstream applications on susceptability hard to scale. To address\nthese limitations, in this work, we propose a computational model to infer\nusers' susceptibility levels given their activities. Since user's\nsusceptibility is a key indicator for their reposting behavior, we utilize the\nsupervision from the observable sharing behavior to infer the underlying\nsusceptibility tendency. The evaluation shows that our model yields estimations\nthat are highly aligned with human judgment on users' susceptibility level\ncomparisons. Building upon such large-scale susceptibility labeling, we further\nconduct a comprehensive analysis of how different social factors relate to\nsusceptibility. We find that political leanings and psychological factors are\nassociated with susceptibility in varying degrees.\n","authors":["Yanchen Liu","Mingyu Derek Ma","Wenna Qin","Azure Zhou","Jiaao Chen","Weiyan Shi","Wei Wang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2311.09630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01766v3","updated":"2023-11-16T07:21:10Z","published":"2023-11-03T08:05:54Z","title":"Support or Refute: Analyzing the Stance of Evidence to Detect\n  Out-of-Context Mis- and Disinformation","summary":"  Mis- and disinformation online have become a major societal problem as major\nsources of online harms of different kinds. One common form of mis- and\ndisinformation is out-of-context (OOC) information, where different pieces of\ninformation are falsely associated, e.g., a real image combined with a false\ntextual caption or a misleading textual description. Although some past studies\nhave attempted to defend against OOC mis- and disinformation through external\nevidence, they tend to disregard the role of different pieces of evidence with\ndifferent stances. Motivated by the intuition that the stance of evidence\nrepresents a bias towards different detection results, we propose a stance\nextraction network (SEN) that can extract the stances of different pieces of\nmulti-modal evidence in a unified framework. Moreover, we introduce a\nsupport-refutation score calculated based on the co-occurrence relations of\nnamed entities into the textual SEN. Extensive experiments on a public\nlarge-scale dataset demonstrated that our proposed method outperformed the\nstate-of-the-art baselines, with the best model achieving a performance gain of\n3.2% in accuracy.\n","authors":["Xin Yuan","Jie Guo","Weidong Qiu","Zheng Huang","Shujun Li"],"pdf_url":"https://arxiv.org/pdf/2311.01766v3.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09627v1","updated":"2023-11-16T07:16:55Z","published":"2023-11-16T07:16:55Z","title":"CRISPR: Eliminating Bias Neurons from an Instruction-following Language\n  Model","summary":"  Large language models (LLMs) executing tasks through instruction-based\nprompts often face challenges stemming from distribution differences between\nuser instructions and training instructions. This leads to distractions and\nbiases, especially when dealing with inconsistent dynamic labels. In this\npaper, we introduces a novel bias mitigation method, CRISPR, designed to\nalleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods\nto identify bias neurons influencing biased outputs and employs pruning to\neliminate the bias neurons. Experimental results demonstrate the method's\neffectiveness in mitigating biases in instruction-based prompting, enhancing\nlanguage model performance on social bias benchmarks without compromising\npre-existing knowledge. CRISPR proves highly practical, model-agnostic,\noffering flexibility in adapting to evolving social biases.\n","authors":["Nakyeong Yang","Taegwan Kang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2311.09627v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2305.02547v3","updated":"2023-11-16T07:11:11Z","published":"2023-05-04T04:58:00Z","title":"PersonaLLM: Investigating the Ability of Large Language Models to\n  Express Big Five Personality Traits","summary":"  Despite the many use cases for large language models (LLMs) in creating\npersonalized chatbots, there has been limited research on evaluating the extent\nto which the behaviors of personalized LLMs accurately and consistently reflect\nspecific personality traits. We consider studying the behavior of LLM-based\nagents, referred to as LLM personas, and present a case study with ChatGPT and\nGPT-4. The study investigates whether LLMs can generate content that aligns\nwith their assigned personality profiles. To this end, we create distinct LLM\npersonas based on the Big Five personality model, have them complete the\n44-item Big Five Inventory (BFI) personality test and a story writing task, and\nthen assess their essays with automatic and human evaluations. Results show\nthat LLM personas' self-reported BFI scores are consistent with their\ndesignated personality types, with large effect sizes observed across five\ntraits. Additionally, there are significant correlations between the assigned\npersonality types and certain psycholinguistic features of their writings, as\nmeasured by the Linguistic Inquiry and Word Count (LIWC) tool. Interestingly,\nhuman evaluators perceive the stories as less personal when told that the\nstories are authored by AI. However, their judgments on other aspects of the\nwriting such as readability, cohesiveness, redundancy, likeability, and\nbelievability remain largely unaffected. Notably, when evaluators were informed\nabout the AI authorship, their accuracy in identifying the intended personality\ntraits from the stories decreased by more than 10% for some traits. This\nresearch marks a significant step forward in understanding the capabilities of\nLLMs to express personality traits.\n","authors":["Hang Jiang","Xiajie Zhang","Xubo Cao","Jad Kabbara"],"pdf_url":"https://arxiv.org/pdf/2305.02547v3.pdf","comment":"First version uploaded at IC2S2 in May 2023. Full paper submitted in\n  November 2023"},{"id":"http://arxiv.org/abs/2311.09619v1","updated":"2023-11-16T07:03:54Z","published":"2023-11-16T07:03:54Z","title":"Take One Step at a Time to Know Incremental Utility of Demonstration: An\n  Analysis on Reranking for Few-Shot In-Context Learning","summary":"  In-Context Learning (ICL) is an emergent capability of Large Language Models\n(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new\ntasks. Previous studies have shown that using LLMs' outputs as labels is\neffective in training models to select demonstrations. Such a label is expected\nto estimate utility of a demonstration in ICL; however, it has not been well\nunderstood how different labeling strategies affect results on target tasks.\nThis paper presents an analysis on different utility functions by focusing on\nLLMs' output probability given ground-truth output, and task-specific reward\ngiven LLMs' prediction. Unlike the previous work, we introduce a novel labeling\nmethod, incremental utility, which estimates how much incremental knowledge is\nbrought into the LLMs by a demonstration. We conduct experiments with\ninstruction-tuned LLMs on binary/multi-class classification, segmentation, and\ntranslation across Arabic, English, Finnish, Japanese, and Spanish. Our results\nshow that (1) the probability is effective when the probability values are\ndistributed across the whole value range (on the classification tasks), and (2)\nthe downstream metric is more robust when nuanced reward values are provided\nwith long outputs (on the segmentation and translation tasks). We then show\nthat the proposed incremental utility further helps ICL by contrasting how the\nLLMs perform with and without the demonstrations.\n","authors":["Kazuma Hashimoto","Karthik Raman","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2311.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09618v1","updated":"2023-11-16T07:01:48Z","published":"2023-11-16T07:01:48Z","title":"Simulating Opinion Dynamics with Networks of LLM-based Agents","summary":"  Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations lack fidelity to human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\naccurate information, leading to consensus in line with scientific reality.\nHowever, this bias limits the simulation of individuals with resistant views on\nissues like climate change. After inducing confirmation bias through prompt\nengineering, we observed opinion fragmentation in line with existing\nagent-based research. These insights highlight the promise and limitations of\nLLM agents in this domain and suggest a path forward: refining LLMs with\nreal-world discourse to better simulate the evolution of human beliefs.\n","authors":["Yun-Shiuan Chuang","Agam Goyal","Nikunj Harlalka","Siddharth Suresh","Robert Hawkins","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2311.09618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09615v1","updated":"2023-11-16T06:59:54Z","published":"2023-11-16T06:59:54Z","title":"On Retrieval Augmentation and the Limitations of Language Model Training","summary":"  Augmenting a language model (LM) with $k$-nearest neighbors (kNN) retrieval\non its training data alone can decrease its perplexity, though the underlying\nreasons for this remains elusive. In this work, we first rule out one\npreviously posited possibility -- the \"softmax bottleneck.\" We further identify\nthe MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM\noptimization early on. We explore memorization and generalization in language\nmodels with two new datasets, where advanced model like GPT-3.5-turbo find\ngeneralizing to irrelevant information in the training data challenging.\nHowever, incorporating kNN retrieval to vanilla GPT-2 117M can consistently\nimprove performance in this setting.\n","authors":["Ting-Rui Chiang","Xinyan Velocity Yu","Joshua Robinson","Ollie Liu","Isabelle Lee","Dani Yogatama"],"pdf_url":"https://arxiv.org/pdf/2311.09615v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09613v1","updated":"2023-11-16T06:51:46Z","published":"2023-11-16T06:51:46Z","title":"Digital Socrates: Evaluating LLMs through explanation critiques","summary":"  While LLMs can provide reasoned explanations along with their answers, the\nnature and quality of those explanations are still poorly understood. In\nresponse, our goal is to define a detailed way of characterizing the\nexplanation capabilities of modern models and to create a nuanced,\ninterpretable explanation evaluation tool that can generate such\ncharacterizations automatically, without relying on expensive API calls or\nhuman annotations. Our approach is to (a) define the new task of explanation\ncritiquing - identifying and categorizing any main flaw in an explanation and\nproviding suggestions to address the flaw, (b) create a sizeable,\nhuman-verified dataset for this task, and (c) train an open-source, automatic\ncritiquing model (called Digital Socrates) using this data. Through\nquantitative and qualitative analysis, we demonstrate how Digital Socrates is\nuseful for revealing insights about student models by examining their reasoning\nchains, and how it can provide high-quality, nuanced, automatic evaluation of\nthose model explanations for the first time. Digital Socrates thus fills an\nimportant gap in evaluation tools for understanding and improving the\nexplanation behavior of models.\n","authors":["Yuling Gu","Oyvind Tafjord","Peter Clark"],"pdf_url":"https://arxiv.org/pdf/2311.09613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09612v1","updated":"2023-11-16T06:50:26Z","published":"2023-11-16T06:50:26Z","title":"Efficient End-to-End Visual Document Understanding with Rationale\n  Distillation","summary":"  Understanding visually situated language requires recognizing text and visual\nelements, and interpreting complex layouts. State-of-the-art methods commonly\nuse specialized pre-processing tools, such as optical character recognition\n(OCR) systems, that map document image inputs to extracted information in the\nspace of textual tokens, and sometimes also employ large language models (LLMs)\nto reason in text token space. However, the gains from external tools and LLMs\ncome at the cost of increased computational and engineering complexity. In this\npaper, we ask whether small pretrained image-to-text models can learn selective\ntext or layout recognition and reasoning as an intermediate inference step in\nan end-to-end model for pixel-level visual language understanding. We\nincorporate the outputs of such OCR tools, LLMs, and larger multimodal models\nas intermediate ``rationales'' on training data, and train a small student\nmodel to predict both rationales and answers for input questions based on those\ntraining examples. A student model based on Pix2Struct (282M parameters)\nachieves consistent improvements on three visual document understanding\nbenchmarks representing infographics, scanned documents, and figures, with\nimprovements of more than 4\\% absolute over a comparable Pix2Struct model that\npredicts answers directly.\n","authors":["Wang Zhu","Alekh Agarwal","Mandar Joshi","Robin Jia","Jesse Thomason","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2311.09612v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.09606v1","updated":"2023-11-16T06:28:05Z","published":"2023-11-16T06:28:05Z","title":"GistScore: Learning Better Representations for In-Context Example\n  Selection with Gist Bottlenecks","summary":"  Large language models (LLMs) have the ability to perform in-context learning\n(ICL) of new tasks by conditioning on prompts comprising a few task examples.\nThis work studies the problem of selecting the best examples given a candidate\npool to improve ICL performance on given a test input. Existing approaches\neither require training with feedback from a much larger LLM or are\ncomputationally expensive. We propose a novel metric, GistScore, based on\nExample Gisting, a novel approach for training example retrievers for ICL using\nan attention bottleneck via Gisting, a recent technique for compressing task\ninstructions. To tradeoff performance with ease of use, we experiment with both\nfine-tuning gist models on each dataset and multi-task training a single model\non a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we\nshow that our fine-tuned models get state-of-the-art ICL performance with 20%\nabsolute average gain over off-the-shelf retrievers and 7% over the best prior\nmethods. Our multi-task model generalizes well out-of-the-box to new task\ncategories, datasets, and prompt templates with retrieval speeds that are\nconsistently thousands of times faster than the best prior training-free\nmethod.\n","authors":["Shivanshu Gupta","Clemens Rosenbaum","Ethan R. Elenberg"],"pdf_url":"https://arxiv.org/pdf/2311.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09605v1","updated":"2023-11-16T06:27:35Z","published":"2023-11-16T06:27:35Z","title":"Measuring and Improving Attentiveness to Partial Inputs with\n  Counterfactuals","summary":"  The inevitable appearance of spurious correlations in training datasets hurts\nthe generalization of NLP models on unseen data. Previous work has found that\ndatasets with paired inputs are prone to correlations between a specific part\nof the input (e.g., the hypothesis in NLI) and the label; consequently, models\ntrained only on those outperform chance. Are these correlations picked up by\nmodels trained on the full input data? To address this question, we propose a\nnew evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses\ncounterfactuals by replacing part of the input with its counterpart from a\ndifferent example (subject to some restrictions), expecting an attentive model\nto change its prediction. Using CAT, we systematically investigate established\nsupervised and in-context learning models on ten datasets spanning four tasks:\nnatural language inference, reading comprehension, paraphrase detection, and\nvisual & language reasoning. CAT reveals that reliance on such correlations is\nmainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive\nwith an increased number of demonstrations, while its accuracy on the test data\nimproves. Our results demonstrate that augmenting training or demonstration\ndata with counterfactuals is effective in improving models' attentiveness. We\nshow that models' attentiveness measured by CAT reveals different conclusions\nfrom solely measuring correlations in data.\n","authors":["Yanai Elazar","Bhargavi Paranjape","Hao Peng","Sarah Wiegreffe","Khyathi Raghavi","Vivek Srikumar","Sameer Singh","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2311.09605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09603v1","updated":"2023-11-16T06:22:17Z","published":"2023-11-16T06:22:17Z","title":"SCORE: A framework for Self-Contradictory Reasoning Evaluation","summary":"  Large language models (LLMs) have demonstrated impressive reasoning ability\nin various language-based tasks. Despite many proposed reasoning methods aimed\nat enhancing performance in downstream tasks, two fundamental questions\npersist: Does reasoning genuinely support predictions, and how reliable is the\nquality of reasoning? In this paper, we propose a framework \\textsc{SCORE} to\nanalyze how well LLMs can reason. Specifically, we focus on self-contradictory\nreasoning, where reasoning does not support the prediction. We find that LLMs\noften contradict themselves when performing reasoning tasks that involve\ncontextual information and commonsense. The model may miss evidence or use\nshortcuts, thereby exhibiting self-contradictory behaviors. We also employ the\nPoint-of-View (POV) method, which probes models to generate reasoning from\nmultiple perspectives, as a diagnostic tool for further analysis. We find that\nthough LLMs may appear to perform well in one-perspective settings, they fail\nto stabilize such behavior in multi-perspectives settings. Even for correct\npredictions, the reasoning may be messy and incomplete, and LLMs can easily be\nled astray from good reasoning. \\textsc{SCORE}'s results underscore the lack of\nrobustness required for trustworthy reasoning and the urgency for further\nresearch to establish best practices for a comprehensive evaluation of\nreasoning beyond accuracy-based metrics.\n","authors":["Ziyi Liu","Isabelle Lee","Yongkang Du","Soumya Sanyal","Jieyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.09603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09602v1","updated":"2023-11-16T06:20:13Z","published":"2023-11-16T06:20:13Z","title":"Language Models (Mostly) Do Not Consider Emotion Triggers When\n  Predicting Emotion","summary":"  Situations and events evoke emotions in humans, but to what extent do they\ninform the prediction of emotion detection models? Prior work in emotion\ntrigger or cause identification focused on training models to recognize events\nthat trigger an emotion. Instead, this work investigates how well\nhuman-annotated emotion triggers correlate with features that models deemed\nsalient in their prediction of emotions. First, we introduce a novel dataset\nEmoTrigger, consisting of 900 social media posts sourced from three different\ndatasets; these were annotated by experts for emotion triggers with high\nagreement. Using EmoTrigger, we evaluate the ability of large language models\n(LLMs) to identify emotion triggers, and conduct a comparative analysis of the\nfeatures considered important for these tasks between LLMs and fine-tuned\nmodels. Our analysis reveals that emotion triggers are largely not considered\nsalient features for emotion prediction models, instead there is intricate\ninterplay between various features and the task of emotion detection.\n","authors":["Smriti Singh","Cornelia Caragea","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2311.09602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03608v2","updated":"2023-11-16T06:13:28Z","published":"2023-03-07T02:49:50Z","title":"Towards Interpretable and Efficient Automatic Reference-Based\n  Summarization Evaluation","summary":"  Interpretability and efficiency are two important considerations for the\nadoption of neural automatic metrics. In this work, we develop\nstrong-performing automatic metrics for reference-based summarization\nevaluation, based on a two-stage evaluation pipeline that first extracts basic\ninformation units from one text sequence and then checks the extracted units in\nanother sequence. The metrics we developed include two-stage metrics that can\nprovide high interpretability at both the fine-grained unit level and summary\nlevel, and one-stage metrics that achieve a balance between efficiency and\ninterpretability. We make the developed tools publicly available at\nhttps://github.com/Yale-LILY/AutoACU.\n","authors":["Yixin Liu","Alexander R. Fabbri","Yilun Zhao","Pengfei Liu","Shafiq Joty","Chien-Sheng Wu","Caiming Xiong","Dragomir Radev"],"pdf_url":"https://arxiv.org/pdf/2303.03608v2.pdf","comment":"EMNLP 2023 Camera Ready Version"},{"id":"http://arxiv.org/abs/2311.09593v1","updated":"2023-11-16T06:05:47Z","published":"2023-11-16T06:05:47Z","title":"Multi-Step Dialogue Workflow Action Prediction","summary":"  In task-oriented dialogue, a system often needs to follow a sequence of\nactions, called a workflow, that complies with a set of guidelines in order to\ncomplete a task. In this paper, we propose the novel problem of multi-step\nworkflow action prediction, in which the system predicts multiple future\nworkflow actions. Accurate prediction of multiple steps allows for multi-turn\nautomation, which can free up time to focus on more complex tasks. We propose\nthree modeling approaches that are simple to implement yet lead to more action\nautomation: 1) fine-tuning on a training dataset, 2) few-shot in-context\nlearning leveraging retrieval and large language model prompting, and 3)\nzero-shot graph traversal, which aggregates historical action sequences into a\ngraph for prediction. We show that multi-step action prediction produces\nfeatures that improve accuracy on downstream dialogue tasks like predicting\ntask success, and can increase automation of steps by 20% without requiring as\nmuch feedback from a human overseeing the system.\n","authors":["Ramya Ramakrishnan","Ethan Elenberg","Hashan Narangodage","Ryan McDonald"],"pdf_url":"https://arxiv.org/pdf/2311.09593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12770v2","updated":"2023-11-16T06:04:52Z","published":"2023-08-24T13:17:35Z","title":"WavMark: Watermarking for Audio Generation","summary":"  Recent breakthroughs in zero-shot voice synthesis have enabled imitating a\nspeaker's voice using just a few seconds of recording while maintaining a high\nlevel of realism. Alongside its potential benefits, this powerful technology\nintroduces notable risks, including voice fraud and speaker impersonation.\nUnlike the conventional approach of solely relying on passive methods for\ndetecting synthetic data, watermarking presents a proactive and robust defence\nmechanism against these looming risks. This paper introduces an innovative\naudio watermarking framework that encodes up to 32 bits of watermark within a\nmere 1-second audio snippet. The watermark is imperceptible to human senses and\nexhibits strong resilience against various attacks. It can serve as an\neffective identifier for synthesized voices and holds potential for broader\napplications in audio copyright protection. Moreover, this framework boasts\nhigh flexibility, allowing for the combination of multiple watermark segments\nto achieve heightened robustness and expanded capacity. Utilizing 10 to\n20-second audio as the host, our approach demonstrates an average Bit Error\nRate (BER) of 0.48\\% across ten common attacks, a remarkable reduction of over\n2800\\% in BER compared to the state-of-the-art watermarking tool. See\nhttps://aka.ms/wavmark for demos of our work.\n","authors":["Guangyu Chen","Yu Wu","Shujie Liu","Tao Liu","Xiaoyong Du","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2308.12770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14239v2","updated":"2023-11-16T05:55:43Z","published":"2023-05-23T16:56:04Z","title":"On Learning to Summarize with Large Language Models as References","summary":"  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we investigate a new learning\nsetting of text summarization models that considers the LLMs as the reference\nor the gold-standard oracle on these datasets. To examine the standard\npractices that are aligned with this new learning setting, we investigate two\nLLM-based summary quality evaluation methods for model training and adopt a\ncontrastive learning training method to leverage the LLM-guided learning\nsignals. Our experiments on the CNN/DailyMail and XSum datasets demonstrate\nthat smaller summarization models can achieve similar performance as LLMs under\nLLM-based evaluation. However, we found that the smaller models can not yet\nreach LLM-level performance under human evaluation despite promising\nimprovements brought by our proposed training methods. Meanwhile, we perform a\nmeta-analysis on this new learning setting that reveals a discrepancy between\nhuman and LLM-based evaluation, highlighting the benefits and risks of this\nLLM-as-reference setting we investigated.\n","authors":["Yixin Liu","Kejian Shi","Katherine S He","Longtian Ye","Alexander R. Fabbri","Pengfei Liu","Dragomir Radev","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2305.14239v2.pdf","comment":"GitHub Repo: https://github.com/yixinL7/SumLLM"},{"id":"http://arxiv.org/abs/2311.03220v2","updated":"2023-11-16T05:43:22Z","published":"2023-11-06T16:03:46Z","title":"ALYMPICS: Language Agents Meet Game Theory","summary":"  This paper introduces Alympics, a platform that leverages Large Language\nModel (LLM) agents to facilitate investigations in game theory. By employing\nLLMs and autonomous agents to simulate human behavior and enable multi-agent\ncollaborations, we can construct realistic and dynamic models of human\ninteractions for game theory hypothesis formulating and testing. To demonstrate\nthis, we present and implement a survival game involving unequal competition\nfor limited resources. Through manipulation of resource availability and agent\npersonalities, we observe how different agents engage in the competition and\nadapt their strategies. The use of LLM agents in game theory research offers\nsignificant advantages, including simulating realistic behavior, providing a\ncontrolled, scalable, and reproducible environment. Our work highlights the\npotential of LLM agents in enhancing the understanding of strategic\ndecision-making within complex socioeconomic contexts. All codes are available\nat https://github.com/microsoft/Alympics\n","authors":["Shaoguang Mao","Yuzhe Cai","Yan Xia","Wenshan Wu","Xun Wang","Fengyi Wang","Tao Ge","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2311.03220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09585v1","updated":"2023-11-16T05:43:02Z","published":"2023-11-16T05:43:02Z","title":"LifeTox: Unveiling Implicit Toxicity in Life Advice","summary":"  As large language models become increasingly integrated into daily life,\ndetecting implicit toxicity across diverse contexts is crucial. To this end, we\nintroduce LifeTox, a dataset designed for identifying implicit toxicity within\na broad range of advice-seeking scenarios. Unlike existing safety datasets,\nLifeTox comprises diverse contexts derived from personal experiences through\nopen-ended questions. Experiments demonstrate that RoBERTa fine-tuned on\nLifeTox matches or surpasses the zero-shot performance of large language models\nin toxicity classification tasks. These results underscore the efficacy of\nLifeTox in addressing the complex challenges inherent in implicit toxicity.\n","authors":["Minbeom Kim","Jahyun Koo","Hwanhee Lee","Joonsuk Park","Hwaran Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2311.09585v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.09581v1","updated":"2023-11-16T05:32:09Z","published":"2023-11-16T05:32:09Z","title":"Enhancing Medical Text Evaluation with GPT-4","summary":"  In the evaluation of medical text generation, it is essential to scrutinize\neach piece of information and ensure the utmost accuracy of the evaluation.\nExisting evaluation metrics either focus on coarse-level evaluation that\nassigns one score for the whole generated output or rely on evaluation models\ntrained on general domain, resulting in inaccuracies when adapted to the\nmedical domain. To address these issues, we propose a set of factuality-centric\nevaluation aspects and design corresponding GPT-4-based metrics for medical\ntext generation. We systematically compare these metrics with existing ones on\nclinical note generation and medical report summarization tasks, revealing low\ninter-metric correlation. A comprehensive human evaluation confirms that the\nproposed GPT-4-based metrics exhibit substantially higher agreement with human\njudgments than existing evaluation metrics. Our study contributes to the\nunderstanding of medical text generation evaluation and offers a more reliable\nalternative to existing metrics.\n","authors":["Yiqing Xie","Sheng Zhang","Hao Cheng","Zelalem Gero","Cliff Wong","Tristan Naumann","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2311.09581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09580v1","updated":"2023-11-16T05:31:21Z","published":"2023-11-16T05:31:21Z","title":"MMOE: Mixture of Multimodal Interaction Experts","summary":"  Multimodal machine learning, which studies the information and interactions\nacross various input modalities, has made significant advancements in\nunderstanding the relationship between images and descriptive text. However,\nthis is just a portion of the potential multimodal interactions seen in the\nreal world and does not include new interactions between conflicting utterances\nand gestures in predicting sarcasm, for example. Notably, the current methods\nfor capturing shared information often do not extend well to these more nuanced\ninteractions, sometimes performing as low as 50% in binary classification. In\nthis paper, we address this problem via a new approach called MMOE, which\nstands for a mixture of multimodal interaction experts. Our method\nautomatically classifies data points from unlabeled multimodal datasets by\ntheir interaction type and employs specialized models for each specific\ninteraction. Based on our experiments, this approach improves performance on\nthese challenging interactions by more than 10%, leading to an overall increase\nof 2% for tasks like sarcasm prediction. As a result, interaction\nquantification provides new insights for dataset analysis and yields simple\napproaches that obtain state-of-the-art performance.\n","authors":["Haofei Yu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2311.09580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09579v1","updated":"2023-11-16T05:30:07Z","published":"2023-11-16T05:30:07Z","title":"Crafting In-context Examples according to LMs' Parametric Knowledge","summary":"  In-context learning has been applied to knowledge-rich tasks such as question\nanswering. In such scenarios, in-context examples are used to trigger a\nbehaviour in the language model: namely, it should surface information stored\nin its parametric knowledge. We study the construction of in-context example\nsets, with a focus on the parametric knowledge of the model regarding\nin-context examples. We identify 'known' examples, where models can correctly\nanswer from its parametric knowledge, and 'unknown' ones. Our experiments show\nthat prompting with 'unknown' examples decreases the performance, potentially\nas it encourages hallucination rather than searching its parametric knowledge.\nConstructing an in-context example set that presents both known and unknown\ninformation performs the best across diverse settings. We perform analysis on\nthree multi-answer question answering datasets, which allows us to further\nstudy answer set ordering strategies based on the LM's knowledge about each\nanswer. Together, our study sheds lights on how to best construct in-context\nexample sets for knowledge-rich tasks.\n","authors":["Yoonsang Lee","Pranav Atreya","Xi Ye","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2311.09579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09578v1","updated":"2023-11-16T05:29:39Z","published":"2023-11-16T05:29:39Z","title":"Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying","summary":"  We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective\ntraining to further increase parameter efficiency of the Low-rank adaptation\n(LoRA) method. Our investigations include all feasible combinations parameter\ntraining/freezing in conjunction with weight tying to identify the optimal\nbalance between performance and the number of trainable parameters. Through\nexperiments covering a variety of tasks and two base language models, we\nprovide analysis revealing trade-offs between efficiency and performance. Our\nexperiments uncovered a particular Tied-LoRA configuration that stands out by\ndemonstrating comparable performance across several tasks while employing only\n13~\\% percent of parameters utilized by the standard LoRA method.\n","authors":["Adithya Renduchintala","Tugrul Konuk","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2311.09578v1.pdf","comment":"8 pages 4 figures"},{"id":"http://arxiv.org/abs/2311.09576v1","updated":"2023-11-16T05:21:25Z","published":"2023-11-16T05:21:25Z","title":"Work State-Centric AI Agents: Design, Implementation, and Management of\n  Cognitive Work Threads","summary":"  AI agents excel in executing predefined tasks, but the dynamic management of\nwork state information during task execution remains an underexplored area. We\npropose a work state-centric AI agent model employing \"work notes\" to record\nand reflect the state throughout task execution. This paper details the model's\narchitecture, featuring worker threads for task oversight, planner modules for\ntask decomposition and planning, and executor modules for performing subtasks\nusing a ReAct-inspired thought-action loop. We provide an exhaustive work state\nrecord incorporating plans and outcomes, constituting a comprehensive work\njournal. Our results show that this model not only improves task execution\nefficiency but also lays a solid foundation for subsequent task analysis and\nauditing.\n","authors":["Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09569v1","updated":"2023-11-16T05:08:33Z","published":"2023-11-16T05:08:33Z","title":"Prompt Optimisation with Random Sampling","summary":"  Using the generative nature of a language model to generate task-relevant\nseparators has shown competitive results compared to human-curated prompts like\n\"TL;DR\". We demonstrate that even randomly chosen tokens from the vocabulary as\nseparators can achieve near-state-of-the-art performance. We analyse this\nphenomenon in detail using three different random generation strategies,\nestablishing that the language space is rich with potential good separators,\nregardless of the underlying language model size. These observations challenge\nthe common assumption that an effective prompt should be human-readable or\ntask-relevant. Experimental results show that using random separators leads to\nan average 16% relative improvement across nine text classification tasks on\nseven language models, compared to human-curated separators, and is on par with\nautomatic prompt searching methods.\n","authors":["Yao Lu","Jiayi Wang","Sebastian Riedel","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2311.09569v1.pdf","comment":"Supplementary files are available at\n  https://github.com/yaolu/random-prompt"},{"id":"http://arxiv.org/abs/2311.09564v1","updated":"2023-11-16T04:57:49Z","published":"2023-11-16T04:57:49Z","title":"LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks","summary":"  Many large language models (LLMs) for medicine have largely been evaluated on\nshort texts, and their ability to handle longer sequences such as a complete\nelectronic health record (EHR) has not been systematically explored. Assessing\nthese models on long sequences is crucial since prior work in the general\ndomain has demonstrated performance degradation of LLMs on longer texts.\nMotivated by this, we introduce LongBoX, a collection of seven medical datasets\nin text-to-text format, designed to investigate model performance on long\nsequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)\nand strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We\nfurther evaluate two techniques designed for long-sequence handling: (i)\nlocal-global attention, and (ii) Fusion-in-Decoder (FiD). Our results\ndemonstrate mixed results with long-sequence handling - while scores on some\ndatasets increase, there is substantial room for improvement. We hope that\nLongBoX facilitates the development of more effective long-sequence techniques\nfor the medical domain. Data and source code are available at\nhttps://github.com/Mihir3009/LongBoX.\n","authors":["Mihir Parmar","Aakanksha Naik","Himanshu Gupta","Disha Agrawal","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2311.09564v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2305.14456v2","updated":"2023-11-16T04:46:27Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  It is important that language models appropriately adapt to specific cultural\ncontexts. However, as we show in this paper, multilingual and Arabic\nmonolingual language models default to Western culture even when prompted in\nArabic and contextualized by an Arab cultural setting. To measure this Western\nbias, we introduce CAMeL, a dataset of naturally occurring Arabic prompts\nspanning eight diverse cultural aspects and an extensive list of 20,504\ncultural targets corresponding to Arab or Western culture. Using CAMeL, we show\nthat models favor Western targets and demonstrate cultural unfairness on\ndownstream tasks such as named entity recognition and sentiment analysis. Our\nanalyses of pretraining corpora also reveal that commonly used sources such as\nWikipedia may not be suited to build culturally aware models, underscoring the\nimportance of carefully curating pretraining data in constructing language\nmodels to serve a global population.\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09562v1","updated":"2023-11-16T04:43:03Z","published":"2023-11-16T04:43:03Z","title":"A Reevaluation of Event Extraction: Past, Present, and Future Challenges","summary":"  Event extraction has attracted much attention in recent years due to its\npotential for many applications. However, recent studies observe some\nevaluation challenges, suggesting that reported scores might not reflect the\ntrue performance. In this work, we first identify and discuss these evaluation\nchallenges, including the unfair comparisons resulting from different\nassumptions about data or different data preprocessing steps, the\nincompleteness of the current evaluation framework leading to potential dataset\nbias or data split bias, and low reproducibility of prior studies. To address\nthese challenges, we propose TextEE, a standardized, fair, and reproducible\nbenchmark for event extraction. TextEE contains standardized data preprocessing\nscripts and splits for more than ten datasets across different domains. In\naddition, we aggregate and re-implement over ten event extraction approaches\npublished in recent years and conduct a comprehensive reevaluation. Finally, we\nexplore the capability of large language models in event extraction and discuss\nsome future challenges. We expect TextEE will serve as a reliable benchmark for\nevent extraction, facilitating future research in the field.\n","authors":["Kuan-Hao Huang","I-Hung Hsu","Tanmay Parekh","Zhiyu Xie","Zixuan Zhang","Premkumar Natarajan","Kai-Wei Chang","Nanyun Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2311.09562v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2311.09559v1","updated":"2023-11-16T04:29:41Z","published":"2023-11-16T04:29:41Z","title":"Enchancing Semi-Supervised Learning for Extractive Summarization with an\n  LLM-based pseudolabeler","summary":"  This work tackles the task of extractive text summarization in a limited\nlabeled data scenario using a semi-supervised approach. Specifically, we\npropose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate\nour method on three text summarization datasets: TweetSumm, WikiHow, and\nArXiv/PubMed. Our experiments show that by using an LLM to evaluate and\ngenerate pseudolabels, we can improve the ROUGE-1 by 10-20\\% on the different\ndatasets, which is akin to enhancing pretrained models. We also show that such\na method needs a smaller pool of unlabeled examples to perform better.\n","authors":["Gaurav Sahu","Olga Vechtomova","Issam H. Laradji"],"pdf_url":"https://arxiv.org/pdf/2311.09559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09558v1","updated":"2023-11-16T04:26:32Z","published":"2023-11-16T04:26:32Z","title":"Pachinko: Patching Interpretable QA Models through Natural Language\n  Feedback","summary":"  Eliciting feedback from end users of NLP models can be beneficial for\nimproving models. However, how should we present model responses to users so\nthey are most amenable to be corrected from user feedback? Further, what\nproperties do users value to understand and trust responses? We answer these\nquestions by analyzing the effect of rationales generated by QA models to\nsupport their answers. We specifically consider decomposed question-answering\nmodels that first extract an intermediate rationale based on a context and a\nquestion and then use solely this rationale to answer the question. A rationale\noutlines the approach followed by the model to answer the question. Our work\nconsiders various formats of these rationales that vary according to\nwell-defined properties of interest. We sample these rationales from large\nlanguage models using few-shot prompting for two reading comprehension\ndatasets, and then perform two user studies. In the first one, we present users\nwith incorrect answers and corresponding rationales of various formats and ask\nthem to provide natural language feedback to revise the rationale. We then\nmeasure the effectiveness of this feedback in patching these rationales through\nin-context learning. The second study evaluates how well different rationale\nformats enable users to understand and trust model answers, when they are\ncorrect. We find that rationale formats significantly affect how easy it is (1)\nfor users to give feedback for rationales, and (2) for models to subsequently\nexecute this feedback. In addition to influencing critiquablity, certain\nformats significantly enhance user reported understanding and trust of model\noutputs.\n","authors":["Chaitanya Malaviya","Subin Lee","Dan Roth","Mark Yatskar"],"pdf_url":"https://arxiv.org/pdf/2311.09558v1.pdf","comment":"Code & data available at\n  https://github.com/chaitanyamalaviya/pachinko"},{"id":"http://arxiv.org/abs/2311.09552v1","updated":"2023-11-16T04:17:47Z","published":"2023-11-16T04:17:47Z","title":"Large Language Models are Few-Shot Training Example Generators: A Case\n  Study in Fallacy Recognition","summary":"  Recognizing fallacies is crucial for ensuring the quality and validity of\narguments across various domains. However, computational fallacy recognition\nfaces challenges due to the diverse genres, domains, and types of fallacies\nfound in datasets. This leads to a highly multiclass, and even multi-label,\nsetup with substantial class imbalance. In this study, we aim to enhance\nexisting models for fallacy recognition by incorporating additional context and\nby leveraging large language models to generate synthetic data, thus increasing\nthe representation of the infrequent classes. We experiment with GPT3.5 to\ngenerate synthetic examples and we examine the impact of prompt settings for\nthis. Moreover, we explore zero-shot and few-shot scenarios to evaluate the\neffectiveness of using the generated examples for training smaller models\nwithin a unified fallacy recognition framework. Furthermore, we analyze the\noverlap between the synthetic data and existing fallacy datasets. Finally, we\ninvestigate the usefulness of providing supplementary context for detecting\nfallacy types that need such context, e.g., diversion fallacies. Our evaluation\nresults demonstrate consistent improvements across fallacy types, datasets, and\ngenerators.\n","authors":["Tariq Alhindi","Smaranda Muresan","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2311.09552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09000v2","updated":"2023-11-16T04:15:22Z","published":"2023-11-15T14:41:57Z","title":"Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and\n  Correction of LLM Output","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We design and\nbuild an annotation tool to speed up the labelling procedure and ease the\nworkload of raters. It allows flexible incorporation of automatic results in\nany stage, e.g. automatically-retrieved evidence. We further construct an\nopen-domain document-level factuality benchmark in three-level granularity:\nclaim, sentence and document. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims with the\nbest F1=0.53. Annotation tool, benchmark and code are available at\nhttps://github.com/yuxiaw/Factcheck-GPT.\n","authors":["Yuxia Wang","Revanth Gangi Reddy","Zain Muhammad Mujahid","Arnav Arora","Aleksandr Rubashevskii","Jiahui Geng","Osama Mohammed Afzal","Liangming Pan","Nadav Borenstein","Aditya Pillai","Isabelle Augenstein","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2311.09000v2.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2311.09550v1","updated":"2023-11-16T04:11:19Z","published":"2023-11-16T04:11:19Z","title":"A Speed Odyssey for Deployable Quantization of LLMs","summary":"  The large language model era urges faster and less costly inference. Prior\nmodel compression works on LLMs tend to undertake a software-centric approach\nprimarily focused on the simulated quantization performance. By neglecting the\nfeasibility of deployment, these approaches are typically disabled in real\npractice. They used to drastically push down the quantization bit range for a\nreduced computation which might not be supported by the mainstream hardware, or\ninvolve sophisticated algorithms that introduce extra computation or memory\naccess overhead. We argue that pursuing a hardware-centric approach in the\nconstruction of quantization algorithms is crucial. In this regard, we are\ndriven to build our compression method on top of hardware awareness,\neliminating impractical algorithm choices while maximizing the benefit of\nhardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel\nimplementation called FastGEMM and a combined recipe of quantization\nstrategies. Extensive experiments manifest the superiority of our W4A8 method\nwhich brings the actual speed boosting up to \\textbf{4$\\times$} compared to\nHugging Face FP16 inference and \\textbf{2.23$\\times$} vs. the state-of-the-art\ninference engine TensorRT-LLM in FP16, and \\textbf{1.45$\\times$} vs.\nTensorRT-LLM in INT8, yet without substantially harming the performance.\n","authors":["Qingyuan Li","Ran Meng","Yiduo Li","Bo Zhang","Liang Li","Yifan Lu","Xiangxiang Chu","Yerui Sun","Yuchen Xie"],"pdf_url":"https://arxiv.org/pdf/2311.09550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09542v1","updated":"2023-11-16T03:33:01Z","published":"2023-11-16T03:33:01Z","title":"Towards Pragmatic Awareness in Question Answering: A Case Study in\n  Maternal and Infant Health","summary":"  Questions posed by information-seeking users often contain implicit false or\npotentially harmful assumptions. In a high-risk domain such as maternal and\ninfant health, a question-answering system must recognize these pragmatic\nconstraints and go beyond simply answering user questions, examining them in\ncontext to respond helpfully. To achieve this, we study pragmatic inferences\nmade when mothers ask questions about pregnancy and infant care. Some of the\ninferences in these questions evade detection by existing methods, risking the\npossibility of QA systems failing to address them which can have dangerous\nhealth and policy implications. We explore the viability of detecting\ninferences from questions using large language models and illustrate that\ninforming existing QA pipelines with pragmatic inferences produces responses\nthat can mitigate the propagation of harmful beliefs.\n","authors":["Neha Srikanth","Rupak Sarkar","Rachel Rudinger","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2311.09542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09538v1","updated":"2023-11-16T03:28:43Z","published":"2023-11-16T03:28:43Z","title":"Reducing Privacy Risks in Online Self-Disclosures with Language Models","summary":"  Self-disclosure, while being common and rewarding in social media\ninteraction, also poses privacy risks. In this paper, we take the initiative to\nprotect the user-side privacy associated with online self-disclosure through\nidentification and abstraction. We develop a taxonomy of 19 self-disclosure\ncategories, and curate a large corpus consisting of 4.8K annotated disclosure\nspans. We then fine-tune a language model for identification, achieving over\n75% in Token F$_1$. We further conduct a HCI user study, with 82\\% of\nparticipants viewing the model positively, highlighting its real world\napplicability. Motivated by the user feedback, we introduce the task of\nself-disclosure abstraction. We experiment with both one-span abstraction and\nthree-span abstraction settings, and explore multiple fine-tuning strategies.\nOur best model can generate diverse abstractions that moderately reduce privacy\nrisks while maintaining high utility according to human evaluation.\n","authors":["Yao Dou","Isadora Krsek","Tarek Naous","Anubha Kabra","Sauvik Das","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2311.09538v1.pdf","comment":"LLMs, Privacy, HCI"},{"id":"http://arxiv.org/abs/2311.09533v1","updated":"2023-11-16T03:22:25Z","published":"2023-11-16T03:22:25Z","title":"Effective Large Language Model Adaptation for Improved Grounding","summary":"  Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage understanding, generation, and manipulation of text-based data.\nHowever, one major issue towards their widespread deployment in the real world\nis that they can generate \"hallucinated\" answers that are not factual. Towards\nthis end, this paper focuses on improving grounding from a holistic perspective\nwith a novel framework, AGREE, Adaptation of LLMs for GRounding EnhancEment. We\nstart with the design of an iterative test-time adaptation (TTA) capability\nthat takes into account the support information generated in self-grounded\nresponses. To effectively enable this capability, we tune LLMs to ground the\nclaims in their responses to retrieved documents by providing citations. This\ntuning on top of the pre-trained LLMs requires a small amount of data that\nneeds to be constructed in a particular way to learn the grounding information,\nfor which we introduce a data construction method. Our results show that the\ntuning-based AGREE framework generates better grounded responses with more\naccurate citations compared to prompting-based approaches.\n","authors":["Xi Ye","Ruoxi Sun","Sercan Ö. Arik","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2311.09533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09528v1","updated":"2023-11-16T03:13:29Z","published":"2023-11-16T03:13:29Z","title":"HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM","summary":"  Existing open-source helpfulness preference datasets do not specify what\nmakes some responses more helpful and others less so. Models trained on these\ndatasets can incidentally learn to model dataset artifacts (e.g. preferring\nlonger but unhelpful responses only due to their length). To alleviate this\nproblem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated\nfor the various aspects that make responses helpful. Specifically, our\n37k-sample dataset has annotations for correctness, coherence, complexity, and\nverbosity in addition to overall helpfulness of responses. Training Llama 2 70B\nusing the HelpSteer dataset with SteerLM technique produces a model that scores\n7.54 on MT Bench, which is currently the highest score for open models that do\nnot require training data from more powerful models (e.g. GPT4). We release\nthis dataset with CC-BY-4.0 license at\nhttps://huggingface.co/datasets/nvidia/HelpSteer\n","authors":["Zhilin Wang","Yi Dong","Jiaqi Zeng","Virginia Adams","Makesh Narsimhan Sreedhar","Daniel Egert","Olivier Delalleau","Jane Polak Scowcroft","Neel Kant","Aidan Swope","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2311.09528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09521v1","updated":"2023-11-16T02:56:29Z","published":"2023-11-16T02:56:29Z","title":"AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven\n  Training Data Generation","summary":"  Ensuring factual consistency is crucial in various natural language\nprocessing tasks, particularly in abstractive summarization, where preserving\nthe integrity of information is paramount. Prior entailment-based approaches\noften generate factually inconsistent summaries and then train a classifier on\nthe generated data. However, summaries produced by these approaches are either\nof low coherence or lack error-type coverage. To address these issues, we\npropose AMRFact, a novel framework that generates factually inconsistent\nsummaries using Abstract Meaning Representation (AMR). Our approach parses\nfactually correct summaries into AMR graphs and injects controlled factual\ninconsistencies to create negative examples, allowing for coherent factually\ninconsistent summaries to be generated with high error-type coverage.\nAdditionally, we present a data selection module NegFilter based on natural\nlanguage inference and BARTScore to ensure the quality of the generated\nnegative samples. Experimental results demonstrate that our approach\nsignificantly outperforms previous systems on the AggreFact-SOTA dataset,\nshowcasing its efficacy in assessing factuality in abstractive summarization.\n","authors":["Haoyi Qiu","Kung-Hsiang Huang","Jingnong Qu","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2311.09521v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09519v1","updated":"2023-11-16T02:50:06Z","published":"2023-11-16T02:50:06Z","title":"Leveraging Code to Improve In-context Learning for Semantic Parsing","summary":"  In-context learning (ICL) is an appealing approach for semantic parsing due\nto its few-shot nature and improved generalization. However, learning to parse\nto rare domain-specific languages (DSLs) from just a few demonstrations is\nchallenging, limiting the performance of even the most capable LLMs. In this\nwork, we improve the effectiveness of ICL for semantic parsing by (1) using\ngeneral-purpose programming languages such as Python instead of DSLs, and (2)\naugmenting prompts with a structured domain description that includes, e.g.,\nthe available classes and functions. We show that both these changes\nsignificantly improve accuracy across three popular datasets. Combined, they\nlead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional\nsplit), nearly closing the performance gap between easier i.i.d.\\ and harder\ncompositional splits when used with a strong model, and reducing the need for a\nlarge number of demonstrations. We find that the resemblance of the target\nparse language to general-purpose code is a more important factor than the\nlanguage's popularity in pre-training corpora. Our findings provide an improved\nmethodology for building semantic parsers in the modern context of ICL with\nLLMs.\n","authors":["Ben Bogin","Shivanshu Gupta","Peter Clark","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2311.09519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09517v1","updated":"2023-11-16T02:45:47Z","published":"2023-11-16T02:45:47Z","title":"GEE! Grammar Error Explanation with Large Language Models","summary":"  Grammatical error correction tools are effective at correcting grammatical\nerrors in users' input sentences but do not provide users with \\textit{natural\nlanguage} explanations about their errors. Such explanations are essential for\nhelping users learn the language by gaining a deeper understanding of its\ngrammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we\npropose the task of grammar error explanation, where a system needs to provide\none-sentence explanations for each grammatical error in a pair of erroneous and\ncorrected sentences. We analyze the capability of GPT-4 in grammar error\nexplanation, and find that it only produces explanations for 60.2% of the\nerrors using one-shot prompting. To improve upon this performance, we develop a\ntwo-step pipeline that leverages fine-tuned and prompted large language models\nto perform structured atomic token edit extraction, followed by prompting GPT-4\nto generate explanations. We evaluate our pipeline on German and Chinese\ngrammar error correction data sampled from language learners with a wide range\nof proficiency levels. Human evaluation reveals that our pipeline produces\n93.9% and 98.0% correct explanations for German and Chinese data, respectively.\nTo encourage further research in this area, we will open-source our data and\ncode.\n","authors":["Yixiao Song","Kalpesh Krishna","Rajesh Bhatt","Kevin Gimpel","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2311.09517v1.pdf","comment":"Preprint, 24 pages, code and data available in\n  https://github.com/Yixiao-Song/GEE-with-LLMs"},{"id":"http://arxiv.org/abs/2311.09513v1","updated":"2023-11-16T02:37:58Z","published":"2023-11-16T02:37:58Z","title":"Sequencing Matters: A Generate-Retrieve-Generate Model for Building\n  Conversational Agents","summary":"  This paper contains what the Georgetown InfoSense group has done in regard to\nsolving the challenges presented by TREC iKAT 2023. Our submitted runs\noutperform the median runs by a significant margin, exhibiting superior\nperformance in nDCG across various cut numbers and in overall success rate. Our\napproach uses a Generate-Retrieve-Generate method, which we've found to greatly\noutpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our\nsolution involves the use of Large Language Models (LLMs) for initial answers,\nanswer grounding by BM25, passage quality filtering by logistic regression, and\nanswer generation by LLMs again. We leverage several purpose-built Language\nModels, including BERT, Chat-based, and text-to-transfer-based models, for text\nunderstanding, classification, generation, and summarization. The official\nresults of the TREC evaluation contradict our initial self-evaluation, which\nmay suggest that a decrease in the reliance on our retrieval and classification\nmethods is better. Nonetheless, our findings suggest that the sequence of\ninvolving these different components matters, where we see an essentiality of\nusing LLMs before using search engines.\n","authors":["Quinn Patwardhan","Grace Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2311.09513v1.pdf","comment":"Submitted as part of the Thirty-Second Text REtrieval Conference\n  (TREC 2023)"},{"id":"http://arxiv.org/abs/2310.16776v3","updated":"2023-11-16T02:35:06Z","published":"2023-10-25T17:06:42Z","title":"DEFT: Data Efficient Fine-Tuning for Large Language Models via\n  Unsupervised Core-Set Selection","summary":"  Recent advances have led to the availability of many pre-trained language\nmodels (PLMs); however, a question that remains is how much data is truly\nneeded to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,\na data-efficient fine-tuning framework that leverages unsupervised core-set\nselection to minimize the amount of data needed to fine-tune PLMs for\ndownstream tasks. We demonstrate the efficacy of our DEFT framework in the\ncontext of text-editing LMs, and compare to the state-of-the art text-editing\nmodel, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT\nmodels are just as accurate as CoEDIT while being finetuned on ~70% less data.\n","authors":["Devleena Das","Vivek Khetan"],"pdf_url":"https://arxiv.org/pdf/2310.16776v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09510v1","updated":"2023-11-16T02:25:36Z","published":"2023-11-16T02:25:36Z","title":"One Size Does Not Fit All: Customizing Open-Domain Procedures","summary":"  How-to procedures, such as how to plant a garden, are ubiquitous. But one\nsize does not fit all - humans often need to customize these procedural plans\naccording to their specific needs, e.g., planting a garden without pesticides.\nWhile LLMs can fluently generate generic procedures, we present the first study\non how well LLMs can customize open-domain procedures. We introduce\nCustomPlans, a probe dataset of customization hints that encodes diverse user\nneeds for open-domain How-to procedures. Using LLMs as CustomizationAgent and\nExecutionAgent in different settings, we establish their abilities to perform\nopen-domain procedure customization. Human evaluation shows that using these\nagents in a Sequential setting is the best, but they are good enough only ~51%\nof the time. Error analysis shows that LLMs do not sufficiently address user\ncustomization needs in their generated procedures.\n","authors":["Yash Kumar Lal","Li Zhang","Faeze Brahman","Bodhisattwa Prasad Majumder","Peter Clark","Niket Tandon"],"pdf_url":"https://arxiv.org/pdf/2311.09510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09505v1","updated":"2023-11-16T02:05:15Z","published":"2023-11-16T02:05:15Z","title":"SegMix: A Simple Structure-Aware Data Augmentation Method","summary":"  Interpolation-based Data Augmentation (DA) methods (Mixup) linearly\ninterpolate the inputs and labels of two or more training examples. Mixup has\nmore recently been adapted to the field of Natural Language Processing (NLP),\nmainly for sequence labeling tasks. However, such a simple adoption yields\nmixed or unstable improvements over the baseline models. We argue that the\ndirect-adoption methods do not account for structures in NLP tasks. To this\nend, we propose SegMix, a collection of interpolation-based DA algorithms that\ncan adapt to task-specific structures. SegMix poses fewer constraints on data\nstructures, is robust to various hyperparameter settings, applies to more task\nsettings, and adds little computational overhead. In the algorithm's core, we\napply interpolation methods on task-specific meaningful segments, in contrast\nto applying them on sequences as in prior work. We find SegMix to be a flexible\nframework that combines rule-based DA methods with interpolation-based methods,\ncreating interesting mixtures of DA techniques. We show that SegMix\nconsistently improves performance over strong baseline models in Named Entity\nRecognition (NER) and Relation Extraction (RE) tasks, especially under\ndata-scarce settings. Furthermore, this method is easy to implement and adds\nnegligible training overhead.\n","authors":["Yuxin Pei","Pushkar Bhuse","Zhengzhong Liu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2311.09505v1.pdf","comment":"Upload of a work done in 2022"},{"id":"http://arxiv.org/abs/2311.09502v1","updated":"2023-11-16T01:57:00Z","published":"2023-11-16T01:57:00Z","title":"SQATIN: Supervised Instruction Tuning Meets Question Answering for\n  Improved Dialogue NLU","summary":"  Task-oriented dialogue (ToD) systems help users execute well-defined tasks\nacross a variety of domains (e.g., $\\textit{flight booking}$ or $\\textit{food\nordering}$), with their Natural Language Understanding (NLU) components being\ndedicated to the analysis of user utterances, predicting users' intents\n($\\textit{Intent Detection}$, ID) and extracting values for informational slots\n($\\textit{Value Extraction}$, VE). In most domains, labelled NLU data is\nscarce, making sample-efficient learning -- enabled with effective transfer\nparadigms -- paramount. In this work, we introduce SQATIN, a new framework for\ndialog NLU based on (i) instruction tuning and (ii) question-answering-based\nformulation of ID and VE tasks. According to the evaluation on established NLU\nbenchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially\nsurpassing the performance of current models based on standard fine-tuning\nobjectives in both in-domain training and cross-domain transfer. SQATIN yields\nparticularly large performance gains in cross-domain transfer, owing to the\nfact that our QA-based instruction tuning leverages similarities between\nnatural language descriptions of classes (i.e., slots and intents) across\ndomains.\n","authors":["Evgeniia Razumovskaia","Goran Glavaš","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2311.09502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06626v4","updated":"2023-11-16T01:17:35Z","published":"2023-05-11T07:55:20Z","title":"When the Majority is Wrong: Modeling Annotator Disagreement for\n  Subjective Tasks","summary":"  Though majority vote among annotators is typically used for ground truth\nlabels in natural language processing, annotator disagreement in tasks such as\nhate speech detection may reflect differences in opinion across groups, not\nnoise. Thus, a crucial problem in hate speech detection is determining whether\na statement is offensive to the demographic group that it targets, when that\ngroup may constitute a small fraction of the annotator pool. We construct a\nmodel that predicts individual annotator ratings on potentially offensive text\nand combines this information with the predicted target group of the text to\nmodel the opinions of target group members. We show gains across a range of\nmetrics, including raising performance over the baseline by 22% at predicting\nindividual annotators' ratings and by 33% at predicting variance among\nannotators, which provides a metric for model uncertainty downstream. We find\nthat annotator ratings can be predicted using their demographic information and\nopinions on online content, without the need to track identifying annotator IDs\nthat link each annotator to their ratings. We also find that use of\nnon-invasive survey questions on annotators' online experiences helps to\nmaximize privacy and minimize unnecessary collection of demographic information\nwhen predicting annotators' opinions.\n","authors":["Eve Fleisig","Rediet Abebe","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2305.06626v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02802v2","updated":"2023-11-16T01:13:57Z","published":"2023-11-06T00:06:11Z","title":"Incorporating Worker Perspectives into MTurk Annotation Practices for\n  NLP","summary":"  Current practices regarding data collection for natural language processing\non Amazon Mechanical Turk (MTurk) often rely on a combination of studies on\ndata quality and heuristics shared among NLP researchers. However, without\nconsidering the perspectives of MTurk workers, these approaches are susceptible\nto issues regarding workers' rights and poor response quality. We conducted a\ncritical literature review and a survey of MTurk workers aimed at addressing\nopen questions regarding best practices for fair payment, worker privacy, data\nquality, and considering worker incentives. We found that worker preferences\nare often at odds with received wisdom among NLP researchers. Surveyed workers\npreferred reliable, reasonable payments over uncertain, very high payments;\nreported frequently lying on demographic questions; and expressed frustration\nat having work rejected with no explanation. We also found that workers view\nsome quality control methods, such as requiring minimum response times or\nMaster's qualifications, as biased and largely ineffective. Based on the survey\nresults, we provide recommendations on how future NLP studies may better\naccount for MTurk workers' experiences in order to respect workers' rights and\nimprove data quality.\n","authors":["Olivia Huang","Eve Fleisig","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2311.02802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09481v1","updated":"2023-11-16T00:51:25Z","published":"2023-11-16T00:51:25Z","title":"Personalized Jargon Identification for Enhanced Interdisciplinary\n  Communication","summary":"  Scientific jargon can impede researchers when they read materials from other\ndomains. Current methods of jargon identification mainly use corpus-level\nfamiliarity indicators (e.g., Simple Wikipedia represents plain language).\nHowever, researchers' familiarity of a term can vary greatly based on their own\nbackground. We collect a dataset of over 10K term familiarity annotations from\n11 computer science researchers for terms drawn from 100 paper abstracts.\nAnalysis of this data reveals that jargon familiarity and information needs\nvary widely across annotators, even within the same sub-domain (e.g., NLP). We\ninvestigate features representing individual, sub-domain, and domain knowledge\nto predict individual jargon familiarity. We compare supervised and\nprompt-based approaches, finding that prompt-based methods including personal\npublications yields the highest accuracy, though zero-shot prompting provides a\nstrong baseline. This research offers insight into features and methods to\nintegrate personal data into scientific jargon identification.\n","authors":["Yue Guo","Joseph Chee Chang","Maria Antoniak","Erin Bransom","Trevor Cohen","Lucy Lu Wang","Tal August"],"pdf_url":"https://arxiv.org/pdf/2311.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09480v1","updated":"2023-11-16T00:50:37Z","published":"2023-11-16T00:50:37Z","title":"Show Your Work with Confidence: Confidence Bands for Tuning Curves","summary":"  The choice of hyperparameters greatly impacts performance in natural language\nprocessing. Often, it is hard to tell if a method is better than another or\njust better tuned. Tuning curves fix this ambiguity by accounting for tuning\neffort. Specifically, they plot validation performance as a function of the\nnumber of hyperparameter choices tried so far. While several estimators exist\nfor these curves, it is common to use point estimates, which we show fail\nsilently and give contradictory results when given too little data.\n  Beyond point estimates, confidence bands are necessary to rigorously\nestablish the relationship between different approaches. We present the first\nmethod to construct valid confidence bands for tuning curves. The bands are\nexact, simultaneous, and distribution-free, thus they provide a robust basis\nfor comparing methods.\n  Empirical analysis shows that while bootstrap confidence bands, which serve\nas a baseline, fail to approximate their target confidence, ours achieve it\nexactly. We validate our design with ablations, analyze the effect of sample\nsize, and provide guidance on comparing models with our method. To promote\nconfident comparisons in future work, we release a library implementing the\nmethod at https://github.com/nalourie/opda .\n","authors":["Nicholas Lourie","Kyunghyun Cho","He He"],"pdf_url":"https://arxiv.org/pdf/2311.09480v1.pdf","comment":"15 pages, 15 figures"},{"id":"http://arxiv.org/abs/2311.10236v1","updated":"2023-11-16T23:49:55Z","published":"2023-11-16T23:49:55Z","title":"Latent Feature-based Data Splits to Improve Generalisation Evaluation: A\n  Hate Speech Detection Case Study","summary":"  With the ever-growing presence of social media platforms comes the increased\nspread of harmful content and the need for robust hate speech detection\nsystems. Such systems easily overfit to specific targets and keywords, and\nevaluating them without considering distribution shifts that might occur\nbetween train and test data overestimates their benefit. We challenge hate\nspeech models via new train-test splits of existing datasets that rely on the\nclustering of models' hidden representations. We present two split variants\n(Subset-Sum-Split and Closest-Split) that, when applied to two datasets using\nfour pretrained models, reveal how models catastrophically fail on blind spots\nin the latent space. This result generalises when developing a split with one\nmodel and evaluating it on another. Our analysis suggests that there is no\nclear surface-level property of the data split that correlates with the\ndecreased performance, which underscores that task difficulty is not always\nhumanly interpretable. We recommend incorporating latent feature-based splits\nin model development and release two splits via the GenBench benchmark.\n","authors":["Maike Züfle","Verna Dankers","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2311.10236v1.pdf","comment":"Accepted at the GenBench workshop at EMNLP 2023; 9 pages in the main\n  paper, 5 pages with references and 4 pages with appendices"},{"id":"http://arxiv.org/abs/2311.10227v1","updated":"2023-11-16T22:49:27Z","published":"2023-11-16T22:49:27Z","title":"Think Twice: Perspective-Taking Improves Large Language Models'\n  Theory-of-Mind Capabilities","summary":"  Human interactions are deeply rooted in the interplay of thoughts, beliefs,\nand desires made possible by Theory of Mind (ToM): our cognitive ability to\nunderstand the mental states of ourselves and others. Although ToM may come\nnaturally to us, emulating it presents a challenge to even the most advanced\nLarge Language Models (LLMs). Recent improvements to LLMs' reasoning\ncapabilities from simple yet effective prompting techniques such as\nChain-of-Thought have seen limited applicability to ToM. In this paper, we turn\nto the prominent cognitive science theory \"Simulation Theory\" to bridge this\ngap. We introduce SimToM, a novel two-stage prompting framework inspired by\nSimulation Theory's notion of perspective-taking. To implement this idea on\ncurrent ToM benchmarks, SimToM first filters context based on what the\ncharacter in question knows before answering a question about their mental\nstate. Our approach, which requires no additional training and minimal\nprompt-tuning, shows substantial improvement over existing methods, and our\nanalysis reveals the importance of perspective-taking to Theory-of-Mind\ncapabilities. Our findings suggest perspective-taking as a promising direction\nfor future research into improving LLMs' ToM capabilities.\n","authors":["Alex Wilf","Sihyun Shawn Lee","Paul Pu Liang","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2311.10227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06668v2","updated":"2023-11-16T22:39:00Z","published":"2023-11-11T21:19:44Z","title":"In-context Vectors: Making In Context Learning More Effective and\n  Controllable Through Latent Space Steering","summary":"  Large language models (LLMs) demonstrate emergent in-context learning\ncapabilities, where they adapt to new tasks based on example demonstrations.\nHowever, in-context learning has seen limited effectiveness in many settings,\nis difficult to quantitatively control and takes up context window space. To\novercome these limitations, we propose an alternative approach that recasts\nin-context learning as in-context vectors (ICV). Using ICV has two steps. We\nfirst use a forward pass on demonstration examples to create the in-context\nvector from the latent embedding of the LLM. This vector captures essential\ninformation about the intended task. On a new query, instead of adding\ndemonstrations to the prompt, we shift the latent states of the LLM using the\nICV. The ICV approach has several benefits: 1) it enables the LLM to more\neffectively follow the demonstration examples; 2) it's easy to control by\nadjusting the magnitude of the ICV; 3) it reduces the length of the prompt by\nremoving the in-context demonstrations; 4) ICV is computationally much more\nefficient than fine-tuning. We demonstrate that ICV achieves better performance\ncompared to standard in-context learning and fine-tuning on diverse tasks\nincluding safety, style transfer, role-playing and formatting. Moreover, we\nshow that we can flexibly teach LLM to simultaneously follow different types of\ninstructions by simple vector arithmetics on the corresponding ICVs.\n","authors":["Sheng Liu","Lei Xing","James Zou"],"pdf_url":"https://arxiv.org/pdf/2311.06668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10217v1","updated":"2023-11-16T22:15:15Z","published":"2023-11-16T22:15:15Z","title":"A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal\n  Structures","summary":"  The present paper introduces a novel object of study - a language fractal\nstructure. We hypothesize that a set of embeddings of all $n$-grams of a\nnatural language constitutes a representative sample of this fractal set. (We\nuse the term Hailonakea to refer to the sum total of all language fractal\nstructures, over all $n$). The paper estimates intrinsic (genuine) dimensions\nof language fractal structures for the Russian and English languages. To this\nend, we employ methods based on (1) topological data analysis and (2) a minimum\nspanning tree of a data graph for a cloud of points considered (Steele\ntheorem). For both languages, for all $n$, the intrinsic dimensions appear to\nbe non-integer values (typical for fractal sets), close to 9 for both of the\nRussian and English language.\n","authors":["Vasilii A. Gromov","Nikita S. Borodin","Asel S. Yerbolova"],"pdf_url":"https://arxiv.org/pdf/2311.10217v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2311.10215v1","updated":"2023-11-16T22:11:12Z","published":"2023-11-16T22:11:12Z","title":"Predictive Minds: LLMs As Atypical Active Inference Agents","summary":"  Large language models (LLMs) like GPT are often conceptualized as passive\npredictors, simulators, or even stochastic parrots. We instead conceptualize\nLLMs by drawing on the theory of active inference originating in cognitive\nscience and neuroscience. We examine similarities and differences between\ntraditional active inference systems and LLMs, leading to the conclusion that,\ncurrently, LLMs lack a tight feedback loop between acting in the world and\nperceiving the impacts of their actions, but otherwise fit in the active\ninference paradigm. We list reasons why this loop may soon be closed, and\npossible consequences of this including enhanced model self-awareness and the\ndrive to minimize prediction error by changing the world.\n","authors":["Jan Kulveit","Clem von Stengel","Roman Leventov"],"pdf_url":"https://arxiv.org/pdf/2311.10215v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2303.17491v3","updated":"2023-11-16T20:15:14Z","published":"2023-03-30T16:01:52Z","title":"Language Models can Solve Computer Tasks","summary":"  Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent Recursively Criticizes\nand Improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. We compare multiple LLMs and find that RCI with the\nInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful\nof demonstrations per task rather than tens of thousands, and without a\ntask-specific reward function. Furthermore, we demonstrate RCI prompting's\neffectiveness in enhancing LLMs' reasoning abilities on a suite of natural\nlanguage reasoning tasks, outperforming chain of thought (CoT) prompting with\nexternal feedback. We find that RCI combined with CoT performs better than\neither separately. Our code can be found here:\nhttps://github.com/posgnu/rci-agent.\n","authors":["Geunwoo Kim","Pierre Baldi","Stephen McAleer"],"pdf_url":"https://arxiv.org/pdf/2303.17491v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10181v1","updated":"2023-11-16T20:13:24Z","published":"2023-11-16T20:13:24Z","title":"The Impact of Familiarity on Naming Variation: A Study on Object Naming\n  in Mandarin Chinese","summary":"  Different speakers often produce different names for the same object or\nentity (e.g., \"woman\" vs. \"tourist\" for a female tourist). The reasons behind\nvariation in naming are not well understood. We create a Language and Vision\ndataset for Mandarin Chinese that provides an average of 20 names for 1319\nnaturalistic images, and investigate how familiarity with a given kind of\nobject relates to the degree of naming variation it triggers across subjects.\nWe propose that familiarity influences naming variation in two competing ways:\nincreasing familiarity can either expand vocabulary, leading to higher\nvariation, or promote convergence on conventional names, thereby reducing\nvariation. We find evidence for both factors being at play. Our study\nillustrates how computational resources can be used to address research\nquestions in Cognitive Science.\n","authors":["Yunke He","Xixian Liao","Jialing Liang","Gemma Boleda"],"pdf_url":"https://arxiv.org/pdf/2311.10181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10174v1","updated":"2023-11-16T20:02:44Z","published":"2023-11-16T20:02:44Z","title":"JWSign: A Highly Multilingual Corpus of Bible Translations for more\n  Diversity in Sign Language Processing","summary":"  Advancements in sign language processing have been hindered by a lack of\nsufficient data, impeding progress in recognition, translation, and production\ntasks. The absence of comprehensive sign language datasets across the world's\nsign languages has widened the gap in this field, resulting in a few sign\nlanguages being studied more than others, making this research area extremely\nskewed mostly towards sign languages from high-income countries. In this work\nwe introduce a new large and highly multilingual dataset for sign language\ntranslation: JWSign. The dataset consists of 2,530 hours of Bible translations\nin 98 sign languages, featuring more than 1,500 individual signers. On this\ndataset, we report neural machine translation experiments. Apart from bilingual\nbaseline systems, we also train multilingual systems, including some that take\ninto account the typological relatedness of signed or spoken languages. Our\nexperiments highlight that multilingual systems are superior to bilingual\nbaselines, and that in higher-resource scenarios, clustering language pairs\nthat are related improves translation quality.\n","authors":["Shester Gueuwou","Sophie Siake","Colin Leong","Mathias Müller"],"pdf_url":"https://arxiv.org/pdf/2311.10174v1.pdf","comment":"EMNLP 20223 (Findings)"},{"id":"http://arxiv.org/abs/2311.10537v1","updated":"2023-11-16T11:47:58Z","published":"2023-11-16T11:47:58Z","title":"MedAgents: Large Language Models as Collaborators for Zero-shot Medical\n  Reasoning","summary":"  Large Language Models (LLMs), despite their remarkable progress across\nvarious general domains, encounter significant barriers in medicine and\nhealthcare. This field faces unique challenges such as domain-specific\nterminologies and the reasoning over specialized knowledge. To address these\nobstinate issues, we propose a novel Multi-disciplinary Collaboration (MC)\nframework for the medical domain that leverages role-playing LLM-based agents\nwho participate in a collaborative multi-round discussion, thereby enhancing\nLLM proficiency and reasoning capabilities. This training-free and\ninterpretable framework encompasses five critical steps: gathering domain\nexperts, proposing individual analyses, summarising these analyses into a\nreport, iterating over discussions until a consensus is reached, and ultimately\nmaking a decision. Our work particularly focuses on the zero-shot scenario, our\nresults on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from\nMMLU) establish that our proposed MC framework excels at mining and harnessing\nthe medical expertise in LLMs, as well as extending its reasoning abilities.\nBased on these outcomes, we further conduct a human evaluation to pinpoint and\ncategorize common errors within our method, as well as ablation studies aimed\nat understanding the impact of various factors on overall performance. Our code\ncan be found at \\url{https://github.com/gersteinlab/MedAgents}.\n","authors":["Xiangru Tang","Anni Zou","Zhuosheng Zhang","Yilun Zhao","Xingyao Zhang","Arman Cohan","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2311.10537v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2311.10093v1","updated":"2023-11-16T18:59:51Z","published":"2023-11-16T18:59:51Z","title":"The Chosen One: Consistent Characters in Text-to-Image Diffusion Models","summary":"  Recent advances in text-to-image generation models have unlocked vast\npotential for visual creativity. However, these models struggle with generation\nof consistent characters, a crucial aspect for numerous real-world applications\nsuch as story visualization, game development asset design, advertising, and\nmore. Current methods typically rely on multiple pre-existing images of the\ntarget character or involve labor-intensive manual processes. In this work, we\npropose a fully automated solution for consistent character generation, with\nthe sole input being a text prompt. We introduce an iterative procedure that,\nat each stage, identifies a coherent set of images sharing a similar identity\nand extracts a more consistent identity from this set. Our quantitative\nanalysis demonstrates that our method strikes a better balance between prompt\nalignment and identity consistency compared to the baseline methods, and these\nfindings are reinforced by a user study. To conclude, we showcase several\npractical applications of our approach. Project page is available at\nhttps://omriavrahami.com/the-chosen-one\n","authors":["Omri Avrahami","Amir Hertz","Yael Vinker","Moab Arar","Shlomi Fruchter","Ohad Fried","Daniel Cohen-Or","Dani Lischinski"],"pdf_url":"https://arxiv.org/pdf/2311.10093v1.pdf","comment":"Project page is available at https://omriavrahami.com/the-chosen-one"},{"id":"http://arxiv.org/abs/2311.10092v1","updated":"2023-11-16T18:59:46Z","published":"2023-11-16T18:59:46Z","title":"Traffic Video Object Detection using Motion Prior","summary":"  Traffic videos inherently differ from generic videos in their stationary\ncamera setup, thus providing a strong motion prior where objects often move in\na specific direction over a short time interval. Existing works predominantly\nemploy generic video object detection framework for traffic video object\ndetection, which yield certain advantages such as broad applicability and\nrobustness to diverse scenarios. However, they fail to harness the strength of\nmotion prior to enhance detection accuracy. In this work, we propose two\ninnovative methods to exploit the motion prior and boost the performance of\nboth fully-supervised and semi-supervised traffic video object detection.\nFirstly, we introduce a new self-attention module that leverages the motion\nprior to guide temporal information integration in the fully-supervised\nsetting. Secondly, we utilise the motion prior to develop a pseudo-labelling\nmechanism to eliminate noisy pseudo labels for the semi-supervised setting.\nBoth of our motion-prior-centred methods consistently demonstrates superior\nperformance, outperforming existing state-of-the-art approaches by a margin of\n2% in terms of mAP.\n","authors":["Lihao Liu","Yanqi Cheng","Dongdong Chen","Jing He","Pietro Liò","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2311.10092v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.10091v1","updated":"2023-11-16T18:58:55Z","published":"2023-11-16T18:58:55Z","title":"Adaptive Shells for Efficient Neural Radiance Field Rendering","summary":"  Neural radiance fields achieve unprecedented quality for novel view\nsynthesis, but their volumetric formulation remains expensive, requiring a huge\nnumber of samples to render high-resolution images. Volumetric encodings are\nessential to represent fuzzy geometry such as foliage and hair, and they are\nwell-suited for stochastic optimization. Yet, many scenes ultimately consist\nlargely of solid surfaces which can be accurately rendered by a single sample\nper pixel. Based on this insight, we propose a neural radiance formulation that\nsmoothly transitions between volumetric- and surface-based rendering, greatly\naccelerating rendering speed and even improving visual fidelity. Our method\nconstructs an explicit mesh envelope which spatially bounds a neural volumetric\nrepresentation. In solid regions, the envelope nearly converges to a surface\nand can often be rendered with a single sample. To this end, we generalize the\nNeuS formulation with a learned spatially-varying kernel size which encodes the\nspread of the density, fitting a wide kernel to volume-like regions and a tight\nkernel to surface-like regions. We then extract an explicit mesh of a narrow\nband around the surface, with width determined by the kernel size, and\nfine-tune the radiance field within this band. At inference time, we cast rays\nagainst the mesh and evaluate the radiance field only within the enclosed\nregion, greatly reducing the number of samples required. Experiments show that\nour approach enables efficient rendering at very high fidelity. We also\ndemonstrate that the extracted envelope enables downstream applications such as\nanimation and simulation.\n","authors":["Zian Wang","Tianchang Shen","Merlin Nimier-David","Nicholas Sharp","Jun Gao","Alexander Keller","Sanja Fidler","Thomas Müller","Zan Gojcic"],"pdf_url":"https://arxiv.org/pdf/2311.10091v1.pdf","comment":"SIGGRAPH Asia 2023. Project page:\n  research.nvidia.com/labs/toronto-ai/adaptive-shells/"},{"id":"http://arxiv.org/abs/2311.10089v1","updated":"2023-11-16T18:55:58Z","published":"2023-11-16T18:55:58Z","title":"Emu Edit: Precise Image Editing via Recognition and Generation Tasks","summary":"  Instruction-based image editing holds immense potential for a variety of\napplications, as it enables users to perform any editing operation using a\nnatural language instruction. However, current models in this domain often\nstruggle with accurately executing user instructions. We present Emu Edit, a\nmulti-task image editing model which sets state-of-the-art results in\ninstruction-based image editing. To develop Emu Edit we train it to multi-task\nacross an unprecedented range of tasks, such as region-based editing, free-form\nediting, and Computer Vision tasks, all of which are formulated as generative\ntasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we\nprovide it with learned task embeddings which guide the generation process\ntowards the correct edit type. Both these elements are essential for Emu Edit's\noutstanding performance. Furthermore, we show that Emu Edit can generalize to\nnew tasks, such as image inpainting, super-resolution, and compositions of\nediting tasks, with just a few labeled examples. This capability offers a\nsignificant advantage in scenarios where high-quality samples are scarce.\nLastly, to facilitate a more rigorous and informed assessment of instructable\nimage editing models, we release a new challenging and versatile benchmark that\nincludes seven different image editing tasks.\n","authors":["Shelly Sheynin","Adam Polyak","Uriel Singer","Yuval Kirstain","Amit Zohar","Oron Ashual","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2311.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10081v1","updated":"2023-11-16T18:37:29Z","published":"2023-11-16T18:37:29Z","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback","summary":"  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2311.10081v1.pdf","comment":"The feedback datasets will be released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF"},{"id":"http://arxiv.org/abs/2311.10065v1","updated":"2023-11-16T18:02:10Z","published":"2023-11-16T18:02:10Z","title":"Visual Environment Assessment for Safe Autonomous Quadrotor Landing","summary":"  Autonomous identification and evaluation of safe landing zones are of\nparamount importance for ensuring the safety and effectiveness of aerial robots\nin the event of system failures, low battery, or the successful completion of\nspecific tasks. In this paper, we present a novel approach for detection and\nassessment of potential landing sites for safe quadrotor landing. Our solution\nefficiently integrates 2D and 3D environmental information, eliminating the\nneed for external aids such as GPS and computationally intensive elevation\nmaps. The proposed pipeline combines semantic data derived from a Neural\nNetwork (NN), to extract environmental features, with geometric data obtained\nfrom a disparity map, to extract critical geometric attributes such as slope,\nflatness, and roughness. We define several cost metrics based on these\nattributes to evaluate safety, stability, and suitability of regions in the\nenvironments and identify the most suitable landing area. Our approach runs in\nreal-time on quadrotors equipped with limited computational capabilities.\nExperimental results conducted in diverse environments demonstrate that the\nproposed method can effectively assess and identify suitable landing areas,\nenabling the safe and autonomous landing of a quadrotor.\n","authors":["Mattia Secchiero","Nishanth Bobbili","Yang Zhou","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2311.10065v1.pdf","comment":"7 pages, 5 figures, 1 table, submitted to IEEE International\n  Conference on Robotics and Automation (ICRA), 2023"},{"id":"http://arxiv.org/abs/2311.10064v1","updated":"2023-11-16T18:00:38Z","published":"2023-11-16T18:00:38Z","title":"Analyzing Deviations of Dyadic Lines in Fast Hough Transform","summary":"  Fast Hough transform is a widely used algorithm in pattern recognition. The\nalgorithm relies on approximating lines using a specific discrete line model\ncalled dyadic lines. The worst-case deviation of a dyadic line from the ideal\nline it used to construct grows as $O(log(n))$, where $n$ is the linear size of\nthe image. But few lines actually reach the worst-case bound. The present paper\naddresses a statistical analysis of the deviation of a dyadic line from its\nideal counterpart. Specifically, our findings show that the mean deviation is\nzero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution\nof these (suitably normalized) deviations converges towards a normal\ndistribution with zero mean and a small variance. This limiting result makes an\nessential use of ergodic theory.\n","authors":["Gleb Smirnov","Simon Karpenko"],"pdf_url":"https://arxiv.org/pdf/2311.10064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06978v2","updated":"2023-11-16T17:59:20Z","published":"2023-09-13T14:13:08Z","title":"Differentiable JPEG: The Devil is in the Details","summary":"  JPEG remains one of the most widespread lossy image coding methods. However,\nthe non-differentiable nature of JPEG restricts the application in deep\nlearning pipelines. Several differentiable approximations of JPEG have recently\nbeen proposed to address this issue. This paper conducts a comprehensive review\nof existing diff. JPEG approaches and identifies critical details that have\nbeen missed by previous methods. To this end, we propose a novel diff. JPEG\napproach, overcoming previous limitations. Our approach is differentiable\nw.r.t. the input image, the JPEG quality, the quantization tables, and the\ncolor conversion parameters. We evaluate the forward and backward performance\nof our diff. JPEG approach against existing methods. Additionally, extensive\nablations are performed to evaluate crucial design choices. Our proposed diff.\nJPEG resembles the (non-diff.) reference implementation best, significantly\nsurpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. For\nstrong compression rates, we can even improve PSNR by $9.51$dB. Strong\nadversarial attack results are yielded by our diff. JPEG, demonstrating the\neffective gradient approximation. Our code is available at\nhttps://github.com/necla-ml/Diff-JPEG.\n","authors":["Christoph Reich","Biplob Debnath","Deep Patel","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2309.06978v2.pdf","comment":"Accepted at WACV 2024. Project page:\n  https://christophreich1996.github.io/differentiable_jpeg/"},{"id":"http://arxiv.org/abs/2311.05237v2","updated":"2023-11-16T17:46:58Z","published":"2023-11-09T09:39:12Z","title":"Widely Applicable Strong Baseline for Sports Ball Detection and Tracking","summary":"  In this work, we present a novel Sports Ball Detection and Tracking (SBDT)\nmethod that can be applied to various sports categories. Our approach is\ncomposed of (1) high-resolution feature extraction, (2) position-aware model\ntraining, and (3) inference considering temporal consistency, all of which are\nput together as a new SBDT baseline. Besides, to validate the\nwide-applicability of our approach, we compare our baseline with 6\nstate-of-the-art SBDT methods on 5 datasets from different sports categories.\nWe achieve this by newly introducing two SBDT datasets, providing new ball\nannotations for two datasets, and re-implementing all the methods to ease\nextensive comparison. Experimental results demonstrate that our approach is\nsubstantially superior to existing methods on all the sports categories covered\nby the datasets. We believe our proposed method can play as a Widely Applicable\nStrong Baseline (WASB) of SBDT, and our datasets and codebase will promote\nfuture SBDT research. Datasets and codes are available at\nhttps://github.com/nttcom/WASB-SBDT .\n","authors":["Shuhei Tarashima","Muhammad Abdul Haq","Yushan Wang","Norio Tagawa"],"pdf_url":"https://arxiv.org/pdf/2311.05237v2.pdf","comment":"BMVC2023. Code & dataset : https://github.com/nttcom/WASB-SBDT"},{"id":"http://arxiv.org/abs/2311.10042v1","updated":"2023-11-16T17:38:21Z","published":"2023-11-16T17:38:21Z","title":"Depth Insight -- Contribution of Different Features to Indoor\n  Single-image Depth Estimation","summary":"  Depth estimation from a single image is a challenging problem in computer\nvision because binocular disparity or motion information is absent. Whereas\nimpressive performances have been reported in this area recently using\nend-to-end trained deep neural architectures, as to what cues in the images\nthat are being exploited by these black box systems is hard to know. To this\nend, in this work, we quantify the relative contributions of the known cues of\ndepth in a monocular depth estimation setting using an indoor scene data set.\nOur work uses feature extraction techniques to relate the single features of\nshape, texture, colour and saturation, taken in isolation, to predict depth. We\nfind that the shape of objects extracted by edge detection substantially\ncontributes more than others in the indoor setting considered, while the other\nfeatures also have contributions in varying degrees. These insights will help\noptimise depth estimation models, boosting their accuracy and robustness. They\npromise to broaden the practical applications of vision-based depth estimation.\nThe project code is attached to the supplementary material and will be\npublished on GitHub.\n","authors":["Yihong Wu","Yuwen Heng","Mahesan Niranjan","Hansung Kim"],"pdf_url":"https://arxiv.org/pdf/2311.10042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10034v1","updated":"2023-11-16T17:32:58Z","published":"2023-11-16T17:32:58Z","title":"Match and Locate: low-frequency monocular odometry based on deep feature\n  matching","summary":"  Accurate and robust pose estimation plays a crucial role in many robotic\nsystems. Popular algorithms for pose estimation typically rely on high-fidelity\nand high-frequency signals from various sensors. Inclusion of these sensors\nmakes the system less affordable and much more complicated. In this work we\nintroduce a novel approach for the robotic odometry which only requires a\nsingle camera and, importantly, can produce reliable estimates given even\nextremely low-frequency signal of around one frame per second. The approach is\nbased on matching image features between the consecutive frames of the video\nstream using deep feature matching models. The resulting coarse estimate is\nthen adjusted by a convolutional neural network, which is also responsible for\nestimating the scale of the transition, otherwise irretrievable using only the\nfeature matching information. We evaluate the performance of the approach in\nthe AISG-SLA Visual Localisation Challenge and find that while being\ncomputationally efficient and easy to implement our method shows competitive\nresults with only around $3^{\\circ}$ of orientation estimation error and $2m$\nof translation estimation error taking the third place in the challenge.\n","authors":["Stepan Konev","Yuriy Biktairov"],"pdf_url":"https://arxiv.org/pdf/2311.10034v1.pdf","comment":"3 pages 1 figure"},{"id":"http://arxiv.org/abs/2309.02340v2","updated":"2023-11-16T17:31:07Z","published":"2023-09-05T15:57:23Z","title":"Generating Infinite-Resolution Texture using GANs with Patch-by-Patch\n  Paradigm","summary":"  In this paper, we introduce a novel approach for generating texture images of\ninfinite resolutions using Generative Adversarial Networks (GANs) based on a\npatch-by-patch paradigm. Existing texture synthesis techniques often rely on\ngenerating a large-scale texture using a one-forward pass to the generating\nmodel, this limits the scalability and flexibility of the generated images. In\ncontrast, the proposed approach trains GANs models on a single texture image to\ngenerate relatively small patches that are locally correlated and can be\nseamlessly concatenated to form a larger image while using a constant GPU\nmemory footprint. Our method learns the local texture structure and is able to\ngenerate arbitrary-size textures, while also maintaining coherence and\ndiversity. The proposed method relies on local padding in the generator to\nensure consistency between patches and utilizes spatial stochastic modulation\nto allow for local variations and diversity within the large-scale image.\nExperimental results demonstrate superior scalability compared to existing\napproaches while maintaining visual coherence of generated textures.\n","authors":["Alhasan Abdellatif","Ahmed H. Elsheikh"],"pdf_url":"https://arxiv.org/pdf/2309.02340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10018v1","updated":"2023-11-16T17:02:34Z","published":"2023-11-16T17:02:34Z","title":"On the Overconfidence Problem in Semantic 3D Mapping","summary":"  Semantic 3D mapping, the process of fusing depth and image segmentation\ninformation between multiple views to build 3D maps annotated with object\nclasses in real-time, is a recent topic of interest. This paper highlights the\nfusion overconfidence problem, in which conventional mapping methods assign\nhigh confidence to the entire map even when they are incorrect, leading to\nmiscalibrated outputs. Several methods to improve uncertainty calibration at\ndifferent stages in the fusion pipeline are presented and compared on the\nScanNet dataset. We show that the most widely used Bayesian fusion strategy is\namong the worst calibrated, and propose a learned pipeline that combines fusion\nand calibration, GLFS, which achieves simultaneously higher accuracy and 3D map\ncalibration while retaining real-time capability. We further illustrate the\nimportance of map calibration on a downstream task by showing that\nincorporating proper semantic fusion on a modular ObjectNav agent improves its\nsuccess rates. Our code will be provided on Github for reproducibility upon\nacceptance.\n","authors":["Joao Marcos Correia Marques","Albert Zhai","Shenlong Wang","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2311.10018v1.pdf","comment":"This is a preprint for the work submitted to the ICRA 2024 conference"},{"id":"http://arxiv.org/abs/2311.10011v1","updated":"2023-11-16T16:50:56Z","published":"2023-11-16T16:50:56Z","title":"SQLNet: Scale-Modulated Query and Localization Network for Few-Shot\n  Class-Agnostic Counting","summary":"  The class-agnostic counting (CAC) task has recently been proposed to solve\nthe problem of counting all objects of an arbitrary class with several\nexemplars given in the input image. To address this challenging task, existing\nleading methods all resort to density map regression, which renders them\nimpractical for downstream tasks that require object locations and restricts\ntheir ability to well explore the scale information of exemplars for\nsupervision. To address the limitations, we propose a novel localization-based\nCAC approach, termed Scale-modulated Query and Localization Network (SQLNet).\nIt fully explores the scales of exemplars in both the query and localization\nstages and achieves effective counting by accurately locating each object and\npredicting its approximate size. Specifically, during the query stage, rich\ndiscriminative representations of the target class are acquired by the\nHierarchical Exemplars Collaborative Enhancement (HECE) module from the few\nexemplars through multi-scale exemplar cooperation with equifrequent size\nprompt embedding. These representations are then fed into the Exemplars-Unified\nQuery Correlation (EUQC) module to interact with the query features in a\nunified manner and produce the correlated query tensor. In the localization\nstage, the Scale-aware Multi-head Localization (SAML) module utilizes the query\ntensor to predict the confidence, location, and size of each potential object.\nMoreover, a scale-aware localization loss is introduced, which exploits\nflexible location associations and exemplar scales for supervision to optimize\nthe model performance. Extensive experiments demonstrate that SQLNet\noutperforms state-of-the-art methods on popular CAC benchmarks, achieving\nexcellent performance not only in counting accuracy but also in localization\nand bounding box generation. Our codes will be available at\nhttps://github.com/HCPLab-SYSU/SQLNet\n","authors":["Hefeng Wu","Yandong Chen","Lingbo Liu","Tianshui Chen","Keze Wang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2311.10011v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2305.15270v3","updated":"2023-11-16T16:45:45Z","published":"2023-05-24T15:56:26Z","title":"Reversible Graph Neural Network-based Reaction Distribution Learning for\n  Multiple Appropriate Facial Reactions Generation","summary":"  Generating facial reactions in a human-human dyadic interaction is complex\nand highly dependent on the context since more than one facial reactions can be\nappropriate for the speaker's behaviour. This has challenged existing machine\nlearning (ML) methods, whose training strategies enforce models to reproduce a\nspecific (not multiple) facial reaction from each input speaker behaviour. This\npaper proposes the first multiple appropriate facial reaction generation\nframework that re-formulates the one-to-many mapping facial reaction generation\nproblem as a one-to-one mapping problem. This means that we approach this\nproblem by considering the generation of a distribution of the listener's\nappropriate facial reactions instead of multiple different appropriate facial\nreactions, i.e., 'many' appropriate facial reaction labels are summarised as\n'one' distribution label during training. Our model consists of a perceptual\nprocessor, a cognitive processor, and a motor processor. The motor processor is\nimplemented with a novel Reversible Multi-dimensional Edge Graph Neural Network\n(REGNN). This allows us to obtain a distribution of appropriate real facial\nreactions during the training process, enabling the cognitive processor to be\ntrained to predict the appropriate facial reaction distribution. At the\ninference stage, the REGNN decodes an appropriate facial reaction by using this\ndistribution as input. Experimental results demonstrate that our approach\noutperforms existing models in generating more appropriate, realistic, and\nsynchronized facial reactions. The improved performance is largely attributed\nto the proposed appropriate facial reaction distribution learning strategy and\nthe use of a REGNN. The code is available at\nhttps://github.com/TongXu-05/REGNN-Multiple-Appropriate-Facial-Reaction-Generation.\n","authors":["Tong Xu","Micol Spitale","Hao Tang","Lu Liu","Hatice Gunes","Siyang Song"],"pdf_url":"https://arxiv.org/pdf/2305.15270v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05400v2","updated":"2023-11-16T16:37:06Z","published":"2023-05-09T12:45:43Z","title":"Investigating the Corruption Robustness of Image Classifiers with Random\n  Lp-norm Corruptions","summary":"  Robustness is a fundamental property of machine learning classifiers to\nachieve safety and reliability. In the fields of adversarial robustness and\nformal robustness verification of image classification models, robustness is\ncommonly defined as the stability to all input variations within an Lp-norm\ndistance. However, robustness to random corruptions is usually improved and\nevaluated using variations observed in the real-world, while mathematically\ndefined Lp-norm corruptions are rarely considered. This study investigates the\nuse of random Lp-norm corruptions to augment the training and test data of\nimage classifiers. We adapt an approach from the field of adversarial\nrobustness to assess the model robustness to imperceptible random corruptions.\nWe empirically and theoretically investigate whether robustness is transferable\nacross different Lp-norms and derive conclusions on which Lp-norm corruptions a\nmodel should be trained and evaluated on. We find that training data\naugmentation with L0-norm corruptions improves corruption robustness while\nmaintaining accuracy compared to standard training and when applied on top of\nselected state-of-the-art data augmentation techniques.\n","authors":["Georg Siedel","Weijia Shao","Silvia Vock","Andrey Morozov"],"pdf_url":"https://arxiv.org/pdf/2305.05400v2.pdf","comment":"Preprint submitted to VISAPP 2024"},{"id":"http://arxiv.org/abs/2311.09999v1","updated":"2023-11-16T16:23:11Z","published":"2023-11-16T16:23:11Z","title":"TransFusion -- A Transparency-Based Diffusion Model for Anomaly\n  Detection","summary":"  Surface anomaly detection is a vital component in manufacturing inspection.\nReconstructive anomaly detection methods restore the normal appearance of an\nobject, ideally modifying only the anomalous regions. Due to the limitations of\ncommonly used reconstruction architectures, the produced reconstructions are\noften poor and either still contain anomalies or lack details in anomaly-free\nregions. Recent reconstructive methods adopt diffusion models, however with the\nstandard diffusion process the problems are not adequately addressed. We\npropose a novel transparency-based diffusion process, where the transparency of\nanomalous regions is progressively increased, restoring their normal appearance\naccurately and maintaining the appearance of anomaly-free regions without loss\nof detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative\nanomaly detection method that implements the proposed diffusion process,\nenabling accurate downstream anomaly detection. TransFusion achieves\nstate-of-the-art performance on both the VisA and the MVTec AD datasets, with\nan image-level AUROC of 98.5% and 99.2%, respectively.\n","authors":["Matic Fučka","Vitjan Zavrtanik","Danijel Skočaj"],"pdf_url":"https://arxiv.org/pdf/2311.09999v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09998v1","updated":"2023-11-16T16:14:58Z","published":"2023-11-16T16:14:58Z","title":"DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's\n  Distance","summary":"  The Earth Mover's Distance (EMD) is the measure of choice between point\nclouds. However the computational cost to compute it makes it prohibitive as a\ntraining loss, and the standard approach is to use a surrogate such as the\nChamfer distance. We propose an attention-based model to compute an accurate\napproximation of the EMD that can be used as a training loss for generative\nmodels. To get the necessary accurate estimation of the gradients we train our\nmodel to explicitly compute the matching between point clouds instead of EMD\nitself. We cast this new objective as the estimation of an attention matrix\nthat approximates the ground truth matching matrix. Experiments show that this\nmodel provides an accurate estimate of the EMD and its gradient with a wall\nclock speed-up of more than two orders of magnitude with respect to the exact\nHungarian matching algorithm and one order of magnitude with respect to the\nstandard approximate Sinkhorn algorithm, allowing in particular to train a\npoint cloud VAE with the EMD itself. Extensive evaluation show the remarkable\nbehaviour of this model when operating out-of-distribution, a key requirement\nfor a distance surrogate. Finally, the model generalizes very well to point\nclouds during inference several times larger than during training.\n","authors":["Atul Kumar Sinha","Francois Fleuret"],"pdf_url":"https://arxiv.org/pdf/2311.09998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16872v3","updated":"2023-11-16T16:12:46Z","published":"2023-10-25T16:42:26Z","title":"SonoSAMTrack -- Segment and Track Anything on Ultrasound Images","summary":"  In this paper, we present SonoSAMTrack - that combines a promptable\nfoundational model for segmenting objects of interest on ultrasound images\ncalled SonoSAM, with a state-of-the art contour tracking model to propagate\nsegmentations on 2D+t and 3D ultrasound datasets. Fine-tuned and tested\nexclusively on a rich, diverse set of objects from $\\approx200$k ultrasound\nimage-mask pairs, SonoSAM demonstrates state-of-the-art performance on 7 unseen\nultrasound data-sets, outperforming competing methods by a significant margin.\nWe also extend SonoSAM to 2-D +t applications and demonstrate superior\nperformance making it a valuable tool for generating dense annotations and\nsegmentation of anatomical structures in clinical workflows. Further, to\nincrease practical utility of the work, we propose a two-step process of\nfine-tuning followed by knowledge distillation to a smaller footprint model\nwithout comprising the performance. We present detailed qualitative and\nquantitative comparisons of SonoSAM with state-of-the-art methods showcasing\nefficacy of the method. This is followed by demonstrating the reduction in\nnumber of clicks in a dense video annotation problem of adult cardiac\nultrasound chamber segmentation using SonoSAMTrack.\n","authors":["Hariharan Ravishankar","Rohan Patil","Vikram Melapudi","Harsh Suthar","Stephan Anzengruber","Parminder Bhatia","Kass-Hout Taha","Pavan Annangi"],"pdf_url":"https://arxiv.org/pdf/2310.16872v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09974v1","updated":"2023-11-16T15:47:49Z","published":"2023-11-16T15:47:49Z","title":"From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning","summary":"  In recent years, self-supervised contrastive learning has emerged as a\ndistinguished paradigm in the artificial intelligence landscape. It facilitates\nunsupervised feature learning through contrastive delineations at the instance\nlevel. However, crafting an effective self-supervised paradigm remains a\npivotal challenge within this field. This paper delves into two crucial factors\nimpacting self-supervised contrastive learning-bach size and pretext tasks, and\nfrom a data processing standpoint, proposes an adaptive technique of batch\nfusion. The proposed method, via dimensionality reduction and reconstruction of\nbatch data, enables formerly isolated individual data to partake in intra-batch\ncommunication through the Embedding Layer. Moreover, it adaptively amplifies\nthe self-supervised feature encoding capability as the training progresses. We\nconducted a linear classification test of this method based on the classic\ncontrastive learning framework on ImageNet-1k. The empirical findings\nillustrate that our approach achieves state-of-the-art performance under\nequitable comparisons. Benefiting from its \"plug-and-play\" characteristics, we\nfurther explored other contrastive learning methods. On the ImageNet-100,\ncompared to the original performance, the top1 has seen a maximum increase of\n1.25%. We suggest that the proposed method may contribute to the advancement of\ndata-driven self-supervised learning research, bringing a fresh perspective to\nthis community.\n","authors":["Jiansong Zhang","Peizhong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09974v1.pdf","comment":"9 pages, 3 figures, the code of this paper is releasing soon"},{"id":"http://arxiv.org/abs/2311.08870v2","updated":"2023-11-16T15:43:52Z","published":"2023-11-15T11:11:25Z","title":"One-Shot Federated Learning with Classifier-Guided Diffusion Models","summary":"  One-shot federated learning (OSFL) has gained attention in recent years due\nto its low communication cost. However, most of the existing methods require\nauxiliary datasets or training generators, which hinders their practicality in\nreal-world scenarios. In this paper, we explore the novel opportunities that\ndiffusion models bring to OSFL and propose FedCADO, utilizing guidance from\nclient classifiers to generate data that complies with clients' distributions\nand subsequently training the aggregated model on the server. Specifically, our\nmethod involves targeted optimizations in two aspects. On one hand, we\nconditionally edit the randomly sampled initial noises, embedding them with\nspecified semantics and distributions, resulting in a significant improvement\nin both the quality and stability of generation. On the other hand, we employ\nthe BN statistics from the classifiers to provide detailed guidance during\ngeneration. These tailored optimizations enable us to limitlessly generate\ndatasets, which closely resemble the distribution and quality of the original\nclient dataset. Our method effectively handles the heterogeneous client models\nand the problems of non-IID features or labels. In terms of privacy protection,\nour method avoids training any generator or transferring any auxiliary\ninformation on clients, eliminating any additional privacy leakage risks.\nLeveraging the extensive knowledge stored in the pre-trained diffusion model,\nthe synthetic datasets can assist us in surpassing the knowledge limitations of\nthe client samples, resulting in aggregation models that even outperform the\nperformance ceiling of centralized training in some cases, which is\nconvincingly demonstrated in the sufficient quantification and visualization\nexperiments conducted on three large-scale multi-domain image datasets.\n","authors":["Mingzhao Yang","Shangchao Su","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2311.08870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09965v1","updated":"2023-11-16T15:39:01Z","published":"2023-11-16T15:39:01Z","title":"SurgPLAN: Surgical Phase Localization Network for Phase Recognition","summary":"  Surgical phase recognition is crucial to providing surgery understanding in\nsmart operating rooms. Despite great progress in automatic surgical phase\nrecognition, most existing methods are still restricted by two problems. First,\nthese methods cannot capture discriminative visual features for each frame and\nmotion information with simple 2D networks. Second, the frame-by-frame\nrecognition paradigm degrades the performance due to unstable predictions\nwithin each phase, termed as phase shaking. To address these two challenges, we\npropose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a\nmore accurate and stable surgical phase recognition with the principle of\ntemporal detection. Specifically, we first devise a Pyramid SlowFast (PSF)\narchitecture to serve as the visual backbone to capture multi-scale spatial and\ntemporal features by two branches with different frame sampling rates.\nMoreover, we propose a Temporal Phase Localization (TPL) module to generate the\nphase prediction based on temporal region proposals, which ensures accurate and\nconsistent predictions within each surgical phase. Extensive experiments\nconfirm the significant advantages of our SurgPLAN over frame-by-frame\napproaches in terms of both accuracy and stability.\n","authors":["Xingjian Luo","You Pang","Zhen Chen","Jinlin Wu","Zongmin Zhang","Zhen Lei","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09958v1","updated":"2023-11-16T15:29:21Z","published":"2023-11-16T15:29:21Z","title":"VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model","summary":"  Vertebral detection and segmentation are critical steps for treatment\nplanning in spine surgery and radiation therapy. Accurate identification and\nsegmentation are complicated in imaging that does not include the full spine,\nin cases with variations in anatomy (T13 and/or L6 vertebrae), and in the\npresence of fracture or hardware. This paper proposes VertDetect, a fully\nautomated end-to-end 3D vertebral instance segmentation Convolutional Neural\nNetwork (CNN) model to predict vertebral level labels and segmentations for all\nvertebrae present in a CT scan. The utilization of a shared CNN backbone\nprovides the detection and segmentation branches of the network with feature\nmaps containing both spinal and vertebral level information. A Graph\nConvolutional Network (GCN) layer is used to improve vertebral labelling by\nusing the known structure of the spine. This model achieved a Dice Similarity\nCoefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI,\n0.835-0.909) in the VerSe 2019 and 0.868 (95\\% CI, 0.834-0.890) and 0.869 (95\\%\nCI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively.\nThis model achieved state-of-the-art performance for an end-to-end\narchitecture, whose design facilitates the extraction of features that can be\nsubsequently used for downstream tasks.\n","authors":["Geoff Klein","Michael Hardisty","Cari Whyne","Anne L. Martel"],"pdf_url":"https://arxiv.org/pdf/2311.09958v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09952v1","updated":"2023-11-16T15:15:15Z","published":"2023-11-16T15:15:15Z","title":"Score-based generative models learn manifold-like structures with\n  constrained mixing","summary":"  How do score-based generative models (SBMs) learn the data distribution\nsupported on a low-dimensional manifold? We investigate the score model of a\ntrained SBM through its linear approximations and subspaces spanned by local\nfeature vectors. During diffusion as the noise decreases, the local\ndimensionality increases and becomes more varied between different sample\nsequences. Importantly, we find that the learned vector field mixes samples by\na non-conservative field within the manifold, although it denoises with normal\nprojections as if there is an energy function in off-manifold directions. At\neach noise level, the subspace spanned by the local features overlap with an\neffective density function. These observations suggest that SBMs can flexibly\nmix samples with the learned score field while carefully maintaining a\nmanifold-like structure of the data distribution.\n","authors":["Li Kevin Wenliang","Ben Moran"],"pdf_url":"https://arxiv.org/pdf/2311.09952v1.pdf","comment":"NeurIPS 2022 Workshop on Score-Based Methods"},{"id":"http://arxiv.org/abs/2306.12983v2","updated":"2023-11-16T14:57:58Z","published":"2023-06-22T15:41:15Z","title":"Towards More Realistic Membership Inference Attacks on Large Diffusion\n  Models","summary":"  Generative diffusion models, including Stable Diffusion and Midjourney, can\ngenerate visually appealing, diverse, and high-resolution images for various\napplications. These models are trained on billions of internet-sourced images,\nraising significant concerns about the potential unauthorized use of\ncopyright-protected images. In this paper, we examine whether it is possible to\ndetermine if a specific image was used in the training set, a problem known in\nthe cybersecurity community and referred to as a membership inference attack.\nOur focus is on Stable Diffusion, and we address the challenge of designing a\nfair evaluation framework to answer this membership question. We propose a\nmethodology to establish a fair evaluation setup and apply it to Stable\nDiffusion, enabling potential extensions to other generative models. Utilizing\nthis evaluation setup, we execute membership attacks (both known and newly\nintroduced). Our research reveals that previously proposed evaluation setups do\nnot provide a full understanding of the effectiveness of membership inference\nattacks. We conclude that the membership inference attack remains a significant\nchallenge for large diffusion models (often deployed as black-box systems),\nindicating that related privacy and copyright issues will persist in the\nforeseeable future.\n","authors":["Jan Dubiński","Antoni Kowalczuk","Stanisław Pawlak","Przemysław Rokita","Tomasz Trzciński","Paweł Morawiecki"],"pdf_url":"https://arxiv.org/pdf/2306.12983v2.pdf","comment":"Accepted at WACV2024"},{"id":"http://arxiv.org/abs/2311.07370v2","updated":"2023-11-16T14:55:15Z","published":"2023-11-13T14:36:29Z","title":"Classification of developmental and brain disorders via graph\n  convolutional aggregation","summary":"  While graph convolution based methods have become the de-facto standard for\ngraph representation learning, their applications to disease prediction tasks\nremain quite limited, particularly in the classification of neurodevelopmental\nand neurodegenerative brain disorders. In this paper, we introduce an\naggregator normalization graph convolutional network by leveraging aggregation\nin graph sampling, as well as skip connections and identity mapping. The\nproposed model learns discriminative graph node representations by\nincorporating both imaging and non-imaging features into the graph nodes and\nedges, respectively, with the aim of augmenting predictive capabilities and\nproviding a holistic perspective on the underlying mechanisms of brain\ndisorders. Skip connections enable the direct flow of information from the\ninput features to later layers of the network, while identity mapping helps\nmaintain the structural information of the graph during feature learning. We\nbenchmark our model against several recent baseline methods on two large\ndatasets, Autism Brain Imaging Data Exchange (ABIDE) and Alzheimer's Disease\nNeuroimaging Initiative (ADNI), for the prediction of autism spectrum disorder\nand Alzheimer's disease, respectively. Experimental results demonstrate the\ncompetitive performance of our approach in comparison with recent baselines in\nterms of several evaluation metrics, achieving relative improvements of 50% and\n13.56% in classification accuracy over graph convolutional networks on ABIDE\nand ADNI, respectively.\n","authors":["Ibrahim Salim","A. Ben Hamza"],"pdf_url":"https://arxiv.org/pdf/2311.07370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09942v1","updated":"2023-11-16T14:50:42Z","published":"2023-11-16T14:50:42Z","title":"Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection","summary":"  This paper discusses the role of Transfer Learning (TL) and transformers in\ncancer detection based on image analysis. With the enormous evolution of cancer\npatients, the identification of cancer cells in a patient's body has emerged as\na trend in the field of Artificial Intelligence (AI). This process involves\nanalyzing medical images, such as Computed Tomography (CT) scans and Magnetic\nResonance Imaging (MRIs), to identify abnormal growths that may help in cancer\ndetection. Many techniques and methods have been realized to improve the\nquality and performance of cancer classification and detection, such as TL,\nwhich allows the transfer of knowledge from one task to another with the same\ntask or domain. TL englobes many methods, particularly those used in image\nanalysis, such as transformers and Convolutional Neural Network (CNN) models\ntrained on the ImageNet dataset. This paper analyzes and criticizes each method\nof TL based on image analysis and compares the results of each method, showing\nthat transformers have achieved the best results with an accuracy of 97.41% for\ncolon cancer detection and 94.71% for Histopathological Lung cancer. Future\ndirections for cancer detection based on image analysis are also discussed.\n","authors":["Amine Bechar","Youssef Elmir","Rafik Medjoudj","Yassine Himeur","Abbes Amira"],"pdf_url":"https://arxiv.org/pdf/2311.09942v1.pdf","comment":"6 pages, 4 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2311.09939v1","updated":"2023-11-16T14:43:45Z","published":"2023-11-16T14:43:45Z","title":"RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection","summary":"  Online misinformation is often multimodal in nature, i.e., it is caused by\nmisleading associations between texts and accompanying images. To support the\nfact-checking process, researchers have been recently developing automatic\nmultimodal methods that gather and analyze external information, evidence,\nrelated to the image-text pairs under examination. However, prior works assumed\nall collected evidence to be relevant. In this study, we introduce a \"Relevant\nEvidence Detection\" (RED) module to discern whether each piece of evidence is\nrelevant, to support or refute the claim. Specifically, we develop the\n\"Relevant Evidence Detection Directed Transformer\" (RED-DOT) and explore\nmultiple architectural variants (e.g., single or dual-stage) and mechanisms\n(e.g., \"guided attention\"). Extensive ablation and comparative experiments\ndemonstrate that RED-DOT achieves significant improvements over the\nstate-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our\nevidence re-ranking and element-wise modality fusion led to RED-DOT achieving\ncompetitive and even improved performance on NewsCLIPings+, without the need\nfor numerous evidence or multiple backbone encoders. Finally, our qualitative\nanalysis demonstrates that the proposed \"guided attention\" module has the\npotential to enhance the architecture's interpretability. We release our code\nat: https://github.com/stevejpapad/relevant-evidence-detection\n","authors":["Stefanos-Iordanis Papadopoulos","Christos Koutlis","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2311.09939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09919v1","updated":"2023-11-16T14:18:10Z","published":"2023-11-16T14:18:10Z","title":"DSR-Diff: Depth Map Super-Resolution with Diffusion Model","summary":"  Color-guided depth map super-resolution (CDSR) improve the spatial resolution\nof a low-quality depth map with the corresponding high-quality color map,\nbenefiting various applications such as 3D reconstruction, virtual reality, and\naugmented reality. While conventional CDSR methods typically rely on\nconvolutional neural networks or transformers, diffusion models (DMs) have\ndemonstrated notable effectiveness in high-level vision tasks. In this work, we\npresent a novel CDSR paradigm that utilizes a diffusion model within the latent\nspace to generate guidance for depth map super-resolution. The proposed method\ncomprises a guidance generation network (GGN), a depth map super-resolution\nnetwork (DSRN), and a guidance recovery network (GRN). The GGN is specifically\ndesigned to generate the guidance while managing its compactness. Additionally,\nwe integrate a simple but effective feature fusion module and a\ntransformer-style feature extraction module into the DSRN, enabling it to\nleverage guided priors in the extraction, fusion, and reconstruction of\nmulti-model images. Taking into account both accuracy and efficiency, our\nproposed method has shown superior performance in extensive experiments when\ncompared to state-of-the-art methods. Our codes will be made available at\nhttps://github.com/shiyuan7/DSR-Diff.\n","authors":["Yuan Shi","Bin Xia","Rui Zhu","Qingmin Liao","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2311.09919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09902v1","updated":"2023-11-16T13:54:38Z","published":"2023-11-16T13:54:38Z","title":"Selection of Distinct Morphologies to Divide & Conquer Gigapixel\n  Pathology Images","summary":"  Whole slide images (WSIs) are massive digital pathology files illustrating\nintricate tissue structures. Selecting a small, representative subset of\npatches from each WSI is essential yet challenging. Therefore, following the\n\"Divide & Conquer\" approach becomes essential to facilitate WSI analysis\nincluding the classification and the WSI matching in computational pathology.\nTo this end, we propose a novel method termed \"Selection of Distinct\nMorphologies\" (SDM) to choose a subset of WSI patches. The aim is to encompass\nall inherent morphological variations within a given WSI while simultaneously\nminimizing the number of selected patches to represent these variations,\nensuring a compact yet comprehensive set of patches. This systematically\ncurated patch set forms what we term a \"montage\". We assess the\nrepresentativeness of the SDM montage across various public and private\nhistopathology datasets. This is conducted by using the leave-one-out WSI\nsearch and matching evaluation method, comparing it with the state-of-the-art\nYottixel's mosaic. SDM demonstrates remarkable efficacy across all datasets\nduring its evaluation. Furthermore, SDM eliminates the necessity for empirical\nparameterization, a crucial aspect of Yottixel's mosaic, by inherently\noptimizing the selection process to capture the distinct morphological features\nwithin the WSI.\n","authors":["Abubakr Shafique","Saghir Alfasly","Areej Alsaafin","Peyman Nejat","Jibran A. Khan","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2311.09902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12327v2","updated":"2023-11-16T13:06:47Z","published":"2023-07-23T13:50:41Z","title":"End-to-end Hyperspectral Image Change Detection Network Based on Band\n  Selection","summary":"  For hyperspectral image change detection (HSI-CD), one key challenge is to\nreduce band redundancy, as only a few bands are crucial for change detection\nwhile other bands may be adverse to it. However, most existing HSI-CD methods\ndirectly extract change feature from full-dimensional HSIs, suffering from a\ndegradation of feature discrimination. To address this issue, we propose an\nend-to-end hyperspectral image change detection network with band selection\n(ECDBS), which effectively retains the critical bands to promote change\ndetection. The main ingredients of the network are a deep learning based band\nselection module and cascading band-specific spatial attention (BSA) blocks.\nThe band selection module can be seamlessly integrated with subsequent CD\nmodels for joint optimization and end-to-end reasoning, rather than as a step\nseparate from change detection. The BSA block extracts features from each band\nusing a tailored strategy. Unlike the typically used feature extraction\nstrategy that uniformly processes all bands, the BSA blocks considers the\ndifferences in feature distributions among widely spaced bands, thereupon\nextracting more sufficient change feature. Experimental evaluations conducted\non three widely used HSI-CD datasets demonstrate the effectiveness and\nsuperiority of our proposed method over other state-of-the-art techniques.\n","authors":["Qingren Yao","Yuan Zhou","Chang Tang","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2307.12327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02463v5","updated":"2023-11-16T12:38:46Z","published":"2023-08-04T17:00:38Z","title":"Towards Generalist Foundation Model for Radiology by Leveraging\n  Web-scale 2D&3D Medical Data","summary":"  In this study, we aim to initiate the development of Radiology Foundation\nModel, termed as RadFM. We consider the construction of foundational models\nfrom three perspectives, namely, dataset construction, model design, and\nthorough evaluation. Our contribution can be concluded as follows: (i), we\nconstruct a large-scale Medical Multi-modal Dataset, MedMD, which consists of\n16M 2D and 3D medical scans with high-quality text descriptions or reports\nacross various data formats, modalities, and tasks, covering over 5000 distinct\ndiseases. To the best of our knowledge, this is the first large-scale,\nhigh-quality, medical visual-language dataset, with both 2D and 3D scans; (ii),\nwe propose an architecture that enables visually conditioned generative\npre-training, i.e., allowing for integration of text input with 2D or 3D\nmedical scans, and generate responses for diverse radiologic tasks. The model\nwas initially pre-trained on MedMD and subsequently fine-tuned on the\ndomain-specific dataset, which is a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs, termed as RadMD; (iii), we\npropose a new evaluation benchmark, RadBench, that comprises five tasks,\nincluding modality recognition, disease diagnosis, visual question answering,\nreport generation and rationale diagnosis, aiming to comprehensively assess the\ncapability of foundation models in handling practical clinical problems. We\nconduct both automatic and human evaluation on RadBench, in both cases, RadFM\noutperforms existing multi-modal foundation models, that are publicaly\naccessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V.\nAdditionally, we also adapt RadFM for different public benchmarks, surpassing\nexisting SOTAs on diverse datasets. All codes, data, and model checkpoint will\nall be made publicly available to promote further research and development in\nthe field.\n","authors":["Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2308.02463v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09849v1","updated":"2023-11-16T12:21:54Z","published":"2023-11-16T12:21:54Z","title":"Rusty Detection Using Image Processing For Maintenance Of Stations","summary":"  This study addresses the challenge of accurately seg-menting rusted areas on\npainted construction surfaces. A method leveraging digital image processing is\nexplored to calculate the percentage of rust present on painted coatings. The\nproposed segmentation approach is based on the HSV color model. To equalize\nluminosity and mitigate the influence of illumination, a fundamental model of\nsingle-scale Retinex is applied specifically to the saturation component.\n  Subsequently, the image undergoes further processing, involv-ing manual color\nfiltering. This step is crucial for refining the identification of rusted\nregions. To enhance precision and filter out noise, the pixel areas selected\nthrough color filtering are subjected to the DBScan algorithm. This multi-step\nprocess aims to achieve a robust segmentation of rusted areas on painted\nconstruction surfaces, providing a valuable contribution to the field of\ncorrosion detection and analysis.\n","authors":["Dao Duy Tung","Ho Xuan Hung"],"pdf_url":"https://arxiv.org/pdf/2311.09849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09847v1","updated":"2023-11-16T12:20:25Z","published":"2023-11-16T12:20:25Z","title":"Overcoming Data Scarcity in Biomedical Imaging with a Foundational\n  Multi-Task Model","summary":"  Foundational models, pretrained on a large scale, have demonstrated\nsubstantial success across non-medical domains. However, training these models\ntypically requires large, comprehensive datasets, which contrasts with the\nsmaller and more heterogeneous datasets common in biomedical imaging. Here, we\npropose a multi-task learning strategy that decouples the number of training\ntasks from memory requirements. We trained a Universal bioMedical PreTrained\nmodel (UMedPT) on a multi-task database including tomographic, microscopic, and\nX-ray images, with various labelling strategies such as classification,\nsegmentation, and object detection. The UMedPT foundational model outperformed\nImageNet pretraining and the previous state-of-the-art models. For tasks\nrelated to the pretraining database, it maintained its performance with only 1%\nof the original training data and without fine-tuning. For out-of-domain tasks\nit required not more than 50% of the original training data. In an external\nindependent validation imaging features extracted using UMedPT proved to be a\nnew standard for cross-center transferability.\n","authors":["Raphael Schäfer","Till Nicke","Henning Höfener","Annkristin Lange","Dorit Merhof","Friedrich Feuerhake","Volkmar Schulz","Johannes Lotz","Fabian Kiessling"],"pdf_url":"https://arxiv.org/pdf/2311.09847v1.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09846v1","updated":"2023-11-16T12:19:48Z","published":"2023-11-16T12:19:48Z","title":"GroupMixer: Patch-based Group Convolutional Neural Network for Breast\n  Cancer Detection from Histopathological Images","summary":"  Diagnosis of breast cancer malignancy at the early stages is a crucial step\nfor controlling its side effects. Histopathological analysis provides a unique\nopportunity for malignant breast cancer detection. However, such a task would\nbe tedious and time-consuming for the histopathologists. Deep Neural Networks\nenable us to learn informative features directly from raw histopathological\nimages without manual feature extraction. Although Convolutional Neural\nNetworks (CNNs) have been the dominant architectures in the computer vision\nrealm, Transformer-based architectures have shown promising results in\ndifferent computer vision tasks. Although harnessing the capability of\nTransformer-based architectures for medical image analysis seems interesting,\nthese architectures are large, have a significant number of trainable\nparameters, and require large datasets to be trained on, which are usually rare\nin the medical domain. It has been claimed and empirically proved that at least\npart of the superior performance of Transformer-based architectures in Computer\nVision domain originates from patch embedding operation. In this paper, we\nborrowed the previously introduced idea of integrating a fully Convolutional\nNeural Network architecture with Patch Embedding operation and presented an\nefficient CNN architecture for breast cancer malignancy detection from\nhistopathological images. Despite the number of parameters that is\nsignificantly smaller than other methods, the accuracy performance metrics\nachieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x\nmagnifications respectively. We took a step forward and modified the\narchitecture using Group Convolution and Channel Shuffling ideas and reduced\nthe number of trainable parameters even more with a negligible decline in\nperformance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the\nmentioned magnifications respectively.\n","authors":["Ardavan Modarres","Erfan Ebrahim Esfahani","Mahsa Bahrami"],"pdf_url":"https://arxiv.org/pdf/2311.09846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09822v1","updated":"2023-11-16T11:49:49Z","published":"2023-11-16T11:49:49Z","title":"MAM-E: Mammographic synthetic image generation with diffusion models","summary":"  Generative models are used as an alternative data augmentation technique to\nalleviate the data scarcity problem faced in the medical imaging field.\nDiffusion models have gathered special attention due to their innovative\ngeneration approach, the high quality of the generated images and their\nrelatively less complex training process compared with Generative Adversarial\nNetworks. Still, the implementation of such models in the medical domain\nremains at early stages. In this work, we propose exploring the use of\ndiffusion models for the generation of high quality full-field digital\nmammograms using state-of-the-art conditional diffusion pipelines.\nAdditionally, we propose using stable diffusion models for the inpainting of\nsynthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of\ngenerative models for high quality mammography synthesis controlled by a text\nprompt and capable of generating synthetic lesions on specific regions of the\nbreast. Finally, we provide quantitative and qualitative assessment of the\ngenerated images and easy-to-use graphical user interfaces for mammography\nsynthesis.\n","authors":["Ricardo Montoya-del-Angel","Karla Sam-Millan","Joan C Vilanova","Robert Martí"],"pdf_url":"https://arxiv.org/pdf/2311.09822v1.pdf","comment":"10 pages + 2 pages of references, 6 figures"},{"id":"http://arxiv.org/abs/2311.09819v1","updated":"2023-11-16T11:48:29Z","published":"2023-11-16T11:48:29Z","title":"PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical\n  Instruments","summary":"  In surgical procedures, correct instrument counting is essential. Instance\nsegmentation is a location method that locates not only an object's bounding\nbox but also each pixel's specific details. However, obtaining mask-level\nannotations is labor-intensive in instance segmentation. To address this issue,\nwe propose a novel yet effective weakly-supervised surgical instrument instance\nsegmentation approach, named Point-based Weakly-supervised Instance\nSegmentation (PWISeg). PWISeg adopts an FCN-based architecture with\npoint-to-box and point-to-mask branches to model the relationships between\nfeature points and bounding boxes, as well as feature points and segmentation\nmasks on FPN, accomplishing instrument detection and segmentation jointly in a\nsingle model. Since mask level annotations are hard to available in the real\nworld, for point-to-mask training, we introduce an unsupervised projection\nloss, utilizing the projected relation between predicted masks and bboxes as\nsupervision signal. On the other hand, we annotate a few pixels as the key\npixel for each instrument. Based on this, we further propose a key pixel\nassociation loss and a key pixel distribution loss, driving the point-to-mask\nbranch to generate more accurate segmentation predictions. To comprehensively\nevaluate this task, we unveil a novel surgical instrument dataset with manual\nannotations, setting up a benchmark for further research. Our comprehensive\nresearch trial validated the superior performance of our PWISeg. The results\nshow that the accuracy of surgical instrument segmentation is improved,\nsurpassing most methods of instance segmentation via weakly supervised bounding\nboxes. This improvement is consistently observed in our proposed dataset and\nwhen applied to the public HOSPI-Tools dataset.\n","authors":["Zhen Sun","Huan Xu","Jinlin Wu","Zhen Chen","Zhen Lei","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09819v1.pdf","comment":"This work has been submitted to IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2024 for possible publication"},{"id":"http://arxiv.org/abs/2311.09817v1","updated":"2023-11-16T11:47:53Z","published":"2023-11-16T11:47:53Z","title":"Neural-Logic Human-Object Interaction Detection","summary":"  The interaction decoder utilized in prevalent Transformer-based HOI detectors\ntypically accepts pre-composed human-object pairs as inputs. Though achieving\nremarkable performance, such paradigm lacks feasibility and cannot explore\nnovel combinations over entities during decoding. We present L OGIC HOI, a new\nHOI detector that leverages neural-logic reasoning and Transformer to infer\nfeasible interactions between entities. Specifically, we modify the\nself-attention mechanism in vanilla Transformer, enabling it to reason over the\n<human, action, object> triplet and constitute novel interactions. Meanwhile,\nsuch reasoning process is guided by two crucial properties for understanding\nHOI: affordances (the potential actions an object can facilitate) and proxemics\n(the spatial relations between humans and objects). We formulate these two\nproperties in first-order logic and ground them into continuous space to\nconstrain the learning process of our approach, leading to improved performance\nand zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and\nHICO-DET under both normal and zero-shot setups, achieving significant\nimprovements over existing methods.\n","authors":["Liulei Li","Jianan Wei","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2311.09817v1.pdf","comment":"Accepted to NeurIPS 2023; Code:\n  https://github.com/weijianan1/LogicHOI"},{"id":"http://arxiv.org/abs/2307.10915v2","updated":"2023-11-16T11:32:17Z","published":"2023-07-20T14:39:46Z","title":"Revisiting Fine-Tuning Strategies for Self-supervised Medical Imaging\n  Analysis","summary":"  Despite the rapid progress in self-supervised learning (SSL), end-to-end\nfine-tuning still remains the dominant fine-tuning strategy for medical imaging\nanalysis. However, it remains unclear whether this approach is truly optimal\nfor effectively utilizing the pre-trained knowledge, especially considering the\ndiverse categories of SSL that capture different types of features. In this\npaper, we present the first comprehensive study that discovers effective\nfine-tuning strategies for self-supervised learning in medical imaging. After\ndeveloping strong contrastive and restorative SSL baselines that outperform\nSOTA methods across four diverse downstream tasks, we conduct an extensive\nfine-tuning analysis across multiple pre-training and fine-tuning datasets, as\nwell as various fine-tuning dataset sizes. Contrary to the conventional wisdom\nof fine-tuning only the last few layers of a pre-trained network, we show that\nfine-tuning intermediate layers is more effective, with fine-tuning the second\nquarter (25-50%) of the network being optimal for contrastive SSL whereas\nfine-tuning the third quarter (50-75%) of the network being optimal for\nrestorative SSL. Compared to the de-facto standard of end-to-end fine-tuning,\nour best fine-tuning strategy, which fine-tunes a shallower network consisting\nof the first three quarters (0-75%) of the pre-trained network, yields\nimprovements of as much as 5.48%. Additionally, using these insights, we\npropose a simple yet effective method to leverage the complementary strengths\nof multiple SSL models, resulting in enhancements of up to 3.57% compared to\nusing the best model alone. Hence, our fine-tuning strategies not only enhance\nthe performance of individual SSL models, but also enable effective utilization\nof the complementary strengths offered by multiple SSL models, leading to\nsignificant improvements in self-supervised medical imaging analysis.\n","authors":["Muhammad Osama Khan","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2307.10915v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 18 pages"},{"id":"http://arxiv.org/abs/2311.09806v1","updated":"2023-11-16T11:30:56Z","published":"2023-11-16T11:30:56Z","title":"EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction\n  on Mobile Devices","summary":"  Reconstructing real-world 3D objects has numerous applications in computer\nvision, such as virtual reality, video games, and animations. Ideally, 3D\nreconstruction methods should generate high-fidelity results with 3D\nconsistency in real-time. Traditional methods match pixels between images using\nphoto-consistency constraints or learned features, while differentiable\nrendering methods like Neural Radiance Fields (NeRF) use surface-based\nrepresentations or differentiable volume rendering to generate high-fidelity\nscenes. However, these methods require excessive runtime for rendering, making\nthem impractical for daily applications. To address these challenges, we\npresent $\\textbf{EvaSurf}$, an $\\textbf{E}$fficient\n$\\textbf{V}$iew-$\\textbf{A}$ware Implicit Textured $\\textbf{Surf}$ace\nReconstruction method on Mobile Devices. In our method, we first employ an\nefficient surface-based model with a multi-view supervision module to ensure\naccurate mesh creation. To enable high-fidelity rendering, we learn an implicit\ntexture embedded with a set of Gaussian lobes to capture view-dependent\ninformation. Furthermore, With the explicit geometry and the implicit texture,\nwe can employ a lightweight neural shader to reduce the expense of computation\nand further support real-time rendering on common mobile devices. Extensive\nexperiments demonstrate that our method can reconstruct high-quality appearance\nand accurate mesh on both synthetic and real-world datasets. Moreover, our\nmethod can be trained in just 1-2 hours using a single GPU and run on mobile\ndevices at over 40FPS (Frames Per Second), with a final package required for\nrendering taking up only 40-50 MB.\n","authors":["Jingnan Gao","Zhuo Chen","Yichao Yan","Bowen Pan","Zhe Wang","Jiangjing Lyu","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2311.09806v1.pdf","comment":"Project Page: http://g-1nonly.github.io/EvaSurf-Website/"},{"id":"http://arxiv.org/abs/2311.09778v1","updated":"2023-11-16T11:02:10Z","published":"2023-11-16T11:02:10Z","title":"Certified Control for Train Sign Classification","summary":"  There is considerable industrial interest in integrating AI techniques into\nrailway systems, notably for fully autonomous train systems. The KI-LOK\nresearch project is involved in developing new methods for certifying such\nAI-based systems. Here we explore the utility of a certified control\narchitecture for a runtime monitor that prevents false positive detection of\ntraffic signs in an AI-based perception system. The monitor uses classical\ncomputer vision algorithms to check if the signs -- detected by an AI object\ndetection model -- fit predefined specifications. We provide such\nspecifications for some critical signs and integrate a Python prototype of the\nmonitor with a popular object detection model to measure relevant performance\nmetrics on generated data. Our initial results are promising, achieving\nconsiderable precision gains with only minor recall reduction; however, further\ninvestigation into generalization possibilities will be necessary.\n","authors":["Jan Roßbach","Michael Leuschel"],"pdf_url":"https://arxiv.org/pdf/2311.09778v1.pdf","comment":"In Proceedings FMAS 2023, arXiv:2311.08987"},{"id":"http://arxiv.org/abs/2311.09768v1","updated":"2023-11-16T10:45:32Z","published":"2023-11-16T10:45:32Z","title":"Utilizing dataset affinity prediction in object detection to assess\n  training data","summary":"  Data pooling offers various advantages, such as increasing the sample size,\nimproving generalization, reducing sampling bias, and addressing data sparsity\nand quality, but it is not straightforward and may even be counterproductive.\nAssessing the effectiveness of pooling datasets in a principled manner is\nchallenging due to the difficulty in estimating the overall information content\nof individual datasets. Towards this end, we propose incorporating a data\nsource prediction module into standard object detection pipelines. The module\nruns with minimal overhead during inference time, providing additional\ninformation about the data source assigned to individual detections. We show\nthe benefits of the so-called dataset affinity score by automatically selecting\nsamples from a heterogeneous pool of vehicle datasets. The results show that\nobject detectors can be trained on a significantly sparser set of training\nsamples without losing detection accuracy.\n","authors":["Stefan Becker","Jens Bayer","Ronny Hug","Wolfgang Hübner","Michael Arens"],"pdf_url":"https://arxiv.org/pdf/2311.09768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19730v2","updated":"2023-11-16T10:41:03Z","published":"2023-05-31T10:49:16Z","title":"Data Representations' Study of Latent Image Manifolds","summary":"  Deep neural networks have been demonstrated to achieve phenomenal success in\nmany domains, and yet their inner mechanisms are not well understood. In this\npaper, we investigate the curvature of image manifolds, i.e., the manifold\ndeviation from being flat in its principal directions. We find that\nstate-of-the-art trained convolutional neural networks for image classification\nhave a characteristic curvature profile along layers: an initial steep\nincrease, followed by a long phase of a plateau, and followed by another\nincrease. In contrast, this behavior does not appear in untrained networks in\nwhich the curvature flattens. We also show that the curvature gap between the\nlast two layers has a strong correlation with the generalization capability of\nthe network. Moreover, we find that the intrinsic dimension of latent codes is\nnot necessarily indicative of curvature. Finally, we observe that common\nregularization methods such as mixup yield flatter representations when\ncompared to other methods. Our experiments show consistent results over a\nvariety of deep learning architectures and multiple data sets. Our code is\npublicly available at https://github.com/azencot-group/CRLM\n","authors":["Ilya Kaufman","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2305.19730v2.pdf","comment":"Accepted to ICML 2023"},{"id":"http://arxiv.org/abs/2311.09759v1","updated":"2023-11-16T10:32:18Z","published":"2023-11-16T10:32:18Z","title":"Scene Text Image Super-resolution based on Text-conditional Diffusion\n  Models","summary":"  Scene Text Image Super-resolution (STISR) has recently achieved great success\nas a preprocessing method for scene text recognition. STISR aims to transform\nblurred and noisy low-resolution (LR) text images in real-world settings into\nclear high-resolution (HR) text images suitable for scene text recognition. In\nthis study, we leverage text-conditional diffusion models (DMs), known for\ntheir impressive text-to-image synthesis capabilities, for STISR tasks. Our\nexperimental results revealed that text-conditional DMs notably surpass\nexisting STISR methods. Especially when texts from LR text images are given as\ninput, the text-conditional DMs are able to produce superior quality\nsuper-resolution text images. Utilizing this capability, we propose a novel\nframework for synthesizing LR-HR paired text image datasets. This framework\nconsists of three specialized text-conditional DMs, each dedicated to text\nimage synthesis, super-resolution, and image degradation. These three modules\nare vital for synthesizing distinct LR and HR paired images, which are more\nsuitable for training STISR methods. Our experiments confirmed that these\nsynthesized image pairs significantly enhance the performance of STISR methods\nin the TextZoom evaluation.\n","authors":["Chihiro Noguchi","Shun Fukuda","Masao Yamanaka"],"pdf_url":"https://arxiv.org/pdf/2311.09759v1.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2311.09757v1","updated":"2023-11-16T10:30:27Z","published":"2023-11-16T10:30:27Z","title":"UFPS: A unified framework for partially-annotated federated segmentation\n  in heterogeneous data distribution","summary":"  Partially supervised segmentation is a label-saving method based on datasets\nwith fractional classes labeled and intersectant. However, it is still far from\nlanding on real-world medical applications due to privacy concerns and data\nheterogeneity. As a remedy without privacy leakage, federated partially\nsupervised segmentation (FPSS) is formulated in this work. The main challenges\nfor FPSS are class heterogeneity and client drift. We propose a Unified\nFederated Partially-labeled Segmentation (UFPS) framework to segment pixels\nwithin all classes for partially-annotated datasets by training a totipotential\nglobal model without class collision. Our framework includes Unified Label\nLearning and sparsed Unified Sharpness Aware Minimization for unification of\nclass and feature space, respectively. We find that vanilla combinations for\ntraditional methods in partially supervised segmentation and federated learning\nare mainly hampered by class collision through empirical study. Our\ncomprehensive experiments on real medical datasets demonstrate better\ndeconflicting and generalization ability of UFPS compared with modified\nmethods.\n","authors":["Le Jiang","Li Yan Ma","Tie Yong Zeng","Shi Hui Ying"],"pdf_url":"https://arxiv.org/pdf/2311.09757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09753v1","updated":"2023-11-16T10:28:59Z","published":"2023-11-16T10:28:59Z","title":"DIFFNAT: Improving Diffusion Image Quality Using Natural Image\n  Statistics","summary":"  Diffusion models have advanced generative AI significantly in terms of\nediting and creating naturalistic images. However, efficiently improving\ngenerated image quality is still of paramount interest. In this context, we\npropose a generic \"naturalness\" preserving loss function, viz., kurtosis\nconcentration (KC) loss, which can be readily applied to any standard diffusion\nmodel pipeline to elevate the image quality. Our motivation stems from the\nprojected kurtosis concentration property of natural images, which states that\nnatural images have nearly constant kurtosis values across different band-pass\nversions of the image. To retain the \"naturalness\" of the generated images, we\nenforce reducing the gap between the highest and lowest kurtosis values across\nthe band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note\nthat our approach does not require any additional guidance like classifier or\nclassifier-free guidance to improve the image quality. We validate the proposed\napproach for three diverse tasks, viz., (1) personalized few-shot finetuning\nusing text guidance, (2) unconditional image generation, and (3) image\nsuper-resolution. Integrating the proposed KC loss has improved the perceptual\nquality across all these tasks in terms of both FID, MUSIQ score, and user\nevaluation.\n","authors":["Aniket Roy","Maiterya Suin","Anshul Shah","Ketul Shah","Jiang Liu","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2311.09753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09744v1","updated":"2023-11-16T10:19:04Z","published":"2023-11-16T10:19:04Z","title":"Redefining the Laparoscopic Spatial Sense: AI-based Intra- and\n  Postoperative Measurement from Stereoimages","summary":"  A significant challenge in image-guided surgery is the accurate measurement\ntask of relevant structures such as vessel segments, resection margins, or\nbowel lengths. While this task is an essential component of many surgeries, it\ninvolves substantial human effort and is prone to inaccuracies. In this paper,\nwe develop a novel human-AI-based method for laparoscopic measurements\nutilizing stereo vision that has been guided by practicing surgeons. Based on a\nholistic qualitative requirements analysis, this work proposes a comprehensive\nmeasurement method, which comprises state-of-the-art machine learning\narchitectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed\nin various realistic experimental evaluation environments. Our results outline\nthe potential of our method achieving high accuracies in distance measurements\nwith errors below 1 mm. Furthermore, on-surface measurements demonstrate\nrobustness when applied in challenging environments with textureless regions.\nOverall, by addressing the inherent challenges of image-guided surgery, we lay\nthe foundation for a more robust and accurate solution for intra- and\npostoperative measurements, enabling more precise, safe, and efficient surgical\nprocedures.\n","authors":["Leopold Müller","Patrick Hemmer","Moritz Queisner","Igor Sauer","Simeon Allmendinger","Johannes Jakubik","Michael Vössing","Niklas Kühl"],"pdf_url":"https://arxiv.org/pdf/2311.09744v1.pdf","comment":"38th AAAI Conference on Artificial Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2311.09737v1","updated":"2023-11-16T10:07:27Z","published":"2023-11-16T10:07:27Z","title":"Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality\n  MRI Segmentation","summary":"  Cross-modal MRI segmentation is of great value for computer-aided medical\ndiagnosis, enabling flexible data acquisition and model generalization.\nHowever, most existing methods have difficulty in handling local variations in\ndomain shift and typically require a significant amount of data for training,\nwhich hinders their usage in practice. To address these problems, we propose a\nnovel adaptive domain generalization framework, which integrates a\nlearning-free cross-domain representation based on image gradient maps and a\nclass prior-informed test-time adaptation strategy for mitigating local domain\nshift. We validate our approach on two multi-modal MRI datasets with six\ncross-modal segmentation tasks. Across all the task settings, our method\nconsistently outperforms competing approaches and shows a stable performance\neven with limited training data.\n","authors":["Bingnan Li","Zhitong Gao","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2311.09737v1.pdf","comment":"9 pages, Machine Learning for Health (ML4H) 2023"},{"id":"http://arxiv.org/abs/2311.09726v1","updated":"2023-11-16T09:57:29Z","published":"2023-11-16T09:57:29Z","title":"MS-Former: Memory-Supported Transformer for Weakly Supervised Change\n  Detection with Patch-Level Annotations","summary":"  Fully supervised change detection methods have achieved significant\nadvancements in performance, yet they depend severely on acquiring costly\npixel-level labels. Considering that the patch-level annotations also contain\nabundant information corresponding to both changed and unchanged objects in\nbi-temporal images, an intuitive solution is to segment the changes with\npatch-level annotations. How to capture the semantic variations associated with\nthe changed and unchanged regions from the patch-level annotations to obtain\npromising change results is the critical challenge for the weakly supervised\nchange detection task. In this paper, we propose a memory-supported transformer\n(MS-Former), a novel framework consisting of a bi-directional attention block\n(BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised\nchange detection with patch-level annotations. More specifically, the BAM\ncaptures contexts associated with the changed and unchanged regions from the\ntemporal difference features to construct informative prototypes stored in the\nmemory bank. On the other hand, the BAM extracts useful information from the\nprototypes as supplementary contexts to enhance the temporal difference\nfeatures, thereby better distinguishing changed and unchanged regions. After\nthat, the PSS guides the network learning valuable knowledge from the\npatch-level annotations, thus further elevating the performance. Experimental\nresults on three benchmark datasets demonstrate the effectiveness of our\nproposed method in the change detection task. The demo code for our work will\nbe publicly available at \\url{https://github.com/guanyuezhen/MS-Former}.\n","authors":["Zhenglai Li","Chang Tang","Xinwang Liu","Changdong Li","Xianju Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09726v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.07661v3","updated":"2023-11-16T09:14:36Z","published":"2023-02-15T13:48:10Z","title":"Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D\n  Panoramic Color Images from LiDAR Point Clouds","summary":"  This work presents a new depth- and semantics-aware conditional generative\nmodel, named TITAN-Next, for cross-domain image-to-image translation in a\nmulti-modal setup between LiDAR and camera sensors. The proposed model\nleverages scene semantics as a mid-level representation and is able to\ntranslate raw LiDAR point clouds to RGB-D camera images by solely relying on\nsemantic scene segments. We claim that this is the first framework of its kind\nand it has practical applications in autonomous vehicles such as providing a\nfail-safe mechanism and augmenting available data in the target image domain.\nThe proposed model is evaluated on the large-scale and challenging\nSemantic-KITTI dataset, and experimental findings show that it considerably\noutperforms the original TITAN-Net and other strong baselines by 23.7$\\%$\nmargin in terms of IoU.\n","authors":["Tiago Cortinhal","Eren Erdal Aksoy"],"pdf_url":"https://arxiv.org/pdf/2302.07661v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02782v3","updated":"2023-11-16T09:10:22Z","published":"2023-11-05T22:13:12Z","title":"Towards Generic Anomaly Detection and Understanding: Large-scale\n  Visual-linguistic Model (GPT-4V) Takes the Lead","summary":"  Anomaly detection is a crucial task across different domains and data types.\nHowever, existing anomaly detection models are often designed for specific\ndomains and modalities. This study explores the use of GPT-4V(ision), a\npowerful visual-linguistic model, to address anomaly detection tasks in a\ngeneric manner. We investigate the application of GPT-4V in multi-modality,\nmulti-domain anomaly detection tasks, including image, video, point cloud, and\ntime series data, across multiple application areas, such as industrial,\nmedical, logical, video, 3D anomaly detection, and localization tasks. To\nenhance GPT-4V's performance, we incorporate different kinds of additional cues\nsuch as class information, human expertise, and reference images as\nprompts.Based on our experiments, GPT-4V proves to be highly effective in\ndetecting and explaining global and fine-grained semantic patterns in\nzero/one-shot anomaly detection. This enables accurate differentiation between\nnormal and abnormal instances. Although we conducted extensive evaluations in\nthis study, there is still room for future evaluation to further exploit\nGPT-4V's generic anomaly detection capacity from different aspects. These\ninclude exploring quantitative metrics, expanding evaluation benchmarks,\nincorporating multi-round interactions, and incorporating human feedback loops.\nNevertheless, GPT-4V exhibits promising performance in generic anomaly\ndetection and understanding, thus opening up a new avenue for anomaly\ndetection.\n","authors":["Yunkang Cao","Xiaohao Xu","Chen Sun","Xiaonan Huang","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2311.02782v3.pdf","comment":"Work in progress. Evaluated GPT-4V on 4 modalities, 9 tasks, and 15\n  datasets. The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2311.09680v1","updated":"2023-11-16T08:49:46Z","published":"2023-11-16T08:49:46Z","title":"Trustworthy Large Models in Vision: A Survey","summary":"  The rapid progress of Large Models (LMs) has recently revolutionized various\nfields of deep learning with remarkable grades, ranging from Natural Language\nProcessing (NLP) to Computer Vision (CV). However, LMs are increasingly\nchallenged and criticized by academia and industry due to their powerful\nperformance but untrustworthy behavior, which urgently needs to be alleviated\nin reliable methods. Despite the abundance of literature on trustworthy LMs in\nlanguage, a systematic survey specifically delving into the trustworthiness of\nLMs in vision remains absent. In order to mitigate this gap, we summarize four\nrelevant concerns that obstruct the trustworthy usage in vision of LMs in this\nsurvey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)\ninterpretability. By highlighting corresponding challenge, countermeasures, and\ndiscussion in each topic, we hope this survey will facilitate readers'\nunderstanding of the field, promote alignment of LMs with human expectations\nand enable trustworthy LMs to serve as welfare rather than disaster for human\nsociety.\n","authors":["Ziyan Guo","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09671v1","updated":"2023-11-16T08:39:58Z","published":"2023-11-16T08:39:58Z","title":"Robust Contrastive Learning With Theory Guarantee","summary":"  Contrastive learning (CL) is a self-supervised training paradigm that allows\nus to extract meaningful features without any label information. A typical CL\nframework is divided into two phases, where it first tries to learn the\nfeatures from unlabelled data, and then uses those features to train a linear\nclassifier with the labeled data. While a fair amount of existing theoretical\nworks have analyzed how the unsupervised loss in the first phase can support\nthe supervised loss in the second phase, none has examined the connection\nbetween the unsupervised loss and the robust supervised loss, which can shed\nlight on how to construct an effective unsupervised loss for the first phase of\nCL. To fill this gap, our work develops rigorous theories to dissect and\nidentify which components in the unsupervised loss can help improve the robust\nsupervised loss and conduct proper experiments to verify our findings.\n","authors":["Ngoc N. Tran","Lam Tran","Hoang Phan","Anh Bui","Tung Pham","Toan Tran","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2311.09671v1.pdf","comment":"27 pages, 0 figures. arXiv admin note: text overlap with\n  arXiv:2305.10252"},{"id":"http://arxiv.org/abs/2301.10365v2","updated":"2023-11-16T08:39:51Z","published":"2023-01-25T00:21:31Z","title":"Data Consistent Deep Rigid MRI Motion Correction","summary":"  Motion artifacts are a pervasive problem in MRI, leading to misdiagnosis or\nmischaracterization in population-level imaging studies. Current retrospective\nrigid intra-slice motion correction techniques jointly optimize estimates of\nthe image and the motion parameters. In this paper, we use a deep network to\nreduce the joint image-motion parameter search to a search over rigid motion\nparameters alone. Our network produces a reconstruction as a function of two\ninputs: corrupted k-space data and motion parameters. We train the network\nusing simulated, motion-corrupted k-space data generated with known motion\nparameters. At test-time, we estimate unknown motion parameters by minimizing a\ndata consistency loss between the motion parameters, the network-based image\nreconstruction given those parameters, and the acquired measurements.\nIntra-slice motion correction experiments on simulated and realistic 2D fast\nspin echo brain MRI achieve high reconstruction fidelity while providing the\nbenefits of explicit data consistency optimization. Our code is publicly\navailable at https://www.github.com/nalinimsingh/neuroMoCo.\n","authors":["Nalini M. Singh","Neel Dey","Malte Hoffmann","Bruce Fischl","Elfar Adalsteinsson","Robert Frost","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2301.10365v2.pdf","comment":"Presented at MIDL 2023. 14 pages, 6 figures. Keywords: motion\n  correction, magnetic resonance imaging, deep learning"},{"id":"http://arxiv.org/abs/2311.09655v1","updated":"2023-11-16T08:17:02Z","published":"2023-11-16T08:17:02Z","title":"Multi-View Spectrogram Transformer for Respiratory Sound Classification","summary":"  Deep neural networks have been applied to audio spectrograms for respiratory\nsound classification. Existing models often treat the spectrogram as a\nsynthetic image while overlooking its physical characteristics. In this paper,\na Multi-View Spectrogram Transformer (MVST) is proposed to embed different\nviews of time-frequency characteristics into the vision transformer.\nSpecifically, the proposed MVST splits the mel-spectrogram into different sized\npatches, representing the multi-view acoustic elements of a respiratory sound.\nThese patches and positional embeddings are then fed into transformer encoders\nto extract the attentional information among patches through a self-attention\nmechanism. Finally, a gated fusion scheme is designed to automatically weigh\nthe multi-view features to highlight the best one in a specific scenario.\nExperimental results on the ICBHI dataset demonstrate that the proposed MVST\nsignificantly outperforms state-of-the-art methods for classifying respiratory\nsounds.\n","authors":["Wentao He","Yuchen Yan","Jianfeng Ren","Ruibin Bai","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.09655v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2311.09653v1","updated":"2023-11-16T08:12:34Z","published":"2023-11-16T08:12:34Z","title":"Improved TokenPose with Sparsity","summary":"  Over the past few years, the vision transformer and its various forms have\ngained significance in human pose estimation. By treating image patches as\ntokens, transformers can capture global relationships wisely, estimate the\nkeypoint tokens by leveraging the visual tokens, and recognize the posture of\nthe human body. Nevertheless, global attention is computationally demanding,\nwhich poses a challenge for scaling up transformer-based methods to\nhigh-resolution features. In this paper, we introduce sparsity in both keypoint\ntoken attention and visual token attention to improve human pose estimation.\nExperimental results on the MPII dataset demonstrate that our model has a\nhigher level of accuracy and proved the feasibility of the method, achieving\nnew state-of-the-art results. The idea can also provide references for other\ntransformer-based models.\n","authors":["Anning Li"],"pdf_url":"https://arxiv.org/pdf/2311.09653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09652v1","updated":"2023-11-16T08:12:10Z","published":"2023-11-16T08:12:10Z","title":"Event-based Motion-Robust Accurate Shape Estimation for Mixed\n  Reflectance Scenes","summary":"  Event-based structured light systems have recently been introduced as an\nexciting alternative to conventional frame-based triangulation systems for the\n3D measurements of diffuse surfaces. Important benefits include the fast\ncapture speed and the high dynamic range provided by the event camera - albeit\nat the cost of lower data quality. So far, both low-accuracy event-based as\nwell as high-accuracy frame-based 3D imaging systems are tailored to a specific\nsurface type, such as diffuse or specular, and can not be used for a broader\nclass of object surfaces (\"mixed reflectance scenes\"). In this paper, we\npresent a novel event-based structured light system that enables fast 3D\nimaging of mixed reflectance scenes with high accuracy. On the captured events,\nwe use epipolar constraints that intrinsically enable decomposing the measured\nreflections into diffuse, two-bounce specular, and other multi-bounce\nreflections. The diffuse objects in the scene are reconstructed using\ntriangulation. Eventually, the reconstructed diffuse scene parts are used as a\n\"display\" to evaluate the specular scene parts via deflectometry. This novel\nprocedure allows us to use the entire scene as a virtual screen, using only a\nscanning laser and an event camera. The resulting system achieves fast and\nmotion-robust (14Hz) reconstructions of mixed reflectance scenes with < 500\n$\\mu$m accuracy. Moreover, we introduce a \"superfast\" capture mode (250Hz) for\nthe 3D measurement of diffuse scenes.\n","authors":["Aniket Dashpute","Jiazhang Wang","James Taylor","Oliver Cossairt","Ashok Veeraraghavan","Florian Willomitzer"],"pdf_url":"https://arxiv.org/pdf/2311.09652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06817v3","updated":"2023-11-16T08:00:17Z","published":"2023-03-13T02:21:38Z","title":"Transformation-Invariant Network for Few-Shot Object Detection in Remote\n  Sensing Images","summary":"  Object detection in remote sensing images relies on a large amount of labeled\ndata for training. However, the increasing number of new categories and class\nimbalance make exhaustive annotation impractical. Few-shot object detection\n(FSOD) addresses this issue by leveraging meta-learning on seen base classes\nand fine-tuning on novel classes with limited labeled samples. Nonetheless, the\nsubstantial scale and orientation variations of objects in remote sensing\nimages pose significant challenges to existing few-shot object detection\nmethods. To overcome these challenges, we propose integrating a feature pyramid\nnetwork and utilizing prototype features to enhance query features, thereby\nimproving existing FSOD methods. We refer to this modified FSOD approach as a\nStrong Baseline, which has demonstrated significant performance improvements\ncompared to the original baselines. Furthermore, we tackle the issue of spatial\nmisalignment caused by orientation variations between the query and support\nimages by introducing a Transformation-Invariant Network (TINet). TINet ensures\ngeometric invariance and explicitly aligns the features of the query and\nsupport branches, resulting in additional performance gains while maintaining\nthe same inference speed as the Strong Baseline. Extensive experiments on three\nwidely used remote sensing object detection datasets, i.e., NWPU VHR-10.v2,\nDIOR, and HRRSD demonstrated the effectiveness of the proposed method.\n","authors":["Nanqing Liu","Xun Xu","Turgay Celik","Zongxin Gan","Heng-Chao Li"],"pdf_url":"https://arxiv.org/pdf/2303.06817v3.pdf","comment":"Accepted by TGRS. Modified some errors from the previous version"},{"id":"http://arxiv.org/abs/2311.09646v1","updated":"2023-11-16T07:59:01Z","published":"2023-11-16T07:59:01Z","title":"Reconstructing Continuous Light Field From Single Coded Image","summary":"  We propose a method for reconstructing a continuous light field of a target\nscene from a single observed image. Our method takes the best of two worlds:\njoint aperture-exposure coding for compressive light-field acquisition, and a\nneural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding\nimplemented in a camera enables effective embedding of 3-D scene information\ninto an observed image, but in previous works, it was used only for\nreconstructing discretized light-field views. NeRF-based neural rendering\nenables high quality view synthesis of a 3-D scene from continuous viewpoints,\nbut when only a single image is given as the input, it struggles to achieve\nsatisfactory quality. Our method integrates these two techniques into an\nefficient and end-to-end trainable pipeline. Trained on a wide variety of\nscenes, our method can reconstruct continuous light fields accurately and\nefficiently without any test time optimization. To our knowledge, this is the\nfirst work to bridge two worlds: camera design for efficiently acquiring 3-D\ninformation and neural rendering.\n","authors":["Yuya Ishikawa","Keita Takahashi","Chihiro Tsutake","Toshiaki Fujii"],"pdf_url":"https://arxiv.org/pdf/2311.09646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09642v1","updated":"2023-11-16T07:53:34Z","published":"2023-11-16T07:53:34Z","title":"Weakly Supervised Anomaly Detection for Chest X-Ray Image","summary":"  Chest X-Ray (CXR) examination is a common method for assessing thoracic\ndiseases in clinical applications. While recent advances in deep learning have\nenhanced the significance of visual analysis for CXR anomaly detection, current\nmethods often miss key cues in anomaly images crucial for identifying disease\nregions, as they predominantly rely on unsupervised training with normal\nimages. This letter focuses on a more practical setup in which few-shot anomaly\nimages with only image-level labels are available during training. For this\npurpose, we propose WSCXR, a weakly supervised anomaly detection framework for\nCXR. WSCXR firstly constructs sets of normal and anomaly image features\nrespectively. It then refines the anomaly image features by eliminating normal\nregion features through anomaly feature mining, thus fully leveraging the\nscarce yet crucial features of diseased areas. Additionally, WSCXR employs a\nlinear mixing strategy to augment the anomaly features, facilitating the\ntraining of anomaly detector with few-shot anomaly images. Experiments on two\nCXR datasets demonstrate the effectiveness of our approach.\n","authors":["Haoqi Ni","Ximiao Zhang","Min Xu","Ning Lang","Xiuzhuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.09642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01125v3","updated":"2023-11-16T07:48:30Z","published":"2022-10-03T03:07:33Z","title":"Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep\n  Reconstruction without Reference","summary":"  Spectral computed tomography based on a photon-counting detector (PCD)\nattracts more and more attentions since it has the capability to provide more\naccurate identification and quantitative analysis for biomedical materials. The\nlimited number of photons within narrow energy bins leads to imaging results of\nlow signal-noise ratio. The existing supervised deep reconstruction networks\nfor CT reconstruction are difficult to address these challenges because it is\nusually impossible to acquire noise-free clinical images with clear structures\nas references. In this paper, we propose an iterative deep reconstruction\nnetwork to synergize unsupervised method and data priors into a unified\nframework, named as Spectral2Spectral. Our Spectral2Spectral employs an\nunsupervised deep training strategy to obtain high-quality images from noisy\ndata in an end-to-end fashion. The structural similarity prior within\nimage-spectral domain is refined as a regularization term to further constrain\nthe network training. The weights of neural network are automatically updated\nto capture image features and structures within the iterative process. Three\nlarge-scale preclinical datasets experiments demonstrate that the\nSpectral2spectral reconstructs better image quality than other the\nstate-of-the-art methods.\n","authors":["Xiaodong Guo","Longhui Li","Dingyue Chang","Peng He","Peng Feng","Hengyong Yu","Weiwen Wu"],"pdf_url":"https://arxiv.org/pdf/2210.01125v3.pdf","comment":"Accepted by IEEE TCI"},{"id":"http://arxiv.org/abs/2311.09639v1","updated":"2023-11-16T07:46:47Z","published":"2023-11-16T07:46:47Z","title":"On the Quantification of Image Reconstruction Uncertainty without\n  Training Data","summary":"  Computational imaging plays a pivotal role in determining hidden information\nfrom sparse measurements. A robust inverse solver is crucial to fully\ncharacterize the uncertainty induced by these measurements, as it allows for\nthe estimation of the complete posterior of unrecoverable targets. This, in\nturn, facilitates a probabilistic interpretation of observational data for\ndecision-making. In this study, we propose a deep variational framework that\nleverages a deep generative model to learn an approximate posterior\ndistribution to effectively quantify image reconstruction uncertainty without\nthe need for training data. We parameterize the target posterior using a\nflow-based model and minimize their Kullback-Leibler (KL) divergence to achieve\naccurate uncertainty estimation. To bolster stability, we introduce a robust\nflow-based model with bi-directional regularization and enhance expressivity\nthrough gradient boosting. Additionally, we incorporate a space-filling design\nto achieve substantial variance reduction on both latent prior space and target\nposterior space. We validate our method on several benchmark tasks and two\nreal-world applications, namely fastMRI and black hole image reconstruction.\nOur results indicate that our method provides reliable and high-quality image\nreconstruction with robust uncertainty estimation.\n","authors":["Sirui Bi","Victor Fung","Jiaxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.09639v1.pdf","comment":"Accepted by IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV 2024)"},{"id":"http://arxiv.org/abs/2311.01766v3","updated":"2023-11-16T07:21:10Z","published":"2023-11-03T08:05:54Z","title":"Support or Refute: Analyzing the Stance of Evidence to Detect\n  Out-of-Context Mis- and Disinformation","summary":"  Mis- and disinformation online have become a major societal problem as major\nsources of online harms of different kinds. One common form of mis- and\ndisinformation is out-of-context (OOC) information, where different pieces of\ninformation are falsely associated, e.g., a real image combined with a false\ntextual caption or a misleading textual description. Although some past studies\nhave attempted to defend against OOC mis- and disinformation through external\nevidence, they tend to disregard the role of different pieces of evidence with\ndifferent stances. Motivated by the intuition that the stance of evidence\nrepresents a bias towards different detection results, we propose a stance\nextraction network (SEN) that can extract the stances of different pieces of\nmulti-modal evidence in a unified framework. Moreover, we introduce a\nsupport-refutation score calculated based on the co-occurrence relations of\nnamed entities into the textual SEN. Extensive experiments on a public\nlarge-scale dataset demonstrated that our proposed method outperformed the\nstate-of-the-art baselines, with the best model achieving a performance gain of\n3.2% in accuracy.\n","authors":["Xin Yuan","Jie Guo","Weidong Qiu","Zheng Huang","Shujun Li"],"pdf_url":"https://arxiv.org/pdf/2311.01766v3.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.09625v1","updated":"2023-11-16T07:16:02Z","published":"2023-11-16T07:16:02Z","title":"DECDM: Document Enhancement using Cycle-Consistent Diffusion Models","summary":"  The performance of optical character recognition (OCR) heavily relies on\ndocument image quality, which is crucial for automatic document processing and\ndocument intelligence. However, most existing document enhancement methods\nrequire supervised data pairs, which raises concerns about data separation and\nprivacy protection, and makes it challenging to adapt these methods to new\ndomain pairs. To address these issues, we propose DECDM, an end-to-end\ndocument-level image translation method inspired by recent advances in\ndiffusion models. Our method overcomes the limitations of paired training by\nindependently training the source (noisy input) and target (clean output)\nmodels, making it possible to apply domain-specific diffusion models to other\npairs. DECDM trains on one dataset at a time, eliminating the need to scan both\ndatasets concurrently, and effectively preserving data privacy from the source\nor target domain. We also introduce simple data augmentation strategies to\nimprove character-glyph conservation during translation. We compare DECDM with\nstate-of-the-art methods on multiple synthetic data and benchmark datasets,\nsuch as document denoising and {\\color{black}shadow} removal, and demonstrate\nthe superiority of performance quantitatively and qualitatively.\n","authors":["Jiaxin Zhang","Joy Rimchala","Lalla Mouatadid","Kamalika Das","Sricharan Kumar"],"pdf_url":"https://arxiv.org/pdf/2311.09625v1.pdf","comment":"Accepted by IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV 2024)"},{"id":"http://arxiv.org/abs/2311.09623v1","updated":"2023-11-16T07:14:50Z","published":"2023-11-16T07:14:50Z","title":"Apoptosis classification using attention based spatio temporal graph\n  convolution neural network","summary":"  Accurate classification of apoptosis plays an important role in cell biology\nresearch. There are many state-of-the-art approaches which use deep CNNs to\nperform the apoptosis classification but these approaches do not account for\nthe cell interaction. Our paper proposes the Attention Graph spatio-temporal\ngraph convolutional network to classify the cell death based on the target\ncells in the video. This method considers the interaction of multiple target\ncells at each time stamp. We model the whole video sequence as a set of graphs\nand classify the target cell in the video as dead or alive. Our method\nencounters both spatial and temporal relationships.\n","authors":["Akash Awasthi"],"pdf_url":"https://arxiv.org/pdf/2311.09623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14752v3","updated":"2023-11-16T07:12:46Z","published":"2023-06-26T15:09:02Z","title":"MedLSAM: Localize and Segment Anything Model for 3D CT Images","summary":"  The Segment Anything Model (SAM) has recently emerged as a groundbreaking\nmodel in the field of image segmentation. Nevertheless, both the original SAM\nand its medical adaptations necessitate slice-by-slice annotations, which\ndirectly increase the annotation workload with the size of the dataset. We\npropose MedLSAM to address this issue, ensuring a constant annotation workload\nirrespective of dataset size and thereby simplifying the annotation process.\nOur model introduces a 3D localization foundation model capable of localizing\nany target anatomical part within the body. To achieve this, we develop a\nLocalize Anything Model for 3D Medical Images (MedLAM), utilizing two\nself-supervision tasks: unified anatomical mapping (UAM) and multi-scale\nsimilarity (MSS) across a comprehensive dataset of 14,012 CT scans. We then\nestablish a methodology for accurate segmentation by integrating MedLAM with\nSAM. By annotating several extreme points across three directions on a few\ntemplates, our model can autonomously identify the target anatomical region on\nall data scheduled for annotation. This allows our framework to generate a 2D\nbbox for every slice of the image, which is then leveraged by SAM to carry out\nsegmentation. We carried out comprehensive experiments on two 3D datasets\nencompassing 38 distinct organs. Our findings are twofold: 1) MedLAM is capable\nof directly localizing any anatomical structure using just a few template\nscans, yet its performance surpasses that of fully supervised models; 2)\nMedLSAM not only aligns closely with the performance of SAM and its specialized\nmedical adaptations with manual prompts but achieves this with minimal reliance\non extreme point annotations across the entire dataset. Furthermore, MedLAM has\nthe potential to be seamlessly integrated with future 3D SAM models, paving the\nway for enhanced performance.\n","authors":["Wenhui Lei","Xu Wei","Xiaofan Zhang","Kang Li","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.14752v3.pdf","comment":"Submitted to MIA. Code is public at\n  https://github.com/openmedlab/MedLSAM"},{"id":"http://arxiv.org/abs/2311.09614v1","updated":"2023-11-16T06:58:46Z","published":"2023-11-16T06:58:46Z","title":"Comprehensive Evaluation and Insights into the Use of Deep Neural\n  Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images","summary":"  This study performs comprehensive evaluation of four neural network\narchitectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion\nsegmentation from PET/CT images. These networks were trained, validated, and\ntested on a diverse, multi-institutional dataset of 611 cases. Internal testing\n(88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed\nSegResNet as the top performer with a median Dice similarity coefficient (DSC)\nof 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a\nmedian false negative volume (FNV) of 0 ml. On the unseen external test set\n(145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best\nmedian DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.\nWe assessed reproducibility of six lesion measures, calculated their prediction\nerrors, and examined DSC performance in relation to these lesion measures,\noffering insights into segmentation accuracy and clinical relevance.\nAdditionally, we introduced three lesion detection criteria, addressing the\nclinical need for identifying lesions, counting them, and segmenting based on\nmetabolic characteristics. We also performed expert intra-observer variability\nanalysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to\nassist in the development of more resilient segmentation algorithms. Finally,\nwe performed inter-observer agreement assessment underscoring the importance of\na standardized ground truth segmentation protocol involving multiple expert\nannotators. Code is available at:\nhttps://github.com/microsoft/lymphoma-segmentation-dnn\n","authors":["Shadab Ahamed","Yixi Xu","Claire Gowdy","Joo H. O","Ingrid Bloise","Don Wilson","Patrick Martineau","François Bénard","Fereshteh Yousefirizi","Rahul Dodhia","Juan M. Lavista","William B. Weeks","Carlos F. Uribe","Arman Rahmim"],"pdf_url":"https://arxiv.org/pdf/2311.09614v1.pdf","comment":"12 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.09612v1","updated":"2023-11-16T06:50:26Z","published":"2023-11-16T06:50:26Z","title":"Efficient End-to-End Visual Document Understanding with Rationale\n  Distillation","summary":"  Understanding visually situated language requires recognizing text and visual\nelements, and interpreting complex layouts. State-of-the-art methods commonly\nuse specialized pre-processing tools, such as optical character recognition\n(OCR) systems, that map document image inputs to extracted information in the\nspace of textual tokens, and sometimes also employ large language models (LLMs)\nto reason in text token space. However, the gains from external tools and LLMs\ncome at the cost of increased computational and engineering complexity. In this\npaper, we ask whether small pretrained image-to-text models can learn selective\ntext or layout recognition and reasoning as an intermediate inference step in\nan end-to-end model for pixel-level visual language understanding. We\nincorporate the outputs of such OCR tools, LLMs, and larger multimodal models\nas intermediate ``rationales'' on training data, and train a small student\nmodel to predict both rationales and answers for input questions based on those\ntraining examples. A student model based on Pix2Struct (282M parameters)\nachieves consistent improvements on three visual document understanding\nbenchmarks representing infographics, scanned documents, and figures, with\nimprovements of more than 4\\% absolute over a comparable Pix2Struct model that\npredicts answers directly.\n","authors":["Wang Zhu","Alekh Agarwal","Mandar Joshi","Robin Jia","Jesse Thomason","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2311.09612v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.09607v1","updated":"2023-11-16T06:35:02Z","published":"2023-11-16T06:35:02Z","title":"Multi-Task Learning Approach for Unified Biometric Estimation from Fetal\n  Ultrasound Anomaly Scans","summary":"  Precise estimation of fetal biometry parameters from ultrasound images is\nvital for evaluating fetal growth, monitoring health, and identifying potential\ncomplications reliably. However, the automated computerized segmentation of the\nfetal head, abdomen, and femur from ultrasound images, along with the\nsubsequent measurement of fetal biometrics, remains challenging. In this work,\nwe propose a multi-task learning approach to classify the region into head,\nabdomen and femur as well as estimate the associated parameters. We were able\nto achieve a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44\nmm on abdomen circumference and 1.10 mm on femur length with a classification\naccuracy of 99.91\\% on a dataset of fetal Ultrasound images. To achieve this,\nwe leverage a weighted joint classification and segmentation loss function to\ntrain a U-Net architecture with an added classification head. The code can be\naccessed through\n\\href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\\texttt{Github}\n","authors":["Mohammad Areeb Qazi","Mohammed Talha Alam","Ibrahim Almakky","Werner Gerhard Diehl","Leanne Bricker","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2311.09607v1.pdf","comment":"10 Pages, 4 Figures, The 4th International Conference on Medical\n  Imaging and Computer-Aided Diagnosis"},{"id":"http://arxiv.org/abs/2210.05673v2","updated":"2023-11-16T06:29:14Z","published":"2022-10-11T03:55:38Z","title":"Performance Deterioration of Deep Learning Models after Clinical\n  Deployment: A Case Study with Auto-segmentation for Definitive Prostate\n  Cancer Radiotherapy","summary":"  We evaluated the temporal performance of a deep learning (DL) based\nartificial intelligence (AI) model for auto segmentation in prostate\nradiotherapy, seeking to correlate its efficacy with changes in clinical\nlandscapes. Our study involved 1328 prostate cancer patients who underwent\ndefinitive radiotherapy from January 2006 to August 2022 at the University of\nTexas Southwestern Medical Center. We trained a UNet based segmentation model\non data from 2006 to 2011 and tested it on data from 2012 to 2022 to simulate\nreal world clinical deployment. We measured the model performance using the\nDice similarity coefficient (DSC), visualized the trends in contour quality\nusing exponentially weighted moving average (EMA) curves. Additionally, we\nperformed Wilcoxon Rank Sum Test to analyze the differences in DSC\ndistributions across distinct periods, and multiple linear regression to\ninvestigate the impact of various clinical factors. The model exhibited peak\nperformance in the initial phase (from 2012 to 2014) for segmenting the\nprostate, rectum, and bladder. However, we observed a notable decline in\nperformance for the prostate and rectum after 2015, while bladder contour\nquality remained stable. Key factors that impacted the prostate contour quality\nincluded physician contouring styles, the use of various hydrogel spacer, CT\nscan slice thickness, MRI-guided contouring, and using intravenous (IV)\ncontrast. Rectum contour quality was influenced by factors such as slice\nthickness, physician contouring styles, and the use of various hydrogel\nspacers. The bladder contour quality was primarily affected by using IV\ncontrast. This study highlights the challenges in maintaining AI model\nperformance consistency in a dynamic clinical setting. It underscores the need\nfor continuous monitoring and updating of AI models to ensure their ongoing\neffectiveness and relevance in patient care.\n","authors":["Biling Wang","Michael Dohopolski","Ti Bai","Junjie Wu","Raquibul Hannan","Neil Desai","Aurelie Garant","Daniel Yang","Dan Nguyen","Mu-Han Lin","Robert Timmerman","Xinlei Wang","Steve Jiang"],"pdf_url":"https://arxiv.org/pdf/2210.05673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09599v1","updated":"2023-11-16T06:18:35Z","published":"2023-11-16T06:18:35Z","title":"Gradual Source Domain Expansion for Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation (UDA) tries to overcome the need for a large\nlabeled dataset by transferring knowledge from a source dataset, with lots of\nlabeled data, to a target dataset, that has no labeled data. Since there are no\nlabels in the target domain, early misalignment might propagate into the later\nstages and lead to an error build-up. In order to overcome this problem, we\npropose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA\ntask several times from scratch, each time reinitializing the network weights,\nbut each time expands the source dataset with target data. In particular, the\nhighest-scoring target data of the previous run are employed as pseudo-source\nsamples with their respective pseudo-label. Using this strategy, the\npseudo-source samples induce knowledge extracted from the previous run directly\nfrom the start of the new training. This helps align the two domains better,\nespecially in the early training epochs. In this study, we first introduce a\nstrong baseline network and apply our GSDE strategy to it. We conduct\nexperiments and ablation studies on three benchmarks (Office-31, OfficeHome,\nand DomainNet) and outperform state-of-the-art methods. We further show that\nthe proposed GSDE strategy can improve the accuracy of a variety of different\nstate-of-the-art UDA approaches.\n","authors":["Thomas Westfechtel","Hao-Wei Yeh","Dexuan Zhang","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2311.09599v1.pdf","comment":"Accepted for WACV 2024"},{"id":"http://arxiv.org/abs/2311.09590v1","updated":"2023-11-16T06:02:03Z","published":"2023-11-16T06:02:03Z","title":"MARformer: An Efficient Metal Artifact Reduction Transformer for Dental\n  CBCT Images","summary":"  Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and\nsurgery. However, the metal teeth implants could bring annoying metal artifacts\nduring the CBCT imaging process, interfering diagnosis and downstream\nprocessing such as tooth segmentation. In this paper, we develop an efficient\nTransformer to perform metal artifacts reduction (MAR) from dental CBCT images.\nThe proposed MAR Transformer (MARformer) reduces computation complexity in the\nmultihead self-attention by a new Dimension-Reduced Self-Attention (DRSA)\nmodule, based on that the CBCT images have globally similar structure. A\nPatch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive\nlocal image information for fine-grained restoration. Experimental results on\nCBCT images with synthetic and real-world metal artifacts show that our\nMARformer is efficient and outperforms previous MAR methods and two restoration\nTransformers.\n","authors":["Yuxuan Shi","Jun Xu","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2311.09590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09574v1","updated":"2023-11-16T05:17:14Z","published":"2023-11-16T05:17:14Z","title":"LymphoML: An interpretable artificial intelligence-based method\n  identifies morphologic features that correlate with lymphoma subtype","summary":"  The accurate classification of lymphoma subtypes using hematoxylin and eosin\n(H&E)-stained tissue is complicated by the wide range of morphological features\nthese cancers can exhibit. We present LymphoML - an interpretable machine\nlearning method that identifies morphologic features that correlate with\nlymphoma subtypes. Our method applies steps to process H&E-stained tissue\nmicroarray cores, segment nuclei and cells, compute features encompassing\nmorphology, texture, and architecture, and train gradient-boosted models to\nmake diagnostic predictions. LymphoML's interpretable models, developed on a\nlimited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy\nto pathologists using whole-slide images and outperform black box deep-learning\non a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using\nSHapley Additive exPlanation (SHAP) analysis, we assess the impact of each\nfeature on model prediction and find that nuclear shape features are most\ndiscriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma\n(F1-score: 74.5%). Finally, we provide the first demonstration that a model\ncombining features from H&E-stained tissue with features from a standardized\npanel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a\n46-stain panel (86.1%).\n","authors":["Vivek Shankar","Xiaoli Yang","Vrishab Krishna","Brent Tan","Oscar Silva","Rebecca Rojansky","Andrew Ng","Fabiola Valvert","Edward Briercheck","David Weinstock","Yasodha Natkunam","Sebastian Fernandez-Pol","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2311.09574v1.pdf","comment":"To be published in Proceedings of the 3rd Machine Learning for Health\n  symposium, Proceedings of Machine Learning Research (PMLR)"},{"id":"http://arxiv.org/abs/2311.09571v1","updated":"2023-11-16T05:13:44Z","published":"2023-11-16T05:13:44Z","title":"3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score\n  Distillation","summary":"  In this work we develop 3D Paintbrush, a technique for automatically\ntexturing local semantic regions on meshes via text descriptions. Our method is\ndesigned to operate directly on meshes, producing texture maps which seamlessly\nintegrate into standard graphics pipelines. We opt to simultaneously produce a\nlocalization map (to specify the edit region) and a texture map which conforms\nto it. This synergistic approach improves the quality of both the localization\nand the stylization. To enhance the details and resolution of the textured\narea, we leverage multiple stages of a cascaded diffusion model to supervise\nour local editing technique with generative priors learned from images at\ndifferent resolutions. Our technique, referred to as Cascaded Score\nDistillation (CSD), simultaneously distills scores at multiple resolutions in a\ncascaded fashion, enabling control over both the granularity and global\nunderstanding of the supervision. We demonstrate the effectiveness of 3D\nPaintbrush to locally texture a variety of shapes within different semantic\nregions. Project page: https://threedle.github.io/3d-paintbrush\n","authors":["Dale Decatur","Itai Lang","Kfir Aberman","Rana Hanocka"],"pdf_url":"https://arxiv.org/pdf/2311.09571v1.pdf","comment":"Project page: https://threedle.github.io/3d-paintbrush"},{"id":"http://arxiv.org/abs/2310.18583v2","updated":"2023-11-16T05:02:34Z","published":"2023-10-28T04:16:08Z","title":"Self-Supervised Multi-Modality Learning for Multi-Label Skin Lesion\n  Classification","summary":"  The clinical diagnosis of skin lesion involves the analysis of dermoscopic\nand clinical modalities. Dermoscopic images provide a detailed view of the\nsurface structures whereas clinical images offer a complementary macroscopic\ninformation. The visual diagnosis of melanoma is also based on seven-point\nchecklist which involves identifying different visual attributes. Recently,\nsupervised learning approaches such as convolutional neural networks (CNNs)\nhave shown great performances using both dermoscopic and clinical modalities\n(Multi-modality). The seven different visual attributes in the checklist are\nalso used to further improve the the diagnosis. The performances of these\napproaches, however, are still reliant on the availability of large-scaled\nlabeled data. The acquisition of annotated dataset is an expensive and\ntime-consuming task, more so with annotating multi-attributes. To overcome this\nlimitation, we propose a self-supervised learning (SSL) algorithm for\nmulti-modality skin lesion classification. Our algorithm enables the\nmulti-modality learning by maximizing the similarities between paired\ndermoscopic and clinical images from different views. In addition, we generate\nsurrogate pseudo-multi-labels that represent seven attributes via clustering\nanalysis. We also propose a label-relation-aware module to refine each\npseudo-label embedding and capture the interrelationships between\npseudo-multi-labels. We validated the effectiveness of our algorithm using\nwell-benchmarked seven-point skin lesion dataset. Our results show that our\nalgorithm achieved better performances than other state-of-the-art SSL\ncounterparts.\n","authors":["Hao Wang","Euijoon Ahn","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2310.18583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09093v2","updated":"2023-11-16T04:12:51Z","published":"2023-11-15T16:41:18Z","title":"Applications of Computer Vision in Autonomous Vehicles: Methods,\n  Challenges and Future Directions","summary":"  Autonomous vehicle refers to a vehicle capable of perceiving its surrounding\nenvironment and driving with little or no human driver input. The perception\nsystem is a fundamental component which enables the autonomous vehicle to\ncollect data and extract relevant information from the environment to drive\nsafely. Benefit from the recent advances in computer vision, the perception\ntask can be achieved by using sensors, such as camera, LiDAR, radar, and\nultrasonic sensor. This paper reviews publications on computer vision and\nautonomous driving that are published during the last ten years. In particular,\nwe first investigate the development of autonomous driving systems and\nsummarize these systems that are developed by the major automotive\nmanufacturers from different countries. Second, we investigate the sensors and\nbenchmark data sets that are commonly utilized for autonomous driving. Then, a\ncomprehensive overview of computer vision applications for autonomous driving\nsuch as depth estimation, object detection, lane detection, and traffic sign\nrecognition are discussed. Additionally, we review public opinions and concerns\non autonomous vehicles. Based on the discussion, we analyze the current\ntechnological challenges that autonomous vehicles meet with. Finally, we\npresent our insights and point out some promising directions for future\nresearch. This paper will help the reader to understand autonomous vehicles\nfrom the perspectives of academia and industry.\n","authors":["Xingshuai Dong","Massimiliano L. Cappuccio"],"pdf_url":"https://arxiv.org/pdf/2311.09093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05405v2","updated":"2023-11-16T04:04:59Z","published":"2023-09-11T12:12:25Z","title":"Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and\n  Accurate Organ and Pan-cancer Segmentation in Abdomen CT","summary":"  Abdominal organ and tumour segmentation has many important clinical\napplications, such as organ quantification, surgical planning, and disease\ndiagnosis. However, manual assessment is inherently subjective with\nconsiderable inter- and intra-expert variability. In the paper, we propose a\nhybrid supervised framework, StMt, that integrates self-training and mean\nteacher for the segmentation of abdominal organs and tumors using partially\nlabeled and unlabeled data. We introduce a two-stage segmentation pipeline and\nwhole-volume-based input strategy to maximize segmentation accuracy while\nmeeting the requirements of inference time and GPU memory usage. Experiments on\nthe validation set of FLARE2023 demonstrate that our method achieves excellent\nsegmentation performance as well as fast and low-resource model inference. Our\nmethod achieved an average DSC score of 89.79\\% and 45.55 \\% for the organs and\nlesions on the validation set and the average running time and area under GPU\nmemory-time cure are 11.25s and 9627.82MB, respectively.\n","authors":["Wentao Liu","Tong Tian","Weijin Xu","Lemeng Wang","Haoyuan Li","Huihua Yang"],"pdf_url":"https://arxiv.org/pdf/2309.05405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00903v3","updated":"2023-11-16T03:53:45Z","published":"2023-02-02T06:41:02Z","title":"No One Left Behind: Real-World Federated Class-Incremental Learning","summary":"  Federated learning (FL) is a hot collaborative training framework via\naggregating model parameters of decentralized local clients. However, most FL\nmethods unreasonably assume data categories of FL framework are known and fixed\nin advance. Moreover, some new local clients that collect novel categories\nunseen by other clients may be introduced to FL training irregularly. These\nissues render global model to undergo catastrophic forgetting on old\ncategories, when local clients receive new categories consecutively under\nlimited memory of storing old categories. To tackle the above issues, we\npropose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local\nclients are left behind as they learn new classes continually, by addressing\nlocal and global catastrophic forgetting. Specifically, considering tackling\nclass imbalance of local client to surmount local forgetting, we develop a\ncategory-balanced gradient-adaptive compensation loss and a category\ngradient-induced semantic distillation loss. They can balance heterogeneous\nforgetting speeds of hard-to-forget and easy-to-forget old categories, while\nensure consistent class-relations within different tasks. Moreover, a proxy\nserver is designed to tackle global forgetting caused by Non-IID class\nimbalance between different clients. It augments perturbed prototype images of\nnew categories collected from local clients via self-supervised prototype\naugmentation, thus improving robustness to choose the best old global model for\nlocal-side semantic distillation loss. Experiments on representative datasets\nverify superior performance of our model against comparison methods. The code\nis available at https://github.com/JiahuaDong/LGA.\n","authors":["Jiahua Dong","Hongliu Li","Yang Cong","Gan Sun","Yulun Zhang","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2302.00903v3.pdf","comment":"Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence 2023 (TPAMI 2023)"},{"id":"http://arxiv.org/abs/2311.09543v1","updated":"2023-11-16T03:35:17Z","published":"2023-11-16T03:35:17Z","title":"Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery","summary":"  Though significant progress in human pose and shape recovery from monocular\nRGB images has been made in recent years, obtaining 3D human motion with high\naccuracy and temporal consistency from videos remains challenging. Existing\nvideo-based methods tend to reconstruct human motion from global image\nfeatures, which lack detailed representation capability and limit the\nreconstruction accuracy. In this paper, we propose a Temporal-Aware Refining\nNetwork (TAR), to synchronously explore temporal-aware global and local image\nfeatures for accurate pose and shape recovery. First, a global transformer\nencoder is introduced to obtain temporal global features from static feature\nsequences. Second, a bidirectional ConvGRU network takes the sequence of\nhigh-resolution feature maps as input, and outputs temporal local feature maps\nthat maintain high resolution and capture the local motion of the human body.\nFinally, a recurrent refinement module iteratively updates estimated SMPL\nparameters by leveraging both global and local temporal information to achieve\naccurate and smooth results. Extensive experiments demonstrate that our TAR\nobtains more accurate results than previous state-of-the-art methods on popular\nbenchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.\n","authors":["Ming Chen","Yan Zhou","Weihua Jian","Pengfei Wan","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09543v1.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.09540v1","updated":"2023-11-16T03:29:19Z","published":"2023-11-16T03:29:19Z","title":"FedFusion: Manifold Driven Federated Learning for Multi-satellite and\n  Multi-modality Fusion","summary":"  Multi-satellite, multi-modality in-orbit fusion is a challenging task as it\nexplores the fusion representation of complex high-dimensional data under\nlimited computational resources. Deep neural networks can reveal the underlying\ndistribution of multi-modal remote sensing data, but the in-orbit fusion of\nmultimodal data is more difficult because of the limitations of different\nsensor imaging characteristics, especially when the multimodal data follows\nnon-independent identically distribution (Non-IID) distributions. To address\nthis problem while maintaining classification performance, this paper proposes\na manifold-driven multi-modality fusion framework, FedFusion, which randomly\nsamples local data on each client to jointly estimate the prominent manifold\nstructure of shallow features of each client and explicitly compresses the\nfeature matrices into a low-rank subspace through cascading and additive\napproaches, which is used as the feature input of the subsequent classifier.\nConsidering the physical space limitations of the satellite constellation, we\ndeveloped a multimodal federated learning module designed specifically for\nmanifold data in a deep latent space. This module achieves iterative updating\nof the sub-network parameters of each client through global weighted averaging,\nconstructing a framework that can represent compact representations of each\nclient. The proposed framework surpasses existing methods in terms of\nperformance on three multimodal datasets, achieving a classification average\naccuracy of 94.35$\\%$ while compressing communication costs by a factor of 4.\nFurthermore, extensive numerical evaluations of real-world satellite images\nwere conducted on the orbiting edge computing architecture based on Jetson TX2\nindustrial modules, which demonstrated that FedFusion significantly reduced\ntraining time by 48.4 minutes (15.18%) while optimizing accuracy.}\n","authors":["DaiXun Li","Weiying Xie","Yunsong Li","Leyuan Fang"],"pdf_url":"https://arxiv.org/pdf/2311.09540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09048v4","updated":"2023-11-16T03:16:03Z","published":"2022-05-18T16:28:56Z","title":"Global Contrast Masked Autoencoders Are Powerful Pathological\n  Representation Learners","summary":"  Based on digital pathology slice scanning technology, artificial intelligence\nalgorithms represented by deep learning have achieved remarkable results in the\nfield of computational pathology. Compared to other medical images, pathology\nimages are more difficult to annotate, and thus, there is an extreme lack of\navailable datasets for conducting supervised learning to train robust deep\nlearning models. In this paper, we propose a self-supervised learning (SSL)\nmodel, the global contrast-masked autoencoder (GCMAE), which can train the\nencoder to have the ability to represent local-global features of pathological\nimages, also significantly improve the performance of transfer learning across\ndata sets. In this study, the ability of the GCMAE to learn migratable\nrepresentations was demonstrated through extensive experiments using a total of\nthree different disease-specific hematoxylin and eosin (HE)-stained pathology\ndatasets: Camelyon16, NCTCRC and BreakHis. In addition, this study designed an\neffective automated pathology diagnosis process based on the GCMAE for clinical\napplications. The source code of this paper is publicly available at\nhttps://github.com/StarUniversus/gcmae.\n","authors":["Hao Quan","Xingyu Li","Weixing Chen","Qun Bai","Mingchen Zou","Ruijie Yang","Tingting Zheng","Ruiqun Qi","Xinghua Gao","Xiaoyu Cui"],"pdf_url":"https://arxiv.org/pdf/2205.09048v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09520v1","updated":"2023-11-16T02:55:21Z","published":"2023-11-16T02:55:21Z","title":"MDFL: Multi-domain Diffusion-driven Feature Learning","summary":"  High-dimensional images, known for their rich semantic information, are\nwidely applied in remote sensing and other fields. The spatial information in\nthese images reflects the object's texture features, while the spectral\ninformation reveals the potential spectral representations across different\nbands. Currently, the understanding of high-dimensional images remains limited\nto a single-domain perspective with performance degradation. Motivated by the\nmasking texture effect observed in the human visual system, we present a\nmulti-domain diffusion-driven feature learning network (MDFL) , a scheme to\nredefine the effective information domain that the model really focuses on.\nThis method employs diffusion-based posterior sampling to explicitly consider\njoint information interactions between the high-dimensional manifold structures\nin the spectral, spatial, and frequency domains, thereby eliminating the\ninfluence of masking texture effects in visual models. Additionally, we\nintroduce a feature reuse mechanism to gather deep and raw features of\nhigh-dimensional data. We demonstrate that MDFL significantly improves the\nfeature extraction performance of high-dimensional data, thereby providing a\npowerful aid for revealing the intrinsic patterns and structures of such data.\nThe experimental results on three multi-modal remote sensing datasets show that\nMDFL reaches an average overall accuracy of 98.25%, outperforming various\nstate-of-the-art baseline schemes. The code will be released, contributing to\nthe computer vision community.\n","authors":["Daixun Li","Weiying Xie","Jiaqing Zhang","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2311.09520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02582v2","updated":"2023-11-16T02:32:51Z","published":"2023-06-05T04:21:00Z","title":"Enhancing Point Annotations with Superpixel and Confidence Learning\n  Guided for Improving Semi-Supervised OCT Fluid Segmentation","summary":"  Automatic segmentation of fluid in Optical Coherence Tomography (OCT) images\nis beneficial for ophthalmologists to make an accurate diagnosis. Although\nsemi-supervised OCT fluid segmentation networks enhance their performance by\nintroducing additional unlabeled data, the performance enhancement is limited.\nTo address this, we propose Superpixel and Confident Learning Guide Point\nAnnotations Network (SCLGPA-Net) based on the teacher-student architecture,\nwhich can learn OCT fluid segmentation from limited fully-annotated data and\nabundant point-annotated data. Specifically, we use points to annotate fluid\nregions in unlabeled OCT images and the Superpixel-Guided Pseudo-Label\nGeneration (SGPLG) module generates pseudo-labels and pixel-level label trust\nmaps from the point annotations. The label trust maps provide an indication of\nthe reliability of the pseudo-labels. Furthermore, we propose the Confident\nLearning Guided Label Refinement (CLGLR) module identifies error information in\nthe pseudo-labels and leads to further refinement. Experiments on the RETOUCH\ndataset show that we are able to reduce the need for fully-annotated data by\n94.22\\%, closing the gap with the best fully supervised baselines to a mean IoU\nof only 2\\%. Furthermore, We constructed a private 2D OCT fluid segmentation\ndataset for evaluation. Compared with other methods, comprehensive experimental\nresults demonstrate that the proposed method can achieve excellent performance\nin OCT fluid segmentation.\n","authors":["Tengjin Weng","Yang Shen","Kai Jin","Zhiming Cheng","Yunxiang Li","Gewen Zhang","Shuai Wang","Yaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.02582v2.pdf","comment":"Submission to MIA"},{"id":"http://arxiv.org/abs/2311.09500v1","updated":"2023-11-16T01:52:24Z","published":"2023-11-16T01:52:24Z","title":"Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation","summary":"  This paper addresses the simulation-to-real domain gap in 6DoF PE, and\nproposes a novel self-supervised keypoint radial voting-based 6DoF PE\nframework, effectively narrowing this gap using a learnable kernel in RKHS. We\nformulate this domain gap as a distance in high-dimensional feature space,\ndistinct from previous iterative matching methods. We propose an adapter\nnetwork, which evolves the network parameters from the source domain, which has\nbeen massively trained on synthetic data with synthetic poses, to the target\ndomain, which is trained on real data. Importantly, the real data training only\nuses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real\ngroundtruth data annotations. RKHSPose achieves state-of-the-art performance on\nthree commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion\nLINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully\nsupervised methods on all six applicable BOP core datasets, achieving within\n-10.8% to -0.3% of the top fully supervised results.\n","authors":["Yangzheng Wu","Michael Greenspan"],"pdf_url":"https://arxiv.org/pdf/2311.09500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09499v1","updated":"2023-11-16T01:52:11Z","published":"2023-11-16T01:52:11Z","title":"Center Focusing Network for Real-Time LiDAR Panoptic Segmentation","summary":"  LiDAR panoptic segmentation facilitates an autonomous vehicle to\ncomprehensively understand the surrounding objects and scenes and is required\nto run in real time. The recent proposal-free methods accelerate the algorithm,\nbut their effectiveness and efficiency are still limited owing to the\ndifficulty of modeling non-existent instance centers and the costly\ncenter-based clustering modules. To achieve accurate and real-time LiDAR\npanoptic segmentation, a novel center focusing network (CFNet) is introduced.\nSpecifically, the center focusing feature encoding (CFFE) is proposed to\nexplicitly understand the relationships between the original LiDAR points and\nvirtual instance centers by shifting the LiDAR points and filling in the center\npoints. Moreover, to leverage the redundantly detected centers, a fast center\ndeduplication module (CDM) is proposed to select only one center for each\ninstance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation\nbenchmarks demonstrate that our CFNet outperforms all existing methods by a\nlarge margin and is 1.6 times faster than the most efficient method. The code\nis available at https://github.com/GangZhang842/CFNet.\n","authors":["Xiaoyan Li","Gang Zhang","Boyue Wang","Yongli Hu","Baocai Yin"],"pdf_url":"https://arxiv.org/pdf/2311.09499v1.pdf","comment":"Published in the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2023)"},{"id":"http://arxiv.org/abs/2304.12470v3","updated":"2023-11-16T00:33:24Z","published":"2023-04-24T21:58:14Z","title":"Vision-based Estimation of Fatigue and Engagement in Cognitive Training\n  Sessions","summary":"  Computerized cognitive training (CCT) is a scalable, well-tolerated\nintervention that has promise for slowing cognitive decline. Outcomes from CCT\nare limited by a lack of effective engagement, which is decreased by factors\nsuch as mental fatigue, particularly in older adults at risk for dementia.\nThere is a need for scalable, automated measures that can monitor mental\nfatigue during CCT. Here, we develop and validate a novel Recurrent Video\nTransformer (RVT) method for monitoring real-time mental fatigue in older\nadults with mild cognitive impairment from video-recorded facial gestures\nduring CCT. The RVT model achieved the highest balanced accuracy(78%) and\nprecision (0.82) compared to the prior state-of-the-art models for binary and\nmulti-class classification of mental fatigue and was additionally validated via\nsignificant association (p=0.023) with CCT reaction time. By leveraging dynamic\ntemporal information, the RVT model demonstrates the potential to accurately\nmeasure real-time mental fatigue, laying the foundation for future personalized\nCCT that increase effective engagement.\n","authors":["Yanchen Wang","Adam Turnbull","Yunlong Xu","Kathi Heffner","Feng Vankee Lin","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2304.12470v3.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.01655v2","updated":"2023-11-16T00:22:27Z","published":"2023-11-03T01:12:35Z","title":"Detecting Spurious Correlations via Robust Visual Concepts in Real and\n  AI-Generated Image Classification","summary":"  Often machine learning models tend to automatically learn associations\npresent in the training data without questioning their validity or\nappropriateness. This undesirable property is the root cause of the\nmanifestation of spurious correlations, which render models unreliable and\nprone to failure in the presence of distribution shifts. Research shows that\nmost methods attempting to remedy spurious correlations are only effective for\na model's known spurious associations. Current spurious correlation detection\nalgorithms either rely on extensive human annotations or are too restrictive in\ntheir formulation. Moreover, they rely on strict definitions of visual\nartifacts that may not apply to data produced by generative models, as they are\nknown to hallucinate contents that do not conform to standard specifications.\nIn this work, we introduce a general-purpose method that efficiently detects\npotential spurious correlations, and requires significantly less human\ninterference in comparison to the prior art. Additionally, the proposed method\nprovides intuitive explanations while eliminating the need for pixel-level\nannotations. We demonstrate the proposed method's tolerance to the peculiarity\nof AI-generated images, which is a considerably challenging task, one where\nmost of the existing methods fall short. Consequently, our method is also\nsuitable for detecting spurious correlations that may propagate to downstream\napplications originating from generative models.\n","authors":["Preetam Prabhu Srikar Dammu","Chirag Shah"],"pdf_url":"https://arxiv.org/pdf/2311.01655v2.pdf","comment":"Paper accepted at 37th Conference on Neural Information Processing\n  Systems (NeurIPS 2023), XAIA Workshop"},{"id":"http://arxiv.org/abs/2306.05399v2","updated":"2023-11-16T23:52:37Z","published":"2023-06-08T17:51:58Z","title":"Matting Anything","summary":"  In this paper, we propose the Matting Anything Model (MAM), an efficient and\nversatile framework for estimating the alpha matte of any instance in an image\nwith flexible and interactive visual or linguistic user prompt guidance. MAM\noffers several significant advantages over previous specialized image matting\nnetworks: (i) MAM is capable of dealing with various types of image matting,\nincluding semantic, instance, and referring image matting with only a single\nmodel; (ii) MAM leverages the feature maps from the Segment Anything Model\n(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha\nmatte through iterative refinement, which has only 2.7 million trainable\nparameters. (iii) By incorporating SAM, MAM simplifies the user intervention\nrequired for the interactive use of image matting from the trimap to the box,\npoint, or text prompt. We evaluate the performance of MAM on various image\nmatting benchmarks, and the experimental results demonstrate that MAM achieves\ncomparable performance to the state-of-the-art specialized image matting models\nunder different metrics on each benchmark. Overall, MAM shows superior\ngeneralization ability and can effectively handle various image matting tasks\nwith fewer parameters, making it a practical solution for unified image\nmatting. Our code and models are open-sourced at\nhttps://github.com/SHI-Labs/Matting-Anything.\n","authors":["Jiachen Li","Jitesh Jain","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2306.05399v2.pdf","comment":"Project web-page:\n  https://chrisjuniorli.github.io/project/Matting-Anything/"},{"id":"http://arxiv.org/abs/2311.10234v1","updated":"2023-11-16T23:49:05Z","published":"2023-11-16T23:49:05Z","title":"The Analysis and Extraction of Structure from Organizational Charts","summary":"  Organizational charts, also known as org charts, are critical representations\nof an organization's structure and the hierarchical relationships between its\ncomponents and positions. However, manually extracting information from org\ncharts can be error-prone and time-consuming. To solve this, we present an\nautomated and end-to-end approach that uses computer vision, deep learning, and\nnatural language processing techniques. Additionally, we propose a metric to\nevaluate the completeness and hierarchical accuracy of the extracted\ninformation. This approach has the potential to improve organizational\nrestructuring and resource utilization by providing a clear and concise\nrepresentation of the organizational structure. Our study lays a foundation for\nfurther research on the topic of hierarchical chart analysis.\n","authors":["Nikhil Manali","David Doermann","Mahesh Desai"],"pdf_url":"https://arxiv.org/pdf/2311.10234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02738v2","updated":"2023-11-16T23:25:25Z","published":"2023-11-05T19:04:25Z","title":"Scenario Diffusion: Controllable Driving Scenario Generation With\n  Diffusion","summary":"  Automated creation of synthetic traffic scenarios is a key part of validating\nthe safety of autonomous vehicles (AVs). In this paper, we propose Scenario\nDiffusion, a novel diffusion-based architecture for generating traffic\nscenarios that enables controllable scenario generation. We combine latent\ndiffusion, object detection and trajectory regression to generate distributions\nof synthetic agent poses, orientations and trajectories simultaneously. To\nprovide additional control over the generated scenario, this distribution is\nconditioned on a map and sets of tokens describing the desired scenario. We\nshow that our approach has sufficient expressive capacity to model diverse\ntraffic patterns and generalizes to different geographical regions.\n","authors":["Ethan Pronovost","Meghana Reddy Ganesina","Noureldin Hendy","Zeyu Wang","Andres Morales","Kai Wang","Nicholas Roy"],"pdf_url":"https://arxiv.org/pdf/2311.02738v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.10224v1","updated":"2023-11-16T22:31:05Z","published":"2023-11-16T22:31:05Z","title":"CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular\n  Segmentation of Enhanced TOF-MRA Images","summary":"  Due to the lack of automated methods, to diagnose cerebrovascular disease,\ntime-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually,\nmaking it time-consuming. The commonly used encoder-decoder architectures for\ncerebrovascular segmentation utilize redundant features, eventually leading to\nthe extraction of low-level features multiple times. Additionally,\nconvolutional neural networks (CNNs) suffer from performance degradation when\nthe batch size is small, and deeper networks experience the vanishing gradient\nproblem. Methods: In this paper, we attempt to solve these limitations and\npropose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet,\nfor precise extraction of brain vessel images. We proposed a sequence of\npreprocessing techniques followed by deeply supervised UNet to improve the\naccuracy of segmentation of the brain vessels leading to a stroke. To combine\nthe low and high semantics, we applied the attention mechanism. This mechanism\nfocuses on relevant associations and neglects irrelevant anatomical\ninformation. Furthermore, the inclusion of deep supervision incorporates\ndifferent levels of features that prove to be beneficial for network\nconvergence. Results: We demonstrate the efficiency of the proposed method by\ncross-validating with an unlabeled dataset, which was further labeled by us. We\nbelieve that the novelty of this algorithm lies in its ability to perform well\non both labeled and unlabeled data with image processing-based enhancement. The\nresults indicate that our method performed better than the existing\nstate-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method\nwill help in accurate segmentation of cerebrovascular structure leading to\nstroke\n","authors":["Syed Farhan Abbas","Nguyen Thanh Duc","Yoonguu Song","Kyungwon Kim","Boreom Lee"],"pdf_url":"https://arxiv.org/pdf/2311.10224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05231v3","updated":"2023-11-16T22:00:04Z","published":"2022-12-10T07:19:43Z","title":"NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view\n  Reconstruction","summary":"  Recent methods for neural surface representation and rendering, for example\nNeuS, have demonstrated the remarkably high-quality reconstruction of static\nscenes. However, the training of NeuS takes an extremely long time (8 hours),\nwhich makes it almost impossible to apply them to dynamic scenes with thousands\nof frames. We propose a fast neural surface reconstruction approach, called\nNeuS2, which achieves two orders of magnitude improvement in terms of\nacceleration without compromising reconstruction quality. To accelerate the\ntraining process, we parameterize a neural surface representation by\nmulti-resolution hash encodings and present a novel lightweight calculation of\nsecond-order derivatives tailored to our networks to leverage CUDA parallelism,\nachieving a factor two speed up. To further stabilize and expedite training, a\nprogressive learning strategy is proposed to optimize multi-resolution hash\nencodings from coarse to fine. We extend our method for fast training of\ndynamic scenes, with a proposed incremental training strategy and a novel\nglobal transformation prediction component, which allow our method to handle\nchallenging long sequences with large movements and deformations. Our\nexperiments on various datasets demonstrate that NeuS2 significantly\noutperforms the state-of-the-arts in both surface reconstruction accuracy and\ntraining speed for both static and dynamic scenes. The code is available at our\nwebsite: https://vcai.mpi-inf.mpg.de/projects/NeuS2/ .\n","authors":["Yiming Wang","Qin Han","Marc Habermann","Kostas Daniilidis","Christian Theobalt","Lingjie Liu"],"pdf_url":"https://arxiv.org/pdf/2212.05231v3.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2311.10207v1","updated":"2023-11-16T21:43:05Z","published":"2023-11-16T21:43:05Z","title":"Stella Nera: Achieving 161 TOp/s/W with Multiplier-free DNN Acceleration\n  based on Approximate Matrix Multiplication","summary":"  From classical HPC to deep learning, MatMul is at the heart of today's\ncomputing. The recent Maddness method approximates MatMul without the need for\nmultiplication by using a hash-based version of product quantization (PQ)\nindexing into a look-up table (LUT). Stella Nera is the first Maddness\naccelerator and it achieves 15x higher area efficiency (GMAC/s/mm^2) and more\nthan 25x higher energy efficiency (TMAC/s/W) than direct MatMul accelerators\nimplemented in the same technology. The hash function is a decision tree, which\nallows for an efficient hardware implementation as the multiply-accumulate\noperations are replaced by decision tree passes and LUT lookups. The entire\nMaddness MatMul can be broken down into parts that allow an effective\nimplementation with small computing units and memories, allowing it to reach\nextreme efficiency while remaining generically applicable for MatMul tasks. In\na commercial 14nm technology and scaled to 3nm, we achieve an energy efficiency\nof 161 TOp/s/W@0.55V with a Top-1 accuracy on CIFAR-10 of more than 92.5% using\nResNet9.\n","authors":["Jannis Schönleber","Lukas Cavigelli","Renzo Andri","Matteo Perotti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2311.10207v1.pdf","comment":"6 pages, 7 figures, preprint under review"},{"id":"http://arxiv.org/abs/2311.05146v2","updated":"2023-11-16T21:23:29Z","published":"2023-11-09T05:06:55Z","title":"OW-SLR: Overlapping Windows on Semi-Local Region for Image\n  Super-Resolution","summary":"  There has been considerable progress in implicit neural representation to\nupscale an image to any arbitrary resolution. However, existing methods are\nbased on defining a function to predict the Red, Green and Blue (RGB) value\nfrom just four specific loci. Relying on just four loci is insufficient as it\nleads to losing fine details from the neighboring region(s). We show that by\ntaking into account the semi-local region leads to an improvement in\nperformance. In this paper, we propose applying a new technique called\nOverlapping Windows on Semi-Local Region (OW-SLR) to an image to obtain any\narbitrary resolution by taking the coordinates of the semi-local region around\na point in the latent space. This extracted detail is used to predict the RGB\nvalue of a point. We illustrate the technique by applying the algorithm to the\nOptical Coherence Tomography-Angiography (OCT-A) images and show that it can\nupscale them to random resolution. This technique outperforms the existing\nstate-of-the-art methods when applied to the OCT500 dataset. OW-SLR provides\nbetter results for classifying healthy and diseased retinal images such as\ndiabetic retinopathy and normals from the given set of OCT-A images. The\nproject page is available at https://rishavbb.github.io/ow-slr/index.html\n","authors":["Rishav Bhardwaj","Janarthanam Jothi Balaji","Vasudevan Lakshminarayanan"],"pdf_url":"https://arxiv.org/pdf/2311.05146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04447v2","updated":"2023-11-16T21:12:53Z","published":"2023-09-08T17:13:22Z","title":"Impact of Blur and Resolution on Demographic Disparities in 1-to-Many\n  Facial Identification","summary":"  Most studies to date that have examined demographic variations in face\nrecognition accuracy have analyzed 1-to-1 matching accuracy, using images that\ncould be described as \"government ID quality\". This paper analyzes the accuracy\nof 1-to-many facial identification across demographic groups, and in the\npresence of blur and reduced resolution in the probe image as might occur in\n\"surveillance camera quality\" images. Cumulative match characteristic curves\n(CMC) are not appropriate for comparing propensity for rank-one recognition\nerrors across demographics, and so we use three metrics for our analysis: (1)\nthe well-known d' metric between mated and non-mated score distributions, and\nintroduced in this work, (2) absolute score difference between thresholds in\nthe high-similarity tail of the non-mated and the low-similarity tail of the\nmated distribution, and (3) distribution of (mated - non-mated rank-one scores)\nacross the set of probe images. We find that demographic variation in 1-to-many\naccuracy does not entirely follow what has been observed in 1-to-1 matching\naccuracy. Also, different from 1-to-1 accuracy, demographic comparison of\n1-to-many accuracy can be affected by different numbers of identities and\nimages across demographics. More importantly, we show that increased blur in\nthe probe image, or reduced resolution of the face in the probe image, can\nsignificantly increase the false positive identification rate. And we show that\nthe demographic variation in these high blur or low resolution conditions is\nmuch larger for male / female than for African-American / Caucasian. The point\nthat 1-to-many accuracy can potentially collapse in the context of processing\n\"surveillance camera quality\" probe images against a \"government ID quality\"\ngallery is an important one.\n","authors":["Aman Bhatta","Gabriella Pangelinan","Michael C. King","Kevin W. Bowyer"],"pdf_url":"https://arxiv.org/pdf/2309.04447v2.pdf","comment":"9 pages, 8 figures, Conference submission"},{"id":"http://arxiv.org/abs/2311.00689v2","updated":"2023-11-16T21:02:05Z","published":"2023-11-01T17:45:22Z","title":"Collaboration in Immersive Environments: Challenges and Solutions","summary":"  Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in\nall engineering fields in order to avoid the use of physical prototypes, to\ntrain in high-risk situations, and to interpret real or simulated results. In\norder to complete a shared task or assign tasks to the agents in such immersive\nenvironments, collaboration or Shared Cooperative Activities are a necessity.\nCollaboration in immersive environments is an emerging field of research that\naims to study and enhance the ways in which people interact and work together\nin Virtual and Augmented Reality settings. Collaboration in immersive\nenvironments is a complex process that involves different factors such as\ncommunication, coordination, and social presence. This paper provides an\noverview of the current state of research on collaboration in immersive\nenvironments. It discusses the different types of immersive environments,\nincluding VR and AR, and the different forms of collaboration that can occur in\nthese environments. The paper also highlights the challenges and limitations of\ncollaboration in immersive environments, such as the lack of physical cues,\ncost and usability and the need for further research in this area. Overall,\ncollaboration in immersive environments is a promising field with a wide range\nof potential applications, from education to industry, and it can benefit both\nindividuals and groups by enhancing their ability to work together effectively.\n","authors":["Shahin Doroudian"],"pdf_url":"https://arxiv.org/pdf/2311.00689v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10177v1","updated":"2023-11-16T20:09:47Z","published":"2023-11-16T20:09:47Z","title":"Towards Improving Robustness Against Common Corruptions using Mixture of\n  Class Specific Experts","summary":"  Neural networks have demonstrated significant accuracy across various\ndomains, yet their vulnerability to subtle input alterations remains a\npersistent challenge. Conventional methods like data augmentation, while\neffective to some extent, fall short in addressing unforeseen corruptions,\nlimiting the adaptability of neural networks in real-world scenarios. In\nresponse, this paper introduces a novel paradigm known as the Mixture of\nClass-Specific Expert Architecture. The approach involves disentangling feature\nlearning for individual classes, offering a nuanced enhancement in scalability\nand overall performance. By training dedicated network segments for each class\nand subsequently aggregating their outputs, the proposed architecture aims to\nmitigate vulnerabilities associated with common neural network structures. The\nstudy underscores the importance of comprehensive evaluation methodologies,\nadvocating for the incorporation of benchmarks like the common corruptions\nbenchmark. This inclusion provides nuanced insights into the vulnerabilities of\nneural networks, especially concerning their generalization capabilities and\nrobustness to unforeseen distortions. The research aligns with the broader\nobjective of advancing the development of highly robust learning systems\ncapable of nuanced reasoning across diverse and challenging real-world\nscenarios. Through this contribution, the paper aims to foster a deeper\nunderstanding of neural network limitations and proposes a practical approach\nto enhance their resilience in the face of evolving and unpredictable\nconditions.\n","authors":["Shashank Kotyan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2311.10177v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.07928;\n  text overlap with arXiv:1903.12261 by other authors"},{"id":"http://arxiv.org/abs/2311.10162v1","updated":"2023-11-16T19:34:18Z","published":"2023-11-16T19:34:18Z","title":"K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without\n  Noise","summary":"  Deep learning-based MRI reconstruction models have achieved superior\nperformance these days. Most recently, diffusion models have shown remarkable\nperformance in image generation, in-painting, super-resolution, image editing\nand more. As a generalized diffusion model, cold diffusion further broadens the\nscope and considers models built around arbitrary image transformations such as\nblurring, down-sampling, etc. In this paper, we propose a k-space cold\ndiffusion model that performs image degradation and restoration in k-space\nwithout the need for Gaussian noise. We provide comparisons with multiple deep\nlearning-based MRI reconstruction models and perform tests on a well-known\nlarge open-source MRI dataset. Our results show that this novel way of\nperforming degradation can generate high-quality reconstruction images for\naccelerated MRI.\n","authors":["Guoyao Shen","Mengyu Li","Chad W. Farris","Stephan Anderson","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.10162v1.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2306.06354v3","updated":"2023-11-16T19:26:02Z","published":"2023-06-10T06:05:35Z","title":"EventCLIP: Adapting CLIP for Event-based Object Recognition","summary":"  Recent advances in zero-shot and few-shot classification heavily rely on the\nsuccess of pre-trained vision-language models (VLMs) such as CLIP. Due to a\nshortage of large-scale datasets, training such models for event camera data\nremains infeasible. Thus, adapting existing VLMs across modalities to event\nvision is an important research challenge. In this work, we introduce\nEventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot\nevent-based object recognition. We first generalize CLIP's image encoder to\nevent data by converting raw events to 2D grid-based representations. To\nfurther enhance performance, we propose a feature adapter to aggregate temporal\ninformation over event frames and refine text embeddings to better align with\nthe visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet\ndatasets, achieving state-of-the-art few-shot performance. When fine-tuned on\nthe entire dataset, our method outperforms all existing event classifiers.\nMoreover, we explore practical applications of EventCLIP including robust event\nclassification and label-free event recognition, where our approach surpasses\nprevious baselines designed specifically for these tasks.\n","authors":["Ziyi Wu","Xudong Liu","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2306.06354v3.pdf","comment":"Add results on 1) EventCLIP with another VLM FLIP 2) inference speed\n  analysis"},{"id":"http://arxiv.org/abs/2311.07623v2","updated":"2023-11-16T17:17:40Z","published":"2023-11-13T07:44:56Z","title":"PadChannel: Improving CNN Performance through Explicit Padding Encoding","summary":"  In convolutional neural networks (CNNs), padding plays a pivotal role in\npreserving spatial dimensions throughout the layers. Traditional padding\ntechniques do not explicitly distinguish between the actual image content and\nthe padded regions, potentially causing CNNs to incorrectly interpret the\nboundary pixels or regions that resemble boundaries. This ambiguity can lead to\nsuboptimal feature extraction. To address this, we propose PadChannel, a novel\npadding method that encodes padding statuses as an additional input channel,\nenabling CNNs to easily distinguish genuine pixels from padded ones. By\nincorporating PadChannel into several prominent CNN architectures, we observed\nsmall performance improvements and notable reductions in the variances on the\nImageNet-1K image classification task at marginal increases in the\ncomputational cost. The source code is available at\nhttps://github.com/AussieSeaweed/pad-channel\n","authors":["Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2311.07623v2.pdf","comment":"7 pages, 4 figures, submitted to the 2024 IEEE/CVF Conference on\n  Computer Vision and Pattern Recognition"},{"id":"http://arxiv.org/abs/2311.10126v1","updated":"2023-11-16T13:07:47Z","published":"2023-11-16T13:07:47Z","title":"I&S-ViT: An Inclusive & Stable Method for Pushing the Limit of\n  Post-Training ViTs Quantization","summary":"  Albeit the scalable performance of vision transformers (ViTs), the dense\ncomputational costs (training & inference) undermine their position in\nindustrial applications. Post-training quantization (PTQ), tuning ViTs with a\ntiny dataset and running in a low-bit format, well addresses the cost issue but\nunluckily bears more performance drops in lower-bit cases. In this paper, we\nintroduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an\ninclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of\nViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for\npost-Softmax activations; (2) Rugged and magnified loss landscape in\ncoarse-grained quantization granularity for post-LayerNorm activations. Then,\nI&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2\nquantizer (SULQ) that incorporates a shift mechanism followed by uniform\nquantization to achieve both an inclusive domain representation and accurate\ndistribution approximation; (2) A three-stage smooth optimization strategy\n(SOS) that amalgamates the strengths of channel-wise and layer-wise\nquantization to enable stable learning. Comprehensive evaluations across\ndiverse vision tasks validate I&S-ViT' superiority over existing PTQ of ViTs\nmethods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the\nperformance of 3-bit ViT-B by an impressive 50.68%.\n","authors":["Yunshan Zhong","Jiawei Hu","Mingbao Lin","Mengzhao Chen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2311.10126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10125v1","updated":"2023-11-16T13:01:25Z","published":"2023-11-16T13:01:25Z","title":"UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized\n  Multimodal Framework","summary":"  In the current landscape of artificial intelligence, foundation models serve\nas the bedrock for advancements in both language and vision domains. OpenAI\nGPT-4 has emerged as the pinnacle in large language models (LLMs), while the\ncomputer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models\nsuch as Meta's SAM and DINO, and YOLOS. However, the financial and\ncomputational burdens of training new models from scratch remain a significant\nbarrier to progress. In response to this challenge, we introduce\nUnifiedVisionGPT, a novel framework designed to consolidate and automate the\nintegration of SOTA vision models, thereby facilitating the development of\nvision-oriented AI. UnifiedVisionGPT distinguishes itself through four key\nfeatures: (1) provides a versatile multimodal framework adaptable to a wide\nrange of applications, building upon the strengths of multimodal foundation\nmodels; (2) seamlessly integrates various SOTA vision models to create a\ncomprehensive multimodal platform, capitalizing on the best components of each\nmodel; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in\nthe CV domain compared to the current trajectory of LLMs; and (4) introduces\nautomation in the selection of SOTA vision models, generating optimal results\nbased on diverse multimodal inputs such as text prompts and images. This paper\noutlines the architecture and capabilities of UnifiedVisionGPT, demonstrating\nits potential to revolutionize the field of computer vision through enhanced\nefficiency, versatility, generalization, and performance. Our implementation,\nalong with the unified multimodal framework and comprehensive dataset, is made\npublicly available at https://github.com/LHBuilder/SA-Segment-Anything.\n","authors":["Chris Kelly","Luhui Hu","Cindy Yang","Yu Tian","Deshun Yang","Bang Yang","Zaoshan Huang","Zihao Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2311.10125v1.pdf","comment":"9 pages, 29 figures"},{"id":"http://arxiv.org/abs/2311.10123v1","updated":"2023-11-16T11:35:10Z","published":"2023-11-16T11:35:10Z","title":"MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry\n  and Texture","summary":"  Generative models for 3D object synthesis have seen significant advancements\nwith the incorporation of prior knowledge distilled from 2D diffusion models.\nNevertheless, challenges persist in the form of multi-view geometric\ninconsistencies and slow generation speeds within the existing 3D synthesis\nframeworks. This can be attributed to two factors: firstly, the deficiency of\nabundant geometric a priori knowledge in optimization, and secondly, the\nentanglement issue between geometry and texture in conventional 3D generation\nmethods.In response, we introduce MetaDreammer, a two-stage optimization\napproach that leverages rich 2D and 3D prior knowledge. In the first stage, our\nemphasis is on optimizing the geometric representation to ensure multi-view\nconsistency and accuracy of 3D objects. In the second stage, we concentrate on\nfine-tuning the geometry and optimizing the texture, thereby achieving a more\nrefined 3D object. Through leveraging 2D and 3D prior knowledge in two stages,\nrespectively, we effectively mitigate the interdependence between geometry and\ntexture. MetaDreamer establishes clear optimization objectives for each stage,\nresulting in significant time savings in the 3D generation process. Ultimately,\nMetaDreamer can generate high-quality 3D objects based on textual prompts\nwithin 20 minutes, and to the best of our knowledge, it is the most efficient\ntext-to-3D generation method. Furthermore, we introduce image control into the\nprocess, enhancing the controllability of 3D generation. Extensive empirical\nevidence confirms that our method is not only highly efficient but also\nachieves a quality level that is at the forefront of current state-of-the-art\n3D generation techniques.\n","authors":["Lincong Feng","Muyu Wang","Maoyu Wang","Kuo Xu","Xiaoli Liu"],"pdf_url":"https://arxiv.org/pdf/2311.10123v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.17843,\n  arXiv:2209.14988 by other authors"},{"id":"http://arxiv.org/abs/2311.10122v1","updated":"2023-11-16T10:59:44Z","published":"2023-11-16T10:59:44Z","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before\n  Projection","summary":"  The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos.\n","authors":["Bin Lin","Bin Zhu","Yang Ye","Munan Ning","Peng Jin","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2311.10122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10121v1","updated":"2023-11-16T10:45:46Z","published":"2023-11-16T10:45:46Z","title":"Slide-SAM: Medical SAM Meets Sliding Window","summary":"  Segment Anything Model (SAM) achieves remarkable results in 2D image\nsegmentation of natural images. However, the huge gap between medical images\nand natural images prevents it directly applied to medical image segmentation\ntasks. Especially in 3D medical image, SAM cannot learn the contextual\nrelationship between slices, which limites application in real scenarios. In\naddition, recent research shows that applying 2D SAM to 3D images requires\nprompting the entire volume, which is time and label comsuming. In order to\nsolve the above problems, we introduced Slide-SAM which extended SAM to 3D\nmedical images. Specifically, you only need to use a single slice prompt to\nsegement the entire volume, which greatly reduces the prompt workload for\nprofessionals. Secondly, unlike traditional 3D medical image segmentation, we\nare free from the influence of computing resources and can still use high\nresolution (H$ \\times $W = 1024$ \\times $1024) for training in 3D images to\nachieve optimal learning for small targets. This is to combine the entire 3D\nvolume is beyond the reach of training. Finally, we collected a large number of\n3D images from large-scale 3D public and private datasets, and extended SAM to\n3D medical image segmentation involving bounding box and point prompts.\nFinally, we perform a comprehensive evaluation and analysis investigating the\nperformance of Slide-SAM in medical image segmentation of different modalities,\nanatomy, and organs. We have verified Slide-SAM's segmentation capabilities on\nmultiple datasets, achieving the most advanced 3D segmentation performance\nwhile maintaining the minimum prompt. Code will be open source soon.\n","authors":["Quan Quan","Fenghe Tang","Zikang Xu","Heqin Zhu","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.10121v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.07870v2","updated":"2023-11-16T17:21:15Z","published":"2023-11-14T03:00:50Z","title":"AutoML for Large Capacity Modeling of Meta's Ranking Systems","summary":"  Web-scale ranking systems at Meta serving billions of users is complex.\nImproving ranking models is essential but engineering heavy. Automated Machine\nLearning (AutoML) can release engineers from labor intensive work of tuning\nranking models; however, it is unknown if AutoML is efficient enough to meet\ntight production timeline in real-world and, at the same time, bring additional\nimprovements to the strong baselines. Moreover, to achieve higher ranking\nperformance, there is an ever-increasing demand to scale up ranking models to\neven larger capacity, which imposes more challenges on the efficiency. The\nlarge scale of models and tight production schedule requires AutoML to\noutperform human baselines by only using a small number of model evaluation\ntrials (around 100). We presents a sampling-based AutoML method, focusing on\nneural architecture search and hyperparameter optimization, addressing these\nchallenges in Meta-scale production when building large capacity models. Our\napproach efficiently handles large-scale data demands. It leverages a\nlightweight predictor-based searcher and reinforcement learning to explore vast\nsearch spaces, significantly reducing the number of model evaluations. Through\nexperiments in large capacity modeling for CTR and CVR applications, we show\nthat our method achieves outstanding Return on Investment (ROI) versus human\ntuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or\n$25\\%$ Query per Second (QPS) increase by only sampling one hundred models on\naverage from a curated search space. The proposed AutoML method has already\nmade real-world impact where a discovered Instagram CTR model with up to -0.36%\nNE gain (over existing production baseline) was selected for large-scale online\nA/B test and show statistically significant gain. These production results\nproved AutoML efficacy and accelerated its adoption in ranking systems at Meta.\n","authors":["Hang Yin","Kuang-Hung Liu","Mengying Sun","Yuxin Chen","Buyun Zhang","Jiang Liu","Vivek Sehgal","Rudresh Rajnikant Panchal","Eugen Hotaj","Xi Liu","Daifeng Guo","Jamey Zhang","Zhou Wang","Shali Jiang","Huayu Li","Zhengxing Chen","Wen-Yen Chen","Jiyan Yang","Wei Wen"],"pdf_url":"https://arxiv.org/pdf/2311.07870v2.pdf","comment":"Hang Yin and Kuang-Hung Liu contribute equally"},{"id":"http://arxiv.org/abs/2303.03290v2","updated":"2023-11-16T12:47:29Z","published":"2023-03-06T17:06:50Z","title":"AmQA: Amharic Question Answering Dataset","summary":"  Question Answering (QA) returns concise answers or answer lists from natural\nlanguage text given a context document. Many resources go into curating QA\ndatasets to advance robust models' development. There is a surge of QA datasets\nfor languages like English, however, this is not true for Amharic. Amharic, the\nofficial language of Ethiopia, is the second most spoken Semitic language in\nthe world. There is no published or publicly available Amharic QA dataset.\nHence, to foster the research in Amharic QA, we present the first Amharic QA\n(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia\narticles. Additionally, we run an XLMR Large-based baseline model to spark\nopen-domain QA research interest. The best-performing baseline achieves an\nF-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension\nsettings respectively.\n","authors":["Tilahun Abedissa","Ricardo Usbeck","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2303.03290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18952v3","updated":"2023-11-16T11:51:04Z","published":"2023-05-27T16:05:00Z","title":"Exploring the Practicality of Generative Retrieval on Dynamic Corpora","summary":"  Benchmarking the performance of information retrieval (IR) methods are mostly\nconducted with a fixed set of documents (static corpora); in realistic\nscenarios, this is rarely the case and the document to be retrieved are\nconstantly updated and added. In this paper, we focus on conducting a\ncomprehensive comparison between two categories of contemporary retrieval\nsystems, Dual Encoders (DE) and Generative Retrievals (GR), in a dynamic\nscenario where the corpora to be retrieved is updated. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor IR systems for real-world deployment. Our results demonstrate that GR is\nmore adaptable to evolving knowledge (+13-18% on the StreamingQA Benchmark),\nrobust in handling data with temporal information (x 10 times), and efficient\nin terms of memory (x 4 times), indexing time (x 6 times), and inference flops\n(x 10 times). Our paper highlights GR's potential for future use in practical\nIR systems.\n","authors":["Soyoung Yoon","Chaeeun Kim","Hyunji Lee","Joel Jang","Sohee Yang","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2305.18952v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2306.10532v4","updated":"2023-11-16T11:48:42Z","published":"2023-06-18T11:51:39Z","title":"Personalized Elastic Embedding Learning for On-Device Recommendation","summary":"  To address privacy concerns and reduce network latency, there has been a\nrecent trend of compressing cumbersome recommendation models trained on the\ncloud and deploying compact recommender models to resource-limited devices for\nthe real-time recommendation. Existing solutions generally overlook device\nheterogeneity and user heterogeneity. They require devices with the same budget\nto share the same model and assume the available device resources (e.g.,\nmemory) are constant, which is not reflective of reality. Considering device\nand user heterogeneities as well as dynamic resource constraints, this paper\nproposes a Personalized Elastic Embedding Learning framework (PEEL) for the\non-device recommendation, which generates Personalized Elastic Embeddings\n(PEEs) for devices with various memory budgets in a once-for-all manner,\nadapting to new or dynamic budgets, and addressing user preference diversity by\nassigning personalized embeddings for different groups of users. Specifically,\nit pretrains a global embedding table with collected user-item interaction\ninstances and clusters users into groups. Then, it refines the embedding tables\nwith local interaction instances within each group. PEEs are generated from the\ngroup-wise embedding blocks and their weights that indicate the contribution of\neach embedding block to the local recommendation performance. Given a memory\nbudget, PEEL efficiently generates PEEs by selecting embedding blocks with the\nlargest weights, making it adaptable to dynamic memory budgets on devices.\nFurthermore, a diversity-driven regularizer is implemented to encourage the\nexpressiveness of embedding blocks, and a controller is utilized to optimize\nthe weights. Extensive experiments are conducted on two public datasets, and\nthe results show that PEEL yields superior performance on devices with\nheterogeneous and dynamic memory budgets.\n","authors":["Ruiqi Zheng","Liang Qu","Tong Chen","Kai Zheng","Yuhui Shi","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2306.10532v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09820v1","updated":"2023-11-16T11:48:36Z","published":"2023-11-16T11:48:36Z","title":"IterCQR: Iterative Conversational Query Reformulation without Human\n  Supervision","summary":"  In conversational search, which aims to retrieve passages containing\nessential information, queries suffer from high dependency on the preceding\ndialogue context. Therefore, reformulating conversational queries into\nstandalone forms is essential for the effective utilization of off-the-shelf\nretrievers. Previous methodologies for conversational query search frequently\ndepend on human-annotated gold labels. However, these manually crafted queries\noften result in sub-optimal retrieval performance and require high collection\ncosts. In response to these challenges, we propose Iterative Conversational\nQuery Reformulation (IterCQR), a methodology that conducts query reformulation\nwithout relying on human oracles. IterCQR iteratively trains the QR model by\ndirectly leveraging signal from information retrieval (IR) as a reward. Our\nproposed IterCQR method shows state-of-the-art performance on two datasets,\ndemonstrating its effectiveness on both sparse and dense retrievers. Notably,\nIterCQR exhibits robustness in domain-shift, low-resource, and topic-shift\nscenarios.\n","authors":["Yunah Jang","Kang-il Lee","Hyunkyung Bae","Seungpil Won","Hwanhee Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2311.09820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09765v1","updated":"2023-11-16T10:42:58Z","published":"2023-11-16T10:42:58Z","title":"Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in\n  Dense Encoders","summary":"  Prevailing research practice today often relies on training dense retrievers\non existing large datasets such as MSMARCO and then experimenting with ways to\nimprove zero-shot generalization capabilities to unseen domains. While prior\nwork has tackled this challenge through resource-intensive steps such as data\naugmentation, architectural modifications, increasing model size, or even\nfurther base model pretraining, comparatively little investigation has examined\nwhether the training procedures themselves can be improved to yield better\ngeneralization capabilities in the resulting models. In this work, we recommend\na simple recipe for training dense encoders: Train on MSMARCO with\nparameter-efficient methods, such as LoRA, and opt for using in-batch negatives\nunless given well-constructed hard negatives. We validate these recommendations\nusing the BEIR benchmark and find results are persistent across choice of dense\nencoder and base model size and are complementary to other resource-intensive\nstrategies for out-of-domain generalization such as architectural modifications\nor additional pretraining. We hope that this thorough and impartial study\naround various training techniques, which augments other resource-intensive\nmethods, offers practical insights for developing a dense retrieval model that\neffectively generalizes, even when trained on a single dataset.\n","authors":["Hyunji Lee","Luca Soldaini","Arman Cohan","Minjoon Seo","Kyle Lo"],"pdf_url":"https://arxiv.org/pdf/2311.09765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09735v1","updated":"2023-11-16T10:06:09Z","published":"2023-11-16T10:06:09Z","title":"GEO: Generative Engine Optimization","summary":"  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of Generative Engines (GEs), has the potential to generate\naccurate and personalized responses, and is rapidly replacing traditional\nsearch engines like Google and Bing. Generative Engines typically satisfy\nqueries by synthesizing information from multiple sources and summarizing them\nwith the help of LLMs. While this shift significantly improves \\textit{user}\nutility and \\textit{generative search engine} traffic, it results in a huge\nchallenge for the third stakeholder -- website and content creators. Given the\nblack-box and fast-moving nature of Generative Engines, content creators have\nlittle to no control over when and how their content is displayed. With\ngenerative engines here to stay, the right tools should be provided to ensure\nthat creator economy is not severely disadvantaged. To address this, we\nintroduce Generative Engine Optimization (GEO), a novel paradigm to aid content\ncreators in improving the visibility of their content in Generative Engine\nresponses through a black-box optimization framework for optimizing and\ndefining visibility metrics. We facilitate systematic evaluation in this new\nparadigm by introducing GEO-bench, a benchmark of diverse user queries across\nmultiple domains, coupled with sources required to answer these queries.\nThrough rigorous evaluation, we show that GEO can boost visibility by up to\n40\\% in generative engine responses. Moreover, we show the efficacy of these\nstrategies varies across domains, underscoring the need for domain-specific\nmethods. Our work opens a new frontier in the field of information discovery\nsystems, with profound implications for generative engines and content\ncreators.\n","authors":["Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik R Narasimhan","Ameet Deshpande"],"pdf_url":"https://arxiv.org/pdf/2311.09735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09624v1","updated":"2023-11-16T07:15:44Z","published":"2023-11-16T07:15:44Z","title":"AI Recommendation System for Enhanced Customer Experience: A Novel\n  Image-to-Text Method","summary":"  Existing fashion recommendation systems encounter difficulties in using\nvisual data for accurate and personalized recommendations. This research\ndescribes an innovative end-to-end pipeline that uses artificial intelligence\nto provide fine-grained visual interpretation for fashion recommendations. When\ncustomers upload images of desired products or outfits, the system\nautomatically generates meaningful descriptions emphasizing stylistic elements.\nThese captions guide retrieval from a global fashion product catalogue to offer\nsimilar alternatives that fit the visual characteristics of the original image.\nOn a dataset of over 100,000 categorized fashion photos, the pipeline was\ntrained and evaluated. The F1-score for the object detection model was 0.97,\nexhibiting exact fashion object recognition capabilities optimized for\nrecommendation. This visually aware system represents a key advancement in\ncustomer engagement through personalized fashion recommendations\n","authors":["Mohamaed Foued Ayedi","Hiba Ben Salem","Soulaimen Hammami","Ahmed Ben Said","Rateb Jabbar","Achraf CHabbouh"],"pdf_url":"https://arxiv.org/pdf/2311.09624v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09577v1","updated":"2023-11-16T05:23:53Z","published":"2023-11-16T05:23:53Z","title":"Group-Aware Interest Disentangled Dual-Training for Personalized\n  Recommendation","summary":"  Personalized recommender systems aim to predict users' preferences for items.\nIt has become an indispensable part of online services. Online social platforms\nenable users to form groups based on their common interests. The users' group\nparticipation on social platforms reveals their interests and can be utilized\nas side information to mitigate the data sparsity and cold-start problem in\nrecommender systems. Users join different groups out of different interests. In\nthis paper, we generate group representation from the user's interests and\npropose IGRec (Interest-based Group enhanced Recommendation) to utilize the\ngroup information accurately. It consists of four modules. (1) Interest\ndisentangler via self-gating that disentangles users' interests from their\ninitial embedding representation. (2) Interest aggregator that generates the\ninterest-based group representation by Gumbel-Softmax aggregation on the group\nmembers' interests. (3) Interest-based group aggregation that fuses user's\nrepresentation with the participated group representation. (4) A dual-trained\nrating prediction module to utilize both user-item and group-item interactions.\nWe conduct extensive experiments on three publicly available datasets. Results\nshow IGRec can effectively alleviate the data sparsity problem and enhance the\nrecommender system with interest-based group representation. Experiments on the\ngroup recommendation task further show the informativeness of interest-based\ngroup representation.\n","authors":["Xiaolong Liu","Liangwei Yang","Zhiwei Liu","Xiaohan Li","Mingdai Yang","Chen Wang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2311.09577v1.pdf","comment":"10 pages, 7 figures, 2023 IEEE International Conference on Big Data"},{"id":"http://arxiv.org/abs/2311.09544v1","updated":"2023-11-16T03:47:48Z","published":"2023-11-16T03:47:48Z","title":"Scaling User Modeling: Large-scale Online User Representations for Ads\n  Personalization in Meta","summary":"  Effective user representations are pivotal in personalized advertising.\nHowever, stringent constraints on training throughput, serving latency, and\nmemory, often limit the complexity and input feature set of online ads ranking\nmodels. This challenge is magnified in extensive systems like Meta's, which\nencompass hundreds of models with diverse specifications, rendering the\ntailoring of user representation learning for each model impractical. To\naddress these challenges, we present Scaling User Modeling (SUM), a framework\nwidely deployed in Meta's ads ranking system, designed to facilitate efficient\nand scalable sharing of online user representation across hundreds of ads\nmodels. SUM leverages a few designated upstream user models to synthesize user\nembeddings from massive amounts of user features with advanced modeling\ntechniques. These embeddings then serve as inputs to downstream online ads\nranking models, promoting efficient representation sharing. To adapt to the\ndynamic nature of user features and ensure embedding freshness, we designed SUM\nOnline Asynchronous Platform (SOAP), a latency free online serving system\ncomplemented with model freshness and embedding stabilization, which enables\nfrequent user model updates and online inference of user embeddings upon each\nuser request. We share our hands-on deployment experiences for the SUM\nframework and validate its superiority through comprehensive experiments. To\ndate, SUM has been launched to hundreds of ads ranking models in Meta,\nprocessing hundreds of billions of user requests daily, yielding significant\nonline metric gains and infrastructure cost savings.\n","authors":["Wei Zhang","Dai Li","Chen Liang","Fang Zhou","Zhongke Zhang","Xuewei Wang","Ru Li","Yi Zhou","Yaning Huang","Dong Liang","Kai Wang","Zhangyuan Wang","Zhengxing Chen","Min Li","Fenggang Wu","Minghai Chen","Huayu Li","Yunnan Wu","Zhan Shu","Mindi Yuan","Sri Reddy"],"pdf_url":"https://arxiv.org/pdf/2311.09544v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.09476v1","updated":"2023-11-16T00:39:39Z","published":"2023-11-16T00:39:39Z","title":"ARES: An Automated Evaluation Framework for Retrieval-Augmented\n  Generation Systems","summary":"  Evaluating retrieval-augmented generation (RAG) systems traditionally relies\non hand annotations for input queries, passages to retrieve, and responses to\ngenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluating\nRAG systems along the dimensions of context relevance, answer faithfulness, and\nanswer relevance. Using synthetic training data, ARES finetunes lightweight LM\njudges to assess the quality of individual RAG components. To mitigate\npotential prediction errors, ARES utilizes a small set of human-annotated\ndatapoints for prediction-powered inference (PPI). Across six different\nknowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG\nsystems while using a few hundred human annotations during evaluation.\nFurthermore, ARES judges remain effective across domain shifts, proving\naccurate even after changing the type of queries and/or documents used in the\nevaluated RAG systems. We make our datasets and code for replication and\ndeployment available at https://github.com/stanford-futuredata/ARES.\n","authors":["Jon Saad-Falcon","Omar Khattab","Christopher Potts","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2311.09476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03488v2","updated":"2023-11-16T21:31:59Z","published":"2023-11-06T19:52:55Z","title":"Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems","summary":"  While recommender systems have become an integral component of the Web\nexperience, their heavy reliance on user data raises privacy and security\nconcerns. Substituting user data with synthetic data can address these\nconcerns, but accurately replicating these real-world datasets has been a\nnotoriously challenging problem. Recent advancements in generative AI have\ndemonstrated the impressive capabilities of diffusion models in generating\nrealistic data across various domains. In this work we introduce a Score-based\nDiffusion Recommendation Module (SDRM), which captures the intricate patterns\nof real-world datasets required for training highly accurate recommender\nsystems. SDRM allows for the generation of synthetic data that can replace\nexisting datasets to preserve user privacy, or augment existing datasets to\naddress excessive data sparsity. Our method outperforms competing baselines\nsuch as generative adversarial networks, variational autoencoders, and recently\nproposed diffusion models in synthesizing various datasets to replace or\naugment the original data by an average improvement of 4.30% in Recall@$k$ and\n4.65% in NDCG@$k$.\n","authors":["Derek Lilienthal","Paul Mello","Magdalini Eirinaki","Stas Tiomkin"],"pdf_url":"https://arxiv.org/pdf/2311.03488v2.pdf","comment":"10 pages, 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.10093v1","updated":"2023-11-16T18:59:51Z","published":"2023-11-16T18:59:51Z","title":"The Chosen One: Consistent Characters in Text-to-Image Diffusion Models","summary":"  Recent advances in text-to-image generation models have unlocked vast\npotential for visual creativity. However, these models struggle with generation\nof consistent characters, a crucial aspect for numerous real-world applications\nsuch as story visualization, game development asset design, advertising, and\nmore. Current methods typically rely on multiple pre-existing images of the\ntarget character or involve labor-intensive manual processes. In this work, we\npropose a fully automated solution for consistent character generation, with\nthe sole input being a text prompt. We introduce an iterative procedure that,\nat each stage, identifies a coherent set of images sharing a similar identity\nand extracts a more consistent identity from this set. Our quantitative\nanalysis demonstrates that our method strikes a better balance between prompt\nalignment and identity consistency compared to the baseline methods, and these\nfindings are reinforced by a user study. To conclude, we showcase several\npractical applications of our approach. Project page is available at\nhttps://omriavrahami.com/the-chosen-one\n","authors":["Omri Avrahami","Amir Hertz","Yael Vinker","Moab Arar","Shlomi Fruchter","Ohad Fried","Daniel Cohen-Or","Dani Lischinski"],"pdf_url":"https://arxiv.org/pdf/2311.10093v1.pdf","comment":"Project page is available at https://omriavrahami.com/the-chosen-one"},{"id":"http://arxiv.org/abs/2311.10090v1","updated":"2023-11-16T18:58:43Z","published":"2023-11-16T18:58:43Z","title":"JaxMARL: Multi-Agent RL Environments in JAX","summary":"  Benchmarks play an important role in the development of machine learning\nalgorithms. For example, research in reinforcement learning (RL) has been\nheavily influenced by available environments and benchmarks. However, RL\nenvironments are traditionally run on the CPU, limiting their scalability with\ntypical academic compute. Recent advancements in JAX have enabled the wider use\nof hardware acceleration to overcome these computational hurdles, enabling\nmassively parallel RL training pipelines and environments. This is particularly\nuseful for multi-agent reinforcement learning (MARL) research. First of all,\nmultiple agents must be considered at each environment step, adding\ncomputational burden, and secondly, the sample complexity is increased due to\nnon-stationarity, decentralised partial observability, or other MARL\nchallenges. In this paper, we present JaxMARL, the first open-source code base\nthat combines ease-of-use with GPU enabled efficiency, and supports a large\nnumber of commonly used MARL environments as well as popular baseline\nalgorithms. When considering wall clock time, our experiments show that per-run\nour JAX-based training pipeline is up to 12500x faster than existing\napproaches. This enables efficient and thorough evaluations, with the potential\nto alleviate the evaluation crisis of the field. We also introduce and\nbenchmark SMAX, a vectorised, simplified version of the popular StarCraft\nMulti-Agent Challenge, which removes the need to run the StarCraft II game\nengine. This not only enables GPU acceleration, but also provides a more\nflexible MARL environment, unlocking the potential for self-play,\nmeta-learning, and other future applications in MARL. We provide code at\nhttps://github.com/flairox/jaxmarl.\n","authors":["Alexander Rutherford","Benjamin Ellis","Matteo Gallici","Jonathan Cook","Andrei Lupu","Gardar Ingvarsson","Timon Willi","Akbir Khan","Christian Schroeder de Witt","Alexandra Souly","Saptarashmi Bandyopadhyay","Mikayel Samvelyan","Minqi Jiang","Robert Tjarko Lange","Shimon Whiteson","Bruno Lacerda","Nick Hawes","Tim Rocktaschel","Chris Lu","Jakob Nicolaus Foerster"],"pdf_url":"https://arxiv.org/pdf/2311.10090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10089v1","updated":"2023-11-16T18:55:58Z","published":"2023-11-16T18:55:58Z","title":"Emu Edit: Precise Image Editing via Recognition and Generation Tasks","summary":"  Instruction-based image editing holds immense potential for a variety of\napplications, as it enables users to perform any editing operation using a\nnatural language instruction. However, current models in this domain often\nstruggle with accurately executing user instructions. We present Emu Edit, a\nmulti-task image editing model which sets state-of-the-art results in\ninstruction-based image editing. To develop Emu Edit we train it to multi-task\nacross an unprecedented range of tasks, such as region-based editing, free-form\nediting, and Computer Vision tasks, all of which are formulated as generative\ntasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we\nprovide it with learned task embeddings which guide the generation process\ntowards the correct edit type. Both these elements are essential for Emu Edit's\noutstanding performance. Furthermore, we show that Emu Edit can generalize to\nnew tasks, such as image inpainting, super-resolution, and compositions of\nediting tasks, with just a few labeled examples. This capability offers a\nsignificant advantage in scenarios where high-quality samples are scarce.\nLastly, to facilitate a more rigorous and informed assessment of instructable\nimage editing models, we release a new challenging and versatile benchmark that\nincludes seven different image editing tasks.\n","authors":["Shelly Sheynin","Adam Polyak","Uriel Singer","Yuval Kirstain","Amit Zohar","Oron Ashual","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2311.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10085v1","updated":"2023-11-16T18:44:22Z","published":"2023-11-16T18:44:22Z","title":"A Computationally Efficient Sparsified Online Newton Method","summary":"  Second-order methods hold significant promise for enhancing the convergence\nof deep neural network training; however, their large memory and computational\ndemands have limited their practicality. Thus there is a need for scalable\nsecond-order methods that can efficiently train large models. In this paper, we\nintroduce the Sparsified Online Newton (SONew) method, a memory-efficient\nsecond-order algorithm that yields a sparsified yet effective preconditioner.\nThe algorithm emerges from a novel use of the LogDet matrix divergence measure;\nwe combine it with sparsity constraints to minimize regret in the online convex\noptimization framework. Empirically, we test our method on large scale\nbenchmarks of up to 1B parameters. We achieve up to 30% faster convergence,\n3.4% relative improvement in validation performance, and 80% relative\nimprovement in training loss, in comparison to memory efficient optimizers\nincluding first order methods. Powering the method is a surprising fact --\nimposing structured sparsity patterns, like tridiagonal and banded structure,\nrequires little to no overhead, making it as efficient and parallelizable as\nfirst-order methods. In wall-clock time, tridiagonal SONew is only about 3%\nslower per step than first-order methods but gives overall gains due to much\nfaster convergence. In contrast, one of the state-of-the-art (SOTA)\nmemory-intensive second-order methods, Shampoo, is unable to scale to large\nbenchmarks. Additionally, while Shampoo necessitates significant engineering\nefforts to scale to large benchmarks, SONew offers a more straightforward\nimplementation, increasing its practical appeal. SONew code is available at:\nhttps://github.com/devvrit/SONew\n","authors":["Fnu Devvrit","Sai Surya Duvvuri","Rohan Anil","Vineet Gupta","Cho-Jui Hsieh","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2311.10085v1.pdf","comment":"30 pages. First two authors contributed equally. Accepted at NeurIPS\n  2023"},{"id":"http://arxiv.org/abs/2311.10081v1","updated":"2023-11-16T18:37:29Z","published":"2023-11-16T18:37:29Z","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback","summary":"  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2311.10081v1.pdf","comment":"The feedback datasets will be released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF"},{"id":"http://arxiv.org/abs/2302.14838v3","updated":"2023-11-16T18:02:19Z","published":"2023-02-28T18:37:25Z","title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search","summary":"  Given the recent impressive accomplishments of language models (LMs) for code\ngeneration, we explore the use of LMs as adaptive mutation and crossover\noperators for an evolutionary neural architecture search (NAS) algorithm. While\nNAS still proves too difficult a task for LMs to succeed at solely through\nprompting, we find that the combination of evolutionary prompt engineering with\nsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverse\nand high performing models. We first demonstrate that EvoPrompting is effective\non the computationally efficient MNIST-1D dataset, where EvoPrompting produces\nconvolutional architecture variants that outperform both those designed by\nhuman experts and naive few-shot prompting in terms of accuracy and model size.\nWe then apply our method to searching for graph neural networks on the CLRS\nAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novel\narchitectures that outperform current state-of-the-art models on 21 out of 30\nalgorithmic reasoning tasks while maintaining similar model size. EvoPrompting\nis successful at designing accurate and efficient neural network architectures\nacross a variety of machine learning tasks, while also being general enough for\neasy adaptation to other tasks beyond neural network design.\n","authors":["Angelica Chen","David M. Dohan","David R. So"],"pdf_url":"https://arxiv.org/pdf/2302.14838v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.04955v2","updated":"2023-11-16T17:57:45Z","published":"2023-10-08T00:39:11Z","title":"Information-Theoretic Bounds on The Removal of Attribute-Specific Bias\n  From Neural Networks","summary":"  Ensuring a neural network is not relying on protected attributes (e.g., race,\nsex, age) for predictions is crucial in advancing fair and trustworthy AI.\nWhile several promising methods for removing attribute bias in neural networks\nhave been proposed, their limitations remain under-explored. In this work, we\nmathematically and empirically reveal an important limitation of attribute bias\nremoval methods in presence of strong bias. Specifically, we derive a general\nnon-vacuous information-theoretical upper bound on the performance of any\nattribute bias removal method in terms of the bias strength. We provide\nextensive experiments on synthetic, image, and census datasets to verify the\ntheoretical bound and its consequences in practice. Our findings show that\nexisting attribute bias removal methods are effective only when the inherent\nbias in the dataset is relatively weak, thus cautioning against the use of\nthese methods in smaller datasets where strong attribute bias can occur, and\nadvocating the need for methods that can overcome this limitation.\n","authors":["Jiazhi Li","Mahyar Khayatkhoei","Jiageng Zhu","Hanchen Xie","Mohamed E. Hussein","Wael AbdAlmageed"],"pdf_url":"https://arxiv.org/pdf/2310.04955v2.pdf","comment":"15 pages, 4 figures, 3 tables. To appear in Algorithmic Fairness\n  through the Lens of Time Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.19548v2","updated":"2023-11-16T17:57:02Z","published":"2023-10-30T13:59:47Z","title":"Approximation Theory, Computing, and Deep Learning on the Wasserstein\n  Space","summary":"  The challenge of approximating functions in infinite-dimensional spaces from\nfinite samples is widely regarded as formidable. In this study, we delve into\nthe challenging problem of the numerical approximation of Sobolev-smooth\nfunctions defined on probability spaces. Our particular focus centers on the\nWasserstein distance function, which serves as a relevant example. In contrast\nto the existing body of literature focused on approximating efficiently\npointwise evaluations, we chart a new course to define functional approximants\nby adopting three machine learning-based approaches: 1. Solving a finite number\nof optimal transport problems and computing the corresponding Wasserstein\npotentials. 2. Employing empirical risk minimization with Tikhonov\nregularization in Wasserstein Sobolev spaces. 3. Addressing the problem through\nthe saddle point formulation that characterizes the weak form of the Tikhonov\nfunctional's Euler-Lagrange equation. As a theoretical contribution, we furnish\nexplicit and quantitative bounds on generalization errors for each of these\nsolutions. In the proofs, we leverage the theory of metric Sobolev spaces and\nwe combine it with techniques of optimal transport, variational calculus, and\nlarge deviation bounds. In our numerical implementation, we harness\nappropriately designed neural networks to serve as basis functions. These\nnetworks undergo training using diverse methodologies. This approach allows us\nto obtain approximating functions that can be rapidly evaluated after training.\nConsequently, our constructive solutions significantly enhance at equal\naccuracy the evaluation speed, surpassing that of state-of-the-art methods by\nseveral orders of magnitude.\n","authors":["Massimo Fornasier","Pascal Heid","Giacomo Enrico Sodini"],"pdf_url":"https://arxiv.org/pdf/2310.19548v2.pdf","comment":"Added link to GitHub repository"},{"id":"http://arxiv.org/abs/2311.10054v1","updated":"2023-11-16T17:48:55Z","published":"2023-11-16T17:48:55Z","title":"Is \"A Helpful Assistant\" the Best Role for Large Language Models? A\n  Systematic Evaluation of Social Roles in System Prompts","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of the\ndefault system prompt. But is \"a helpful assistant\" the best role for LLMs? In\nthis study, we present a systematic evaluation of how social roles in system\nprompts affect model performance. We curate a list of 162 roles covering 6\ntypes of interpersonal relationships and 8 types of occupations. Through\nextensive analysis of 3 popular LLMs and 2457 questions, we show that adding\ninterpersonal roles in prompts consistently improves the models' performance\nover a range of questions. Moreover, while we find that using gender-neutral\nroles and specifying the role as the audience leads to better performances,\npredicting which role leads to the best performance remains a challenging task,\nand that frequency, similarity, and perplexity do not fully explain the effect\nof social roles on model performances. Our results can help inform the design\nof system prompts for AI systems. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10053v1","updated":"2023-11-16T17:48:06Z","published":"2023-11-16T17:48:06Z","title":"Near-optimal Closed-loop Method via Lyapunov Damping for Convex\n  Optimization","summary":"  We introduce an autonomous system with closed-loop damping for first-order\nconvex optimization. While, to this day, optimal rates of convergence are only\nachieved by non-autonomous methods via open-loop damping (e.g., Nesterov's\nalgorithm), we show that our system is the first one featuring a closed-loop\ndamping while exhibiting a rate arbitrarily close to the optimal one. We do so\nby coupling the damping and the speed of convergence of the system via a\nwell-chosen Lyapunov function. We then derive a practical first-order algorithm\ncalled LYDIA by discretizing our system, and present numerical experiments\nsupporting our theoretical findings.\n","authors":["Severin Maier","Camille Castera","Peter Ochs"],"pdf_url":"https://arxiv.org/pdf/2311.10053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10051v1","updated":"2023-11-16T17:45:59Z","published":"2023-11-16T17:45:59Z","title":"Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces","summary":"  Despite the prevalence of tabular datasets, few-shot learning remains\nunder-explored within this domain. Existing few-shot methods are not directly\napplicable to tabular datasets due to varying column relationships, meanings,\nand permutational invariance. To address these challenges, we propose FLAT-a\nnovel approach to tabular few-shot learning, encompassing knowledge sharing\nbetween datasets with heterogeneous feature spaces. Utilizing an encoder\ninspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and\ntheir individual columns, which facilitate knowledge transfer and\ngeneralization to previously unseen datasets. A decoder network parametrizes\nthe predictive target network, implemented as a Graph Attention Network, to\naccommodate the heterogeneous nature of tabular datasets. Experiments on a\ndiverse collection of 118 UCI datasets demonstrate FLAT's successful\ngeneralization to new tabular datasets and a considerable improvement over the\nbaselines.\n","authors":["Max Zhu","Katarzyna Kobalczyk","Andrija Petrovic","Mladen Nikolic","Mihaela van der Schaar","Boris Delibasic","Petro Lio"],"pdf_url":"https://arxiv.org/pdf/2311.10051v1.pdf","comment":"Tabular learning, Deep learning, Few shot learning"},{"id":"http://arxiv.org/abs/2311.10049v1","updated":"2023-11-16T17:45:37Z","published":"2023-11-16T17:45:37Z","title":"Inherently Interpretable Time Series Classification via Multiple\n  Instance Learning","summary":"  Conventional Time Series Classification (TSC) methods are often black boxes\nthat obscure inherent interpretation of their decision-making processes. In\nthis work, we leverage Multiple Instance Learning (MIL) to overcome this issue,\nand propose a new framework called MILLET: Multiple Instance Learning for\nLocally Explainable Time series classification. We apply MILLET to existing\ndeep learning TSC models and show how they become inherently interpretable\nwithout compromising (and in some cases, even improving) predictive\nperformance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel\nsynthetic dataset that is specially designed to facilitate interpretability\nevaluation. On these datasets, we show MILLET produces sparse explanations\nquickly that are of higher quality than other well-known interpretability\nmethods. To the best of our knowledge, our work with MILLET, which is available\non GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the\nfirst to develop general MIL methods for TSC and apply them to an extensive\nvariety of domains\n","authors":["Joseph Early","Gavin KC Cheung","Kurt Cutajar","Hanting Xie","Jas Kandola","Niall Twomey"],"pdf_url":"https://arxiv.org/pdf/2311.10049v1.pdf","comment":"Preprint. Under submission at ICLR 2024"},{"id":"http://arxiv.org/abs/2311.10026v1","updated":"2023-11-16T17:14:26Z","published":"2023-11-16T17:14:26Z","title":"Guaranteeing Control Requirements via Reward Shaping in Reinforcement\n  Learning","summary":"  In addressing control problems such as regulation and tracking through\nreinforcement learning, it is often required to guarantee that the acquired\npolicy meets essential performance and stability criteria such as a desired\nsettling time and steady-state error prior to deployment. Motivated by this\nnecessity, we present a set of results and a systematic reward shaping\nprocedure that (i) ensures the optimal policy generates trajectories that align\nwith specified control requirements and (ii) allows to assess whether any given\npolicy satisfies them. We validate our approach through comprehensive numerical\nexperiments conducted in two representative environments from OpenAI Gym: the\nInverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular\nand deep reinforcement learning methods, our experiments consistently affirm\nthe efficacy of our proposed framework, highlighting its effectiveness in\nensuring policy adherence to the prescribed control requirements.\n","authors":["Francesco De Lellis","Marco Coraggio","Giovanni Russo","Mirco Musolesi","Mario di Bernardo"],"pdf_url":"https://arxiv.org/pdf/2311.10026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10025v1","updated":"2023-11-16T17:14:07Z","published":"2023-11-16T17:14:07Z","title":"A Novel Neural Network-Based Federated Learning System for Imbalanced\n  and Non-IID Data","summary":"  With the growth of machine learning techniques, privacy of data of users has\nbecome a major concern. Most of the machine learning algorithms rely heavily on\nlarge amount of data which may be collected from various sources. Collecting\nthese data yet maintaining privacy policies has become one of the most\nchallenging tasks for the researchers. To combat this issue, researchers have\nintroduced federated learning, where a prediction model is learnt by ensuring\nthe privacy of data of clients data. However, the prevalent federated learning\nalgorithms possess an accuracy and efficiency trade-off, especially for non-IID\ndata. In this research, we propose a centralized, neural network-based\nfederated learning system. The centralized algorithm incorporates micro-level\nparallel processing inspired by the traditional mini-batch algorithm where the\nclient devices and the server handle the forward and backward propagation\nrespectively. We also devise a semi-centralized version of our proposed\nalgorithm. This algorithm takes advantage of edge computing for minimizing the\nload from the central server, where clients handle both the forward and\nbackward propagation while sacrificing the overall train time to some extent.\nWe evaluate our proposed systems on five well-known benchmark datasets and\nachieve satisfactory performance in a reasonable time across various data\ndistribution settings as compared to some existing benchmark algorithms.\n","authors":["Mahfuzur Rahman Chowdhury","Muhammad Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2311.10025v1.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2311.10023v1","updated":"2023-11-16T17:08:27Z","published":"2023-11-16T17:08:27Z","title":"Online Optimization for Network Resource Allocation and Comparison with\n  Reinforcement Learning Techniques","summary":"  We tackle in this paper an online network resource allocation problem with\njob transfers. The network is composed of many servers connected by\ncommunication links. The system operates in discrete time; at each time slot,\nthe administrator reserves resources at servers for future job requests, and a\ncost is incurred for the reservations made. Then, after receptions, the jobs\nmay be transferred between the servers to best accommodate the demands. This\nincurs an additional transport cost. Finally, if a job request cannot be\nsatisfied, there is a violation that engenders a cost to pay for the blocked\njob. We propose a randomized online algorithm based on the exponentially\nweighted method. We prove that our algorithm enjoys a sub-linear in time\nregret, which indicates that the algorithm is adapting and learning from its\nexperiences and is becoming more efficient in its decision-making as it\naccumulates more data. Moreover, we test the performance of our algorithm on\nartificial data and compare it against a reinforcement learning method where we\nshow that our proposed method outperforms the latter.\n","authors":["Ahmed Sid-Ali","Ioannis Lambadaris","Yiqiang Q. Zhao","Gennady Shaikhet","Amirhossein Asgharnia"],"pdf_url":"https://arxiv.org/pdf/2311.10023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10012v1","updated":"2023-11-16T16:53:52Z","published":"2023-11-16T16:53:52Z","title":"Finding Real-World Orbital Motion Laws from Data","summary":"  A novel approach is presented for discovering PDEs that govern the motion of\nsatellites in space. The method is based on SINDy, a data-driven technique\ncapable of identifying the underlying dynamics of complex physical systems from\ntime series data. SINDy is utilized to uncover PDEs that describe the laws of\nphysics in space, which are non-deterministic and influenced by various factors\nsuch as drag or the reference area (related to the attitude of the satellite).\nIn contrast to prior works, the physically interpretable coordinate system is\nmaintained, and no dimensionality reduction technique is applied to the data.\nBy training the model with multiple representative trajectories of LEO -\nencompassing various inclinations, eccentricities, and altitudes - and testing\nit with unseen orbital motion patterns, a mean error of around 140 km for the\npositions and 0.12 km/s for the velocities is achieved. The method offers the\nadvantage of delivering interpretable, accurate, and complex models of orbital\nmotion that can be employed for propagation or as inputs to predictive models\nfor other variables of interest, such as atmospheric drag or the probability of\ncollision in an encounter with a spacecraft or space objects. In conclusion,\nthe work demonstrates the promising potential of using SINDy to discover the\nequations governing the behaviour of satellites in space. The technique has\nbeen successfully applied to uncover PDEs describing the motion of satellites\nin LEO with high accuracy. The method possesses several advantages over\ntraditional models, including the ability to provide physically interpretable,\naccurate, and complex models of orbital motion derived from high-entropy\ndatasets. These models can be utilised for propagation or as inputs to\npredictive models for other variables of interest.\n","authors":["João Funenga","Marta Guimarães","Henrique Costa","Cláudia Soares"],"pdf_url":"https://arxiv.org/pdf/2311.10012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05400v2","updated":"2023-11-16T16:37:06Z","published":"2023-05-09T12:45:43Z","title":"Investigating the Corruption Robustness of Image Classifiers with Random\n  Lp-norm Corruptions","summary":"  Robustness is a fundamental property of machine learning classifiers to\nachieve safety and reliability. In the fields of adversarial robustness and\nformal robustness verification of image classification models, robustness is\ncommonly defined as the stability to all input variations within an Lp-norm\ndistance. However, robustness to random corruptions is usually improved and\nevaluated using variations observed in the real-world, while mathematically\ndefined Lp-norm corruptions are rarely considered. This study investigates the\nuse of random Lp-norm corruptions to augment the training and test data of\nimage classifiers. We adapt an approach from the field of adversarial\nrobustness to assess the model robustness to imperceptible random corruptions.\nWe empirically and theoretically investigate whether robustness is transferable\nacross different Lp-norms and derive conclusions on which Lp-norm corruptions a\nmodel should be trained and evaluated on. We find that training data\naugmentation with L0-norm corruptions improves corruption robustness while\nmaintaining accuracy compared to standard training and when applied on top of\nselected state-of-the-art data augmentation techniques.\n","authors":["Georg Siedel","Weijia Shao","Silvia Vock","Andrey Morozov"],"pdf_url":"https://arxiv.org/pdf/2305.05400v2.pdf","comment":"Preprint submitted to VISAPP 2024"},{"id":"http://arxiv.org/abs/2311.10002v1","updated":"2023-11-16T16:30:04Z","published":"2023-11-16T16:30:04Z","title":"Straggler-resilient Federated Learning: Tackling Computation\n  Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network","summary":"  Federated Learning (FL) enables many resource-limited devices to train a\nmodel collaboratively without data sharing. However, many existing works focus\non model-homogeneous FL, where the global and local models are the same size,\nignoring the inherently heterogeneous computational capabilities of different\ndevices and restricting resource-constrained devices from contributing to FL.\nIn this paper, we consider model-heterogeneous FL and propose Federated Partial\nModel Training (FedPMT), where devices with smaller computational capabilities\nwork on partial models (subsets of the global model) and contribute to the\nglobal model. Different from Dropout-based partial model generation, which\nremoves neurons in hidden layers at random, model training in FedPMT is\nachieved from the back-propagation perspective. As such, all devices in FedPMT\nprioritize the most crucial parts of the global model. Theoretical analysis\nshows that the proposed partial model training design has a similar convergence\nrate to the widely adopted Federated Averaging (FedAvg) algorithm,\n$\\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor\nrelated to the model splitting design in FedPMT. Empirical results show that\nFedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile,\ncompared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the\nlearning target in a shorter completion time, thus achieving a better trade-off\nbetween learning accuracy and completion time.\n","authors":["Hongda Wu","Ping Wang","C V Aswartha Narayana"],"pdf_url":"https://arxiv.org/pdf/2311.10002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08691v3","updated":"2023-11-16T16:17:09Z","published":"2023-03-15T15:21:41Z","title":"Learning to Reconstruct Signals From Binary Measurements","summary":"  Recent advances in unsupervised learning have highlighted the possibility of\nlearning to reconstruct signals from noisy and incomplete linear measurements\nalone. These methods play a key role in medical and scientific imaging and\nsensing, where ground truth data is often scarce or difficult to obtain.\nHowever, in practice, measurements are not only noisy and incomplete but also\nquantized. Here we explore the extreme case of learning from binary\nobservations and provide necessary and sufficient conditions on the number of\nmeasurements required for identifying a set of signals from incomplete binary\ndata. Our results are complementary to existing bounds on signal recovery from\nbinary measurements. Furthermore, we introduce a novel self-supervised learning\napproach, which we name SSBM, that only requires binary data for training. We\ndemonstrate in a series of experiments with real datasets that SSBM performs on\npar with supervised learning and outperforms sparse reconstruction methods with\na fixed wavelet basis by a large margin.\n","authors":["Julián Tachella","Laurent Jacques"],"pdf_url":"https://arxiv.org/pdf/2303.08691v3.pdf","comment":"https://openreview.net/forum?id=ioFIAQOBOS"},{"id":"http://arxiv.org/abs/2311.09998v1","updated":"2023-11-16T16:14:58Z","published":"2023-11-16T16:14:58Z","title":"DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's\n  Distance","summary":"  The Earth Mover's Distance (EMD) is the measure of choice between point\nclouds. However the computational cost to compute it makes it prohibitive as a\ntraining loss, and the standard approach is to use a surrogate such as the\nChamfer distance. We propose an attention-based model to compute an accurate\napproximation of the EMD that can be used as a training loss for generative\nmodels. To get the necessary accurate estimation of the gradients we train our\nmodel to explicitly compute the matching between point clouds instead of EMD\nitself. We cast this new objective as the estimation of an attention matrix\nthat approximates the ground truth matching matrix. Experiments show that this\nmodel provides an accurate estimate of the EMD and its gradient with a wall\nclock speed-up of more than two orders of magnitude with respect to the exact\nHungarian matching algorithm and one order of magnitude with respect to the\nstandard approximate Sinkhorn algorithm, allowing in particular to train a\npoint cloud VAE with the EMD itself. Extensive evaluation show the remarkable\nbehaviour of this model when operating out-of-distribution, a key requirement\nfor a distance surrogate. Finally, the model generalizes very well to point\nclouds during inference several times larger than during training.\n","authors":["Atul Kumar Sinha","Francois Fleuret"],"pdf_url":"https://arxiv.org/pdf/2311.09998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09997v1","updated":"2023-11-16T16:14:39Z","published":"2023-11-16T16:14:39Z","title":"Co-data Learning for Bayesian Additive Regression Trees","summary":"  Medical prediction applications often need to deal with small sample sizes\ncompared to the number of covariates. Such data pose problems for prediction\nand variable selection, especially when the covariate-response relationship is\ncomplicated. To address these challenges, we propose to incorporate co-data,\ni.e. external information on the covariates, into Bayesian additive regression\ntrees (BART), a sum-of-trees prediction model that utilizes priors on the tree\nparameters to prevent overfitting. To incorporate co-data, an empirical Bayes\n(EB) framework is developed that estimates, assisted by a co-data model, prior\ncovariate weights in the BART model. The proposed method can handle multiple\ntypes of co-data simultaneously. Furthermore, the proposed EB framework enables\nthe estimation of the other hyperparameters of BART as well, rendering an\nappealing alternative to cross-validation. We show that the method finds\nrelevant covariates and that it improves prediction compared to default BART in\nsimulations. If the covariate-response relationship is nonlinear, the method\nbenefits from the flexibility of BART to outperform regression-based co-data\nlearners. Finally, the use of co-data enhances prediction in an application to\ndiffuse large B-cell lymphoma prognosis based on clinical covariates, gene\nmutations, DNA translocations, and DNA copy number data.\n  Keywords: Bayesian additive regression trees; Empirical Bayes; Co-data;\nHigh-dimensional data; Omics; Prediction\n","authors":["Jeroen M. Goedhart","Thomas Klausch","Jurriaan Janssen","Mark A. van de Wiel"],"pdf_url":"https://arxiv.org/pdf/2311.09997v1.pdf","comment":"30 pages, 3 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2311.09989v1","updated":"2023-11-16T16:07:19Z","published":"2023-11-16T16:07:19Z","title":"Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI\n  Experience","summary":"  The rapid proliferation of data across diverse fields has accentuated the\nimportance of accurate imputation for missing values. This task is crucial for\nensuring data integrity and deriving meaningful insights. In response to this\nchallenge, we present Xputer, a novel imputation tool that adeptly integrates\nNon-negative Matrix Factorization (NMF) with the predictive strengths of\nXGBoost. One of Xputer's standout features is its versatility: it supports zero\nimputation, enables hyperparameter optimization through Optuna, and allows\nusers to define the number of iterations. For enhanced user experience and\naccessibility, we have equipped Xputer with an intuitive Graphical User\nInterface (GUI) ensuring ease of handling, even for those less familiar with\ncomputational tools. In performance benchmarks, Xputer not only rivals the\ncomputational speed of established tools such as IterativeImputer but also\noften outperforms them in terms of imputation accuracy. Furthermore, Xputer\nautonomously handles a diverse spectrum of data types, including categorical,\ncontinuous, and Boolean, eliminating the need for prior preprocessing. Given\nits blend of performance, flexibility, and user-friendly design, Xputer emerges\nas a state-of-the-art solution in the realm of data imputation.\n","authors":["Saleena Younus","Lars Rönnstrand","Julhash U. Kazi"],"pdf_url":"https://arxiv.org/pdf/2311.09989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08870v2","updated":"2023-11-16T15:43:52Z","published":"2023-11-15T11:11:25Z","title":"One-Shot Federated Learning with Classifier-Guided Diffusion Models","summary":"  One-shot federated learning (OSFL) has gained attention in recent years due\nto its low communication cost. However, most of the existing methods require\nauxiliary datasets or training generators, which hinders their practicality in\nreal-world scenarios. In this paper, we explore the novel opportunities that\ndiffusion models bring to OSFL and propose FedCADO, utilizing guidance from\nclient classifiers to generate data that complies with clients' distributions\nand subsequently training the aggregated model on the server. Specifically, our\nmethod involves targeted optimizations in two aspects. On one hand, we\nconditionally edit the randomly sampled initial noises, embedding them with\nspecified semantics and distributions, resulting in a significant improvement\nin both the quality and stability of generation. On the other hand, we employ\nthe BN statistics from the classifiers to provide detailed guidance during\ngeneration. These tailored optimizations enable us to limitlessly generate\ndatasets, which closely resemble the distribution and quality of the original\nclient dataset. Our method effectively handles the heterogeneous client models\nand the problems of non-IID features or labels. In terms of privacy protection,\nour method avoids training any generator or transferring any auxiliary\ninformation on clients, eliminating any additional privacy leakage risks.\nLeveraging the extensive knowledge stored in the pre-trained diffusion model,\nthe synthetic datasets can assist us in surpassing the knowledge limitations of\nthe client samples, resulting in aggregation models that even outperform the\nperformance ceiling of centralized training in some cases, which is\nconvincingly demonstrated in the sufficient quantification and visualization\nexperiments conducted on three large-scale multi-domain image datasets.\n","authors":["Mingzhao Yang","Shangchao Su","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2311.08870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09965v1","updated":"2023-11-16T15:39:01Z","published":"2023-11-16T15:39:01Z","title":"SurgPLAN: Surgical Phase Localization Network for Phase Recognition","summary":"  Surgical phase recognition is crucial to providing surgery understanding in\nsmart operating rooms. Despite great progress in automatic surgical phase\nrecognition, most existing methods are still restricted by two problems. First,\nthese methods cannot capture discriminative visual features for each frame and\nmotion information with simple 2D networks. Second, the frame-by-frame\nrecognition paradigm degrades the performance due to unstable predictions\nwithin each phase, termed as phase shaking. To address these two challenges, we\npropose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a\nmore accurate and stable surgical phase recognition with the principle of\ntemporal detection. Specifically, we first devise a Pyramid SlowFast (PSF)\narchitecture to serve as the visual backbone to capture multi-scale spatial and\ntemporal features by two branches with different frame sampling rates.\nMoreover, we propose a Temporal Phase Localization (TPL) module to generate the\nphase prediction based on temporal region proposals, which ensures accurate and\nconsistent predictions within each surgical phase. Extensive experiments\nconfirm the significant advantages of our SurgPLAN over frame-by-frame\napproaches in terms of both accuracy and stability.\n","authors":["Xingjian Luo","You Pang","Zhen Chen","Jinlin Wu","Zongmin Zhang","Zhen Lei","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09962v1","updated":"2023-11-16T15:32:22Z","published":"2023-11-16T15:32:22Z","title":"Self-supervised learning of multi-omics embeddings in the low-label,\n  high-data regime","summary":"  Contrastive, self-supervised learning (SSL) is used to train a model that\npredicts cancer type from miRNA, mRNA or RPPA expression data. This model, a\npretrained FT-Transformer, is shown to outperform XGBoost and CatBoost,\nstandard benchmarks for tabular data, when labelled samples are scarce but the\nnumber of unlabelled samples is high. This is despite the fact that the\ndatasets we use have $\\mathcal{O}(10^{1})$ classes and\n$\\mathcal{O}(10^{2})-\\mathcal{O}(10^{4})$ features. After demonstrating the\nefficacy of our chosen method of self-supervised pretraining, we investigate\nSSL for multi-modal models. A late-fusion model is proposed, where each omics\nis passed through its own sub-network, the outputs of which are averaged and\npassed to the pretraining or downstream objective function. Multi-modal\npretraining is shown to improve predictions from a single omics, and we argue\nthat this is useful for datasets with many unlabelled multi-modal samples, but\nfew labelled unimodal samples. Additionally, we show that pretraining each\nomics-specific module individually is highly effective. This enables the\napplication of the proposed model in a variety of contexts where a large amount\nof unlabelled data is available from each omics, but only a few labelled\nsamples.\n","authors":["Christian John Hurry","Emma Slade"],"pdf_url":"https://arxiv.org/pdf/2311.09962v1.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.00534v3","updated":"2023-11-16T15:22:45Z","published":"2023-07-02T10:03:01Z","title":"Shared Growth of Graph Neural Networks via Prompted Free-direction\n  Knowledge Distillation","summary":"  Knowledge distillation (KD) has shown to be effective to boost the\nperformance of graph neural networks (GNNs), where the typical objective is to\ndistill knowledge from a deeper teacher GNN into a shallower student GNN.\nHowever, it is often quite challenging to train a satisfactory deeper GNN due\nto the well-known over-parametrized and over-smoothing issues, leading to\ninvalid knowledge transfer in practical applications. In this paper, we propose\nthe first Free-direction Knowledge Distillation framework via reinforcement\nlearning for GNNs, called FreeKD, which is no longer required to provide a\ndeeper well-optimized teacher GNN. Our core idea is to collaboratively learn\ntwo shallower GNNs to exchange knowledge between them. As we observe that one\ntypical GNN model often exhibits better and worse performances at different\nnodes during training, we devise a dynamic and free-direction knowledge\ntransfer strategy that involves two levels of actions: 1) node-level action\ndetermines the directions of knowledge transfer between the corresponding nodes\nof two networks; and then 2) structure-level action determines which of the\nlocal structures generated by the node-level actions to be propagated.\nAdditionally, considering that different augmented graphs can potentially\ncapture distinct perspectives of the graph data, we propose FreeKD-Prompt that\nlearns undistorted and diverse augmentations based on prompt learning for\nexchanging varied knowledge. Furthermore, instead of confining knowledge\nexchange within two GNNs, we develop FreeKD++ to enable free-direction\nknowledge transfer among multiple GNNs. Extensive experiments on five benchmark\ndatasets demonstrate our approaches outperform the base GNNs in a large margin.\nMore surprisingly, our FreeKD has comparable or even better performance than\ntraditional KD algorithms that distill knowledge from a deeper and stronger\nteacher GNN.\n","authors":["Kaituo Feng","Yikun Miao","Changsheng Li","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2307.00534v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2206.06561"},{"id":"http://arxiv.org/abs/2311.09952v1","updated":"2023-11-16T15:15:15Z","published":"2023-11-16T15:15:15Z","title":"Score-based generative models learn manifold-like structures with\n  constrained mixing","summary":"  How do score-based generative models (SBMs) learn the data distribution\nsupported on a low-dimensional manifold? We investigate the score model of a\ntrained SBM through its linear approximations and subspaces spanned by local\nfeature vectors. During diffusion as the noise decreases, the local\ndimensionality increases and becomes more varied between different sample\nsequences. Importantly, we find that the learned vector field mixes samples by\na non-conservative field within the manifold, although it denoises with normal\nprojections as if there is an energy function in off-manifold directions. At\neach noise level, the subspace spanned by the local features overlap with an\neffective density function. These observations suggest that SBMs can flexibly\nmix samples with the learned score field while carefully maintaining a\nmanifold-like structure of the data distribution.\n","authors":["Li Kevin Wenliang","Ben Moran"],"pdf_url":"https://arxiv.org/pdf/2311.09952v1.pdf","comment":"NeurIPS 2022 Workshop on Score-Based Methods"},{"id":"http://arxiv.org/abs/2311.09948v1","updated":"2023-11-16T15:01:48Z","published":"2023-11-16T15:01:48Z","title":"Hijacking Large Language Models via Adversarial In-Context Learning","summary":"  In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific tasks by utilizing labeled examples as demonstrations in the\nprecondition prompts. Despite its promising performance, ICL suffers from\ninstability with the choice and arrangement of examples. Additionally, crafted\nadversarial attacks pose a notable threat to the robustness of ICL. However,\nexisting attacks are either easy to detect, rely on external models, or lack\nspecificity towards ICL. To address these issues, this work introduces a novel\ntransferable attack for ICL, aiming to hijack LLMs to generate the targeted\nresponse. The proposed LLM hijacking attack leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demonstrations. Extensive experimental results on various tasks and\ndatasets demonstrate the effectiveness of our LLM hijacking attack, resulting\nin a distracted attention towards adversarial tokens, consequently leading to\nthe targeted unwanted outputs.\n","authors":["Yao Qiang","Xiangyu Zhou","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.09948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09947v1","updated":"2023-11-16T15:01:26Z","published":"2023-11-16T15:01:26Z","title":"Natural Disaster Analysis using Satellite Imagery and Social-Media Data\n  for Emergency Response Situations","summary":"  Disaster Management is one of the most promising research areas because of\nits significant economic, environmental and social repercussions. This research\nfocuses on analyzing different types of data (pre and post satellite images and\ntwitter data) related to disaster management for in-depth analysis of\nlocation-wise emergency requirements. This research has been divided into two\nstages, namely, satellite image analysis and twitter data analysis followed by\nintegration using location. The first stage involves pre and post disaster\nsatellite image analysis of the location using multi-class land cover\nsegmentation technique based on U-Net architecture. The second stage focuses on\nmapping the region with essential information about the disaster situation and\nimmediate requirements for relief operations. The severely affected regions are\ndemarcated and twitter data is extracted using keywords respective to that\nlocation. The extraction of situational information from a large corpus of raw\ntweets adopts Content Word based Tweet Summarization (COWTS) technique. An\nintegration of these modules using real-time location-based mapping and\nfrequency analysis technique gathers multi-dimensional information in the\nadvent of disaster occurrence such as the Kerala and Mississippi floods that\nwere analyzed and validated as test cases. The novelty of this research lies in\nthe application of segmented satellite images for disaster relief using\nhighlighted land cover changes and integration of twitter data by mapping these\nregion-specific filters for obtaining a complete overview of the disaster.\n","authors":["Sukeerthi Mandyam","Shanmuga Priya MG","Shalini Suresh","Kavitha Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2311.09947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12983v2","updated":"2023-11-16T14:57:58Z","published":"2023-06-22T15:41:15Z","title":"Towards More Realistic Membership Inference Attacks on Large Diffusion\n  Models","summary":"  Generative diffusion models, including Stable Diffusion and Midjourney, can\ngenerate visually appealing, diverse, and high-resolution images for various\napplications. These models are trained on billions of internet-sourced images,\nraising significant concerns about the potential unauthorized use of\ncopyright-protected images. In this paper, we examine whether it is possible to\ndetermine if a specific image was used in the training set, a problem known in\nthe cybersecurity community and referred to as a membership inference attack.\nOur focus is on Stable Diffusion, and we address the challenge of designing a\nfair evaluation framework to answer this membership question. We propose a\nmethodology to establish a fair evaluation setup and apply it to Stable\nDiffusion, enabling potential extensions to other generative models. Utilizing\nthis evaluation setup, we execute membership attacks (both known and newly\nintroduced). Our research reveals that previously proposed evaluation setups do\nnot provide a full understanding of the effectiveness of membership inference\nattacks. We conclude that the membership inference attack remains a significant\nchallenge for large diffusion models (often deployed as black-box systems),\nindicating that related privacy and copyright issues will persist in the\nforeseeable future.\n","authors":["Jan Dubiński","Antoni Kowalczuk","Stanisław Pawlak","Przemysław Rokita","Tomasz Trzciński","Paweł Morawiecki"],"pdf_url":"https://arxiv.org/pdf/2306.12983v2.pdf","comment":"Accepted at WACV2024"},{"id":"http://arxiv.org/abs/2306.02896v2","updated":"2023-11-16T14:48:16Z","published":"2023-06-05T14:05:04Z","title":"Representational Strengths and Limitations of Transformers","summary":"  Attention layers, as commonly used in transformers, form the backbone of\nmodern deep learning, yet there is no mathematical description of their\nbenefits and deficiencies as compared with other architectures. In this work we\nestablish both positive and negative results on the representation power of\nattention layers, with a focus on intrinsic complexity parameters such as\nwidth, depth, and embedding dimension. On the positive side, we present a\nsparse averaging task, where recurrent networks and feedforward networks all\nhave complexity scaling polynomially in the input size, whereas transformers\nscale merely logarithmically in the input size; furthermore, we use the same\nconstruction to show the necessity and role of a large embedding dimension in a\ntransformer. On the negative side, we present a triple detection task, where\nattention layers in turn have complexity scaling linearly in the input size; as\nthis scenario seems rare in practice, we also present natural variants that can\nbe efficiently solved by attention layers. The proof techniques emphasize the\nvalue of communication complexity in the analysis of transformers and related\nmodels, and the role of sparse averaging as a prototypical attention task,\nwhich even finds use in the analysis of triple detection.\n","authors":["Clayton Sanford","Daniel Hsu","Matus Telgarsky"],"pdf_url":"https://arxiv.org/pdf/2306.02896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09930v1","updated":"2023-11-16T14:32:18Z","published":"2023-11-16T14:32:18Z","title":"A Framework for Monitoring and Retraining Language Models in Real-World\n  Applications","summary":"  In the Machine Learning (ML) model development lifecycle, training candidate\nmodels using an offline holdout dataset and identifying the best model for the\ngiven task is only the first step. After the deployment of the selected model,\ncontinuous model monitoring and model retraining is required in many real-world\napplications. There are multiple reasons for retraining, including data or\nconcept drift, which may be reflected on the model performance as monitored by\nan appropriate metric. Another motivation for retraining is the acquisition of\nincreasing amounts of data over time, which may be used to retrain and improve\nthe model performance even in the absence of drifts. We examine the impact of\nvarious retraining decision points on crucial factors, such as model\nperformance and resource utilization, in the context of Multilabel\nClassification models. We explain our key decision points and propose a\nreference framework for designing an effective model retraining strategy.\n","authors":["Jaykumar Kasundra","Claudia Schulz","Melicaalsadat Mirsafian","Stavroula Skylaki"],"pdf_url":"https://arxiv.org/pdf/2311.09930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09922v1","updated":"2023-11-16T14:21:13Z","published":"2023-11-16T14:21:13Z","title":"Fast multiplication by two's complement addition of numbers represented\n  as a set of polynomial radix 2 indexes, stored as an integer list for\n  massively parallel computation","summary":"  We demonstrate a multiplication method based on numbers represented as set of\npolynomial radix 2 indices stored as an integer list. The 'polynomial integer\nindex multiplication' method is a set of algorithms implemented in python code.\nWe demonstrate the method to be faster than both the Number Theoretic Transform\n(NTT) and Karatsuba for multiplication within a certain bit range. Also\nimplemented in python code for comparison purposes with the polynomial radix 2\ninteger method. We demonstrate that it is possible to express any integer or\nreal number as a list of integer indices, representing a finite series in base\ntwo. The finite series of integer index representation of a number can then be\nstored and distributed across multiple CPUs / GPUs. We show that operations of\naddition and multiplication can be applied as two's complement additions\noperating on the index integer representations and can be fully distributed\nacross a given CPU / GPU architecture. We demonstrate fully distributed\narithmetic operations such that the 'polynomial integer index multiplication'\nmethod overcomes the current limitation of parallel multiplication methods. Ie,\nthe need to share common core memory and common disk for the calculation of\nresults and intermediate results.\n","authors":["Mark Stocks"],"pdf_url":"https://arxiv.org/pdf/2311.09922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.12679v3","updated":"2023-11-16T14:13:12Z","published":"2021-04-26T16:12:04Z","title":"Impact of Spatial Frequency Based Constraints on Adversarial Robustness","summary":"  Adversarial examples mainly exploit changes to input pixels to which humans\nare not sensitive to, and arise from the fact that models make decisions based\non uninterpretable features. Interestingly, cognitive science reports that the\nprocess of interpretability for human classification decision relies\npredominantly on low spatial frequency components. In this paper, we\ninvestigate the robustness to adversarial perturbations of models enforced\nduring training to leverage information corresponding to different spatial\nfrequency ranges. We show that it is tightly linked to the spatial frequency\ncharacteristics of the data at stake. Indeed, depending on the data set, the\nsame constraint may results in very different level of robustness (up to 0.41\nadversarial accuracy difference). To explain this phenomenon, we conduct\nseveral experiments to enlighten influential factors such as the level of\nsensitivity to high frequencies, and the transferability of adversarial\nperturbations between original and low-pass filtered inputs.\n","authors":["Rémi Bernhard","Pierre-Alain Moellic","Martial Mermillod","Yannick Bourrier","Romain Cohendet","Miguel Solinas","Marina Reyboz"],"pdf_url":"https://arxiv.org/pdf/2104.12679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12560v2","updated":"2023-11-16T14:12:10Z","published":"2022-07-25T22:34:08Z","title":"AMLB: an AutoML Benchmark","summary":"  Comparing different AutoML frameworks is notoriously challenging and often\ndone incorrectly. We introduce an open and extensible benchmark that follows\nbest practices and avoids common mistakes when comparing AutoML frameworks. We\nconduct a thorough comparison of 9 well-known AutoML frameworks across 71\nclassification and 33 regression tasks. The differences between the AutoML\nframeworks are explored with a multi-faceted analysis, evaluating model\naccuracy, its trade-offs with inference time, and framework failures. We also\nuse Bradley-Terry trees to discover subsets of tasks where the relative AutoML\nframework rankings differ. The benchmark comes with an open-source tool that\nintegrates with many AutoML frameworks and automates the empirical evaluation\nprocess end-to-end: from framework installation and resource allocation to\nin-depth evaluation. The benchmark uses public data sets, can be easily\nextended with other AutoML frameworks and tasks, and has a website with\nup-to-date results.\n","authors":["Pieter Gijsbers","Marcos L. P. Bueno","Stefan Coors","Erin LeDell","Sébastien Poirier","Janek Thomas","Bernd Bischl","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2207.12560v2.pdf","comment":"UNDER REVIEW: Revised submission to JMLR, with updated results from\n  June 2023"},{"id":"http://arxiv.org/abs/2306.12251v2","updated":"2023-11-16T14:05:25Z","published":"2023-06-21T13:16:10Z","title":"GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection","summary":"  With a long history of traditional Graph Anomaly Detection (GAD) algorithms\nand recently popular Graph Neural Networks (GNNs), it is still not clear (1)\nhow they perform under a standard comprehensive setting, (2) whether GNNs can\noutperform traditional algorithms such as tree ensembles, and (3) how about\ntheir efficiency on large-scale graphs. In response, we introduce GADBench -- a\nbenchmark tool dedicated to supervised anomalous node detection in static\ngraphs. GADBench facilitates a detailed comparison across 29 distinct models on\nten real-world GAD datasets, encompassing thousands to millions ($\\sim$6M)\nnodes. Our main finding is that tree ensembles with simple neighborhood\naggregation can outperform the latest GNNs tailored for the GAD task. We shed\nlight on the current progress of GAD, setting a robust groundwork for\nsubsequent investigations in this domain. GADBench is open-sourced at\nhttps://github.com/squareRoot3/GADBench.\n","authors":["Jianheng Tang","Fengrui Hua","Ziqi Gao","Peilin Zhao","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2306.12251v2.pdf","comment":"NeurIPS 2023 Datasets and Benchmarks Track camera ready version"},{"id":"http://arxiv.org/abs/2306.07255v2","updated":"2023-11-16T13:54:59Z","published":"2023-06-12T17:25:12Z","title":"Conditional Matrix Flows for Gaussian Graphical Models","summary":"  Studying conditional independence among many variables with few observations\nis a challenging task. Gaussian Graphical Models (GGMs) tackle this problem by\nencouraging sparsity in the precision matrix through $l_q$ regularization with\n$q\\leq1$. However, most GMMs rely on the $l_1$ norm because the objective is\nhighly non-convex for sub-$l_1$ pseudo-norms. In the frequentist formulation,\nthe $l_1$ norm relaxation provides the solution path as a function of the\nshrinkage parameter $\\lambda$. In the Bayesian formulation, sparsity is instead\nencouraged through a Laplace prior, but posterior inference for different\n$\\lambda$ requires repeated runs of expensive Gibbs samplers. Here we propose a\ngeneral framework for variational inference with matrix-variate Normalizing\nFlow in GGMs, which unifies the benefits of frequentist and Bayesian\nframeworks. As a key improvement on previous work, we train with one flow a\ncontinuum of sparse regression models jointly for all regularization parameters\n$\\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.\nWithin one model we thus have access to (i) the evolution of the posterior for\nany $\\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood\nfor model selection, and (iii) the frequentist solution paths through simulated\nannealing in the MAP limit.\n","authors":["Marcello Massimo Negri","F. Arend Torres","Volker Roth"],"pdf_url":"https://arxiv.org/pdf/2306.07255v2.pdf","comment":"NeurIPS23 version"},{"id":"http://arxiv.org/abs/2306.11928v2","updated":"2023-11-16T13:38:20Z","published":"2023-06-20T22:31:13Z","title":"Open Problem: Learning with Variational Objectives on Measures","summary":"  The theory of statistical learning has focused on variational objectives\nexpressed on functions. In this note, we discuss motivations to write similar\nobjectives on measures, in particular to discuss out-of-distribution\ngeneralization and weakly-supervised learning. It raises a natural question:\ncan one cast usual statistical learning results to objectives expressed on\nmeasures? Does the resulting construction lead to new algorithms of practical\ninterest?\n","authors":["Vivien Cabannes","Carles Domingo-Enrich"],"pdf_url":"https://arxiv.org/pdf/2306.11928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09891v1","updated":"2023-11-16T13:38:00Z","published":"2023-11-16T13:38:00Z","title":"On some elusive aspects of databases hindering AI based discovery: A\n  case study on superconducting materials","summary":"  It stands to reason that the amount and the quality of big data is of key\nimportance for setting up accurate AI-driven models. Nonetheless, we believe\nthere are still critical roadblocks in the inherent generation of databases,\nthat are often underestimated and poorly discussed in the literature. In our\nview, such issues can seriously hinder the AI-based discovery process, even\nwhen high quality, sufficiently large and highly reputable data sources are\navailable. Here, considering superconducting and thermoelectric materials as\ntwo representative case studies, we specifically discuss three aspects, namely\nintrinsically biased sample selection, possible hidden variables, disparate\ndata age. Importantly, to our knowledge, we suggest and test a first strategy\ncapable of detecting and quantifying the presence of the intrinsic data bias.\n","authors":["Giovanni Trezza","Eliodoro Chiavazzo"],"pdf_url":"https://arxiv.org/pdf/2311.09891v1.pdf","comment":"20 pages, 3 figures (main), 3 figures (supp info)"},{"id":"http://arxiv.org/abs/2205.07394v2","updated":"2023-11-16T13:28:25Z","published":"2022-05-15T22:53:36Z","title":"Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems\n  Using Online Reinforcement Learning","summary":"  Hybrid storage systems (HSS) use multiple different storage devices to\nprovide high and scalable storage capacity at high performance. Recent research\nproposes various techniques that aim to accurately identify\nperformance-critical data to place it in a \"best-fit\" storage device.\nUnfortunately, most of these techniques are rigid, which (1) limits their\nadaptivity to perform well for a wide range of workloads and storage device\nconfigurations, and (2) makes it difficult for designers to extend these\ntechniques to different storage system configurations (e.g., with a different\nnumber or different types of storage devices) than the configuration they are\ndesigned for. We introduce Sibyl, the first technique that uses reinforcement\nlearning for data placement in hybrid storage systems. Sibyl observes different\nfeatures of the running workload as well as the storage devices to make\nsystem-aware data placement decisions. For every decision it makes, Sibyl\nreceives a reward from the system that it uses to evaluate the long-term\nperformance impact of its decision and continuously optimizes its data\nplacement policy online. We implement Sibyl on real systems with various HSS\nconfigurations. Our results show that Sibyl provides 21.6%/19.9% performance\nimprovement in a performance-oriented/cost-oriented HSS configuration compared\nto the best previous data placement technique. Our evaluation using an HSS\nconfiguration with three different storage devices shows that Sibyl outperforms\nthe state-of-the-art data placement policy by 23.9%-48.2%, while significantly\nreducing the system architect's burden in designing a data placement mechanism\nthat can simultaneously incorporate three storage devices. We show that Sibyl\nachieves 80% of the performance of an oracle policy that has complete knowledge\nof future access patterns while incurring a very modest storage overhead of\nonly 124.4 KiB.\n","authors":["Gagandeep Singh","Rakesh Nadig","Jisung Park","Rahul Bera","Nastaran Hajinazar","David Novo","Juan Gómez-Luna","Sander Stuijk","Henk Corporaal","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2205.07394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04876v3","updated":"2023-11-16T13:17:44Z","published":"2022-06-21T09:42:30Z","title":"On the Intrinsic Structures of Spiking Neural Networks","summary":"  Recent years have emerged a surge of interest in SNNs owing to their\nremarkable potential to handle time-dependent and event-driven data. The\nperformance of SNNs hinges not only on selecting an apposite architecture and\nfine-tuning connection weights, similar to conventional ANNs, but also on the\nmeticulous configuration of intrinsic structures within spiking computations.\nHowever, there has been a dearth of comprehensive studies examining the impact\nof intrinsic structures. Consequently, developers often find it challenging to\napply a standardized configuration of SNNs across diverse datasets or tasks.\nThis work delves deep into the intrinsic structures of SNNs. Initially, we\nunveil two pivotal components of intrinsic structures: the integration\noperation and firing-reset mechanism, by elucidating their influence on the\nexpressivity of SNNs. Furthermore, we draw two key conclusions: the membrane\ntime hyper-parameter is intimately linked to the eigenvalues of the integration\noperation, dictating the functional topology of spiking dynamics, and various\nhyper-parameters of the firing-reset mechanism govern the overall firing\ncapacity of an SNN, mitigating the injection ratio or sampling density of input\ndata. These findings elucidate why the efficacy of SNNs hinges heavily on the\nconfiguration of intrinsic structures and lead to a recommendation that\nenhancing the adaptability of these structures contributes to improving the\noverall performance and applicability of SNNs. Inspired by this recognition, we\npropose two feasible approaches to enhance SNN learning. These involve\nleveraging self-connection architectures and employing stochastic spiking\nneurons to augment the adaptability of the integration operation and\nfiring-reset mechanism, respectively. We verify the effectiveness of the\nproposed methods from perspectives of theory and practice.\n","authors":["Shao-Qun Zhang","Jia-Yi Chen","Jin-Hui Wu","Gao Zhang","Huan Xiong","Bin Gu","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2207.04876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09878v1","updated":"2023-11-16T13:12:58Z","published":"2023-11-16T13:12:58Z","title":"Safety Aware Autonomous Path Planning Using Model Predictive\n  Reinforcement Learning for Inland Waterways","summary":"  In recent years, interest in autonomous shipping in urban waterways has\nincreased significantly due to the trend of keeping cars and trucks out of city\ncenters. Classical approaches such as Frenet frame based planning and potential\nfield navigation often require tuning of many configuration parameters and\nsometimes even require a different configuration depending on the situation. In\nthis paper, we propose a novel path planning approach based on reinforcement\nlearning called Model Predictive Reinforcement Learning (MPRL). MPRL calculates\na series of waypoints for the vessel to follow. The environment is represented\nas an occupancy grid map, allowing us to deal with any shape of waterway and\nany number and shape of obstacles. We demonstrate our approach on two scenarios\nand compare the resulting path with path planning using a Frenet frame and path\nplanning based on a proximal policy optimization (PPO) agent. Our results show\nthat MPRL outperforms both baselines in both test scenarios. The PPO based\napproach was not able to reach the goal in either scenario while the Frenet\nframe approach failed in the scenario consisting of a corner with obstacles.\nMPRL was able to safely (collision free) navigate to the goal in both of the\ntest scenarios.\n","authors":["Astrid Vanneste","Simon Vanneste","Olivier Vasseur","Robin Janssens","Mattias Billast","Ali Anwar","Kevin Mets","Tom De Schepper","Siegfried Mercelis","Peter Hellinckx"],"pdf_url":"https://arxiv.org/pdf/2311.09878v1.pdf","comment":"\\c{opyright} 2022 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2311.09858v1","updated":"2023-11-16T12:38:45Z","published":"2023-11-16T12:38:45Z","title":"Polynomially Over-Parameterized Convolutional Neural Networks Contain\n  Structured Strong Winning Lottery Tickets","summary":"  The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised\nneural networks likely contain subnetworks that perform well without any\ntraining. Although unstructured pruning has been extensively studied in this\ncontext, its structured counterpart, which can deliver significant\ncomputational and memory efficiency gains, has been largely unexplored. One of\nthe main reasons for this gap is the limitations of the underlying mathematical\ntools used in formal analyses of the SLTH. In this paper, we overcome these\nlimitations: we leverage recent advances in the multidimensional generalisation\nof the Random Subset-Sum Problem and obtain a variant that admits the\nstochastic dependencies that arise when addressing structured pruning in the\nSLTH. We apply this result to prove, for a wide class of random Convolutional\nNeural Networks, the existence of structured subnetworks that can approximate\nany sufficiently smaller network.\n  This result provides the first sub-exponential bound around the SLTH for\nstructured pruning, opening up new avenues for further research on the\nhypothesis and contributing to the understanding of the role of\nover-parameterization in deep learning.\n","authors":["Arthur da Cunha","Francesco d'Amore","Emanuele Natale"],"pdf_url":"https://arxiv.org/pdf/2311.09858v1.pdf","comment":"To be published in the 37th Conference on Neural Information\n  Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2308.00002v3","updated":"2023-11-16T12:33:12Z","published":"2023-07-28T01:30:15Z","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition","summary":"  Temporal commonsense reasoning refers to the ability to understand the\ntypical temporal context of phrases, actions, and events, and use it to reason\nover problems requiring such knowledge. This trait is essential in temporal\nnatural language processing tasks, with possible applications such as timeline\nsummarization, temporal question answering, and temporal natural language\ninference. Recent research on the performance of large language models suggests\nthat, although they are adept at generating syntactically correct sentences and\nsolving classification tasks, they often take shortcuts in their reasoning and\nfall prey to simple linguistic traps. This article provides an overview of\nresearch in the domain of temporal commonsense reasoning, particularly focusing\non enhancing language model performance through a variety of augmentations and\ntheir evaluation across a growing number of datasets. However, these augmented\nmodels still struggle to approach human performance on reasoning tasks over\ntemporal common sense properties, such as the typical occurrence times,\norderings, or durations of events. We further emphasize the need for careful\ninterpretation of research to guard against overpromising evaluation results in\nlight of the shallow reasoning present in transformers. This can be achieved by\nappropriately preparing datasets and suitable evaluation metrics.\n","authors":["Georg Wenzel","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2308.00002v3.pdf","comment":"27 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2311.09856v1","updated":"2023-11-16T12:32:44Z","published":"2023-11-16T12:32:44Z","title":"Contribution Evaluation in Federated Learning: Examining Current\n  Approaches","summary":"  Federated Learning (FL) has seen increasing interest in cases where entities\nwant to collaboratively train models while maintaining privacy and governance\nover their data. In FL, clients with private and potentially heterogeneous data\nand compute resources come together to train a common model without raw data\never leaving their locale. Instead, the participants contribute by sharing\nlocal model updates, which, naturally, differ in quality. Quantitatively\nevaluating the worth of these contributions is termed the Contribution\nEvaluation (CE) problem. We review current CE approaches from the underlying\nmathematical framework to efficiently calculate a fair value for each client.\nFurthermore, we benchmark some of the most promising state-of-the-art\napproaches, along with a new one we introduce, on MNIST and CIFAR-10, to\nshowcase their differences. Designing a fair and efficient CE method, while a\nsmall part of the overall FL system design, is tantamount to the mainstream\nadoption of FL.\n","authors":["Vasilis Siomos","Jonathan Passerat-Palmbach"],"pdf_url":"https://arxiv.org/pdf/2311.09856v1.pdf","comment":"Published at New Frontiers in Federated Learning: Privacy, Fairness,\n  Robustness, Personalization and Data Ownership workshop @NeurIPS 2021"},{"id":"http://arxiv.org/abs/2311.09854v1","updated":"2023-11-16T12:30:14Z","published":"2023-11-16T12:30:14Z","title":"SurvTimeSurvival: Survival Analysis On The Patient With Multiple\n  Visits/Records","summary":"  The accurate prediction of survival times for patients with severe diseases\nremains a critical challenge despite recent advances in artificial\nintelligence. This study introduces \"SurvTimeSurvival: Survival Analysis On\nPatients With Multiple Visits/Records\", utilizing the Transformer model to not\nonly handle the complexities of time-varying covariates but also covariates\ndata. We also tackle the data sparsity issue common to survival analysis\ndatasets by integrating synthetic data generation into the learning process of\nour model. We show that our method outperforms state-of-the-art deep learning\napproaches on both covariates and time-varying covariates datasets. Our\napproach aims not only to enhance the understanding of individual patient\nsurvival trajectories across various medical conditions, thereby improving\nprediction accuracy, but also to play a pivotal role in designing clinical\ntrials and creating new treatments.\n","authors":["Hung Le","Ong Eng-Jon","Bober Miroslaw"],"pdf_url":"https://arxiv.org/pdf/2311.09854v1.pdf","comment":"Accepted as Findings Track in Machine Learning For Health (ML4H) 2023"},{"id":"http://arxiv.org/abs/2311.09852v1","updated":"2023-11-16T12:28:31Z","published":"2023-11-16T12:28:31Z","title":"Short vs. Long-term Coordination of Drones: When Distributed\n  Optimization Meets Deep Reinforcement Learning","summary":"  Swarms of smart drones, with the support of charging technology, can provide\ncompleting sensing capabilities in Smart Cities, such as traffic monitoring and\ndisaster response. Existing approaches, including distributed optimization and\ndeep reinforcement learning (DRL), aim to coordinate drones to achieve\ncost-effective, high-quality navigation, sensing, and recharging. However, they\nhave distinct challenges: short-term optimization struggles to provide\nsustained benefits, while long-term DRL lacks scalability, resilience, and\nflexibility. To bridge this gap, this paper introduces a new progressive\napproach that encompasses the planning and selection based on distributed\noptimization, as well as DRL-based flying direction scheduling. Extensive\nexperiment with datasets generated from realisitic urban mobility demonstrate\nthe outstanding performance of the proposed solution in traffic monitoring\ncompared to three baseline methods.\n","authors":["Chuhao Qin","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2311.09852v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2311.08433v2","updated":"2023-11-16T12:21:56Z","published":"2023-11-14T06:44:26Z","title":"Clinical Characteristics and Laboratory Biomarkers in ICU-admitted\n  Septic Patients with and without Bacteremia","summary":"  Few studies have investigated the diagnostic utilities of biomarkers for\npredicting bacteremia among septic patients admitted to intensive care units\n(ICU). Therefore, this study evaluated the prediction power of laboratory\nbiomarkers to utilize those markers with high performance to optimize the\npredictive model for bacteremia. This retrospective cross-sectional study was\nconducted at the ICU department of Gyeongsang National University Changwon\nHospital in 2019. Adult patients qualifying SEPSIS-3 (increase in sequential\norgan failure score greater than or equal to 2) criteria with at least two sets\nof blood culture were selected. Collected data was initially analyzed\nindependently to identify the significant predictors, which was then used to\nbuild the multivariable logistic regression (MLR) model. A total of 218\npatients with 48 cases of true bacteremia were analyzed in this research. Both\nCRP and PCT showed a substantial area under the curve (AUC) value for\ndiscriminating bacteremia among septic patients (0.757 and 0.845,\nrespectively). To further enhance the predictive accuracy, we combined PCT,\nbilirubin, neutrophil lymphocyte ratio (NLR), platelets, lactic acid,\nerythrocyte sedimentation rate (ESR), and Glasgow Coma Scale (GCS) score to\nbuild the predictive model with an AUC of 0.907 (95% CI, 0.843 to 0.956). In\naddition, a high association between bacteremia and mortality rate was\ndiscovered through the survival analysis (0.004). While PCT is certainly a\nuseful index for distinguishing patients with and without bacteremia by itself,\nour MLR model indicates that the accuracy of bacteremia prediction\nsubstantially improves by the combined use of PCT, bilirubin, NLR, platelets,\nlactic acid, ESR, and GCS score.\n","authors":["Sangwon Baek","Seung Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2311.08433v2.pdf","comment":"This article is not the right fit to be published as preprint in\n  arXiv"},{"id":"http://arxiv.org/abs/2311.09848v1","updated":"2023-11-16T12:21:06Z","published":"2023-11-16T12:21:06Z","title":"Diffusion-Augmented Neural Processes","summary":"  Over the last few years, Neural Processes have become a useful modelling tool\nin many application areas, such as healthcare and climate sciences, in which\ndata are scarce and prediction uncertainty estimates are indispensable.\nHowever, the current state of the art in the field (AR CNPs; Bruinsma et al.,\n2023) presents a few issues that prevent its widespread deployment. This work\nproposes an alternative, diffusion-based approach to NPs which, through\nconditioning on noised datasets, addresses many of these limitations, whilst\nalso exceeding SOTA performance.\n","authors":["Lorenzo Bonito","James Requeima","Aliaksandra Shysheya","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2311.09848v1.pdf","comment":"Accepted to the NeurIPS 2023 Workshop on Diffusion Models"},{"id":"http://arxiv.org/abs/2311.09847v1","updated":"2023-11-16T12:20:25Z","published":"2023-11-16T12:20:25Z","title":"Overcoming Data Scarcity in Biomedical Imaging with a Foundational\n  Multi-Task Model","summary":"  Foundational models, pretrained on a large scale, have demonstrated\nsubstantial success across non-medical domains. However, training these models\ntypically requires large, comprehensive datasets, which contrasts with the\nsmaller and more heterogeneous datasets common in biomedical imaging. Here, we\npropose a multi-task learning strategy that decouples the number of training\ntasks from memory requirements. We trained a Universal bioMedical PreTrained\nmodel (UMedPT) on a multi-task database including tomographic, microscopic, and\nX-ray images, with various labelling strategies such as classification,\nsegmentation, and object detection. The UMedPT foundational model outperformed\nImageNet pretraining and the previous state-of-the-art models. For tasks\nrelated to the pretraining database, it maintained its performance with only 1%\nof the original training data and without fine-tuning. For out-of-domain tasks\nit required not more than 50% of the original training data. In an external\nindependent validation imaging features extracted using UMedPT proved to be a\nnew standard for cross-center transferability.\n","authors":["Raphael Schäfer","Till Nicke","Henning Höfener","Annkristin Lange","Dorit Merhof","Friedrich Feuerhake","Volkmar Schulz","Johannes Lotz","Fabian Kiessling"],"pdf_url":"https://arxiv.org/pdf/2311.09847v1.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09846v1","updated":"2023-11-16T12:19:48Z","published":"2023-11-16T12:19:48Z","title":"GroupMixer: Patch-based Group Convolutional Neural Network for Breast\n  Cancer Detection from Histopathological Images","summary":"  Diagnosis of breast cancer malignancy at the early stages is a crucial step\nfor controlling its side effects. Histopathological analysis provides a unique\nopportunity for malignant breast cancer detection. However, such a task would\nbe tedious and time-consuming for the histopathologists. Deep Neural Networks\nenable us to learn informative features directly from raw histopathological\nimages without manual feature extraction. Although Convolutional Neural\nNetworks (CNNs) have been the dominant architectures in the computer vision\nrealm, Transformer-based architectures have shown promising results in\ndifferent computer vision tasks. Although harnessing the capability of\nTransformer-based architectures for medical image analysis seems interesting,\nthese architectures are large, have a significant number of trainable\nparameters, and require large datasets to be trained on, which are usually rare\nin the medical domain. It has been claimed and empirically proved that at least\npart of the superior performance of Transformer-based architectures in Computer\nVision domain originates from patch embedding operation. In this paper, we\nborrowed the previously introduced idea of integrating a fully Convolutional\nNeural Network architecture with Patch Embedding operation and presented an\nefficient CNN architecture for breast cancer malignancy detection from\nhistopathological images. Despite the number of parameters that is\nsignificantly smaller than other methods, the accuracy performance metrics\nachieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x\nmagnifications respectively. We took a step forward and modified the\narchitecture using Group Convolution and Channel Shuffling ideas and reduced\nthe number of trainable parameters even more with a negligible decline in\nperformance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the\nmentioned magnifications respectively.\n","authors":["Ardavan Modarres","Erfan Ebrahim Esfahani","Mahsa Bahrami"],"pdf_url":"https://arxiv.org/pdf/2311.09846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09841v1","updated":"2023-11-16T12:13:49Z","published":"2023-11-16T12:13:49Z","title":"Leveraging LLMs in Scholarly Knowledge Graph Question Answering","summary":"  This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.\n","authors":["Tilahun Abedissa Taffa","Ricardo Usbeck"],"pdf_url":"https://arxiv.org/pdf/2311.09841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09986v3","updated":"2023-11-16T12:12:37Z","published":"2023-03-17T14:02:35Z","title":"Towards AI-controlled FES-restoration of movements: Learning cycling\n  stimulation pattern with reinforcement learning","summary":"  Functional electrical stimulation (FES) has been increasingly integrated with\nother rehabilitation devices, including robots. FES cycling is one of the\ncommon FES applications in rehabilitation, which is performed by stimulating\nleg muscles in a certain pattern. The appropriate pattern varies across\nindividuals and requires manual tuning which can be time-consuming and\nchallenging for the individual user. Here, we present an AI-based method for\nfinding the patterns, which requires no extra hardware or sensors. Our method\nhas two phases, starting with finding model-based patterns using reinforcement\nlearning and detailed musculoskeletal models. The models, built using\nopen-source software, can be customised through our automated script and can be\ntherefore used by non-technical individuals without extra cost. Next, our\nmethod fine-tunes the pattern using real cycling data. We test our both in\nsimulation and experimentally on a stationary tricycle. In the simulation test,\nour method can robustly deliver model-based patterns for different cycling\nconfigurations. The experimental evaluation shows that our method can find a\nmodel-based pattern that induces higher cycling speed than an EMG-based\npattern. By using just 100 seconds of cycling data, our method can deliver a\nfine-tuned pattern that gives better cycling performance. Beyond FES cycling,\nthis work is a showcase, displaying the feasibility and potential of\nhuman-in-the-loop AI in real-world rehabilitation.\n","authors":["Nat Wannawas","A. Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2303.09986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09819v1","updated":"2023-11-16T11:48:29Z","published":"2023-11-16T11:48:29Z","title":"PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical\n  Instruments","summary":"  In surgical procedures, correct instrument counting is essential. Instance\nsegmentation is a location method that locates not only an object's bounding\nbox but also each pixel's specific details. However, obtaining mask-level\nannotations is labor-intensive in instance segmentation. To address this issue,\nwe propose a novel yet effective weakly-supervised surgical instrument instance\nsegmentation approach, named Point-based Weakly-supervised Instance\nSegmentation (PWISeg). PWISeg adopts an FCN-based architecture with\npoint-to-box and point-to-mask branches to model the relationships between\nfeature points and bounding boxes, as well as feature points and segmentation\nmasks on FPN, accomplishing instrument detection and segmentation jointly in a\nsingle model. Since mask level annotations are hard to available in the real\nworld, for point-to-mask training, we introduce an unsupervised projection\nloss, utilizing the projected relation between predicted masks and bboxes as\nsupervision signal. On the other hand, we annotate a few pixels as the key\npixel for each instrument. Based on this, we further propose a key pixel\nassociation loss and a key pixel distribution loss, driving the point-to-mask\nbranch to generate more accurate segmentation predictions. To comprehensively\nevaluate this task, we unveil a novel surgical instrument dataset with manual\nannotations, setting up a benchmark for further research. Our comprehensive\nresearch trial validated the superior performance of our PWISeg. The results\nshow that the accuracy of surgical instrument segmentation is improved,\nsurpassing most methods of instance segmentation via weakly supervised bounding\nboxes. This improvement is consistently observed in our proposed dataset and\nwhen applied to the public HOSPI-Tools dataset.\n","authors":["Zhen Sun","Huan Xu","Jinlin Wu","Zhen Chen","Zhen Lei","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09819v1.pdf","comment":"This work has been submitted to IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2024 for possible publication"},{"id":"http://arxiv.org/abs/2311.09816v1","updated":"2023-11-16T11:44:58Z","published":"2023-11-16T11:44:58Z","title":"Performance Trade-offs of Watermarking Large Language Models","summary":"  Amidst growing concerns of large language models (LLMs) being misused for\ngenerating misinformation or completing homework assignments, watermarking has\nemerged as an effective solution for distinguishing human-written and\nLLM-generated text. A prominent watermarking strategy is to embed a signal into\ngenerated text by upsampling a (pseudorandomly-chosen) subset of tokens at\nevery generation step. Although this signal is imperceptible to a human reader,\nit is detectable through statistical testing. However, implanting such signals\nalters the model's output distribution and can have unintended effects when\nwatermarked LLMs are used for downstream applications. In this work, we\nevaluate the performance of watermarked LLMs on a diverse suite of tasks,\nincluding text classification, textual entailment, reasoning, question\nanswering, translation, summarization, and language modeling. We find that\nwatermarking has negligible impact on the performance of tasks posed as k-class\nclassification problems in the average case. However, the accuracy can plummet\nto that of a random classifier for some scenarios (that occur with\nnon-negligible probability). Tasks that are cast as multiple-choice questions\nand short-form generation are surprisingly unaffected by watermarking. For\nlong-form generation tasks, including summarization and translation, we see a\ndrop of 15-20% in the performance due to watermarking. Our findings highlight\nthe trade-offs that users should be cognizant of when using watermarked models,\nand point to cases where future research could improve existing trade-offs.\n","authors":["Anirudh Ajith","Sameer Singh","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2311.09816v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09811v1","updated":"2023-11-16T11:34:37Z","published":"2023-11-16T11:34:37Z","title":"Runtime Verification of Learning Properties for Reinforcement Learning\n  Algorithms","summary":"  Reinforcement learning (RL) algorithms interact with their environment in a\ntrial-and-error fashion. Such interactions can be expensive, inefficient, and\ntimely when learning on a physical system rather than in a simulation. This\nwork develops new runtime verification techniques to predict when the learning\nphase has not met or will not meet qualitative and timely expectations. This\npaper presents three verification properties concerning the quality and\ntimeliness of learning in RL algorithms. With each property, we propose design\nsteps for monitoring and assessing the properties during the system's\noperation.\n","authors":["Tommaso Mannucci","Julio de Oliveira Filho"],"pdf_url":"https://arxiv.org/pdf/2311.09811v1.pdf","comment":"In Proceedings FMAS 2023, arXiv:2311.08987"},{"id":"http://arxiv.org/abs/2311.09809v1","updated":"2023-11-16T11:33:08Z","published":"2023-11-16T11:33:08Z","title":"Comparing Differentiable Logics for Learning Systems: A Research Preview","summary":"  Extensive research on formal verification of machine learning (ML) systems\nindicates that learning from data alone often fails to capture underlying\nbackground knowledge. A variety of verifiers have been developed to ensure that\na machine-learnt model satisfies correctness and safety properties, however,\nthese verifiers typically assume a trained network with fixed weights.\nML-enabled autonomous systems are required to not only detect incorrect\npredictions, but should also possess the ability to self-correct, continuously\nimproving and adapting. A promising approach for creating ML models that\ninherently satisfy constraints is to encode background knowledge as logical\nconstraints that guide the learning process via so-called differentiable\nlogics. In this research preview, we compare and evaluate various logics from\nthe literature in weakly-supervised contexts, presenting our findings and\nhighlighting open problems for future work. Our experimental results are\nbroadly consistent with results reported previously in literature; however,\nlearning with differentiable logics introduces a new hyperparameter that is\ndifficult to tune and has significant influence on the effectiveness of the\nlogics.\n","authors":["Thomas Flinkow","Barak A. Pearlmutter","Rosemary Monahan"],"pdf_url":"https://arxiv.org/pdf/2311.09809v1.pdf","comment":"In Proceedings FMAS 2023, arXiv:2311.08987"},{"id":"http://arxiv.org/abs/2205.00256v2","updated":"2023-11-16T11:23:28Z","published":"2022-04-30T12:57:02Z","title":"Heterogeneous Graph Neural Networks using Self-supervised Reciprocally\n  Contrastive Learning","summary":"  Heterogeneous graph neural network (HGNN) is a very popular technique for the\nmodeling and analysis of heterogeneous graphs. Most existing HGNN-based\napproaches are supervised or semi-supervised learning methods requiring graphs\nto be annotated, which is costly and time-consuming. Self-supervised\ncontrastive learning has been proposed to address the problem of requiring\nannotated data by mining intrinsic information hidden within the given data.\nHowever, the existing contrastive learning methods are inadequate for\nheterogeneous graphs because they construct contrastive views only based on\ndata perturbation or pre-defined structural properties (e.g., meta-path) in\ngraph data while ignore the noises that may exist in both node attributes and\ngraph topologies. We develop for the first time a novel and robust\nheterogeneous graph contrastive learning approach, namely HGCL, which\nintroduces two views on respective guidance of node attributes and graph\ntopologies and integrates and enhances them by reciprocally contrastive\nmechanism to better model heterogeneous graphs. In this new approach, we adopt\ndistinct but most suitable attribute and topology fusion mechanisms in the two\nviews, which are conducive to mining relevant information in attributes and\ntopologies separately. We further use both attribute similarity and topological\ncorrelation to construct high-quality contrastive samples. Extensive\nexperiments on three large real-world heterogeneous graphs demonstrate the\nsuperiority and robustness of HGCL over state-of-the-art methods.\n","authors":["Cuiying Huo","Dongxiao He","Yawen Li","Di Jin","Jianwu Dang","Weixiong Zhang","Witold Pedrycz","Lingfei Wu"],"pdf_url":"https://arxiv.org/pdf/2205.00256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09793v1","updated":"2023-11-16T11:18:21Z","published":"2023-11-16T11:18:21Z","title":"Fossil 2.0: Formal Certificate Synthesis for the Verification and\n  Control of Dynamical Models","summary":"  This paper presents Fossil 2.0, a new major release of a software tool for\nthe synthesis of certificates (e.g., Lyapunov and barrier functions) for\ndynamical systems modelled as ordinary differential and difference equations.\nFossil 2.0 is much improved from its original release, including new\ninterfaces, a significantly expanded certificate portfolio, controller\nsynthesis and enhanced extensibility. We present these new features as part of\nthis tool paper. Fossil implements a counterexample-guided inductive synthesis\n(CEGIS) loop ensuring the soundness of the method. Our tool uses neural\nnetworks as templates to generate candidate functions, which are then formally\nproven by an SMT solver acting as an assertion verifier. Improvements with\nrespect to the first release include a wider range of certificates, synthesis\nof control laws, and support for discrete-time models.\n","authors":["Alec Edwards","Andrea Peruffo","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2311.09793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03397v4","updated":"2023-11-16T11:13:55Z","published":"2022-02-07T18:35:46Z","title":"Bilevel Optimization with a Lower-level Contraction: Optimal Sample\n  Complexity without Warm-start","summary":"  We analyse a general class of bilevel problems, in which the upper-level\nproblem consists in the minimization of a smooth objective function and the\nlower-level problem is to find the fixed point of a smooth contraction map.\nThis type of problems include instances of meta-learning, equilibrium models,\nhyperparameter optimization and data poisoning adversarial attacks. Several\nrecent works have proposed algorithms which warm-start the lower-level problem,\ni.e.~they use the previous lower-level approximate solution as a staring point\nfor the lower-level solver. This warm-start procedure allows one to improve the\nsample complexity in both the stochastic and deterministic settings, achieving\nin some cases the order-wise optimal sample complexity. However, there are\nsituations, e.g., meta learning and equilibrium models, in which the warm-start\nprocedure is not well-suited or ineffective. In this work we show that without\nwarm-start, it is still possible to achieve order-wise (near) optimal sample\ncomplexity. In particular, we propose a simple method which uses (stochastic)\nfixed point iterations at the lower-level and projected inexact gradient\ndescent at the upper-level, that reaches an $\\epsilon$-stationary point using\n$O(\\epsilon^{-2})$ and $\\tilde{O}(\\epsilon^{-1})$ samples for the stochastic\nand the deterministic setting, respectively. Finally, compared to methods using\nwarm-start, our approach yields a simpler analysis that does not need to study\nthe coupled interactions between the upper-level and lower-level iterates.\n","authors":["Riccardo Grazzi","Massimiliano Pontil","Saverio Salzo"],"pdf_url":"https://arxiv.org/pdf/2202.03397v4.pdf","comment":"Corrected Remark 18 + other small edits. Code at\n  https://github.com/CSML-IIT-UCL/bioptexps"},{"id":"http://arxiv.org/abs/2301.12618v3","updated":"2023-11-16T11:11:39Z","published":"2023-01-30T02:27:02Z","title":"ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning","summary":"  Auxiliary-Task Learning (ATL) aims to improve the performance of the target\ntask by leveraging the knowledge obtained from related tasks. Occasionally,\nlearning multiple tasks simultaneously results in lower accuracy than learning\nonly the target task, which is known as negative transfer. This problem is\noften attributed to the gradient conflicts among tasks, and is frequently\ntackled by coordinating the task gradients in previous works. However, these\noptimization-based methods largely overlook the auxiliary-target generalization\ncapability. To better understand the root cause of negative transfer, we\nexperimentally investigate it from both optimization and generalization\nperspectives. Based on our findings, we introduce ForkMerge, a novel approach\nthat periodically forks the model into multiple branches, automatically\nsearches the varying task weights by minimizing target validation errors, and\ndynamically merges all branches to filter out detrimental task-parameter\nupdates. On a series of auxiliary-task learning benchmarks, ForkMerge\noutperforms existing methods and effectively mitigates negative transfer.\n","authors":["Junguang Jiang","Baixu Chen","Junwei Pan","Ximei Wang","Liu Dapeng","Jie Jiang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2301.12618v3.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.09790v1","updated":"2023-11-16T11:10:38Z","published":"2023-11-16T11:10:38Z","title":"Breaking Boundaries: Balancing Performance and Robustness in Deep\n  Wireless Traffic Forecasting","summary":"  Balancing the trade-off between accuracy and robustness is a long-standing\nchallenge in time series forecasting. While most of existing robust algorithms\nhave achieved certain suboptimal performance on clean data, sustaining the same\nperformance level in the presence of data perturbations remains extremely hard.\n% In this paper, we study a wide array of perturbation scenarios and propose\nnovel defense mechanisms against adversarial attacks using real-world telecom\ndata. We compare our strategy against two existing adversarial training\nalgorithms under a range of maximal allowed perturbations, defined using\n$\\ell_{\\infty}$-norm, $\\in [0.1,0.4]$. % Our findings reveal that our hybrid\nstrategy, which is composed of a classifier to detect adversarial examples, a\ndenoiser to eliminate noise from the perturbed data samples, and a standard\nforecaster, achieves the best performance on both clean and perturbed data. %\nOur optimal model can retain up to $92.02\\%$ the performance of the original\nforecasting model in terms of Mean Squared Error (MSE) on clean data, while\nbeing more robust than the standard adversarially trained models on perturbed\ndata. Its MSE is 2.71$\\times$ and 2.51$\\times$ lower than those of comparing\nmethods on normal and perturbed data, respectively. In addition, the components\nof our models can be trained in parallel, resulting in better computational\nefficiency. % Our results indicate that we can optimally balance the trade-off\nbetween the performance and robustness of forecasting models by improving the\nclassifier and denoiser, even in the presence of sophisticated and destructive\npoisoning attacks.\n","authors":["Ilbert Romain","V. Hoang Thai","Zhang Zonghua","Palpanas Themis"],"pdf_url":"https://arxiv.org/pdf/2311.09790v1.pdf","comment":"12 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.00783v2","updated":"2023-11-16T11:08:52Z","published":"2023-03-01T19:10:05Z","title":"Adversarial Examples Exist in Two-Layer ReLU Networks for Low\n  Dimensional Linear Subspaces","summary":"  Despite a great deal of research, it is still not well-understood why trained\nneural networks are highly vulnerable to adversarial examples. In this work we\nfocus on two-layer neural networks trained using data which lie on a low\ndimensional linear subspace. We show that standard gradient methods lead to\nnon-robust neural networks, namely, networks which have large gradients in\ndirections orthogonal to the data subspace, and are susceptible to small\nadversarial $L_2$-perturbations in these directions. Moreover, we show that\ndecreasing the initialization scale of the training algorithm, or adding $L_2$\nregularization, can make the trained network more robust to adversarial\nperturbations orthogonal to the data.\n","authors":["Odelia Melamed","Gilad Yehudai","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2303.00783v2.pdf","comment":"Camera ready version for NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.01052v2","updated":"2023-11-16T11:04:53Z","published":"2023-11-02T07:54:03Z","title":"Resilient Multiple Choice Learning: A learned scoring scheme with\n  application to audio scene analysis","summary":"  We introduce Resilient Multiple Choice Learning (rMCL), an extension of the\nMCL approach for conditional distribution estimation in regression settings\nwhere multiple targets may be sampled for each training input. Multiple Choice\nLearning is a simple framework to tackle multimodal density estimation, using\nthe Winner-Takes-All (WTA) loss for a set of hypotheses. In regression\nsettings, the existing MCL variants focus on merging the hypotheses, thereby\neventually sacrificing the diversity of the predictions. In contrast, our\nmethod relies on a novel learned scoring scheme underpinned by a mathematical\nframework based on Voronoi tessellations of the output space, from which we can\nderive a probabilistic interpretation. After empirically validating rMCL with\nexperiments on synthetic data, we further assess its merits on the sound source\nlocalization problem, demonstrating its practical usefulness and the relevance\nof its interpretation.\n","authors":["Victor Letzelter","Mathieu Fontaine","Mickaël Chen","Patrick Pérez","Slim Essid","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2311.01052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09774v1","updated":"2023-11-16T10:56:24Z","published":"2023-11-16T10:56:24Z","title":"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs","summary":"  Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.\n","authors":["Junying Chen","Xidong Wang","Anningzhe Gao","Feng Jiang","Shunian Chen","Hongbo Zhang","Dingjie Song","Wenya Xie","Chuyi Kong","Jianquan Li","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19730v2","updated":"2023-11-16T10:41:03Z","published":"2023-05-31T10:49:16Z","title":"Data Representations' Study of Latent Image Manifolds","summary":"  Deep neural networks have been demonstrated to achieve phenomenal success in\nmany domains, and yet their inner mechanisms are not well understood. In this\npaper, we investigate the curvature of image manifolds, i.e., the manifold\ndeviation from being flat in its principal directions. We find that\nstate-of-the-art trained convolutional neural networks for image classification\nhave a characteristic curvature profile along layers: an initial steep\nincrease, followed by a long phase of a plateau, and followed by another\nincrease. In contrast, this behavior does not appear in untrained networks in\nwhich the curvature flattens. We also show that the curvature gap between the\nlast two layers has a strong correlation with the generalization capability of\nthe network. Moreover, we find that the intrinsic dimension of latent codes is\nnot necessarily indicative of curvature. Finally, we observe that common\nregularization methods such as mixup yield flatter representations when\ncompared to other methods. Our experiments show consistent results over a\nvariety of deep learning architectures and multiple data sets. Our code is\npublicly available at https://github.com/azencot-group/CRLM\n","authors":["Ilya Kaufman","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2305.19730v2.pdf","comment":"Accepted to ICML 2023"},{"id":"http://arxiv.org/abs/2311.09762v1","updated":"2023-11-16T10:36:08Z","published":"2023-11-16T10:36:08Z","title":"Graph-Guided Reasoning for Multi-Hop Question Answering in Large\n  Language Models","summary":"  Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning\ncapabilities of Large Language Models (LLMs) by generating a series of\nrationales before the final answer. We analyze the reasoning paths generated by\nCoT and find two issues in multi-step reasoning: (i) Generating rationales\nirrelevant to the question, (ii) Unable to compose subquestions or queries for\ngenerating/retrieving all the relevant information. To address them, we propose\na graph-guided CoT prompting method, which guides the LLMs to reach the correct\nanswer with graph representation/verification steps. Specifically, we first\nleverage LLMs to construct a \"question/rationale graph\" by using knowledge\nextraction prompting given the initial question and the rationales generated in\nthe previous steps. Then, the graph verification step diagnoses the current\nrationale triplet by comparing it with the existing question/rationale graph to\nfilter out irrelevant rationales and generate follow-up questions to obtain\nrelevant information. Additionally, we generate CoT paths that exclude the\nextracted graph information to represent the context information missed from\nthe graph extraction. Our graph-guided reasoning method shows superior\nperformance compared to previous CoT prompting and the variants on multi-hop\nquestion answering benchmark datasets.\n","authors":["Jinyoung Park","Ameen Patel","Omar Zia Khan","Hyunwoo J. Kim","Joo-Kyung Kim"],"pdf_url":"https://arxiv.org/pdf/2311.09762v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.09761v1","updated":"2023-11-16T10:35:11Z","published":"2023-11-16T10:35:11Z","title":"MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and\n  Classification","summary":"  Fallacies can be used to spread disinformation, fake news, and propaganda,\nunderlining the importance of their detection. Automated detection and\nclassification of fallacies, however, remain challenging, mainly because of the\ninnate subjectivity of the task and the need for a comprehensive, unified\napproach in existing research. Addressing these limitations, our study\nintroduces a novel taxonomy of fallacies that aligns and refines previous\nclassifications, a new annotation scheme tailored for subjective NLP tasks, and\na new evaluation method designed to handle subjectivity, adapted to precision,\nrecall, and F1-Score metrics. Using our annotation scheme, the paper introduces\nMAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset.\nMAFALDA is based on examples from various previously existing fallacy datasets\nunder our unified taxonomy across three levels of granularity. We then evaluate\nseveral language models under a zero-shot learning setting using MAFALDA to\nassess their fallacy detection and classification capability. Our comprehensive\nevaluation not only benchmarks the performance of these models but also\nprovides valuable insights into their strengths and limitations in addressing\nfallacious reasoning.\n","authors":["Chadi Helwe","Tom Calamai","Pierre-Henri Paris","Chloé Clavel","Fabian Suchanek"],"pdf_url":"https://arxiv.org/pdf/2311.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.10367v2","updated":"2023-11-16T10:26:17Z","published":"2022-02-21T17:04:05Z","title":"Probabilities of the third type: Statistical Relational Learning and\n  Reasoning with Relative Frequencies","summary":"  Dependencies on the relative frequency of a state in the domain are common\nwhen modelling probabilistic dependencies on relational data. For instance, the\nlikelihood of a school closure during an epidemic might depend on the\nproportion of infected pupils exceeding a threshold. Often, rather than\ndepending on discrete thresholds, dependencies are continuous: for instance,\nthe likelihood of any one mosquito bite transmitting an illness depends on the\nproportion of carrier mosquitoes. Current approaches usually only consider\nprobabilities over possible worlds rather than over domain elements themselves.\nAn exception are the recently introduced Lifted Bayesian Networks for\nConditional Probability Logic, which express discrete dependencies on\nprobabilistic data. We introduce functional lifted Bayesian networks, a\nformalism that explicitly incorporates continuous dependencies on relative\nfrequencies into statistical relational artificial intelligence. and compare\nand contrast them with ifted Bayesian Networks for Conditional Probability\nLogic. Incorporating relative frequencies is not only beneficial to modelling;\nit also provides a more rigorous approach to learning problems where training\nand test or application domains have different sizes. To this end, we provide a\nrepresentation of the asymptotic probability distributions induced by\nfunctional lifted Bayesian networks on domains of increasing sizes. Since that\nrepresentation has well-understood scaling behaviour across domain sizes, it\ncan be used to estimate parameters for a large domain consistently from\nrandomly sampled subpopulations. Furthermore, we show that in parametric\nfamilies of FLBN, convergence is uniform in the parameters, which ensures a\nmeaningful dependence of the asymptotic probabilities on the parameters of the\nmodel.\n","authors":["Felix Weitkämper"],"pdf_url":"https://arxiv.org/pdf/2202.10367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09801v2","updated":"2023-11-16T10:20:53Z","published":"2023-09-18T14:18:35Z","title":"Learning Optimal Contracts: How to Exploit Small Action Spaces","summary":"  We study principal-agent problems in which a principal commits to an\noutcome-dependent payment scheme -- called contract -- in order to induce an\nagent to take a costly, unobservable action leading to favorable outcomes. We\nconsider a generalization of the classical (single-round) version of the\nproblem in which the principal interacts with the agent by committing to\ncontracts over multiple rounds. The principal has no information about the\nagent, and they have to learn an optimal contract by only observing the outcome\nrealized at each round. We focus on settings in which the size of the agent's\naction space is small. We design an algorithm that learns an\napproximately-optimal contract with high probability in a number of rounds\npolynomial in the size of the outcome space, when the number of actions is\nconstant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover,\nit can also be employed to provide a $\\tilde{\\mathcal{O}}(T^{4/5})$ regret\nbound in the related online learning setting in which the principal aims at\nmaximizing their cumulative utility, thus considerably improving\npreviously-known regret bounds.\n","authors":["Francesco Bacchiocchi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti"],"pdf_url":"https://arxiv.org/pdf/2309.09801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09741v1","updated":"2023-11-16T10:14:28Z","published":"2023-11-16T10:14:28Z","title":"What Constitutes a Faithful Summary? Preserving Author Perspectives in\n  News Summarization","summary":"  In this work, we take a first step towards designing summarization systems\nthat are faithful to the author's opinions and perspectives. Focusing on a case\nstudy of preserving political perspectives in news summarization, we find that\nexisting approaches alter the political opinions and stances of news articles\nin more than 50% of summaries, misrepresenting the intent and perspectives of\nthe news authors. We thus propose P^3Sum, a diffusion model-based summarization\napproach controlled by political perspective classifiers. In P^3Sum, the\npolitical leaning of a generated summary is iteratively evaluated at each\ndecoding step, and any drift from the article's original stance incurs a loss\nback-propagated to the embedding layers, steering the political stance of the\nsummary at inference time. Extensive experiments on three news summarization\ndatasets demonstrate that P^3Sum outperforms state-of-the-art summarization\nsystems and large language models by up to 11.4% in terms of the success rate\nof stance preservation, with on-par performance on standard summarization\nutility metrics. These findings highlight the lacunae that even for\nstate-of-the-art models it is still challenging to preserve author perspectives\nin news summarization, while P^3Sum presents an important first step towards\nevaluating and developing summarization systems that are faithful to author\nintent and perspectives.\n","authors":["Yuhan Liu","Shangbin Feng","Xiaochuang Han","Vidhisha Balachandran","Chan Young Park","Sachin Kumar","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2311.09741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09740v1","updated":"2023-11-16T10:13:09Z","published":"2023-11-16T10:13:09Z","title":"Redefining Super-Resolution: Fine-mesh PDE predictions without classical\n  simulations","summary":"  In Computational Fluid Dynamics (CFD), coarse mesh simulations offer\ncomputational efficiency but often lack precision. Applying conventional\nsuper-resolution to these simulations poses a significant challenge due to the\nfundamental contrast between downsampling high-resolution images and\nauthentically emulating low-resolution physics. The former method conserves\nmore of the underlying physics, surpassing the usual constraints of real-world\nscenarios. We propose a novel definition of super-resolution tailored for\nPDE-based problems. Instead of simply downsampling from a high-resolution\ndataset, we use coarse-grid simulated data as our input and predict fine-grid\nsimulated outcomes. Employing a physics-infused UNet upscaling method, we\ndemonstrate its efficacy across various 2D-CFD problems such as discontinuity\ndetection in Burger's equation, Methane combustion, and fouling in Industrial\nheat exchangers. Our method enables the generation of fine-mesh solutions\nbypassing traditional simulation, ensuring considerable computational saving\nand fidelity to the original ground truth outcomes. Through diverse boundary\nconditions during training, we further establish the robustness of our method,\npaving the way for its broad applications in engineering and scientific CFD\nsolvers.\n","authors":["Rajat Kumar Sarkar","Ritam Majumdar","Vishal Jadhav","Sagar Srinivas Sakhinana","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2311.09740v1.pdf","comment":"Accepted at Machine Learning and the Physical Sciences Workshop,\n  NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.09735v1","updated":"2023-11-16T10:06:09Z","published":"2023-11-16T10:06:09Z","title":"GEO: Generative Engine Optimization","summary":"  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of Generative Engines (GEs), has the potential to generate\naccurate and personalized responses, and is rapidly replacing traditional\nsearch engines like Google and Bing. Generative Engines typically satisfy\nqueries by synthesizing information from multiple sources and summarizing them\nwith the help of LLMs. While this shift significantly improves \\textit{user}\nutility and \\textit{generative search engine} traffic, it results in a huge\nchallenge for the third stakeholder -- website and content creators. Given the\nblack-box and fast-moving nature of Generative Engines, content creators have\nlittle to no control over when and how their content is displayed. With\ngenerative engines here to stay, the right tools should be provided to ensure\nthat creator economy is not severely disadvantaged. To address this, we\nintroduce Generative Engine Optimization (GEO), a novel paradigm to aid content\ncreators in improving the visibility of their content in Generative Engine\nresponses through a black-box optimization framework for optimizing and\ndefining visibility metrics. We facilitate systematic evaluation in this new\nparadigm by introducing GEO-bench, a benchmark of diverse user queries across\nmultiple domains, coupled with sources required to answer these queries.\nThrough rigorous evaluation, we show that GEO can boost visibility by up to\n40\\% in generative engine responses. Moreover, we show the efficacy of these\nstrategies varies across domains, underscoring the need for domain-specific\nmethods. Our work opens a new frontier in the field of information discovery\nsystems, with profound implications for generative engines and content\ncreators.\n","authors":["Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik R Narasimhan","Ameet Deshpande"],"pdf_url":"https://arxiv.org/pdf/2311.09735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09731v1","updated":"2023-11-16T10:02:40Z","published":"2023-11-16T10:02:40Z","title":"Prudent Silence or Foolish Babble? Examining Large Language Models'\n  Responses to the Unknown","summary":"  Large Language Models (LLMs) often struggle when faced with situations where\nthey lack the prerequisite knowledge to generate a sensical response. In these\ncases, models tend to fabricate and hallucinate, rather than appropriately\nsignaling uncertainty as humans would. This behavior misaligns with human\nconversational norms and presents challenges surrounding responsible and\nethical AI development. This work aims to systematically investigate LLMs'\nbehaviors in such situations. We curate an adversarial question-answering\nbenchmark containing unanswerable questions targeting information absent from\nthe LLM's training data. Concretely, these unanswerable questions contain\nnon-existent concepts or false premises. When presented with such unanswerable\nquestions, an LLM should appropriately convey uncertainty, and be able to\nchallenge the premise and refuse to generate a response. While facing\nanswerable valid questions, a model should demonstrate a positive correlation\nbetween accuracy and confidence. Using a model-agnostic unified confidence\nelicitation approach, we observe that LLMs that have gone through instruction\nfinetuning and reinforcement learning from human feedback (RLHF) perform\nsignificantly better than their counterparts that do not. Moreover, uncertainty\nexpression 1 through our elicitation method does not always stay consistent\nwith the perceived confidence of the direct response of an LLM. Our findings\ncall for further research into teaching LLMs to proactively and reliably\nexpress uncertainty.\n","authors":["Genglin Liu","Xingyao Wang","Lifan Yuan","Yangyi Chen","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2311.09731v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2311.09730v1","updated":"2023-11-16T10:02:24Z","published":"2023-11-16T10:02:24Z","title":"Aligning with Whom? Large Language Models Have Gender and Racial Biases\n  in Subjective NLP Tasks","summary":"  Human perception of language depends on personal backgrounds like gender and\nethnicity. While existing studies have shown that large language models (LLMs)\nhold values that are closer to certain societal groups, it is unclear whether\ntheir prediction behaviors on subjective NLP tasks also exhibit a similar bias.\nIn this study, leveraging the POPQUORN dataset which contains annotations of\ndiverse demographic backgrounds, we conduct a series of experiments on four\npopular LLMs to investigate their capability to understand group differences\nand potential biases in their predictions for politeness and offensiveness. We\nfind that for both tasks, model predictions are closer to the labels from White\nand female participants. We further explore prompting with the target\ndemographic labels and show that including the target demographic in the prompt\nactually worsens the model's performance. More specifically, when being\nprompted to respond from the perspective of \"Black\" and \"Asian\" individuals,\nmodels show lower performance in predicting both overall scores as well as the\nscores from corresponding groups. Our results suggest that LLMs hold gender and\nracial biases for subjective NLP tasks and that demographic-infused prompts\nalone may be insufficient to mitigate such effects. Code and data are available\nat https://github.com/Jiaxin-Pei/LLM-Group-Bias.\n","authors":["Huaman Sun","Jiaxin Pei","Minje Choi","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.09730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12683v2","updated":"2023-11-16T09:43:51Z","published":"2022-05-25T11:44:13Z","title":"Rethinking Fano's Inequality in Ensemble Learning","summary":"  We propose a fundamental theory on ensemble learning that answers the central\nquestion: what factors make an ensemble system good or bad? Previous studies\nused a variant of Fano's inequality of information theory and derived a lower\nbound of the classification error rate on the basis of the $\\textit{accuracy}$\nand $\\textit{diversity}$ of models. We revisit the original Fano's inequality\nand argue that the studies did not take into account the information lost when\nmultiple model predictions are combined into a final prediction. To address\nthis issue, we generalize the previous theory to incorporate the information\nloss, which we name $\\textit{combination loss}$. Further, we empirically\nvalidate and demonstrate the proposed theory through extensive experiments on\nactual systems. The theory reveals the strengths and weaknesses of systems on\neach metric, which will push the theoretical understanding of ensemble learning\nand give us insights into designing systems.\n","authors":["Terufumi Morishita","Gaku Morio","Shota Horiguchi","Hiroaki Ozaki","Nobuo Nukaga"],"pdf_url":"https://arxiv.org/pdf/2205.12683v2.pdf","comment":"ICML2022"},{"id":"http://arxiv.org/abs/2311.09706v1","updated":"2023-11-16T09:34:23Z","published":"2023-11-16T09:34:23Z","title":"Towards Autonomous Hypothesis Verification via Language Models with\n  Minimal Guidance","summary":"  Research automation efforts usually employ AI as a tool to automate specific\ntasks within the research process. To create an AI that truly conduct research\nthemselves, it must independently generate hypotheses, design verification\nplans, and execute verification. Therefore, we investigated if an AI itself\ncould autonomously generate and verify hypothesis for a toy machine learning\nresearch problem. We prompted GPT-4 to generate hypotheses and Python code for\nhypothesis verification with limited methodological guidance. Our findings\nsuggest that, in some instances, GPT-4 can autonomously generate and validate\nhypotheses without detailed guidance. While this is a promising result, we also\nfound that none of the verifications were flawless, and there remain\nsignificant challenges in achieving autonomous, human-level research using only\ngeneric instructions. These findings underscore the need for continued\nexploration to develop a general and autonomous AI researcher.\n","authors":["Shiro Takagi","Ryutaro Yamauchi","Wataru Kumagai"],"pdf_url":"https://arxiv.org/pdf/2311.09706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12620v2","updated":"2023-11-16T09:21:20Z","published":"2023-09-22T05:08:52Z","title":"Data-driven Preference Learning Methods for Sorting Problems with\n  Multiple Temporal Criteria","summary":"  The advent of predictive methodologies has catalyzed the emergence of\ndata-driven decision support across various domains. However, developing models\ncapable of effectively handling input time series data presents an enduring\nchallenge. This study presents novel preference learning approaches to multiple\ncriteria sorting problems in the presence of temporal criteria. We first\nformulate a convex quadratic programming model characterized by fixed time\ndiscount factors, operating within a regularization framework. To enhance\nscalability and accommodate learnable time discount factors, we introduce a\nnovel monotonic Recurrent Neural Network (mRNN). It is designed to capture the\nevolving dynamics of preferences over time while upholding critical properties\ninherent to MCS problems, including criteria monotonicity, preference\nindependence, and the natural ordering of classes. The proposed mRNN can\ndescribe the preference dynamics by depicting marginal value functions and\npersonalized time discount factors along with time, effectively amalgamating\nthe interpretability of traditional MCS methods with the predictive potential\noffered by deep preference learning models. Comprehensive assessments of the\nproposed models are conducted, encompassing synthetic data scenarios and a\nreal-case study centered on classifying valuable users within a mobile gaming\napp based on their historical in-app behavioral sequences. Empirical findings\nunderscore the notable performance improvements achieved by the proposed models\nwhen compared to a spectrum of baseline methods, spanning machine learning,\ndeep learning, and conventional multiple criteria sorting approaches.\n","authors":["Yijun Li","Mengzhuo Guo","Miłosz Kadziński","Qingpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.12620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09692v1","updated":"2023-11-16T09:07:34Z","published":"2023-11-16T09:07:34Z","title":"Augmenting Unsupervised Reinforcement Learning with Self-Reference","summary":"  Humans possess the ability to draw on past experiences explicitly when\nlearning new tasks and applying them accordingly. We believe this capacity for\nself-referencing is especially advantageous for reinforcement learning agents\nin the unsupervised pretrain-then-finetune setting. During pretraining, an\nagent's past experiences can be explicitly utilized to mitigate the\nnonstationarity of intrinsic rewards. In the finetuning phase, referencing\nhistorical trajectories prevents the unlearning of valuable exploratory\nbehaviors. Motivated by these benefits, we propose the Self-Reference (SR)\napproach, an add-on module explicitly designed to leverage historical\ninformation and enhance agent performance within the pretrain-finetune\nparadigm. Our approach achieves state-of-the-art results in terms of\nInterquartile Mean (IQM) performance and Optimality Gap reduction on the\nUnsupervised Reinforcement Learning Benchmark for model-free methods, recording\nan 86% IQM and a 16% Optimality Gap. Additionally, it improves current\nalgorithms by up to 17% IQM and reduces the Optimality Gap by 31%. Beyond\nperformance enhancement, the Self-Reference add-on also increases sample\nefficiency, a crucial attribute for real-world applications.\n","authors":["Andrew Zhao","Erle Zhu","Rui Lu","Matthieu Lin","Yong-Jin Liu","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2311.09692v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.08390v2","updated":"2023-11-16T09:06:20Z","published":"2023-07-17T11:04:27Z","title":"Correlation-aware Spatial-Temporal Graph Learning for Multivariate\n  Time-series Anomaly Detection","summary":"  Multivariate time-series anomaly detection is critically important in many\napplications, including retail, transportation, power grid, and water treatment\nplants. Existing approaches for this problem mostly employ either statistical\nmodels which cannot capture the non-linear relations well or conventional deep\nlearning models (e.g., CNN and LSTM) that do not explicitly learn the pairwise\ncorrelations among variables. To overcome these limitations, we propose a novel\nmethod, correlation-aware spatial-temporal graph learning (termed CST-GL), for\ntime series anomaly detection. CST-GL explicitly captures the pairwise\ncorrelations via a multivariate time series correlation learning module based\non which a spatial-temporal graph neural network (STGNN) can be developed.\nThen, by employing a graph convolution network that exploits one- and multi-hop\nneighbor information, our STGNN component can encode rich spatial information\nfrom complex pairwise dependencies between variables. With a temporal module\nthat consists of dilated convolutional functions, the STGNN can further capture\nlong-range dependence over time. A novel anomaly scoring component is further\nintegrated into CST-GL to estimate the degree of an anomaly in a purely\nunsupervised manner. Experimental results demonstrate that CST-GL can detect\nanomalies effectively in general settings as well as enable early detection\nacross different time delays.\n","authors":["Yu Zheng","Huan Yee Koh","Ming Jin","Lianhua Chi","Khoa T. Phan","Shirui Pan","Yi-Ping Phoebe Chen","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2307.08390v2.pdf","comment":"17 pages, double columns, 10 tables, 3 figures. Accepted to IEEE\n  Transactions on Neural Networks and Learning Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2311.09690v1","updated":"2023-11-16T09:05:52Z","published":"2023-11-16T09:05:52Z","title":"CDMPP: A Device-Model Agnostic Framework for Latency Prediction of\n  Tensor Programs","summary":"  Deep Neural Networks (DNNs) have shown excellent performance in a wide range\nof machine learning applications. Knowing the latency of running a DNN model or\ntensor program on a specific device is useful in various tasks, such as DNN\ngraph- or tensor-level optimization and device selection. Considering the large\nspace of DNN models and devices that impede direct profiling of all\ncombinations, recent efforts focus on building a predictor to model the\nperformance of DNN models on different devices. However, none of the existing\nattempts have achieved a cost model that can accurately predict the performance\nof various tensor programs while supporting both training and inference\naccelerators. We propose CDMPP, an efficient tensor program latency prediction\nframework for both cross-model and cross-device prediction. We design an\ninformative but efficient representation of tensor programs, called compact\nASTs, and a pre-order-based positional encoding method, to capture the internal\nstructure of tensor programs. We develop a domain-adaption-inspired method to\nlearn domain-invariant representations and devise a KMeans-based sampling\nalgorithm, for the predictor to learn from different domains (i.e., different\nDNN operators and devices). Our extensive experiments on a diverse range of DNN\nmodels and devices demonstrate that CDMPP significantly outperforms\nstate-of-the-art baselines with 14.03% and 10.85% prediction error for\ncross-model and cross-device prediction, respectively, and one order of\nmagnitude higher training efficiency. The implementation and the expanded\ndataset are available at https://github.com/joapolarbear/cdmpp.\n","authors":["Hanpeng Hu","Junwei Su","Juntao Zhao","Yanghua Peng","Yibo Zhu","Haibin Lin","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2311.09690v1.pdf","comment":"Accepted by EuroSys 2024"},{"id":"http://arxiv.org/abs/2311.09683v1","updated":"2023-11-16T08:52:31Z","published":"2023-11-16T08:52:31Z","title":"Modelling daily mobility using mobile data traffic at fine\n  spatiotemporal scale","summary":"  We applied a data-driven approach that explores the usability of the NetMob\n2023 dataset in modelling mobility patterns within an urban context. We\ncombined the data with a highly suitable external source, the ENACT dataset,\nwhich provides a 1 km x 1km grid with estimates of the day and night population\nacross Europe. We developed three sets of XGBoost models that predict the\npopulation in each 100m x 100m grid cell used in NetMob2023 based on the mobile\ndata traffic of the 68 online services covered in the dataset, using the ENACT\nvalues as ground truth. The results suggest that the NetMob 2023 data can be\nuseful for the estimation of the day and night population and grid cell level\nand can explain part of the dynamics of urban mobility.\n","authors":["Panayotis Christidis","Maria Vega Gonzalo","Miklos Radics"],"pdf_url":"https://arxiv.org/pdf/2311.09683v1.pdf","comment":"NetMob 2023 Conference"},{"id":"http://arxiv.org/abs/2311.09671v1","updated":"2023-11-16T08:39:58Z","published":"2023-11-16T08:39:58Z","title":"Robust Contrastive Learning With Theory Guarantee","summary":"  Contrastive learning (CL) is a self-supervised training paradigm that allows\nus to extract meaningful features without any label information. A typical CL\nframework is divided into two phases, where it first tries to learn the\nfeatures from unlabelled data, and then uses those features to train a linear\nclassifier with the labeled data. While a fair amount of existing theoretical\nworks have analyzed how the unsupervised loss in the first phase can support\nthe supervised loss in the second phase, none has examined the connection\nbetween the unsupervised loss and the robust supervised loss, which can shed\nlight on how to construct an effective unsupervised loss for the first phase of\nCL. To fill this gap, our work develops rigorous theories to dissect and\nidentify which components in the unsupervised loss can help improve the robust\nsupervised loss and conduct proper experiments to verify our findings.\n","authors":["Ngoc N. Tran","Lam Tran","Hoang Phan","Anh Bui","Tung Pham","Toan Tran","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2311.09671v1.pdf","comment":"27 pages, 0 figures. arXiv admin note: text overlap with\n  arXiv:2305.10252"},{"id":"http://arxiv.org/abs/2301.10365v2","updated":"2023-11-16T08:39:51Z","published":"2023-01-25T00:21:31Z","title":"Data Consistent Deep Rigid MRI Motion Correction","summary":"  Motion artifacts are a pervasive problem in MRI, leading to misdiagnosis or\nmischaracterization in population-level imaging studies. Current retrospective\nrigid intra-slice motion correction techniques jointly optimize estimates of\nthe image and the motion parameters. In this paper, we use a deep network to\nreduce the joint image-motion parameter search to a search over rigid motion\nparameters alone. Our network produces a reconstruction as a function of two\ninputs: corrupted k-space data and motion parameters. We train the network\nusing simulated, motion-corrupted k-space data generated with known motion\nparameters. At test-time, we estimate unknown motion parameters by minimizing a\ndata consistency loss between the motion parameters, the network-based image\nreconstruction given those parameters, and the acquired measurements.\nIntra-slice motion correction experiments on simulated and realistic 2D fast\nspin echo brain MRI achieve high reconstruction fidelity while providing the\nbenefits of explicit data consistency optimization. Our code is publicly\navailable at https://www.github.com/nalinimsingh/neuroMoCo.\n","authors":["Nalini M. Singh","Neel Dey","Malte Hoffmann","Bruce Fischl","Elfar Adalsteinsson","Robert Frost","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2301.10365v2.pdf","comment":"Presented at MIDL 2023. 14 pages, 6 figures. Keywords: motion\n  correction, magnetic resonance imaging, deep learning"},{"id":"http://arxiv.org/abs/2311.09668v1","updated":"2023-11-16T08:36:00Z","published":"2023-11-16T08:36:00Z","title":"Improving the Generation Quality of Watermarked Large Language Models\n  via Word Importance Scoring","summary":"  The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.\n","authors":["Yuhang Li","Yihan Wang","Zhouxing Shi","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2311.09668v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.07468v2","updated":"2023-11-16T08:35:05Z","published":"2023-11-13T17:01:12Z","title":"Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation\n  of the Reversal Curse","summary":"  Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B as the answer. However, it may\nencounter confusion when presented with questions concerning B. We contend that\nthe reversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.\n","authors":["Ang Lv","Kaiyi Zhang","Shufang Xie","Quan Tu","Yuhan Chen","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2311.07468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09663v1","updated":"2023-11-16T08:29:26Z","published":"2023-11-16T08:29:26Z","title":"Zenkai -- Framework For Exploring Beyond Backpropagation","summary":"  Zenkai is an open-source framework designed to give researchers more control\nand flexibility over building and training deep learning machines. It does this\nby dividing the deep learning machine into layers of semi-autonomous learning\nmachines with their own target and learning algorithm. This is to allow\nresearchers greater exploration such as the use of non-differentiable layers or\nlearning algorithms beyond those based on error backpropagation.\n  Backpropagation Rumelhart et al. [1986] has powered deep learning to become\none of the most exciting fields of the 21st century. As a result, a large\nnumber of software tools have been developed to support efficient\nimplementation and training of neural networks through the use of backpropa-\ngation. While these have been critical to the success of deep learning,\nbuilding frameworks around backpropagation can make it challenging to implement\nsolutions that do not adhere to it. Zenkai aims to make it easier to get around\nthese limitations and help researchers more easily explore new frontiers in\ndeep learning that do not strictly adhere to the backpropagation framework.\n","authors":["Greg Short"],"pdf_url":"https://arxiv.org/pdf/2311.09663v1.pdf","comment":"14 pages. Source code available at\n  https://www.github.com/short-greg/zenkai with documentation at\n  https://zenkai.readthedocs.io/en/latest/. Can be installed with pip install\n  zenkai"},{"id":"http://arxiv.org/abs/2305.18787v2","updated":"2023-11-16T08:26:59Z","published":"2023-05-30T06:47:07Z","title":"Universality and Limitations of Prompt Tuning","summary":"  Despite the demonstrated empirical efficacy of prompt tuning to adapt a\npretrained language model for a new task, the theoretical underpinnings of the\ndifference between \"tuning parameters before the input\" against \"the tuning of\nmodel weights\" are limited. We thus take one of the first steps to understand\nthe role of soft-prompt tuning for transformer-based architectures. By\nconsidering a general purpose architecture, we analyze prompt tuning from the\nlens of both: universal approximation and limitations with finite-depth\nfixed-weight pretrained transformers for continuous-valued functions. Our\nuniversality result guarantees the existence of a strong transformer with a\nprompt to approximate any sequence-to-sequence function in the set of Lipschitz\nfunctions. The limitations of prompt tuning for limited-depth transformers are\nfirst proved by constructing a set of datasets, that cannot be memorized by a\nprompt of any length for a given single encoder layer. We also provide a lower\nbound on the required number of tunable prompt parameters and compare the\nresult with the number of parameters required for a low-rank update (based on\nLoRA) for a single-layer setting. We finally extend our analysis to multi-layer\nsettings by providing sufficient conditions under which the transformer can at\nbest learn datasets from invertible functions only. Our theoretical claims are\nalso corroborated by empirical results.\n","authors":["Yihan Wang","Jatin Chauhan","Wei Wang","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2305.18787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15798v3","updated":"2023-11-16T08:13:06Z","published":"2023-05-25T07:28:28Z","title":"BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion","summary":"  Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. We hope this work can help build small yet powerful diffusion models\nwith feasible training budgets. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM\n","authors":["Bo-Kyeong Kim","Hyoung-Kyu Song","Thibault Castells","Shinkook Choi"],"pdf_url":"https://arxiv.org/pdf/2305.15798v3.pdf","comment":"Updated results; Preliminary version at ICML Workshop on ES-FoMo\n  (2023): https://openreview.net/forum?id=bOVydU0XKC"},{"id":"http://arxiv.org/abs/2311.09649v1","updated":"2023-11-16T08:01:17Z","published":"2023-11-16T08:01:17Z","title":"ICXML: An In-Context Learning Framework for Zero-Shot Extreme\n  Multi-Label Classification","summary":"  This paper focuses on the task of Extreme Multi-Label Classification (XMC)\nwhose goal is to predict multiple labels for each instance from an extremely\nlarge label space. While existing research has primarily focused on fully\nsupervised XMC, real-world scenarios often lack complete supervision signals,\nhighlighting the importance of zero-shot settings. Given the large label space,\nutilizing in-context learning approaches is not trivial. We address this issue\nby introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage\nframework that cuts down the search space by generating a set of candidate\nlabels through incontext learning and then reranks them. Extensive experiments\nsuggest that ICXML advances the state of the art on two diverse public\nbenchmarks.\n","authors":["Yaxin Zhu","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2311.09649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01125v3","updated":"2023-11-16T07:48:30Z","published":"2022-10-03T03:07:33Z","title":"Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep\n  Reconstruction without Reference","summary":"  Spectral computed tomography based on a photon-counting detector (PCD)\nattracts more and more attentions since it has the capability to provide more\naccurate identification and quantitative analysis for biomedical materials. The\nlimited number of photons within narrow energy bins leads to imaging results of\nlow signal-noise ratio. The existing supervised deep reconstruction networks\nfor CT reconstruction are difficult to address these challenges because it is\nusually impossible to acquire noise-free clinical images with clear structures\nas references. In this paper, we propose an iterative deep reconstruction\nnetwork to synergize unsupervised method and data priors into a unified\nframework, named as Spectral2Spectral. Our Spectral2Spectral employs an\nunsupervised deep training strategy to obtain high-quality images from noisy\ndata in an end-to-end fashion. The structural similarity prior within\nimage-spectral domain is refined as a regularization term to further constrain\nthe network training. The weights of neural network are automatically updated\nto capture image features and structures within the iterative process. Three\nlarge-scale preclinical datasets experiments demonstrate that the\nSpectral2spectral reconstructs better image quality than other the\nstate-of-the-art methods.\n","authors":["Xiaodong Guo","Longhui Li","Dingyue Chang","Peng He","Peng Feng","Hengyong Yu","Weiwen Wu"],"pdf_url":"https://arxiv.org/pdf/2210.01125v3.pdf","comment":"Accepted by IEEE TCI"},{"id":"http://arxiv.org/abs/2311.07141v2","updated":"2023-11-16T07:23:17Z","published":"2023-11-13T08:13:55Z","title":"SABAF: Removing Strong Attribute Bias from Neural Networks with\n  Adversarial Filtering","summary":"  Ensuring a neural network is not relying on protected attributes (e.g., race,\nsex, age) for prediction is crucial in advancing fair and trustworthy AI. While\nseveral promising methods for removing attribute bias in neural networks have\nbeen proposed, their limitations remain under-explored. To that end, in this\nwork, we mathematically and empirically reveal the limitation of existing\nattribute bias removal methods in presence of strong bias and propose a new\nmethod that can mitigate this limitation. Specifically, we first derive a\ngeneral non-vacuous information-theoretical upper bound on the performance of\nany attribute bias removal method in terms of the bias strength, revealing that\nthey are effective only when the inherent bias in the dataset is relatively\nweak. Next, we derive a necessary condition for the existence of any method\nthat can remove attribute bias regardless of the bias strength. Inspired by\nthis condition, we then propose a new method using an adversarial objective\nthat directly filters out protected attributes in the input space while\nmaximally preserving all other attributes, without requiring any specific\ntarget label. The proposed method achieves state-of-the-art performance in both\nstrong and moderate bias settings. We provide extensive experiments on\nsynthetic, image, and census datasets, to verify the derived theoretical bound\nand its consequences in practice, and evaluate the effectiveness of the\nproposed method in removing strong attribute bias.\n","authors":["Jiazhi Li","Mahyar Khayatkhoei","Jiageng Zhu","Hanchen Xie","Mohamed E. Hussein","Wael AbdAlmageed"],"pdf_url":"https://arxiv.org/pdf/2311.07141v2.pdf","comment":"35 pages, 18 figures, 32 tables. This work is an extended version of\n  our paper (arXiv:2310.04955). Code will be released at\n  https://github.com/jiazhi412/strong_attribute_bias"},{"id":"http://arxiv.org/abs/2309.10923v2","updated":"2023-11-16T07:23:03Z","published":"2023-09-19T20:53:13Z","title":"Semi-automatic staging area for high-quality structured data extraction\n  from scientific literature","summary":"  We propose a semi-automatic staging area for efficiently building an accurate\ndatabase of experimental physical properties of superconductors from\nliterature, called SuperCon2, to enrich the existing manually-built\nsuperconductor database SuperCon. Here we report our curation interface\n(SuperCon2 Interface) and a workflow managing the state transitions of each\nexamined record, to validate the dataset of superconductors from PDF documents\ncollected using Grobid-superconductors in a previous work. This curation\nworkflow allows both automatic and manual operations, the former contains\n``anomaly detection'' that scans new data identifying outliers, and a\n``training data collector'' mechanism that collects training data examples\nbased on manual corrections. Such training data collection policy is effective\nin improving the machine-learning models with a reduced number of examples. For\nmanual operations, the interface (SuperCon2 interface) is developed to increase\nefficiency during manual correction by providing a smart interface and an\nenhanced PDF document viewer. We show that our interface significantly improves\nthe curation quality by boosting precision and recall as compared with the\ntraditional ``manual correction''. Our semi-automatic approach would provide a\nsolution for achieving a reliable database with text-data mining of scientific\ndocuments.\n","authors":["Luca Foppiano","Tomoya Mato","Kensei Terashima","Pedro Ortiz Suarez","Taku Tou","Chikako Sakai","Wei-Sheng Wang","Toshiyuki Amagasa","Yoshihiko Takano","Masashi Ishii"],"pdf_url":"https://arxiv.org/pdf/2309.10923v2.pdf","comment":"5 tables, 6 figures, 18 pages"},{"id":"http://arxiv.org/abs/2311.09627v1","updated":"2023-11-16T07:16:55Z","published":"2023-11-16T07:16:55Z","title":"CRISPR: Eliminating Bias Neurons from an Instruction-following Language\n  Model","summary":"  Large language models (LLMs) executing tasks through instruction-based\nprompts often face challenges stemming from distribution differences between\nuser instructions and training instructions. This leads to distractions and\nbiases, especially when dealing with inconsistent dynamic labels. In this\npaper, we introduces a novel bias mitigation method, CRISPR, designed to\nalleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods\nto identify bias neurons influencing biased outputs and employs pruning to\neliminate the bias neurons. Experimental results demonstrate the method's\neffectiveness in mitigating biases in instruction-based prompting, enhancing\nlanguage model performance on social bias benchmarks without compromising\npre-existing knowledge. CRISPR proves highly practical, model-agnostic,\noffering flexibility in adapting to evolving social biases.\n","authors":["Nakyeong Yang","Taegwan Kang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2311.09627v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2311.09620v1","updated":"2023-11-16T07:05:12Z","published":"2023-11-16T07:05:12Z","title":"GAIA: Delving into Gradient-based Attribution Abnormality for\n  Out-of-distribution Detection","summary":"  Detecting out-of-distribution (OOD) examples is crucial to guarantee the\nreliability and safety of deep neural networks in real-world settings. In this\npaper, we offer an innovative perspective on quantifying the disparities\nbetween in-distribution (ID) and OOD data -- analyzing the uncertainty that\narises when models attempt to explain their predictive decisions. This\nperspective is motivated by our observation that gradient-based attribution\nmethods encounter challenges in assigning feature importance to OOD data,\nthereby yielding divergent explanation patterns. Consequently, we investigate\nhow attribution gradients lead to uncertain explanation outcomes and introduce\ntwo forms of abnormalities for OOD detection: the zero-deflation abnormality\nand the channel-wise average abnormality. We then propose GAIA, a simple and\neffective approach that incorporates Gradient Abnormality Inspection and\nAggregation. The effectiveness of GAIA is validated on both commonly utilized\n(CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces\nthe average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to\nadvanced post-hoc methods.\n","authors":["Jinggang Chen","Junjie Li","Xiaoyang Qu","Jianzong Wang","Jiguang Wan","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2311.09620v1.pdf","comment":"Accepted by NeurIPS2023"},{"id":"http://arxiv.org/abs/2311.09614v1","updated":"2023-11-16T06:58:46Z","published":"2023-11-16T06:58:46Z","title":"Comprehensive Evaluation and Insights into the Use of Deep Neural\n  Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images","summary":"  This study performs comprehensive evaluation of four neural network\narchitectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion\nsegmentation from PET/CT images. These networks were trained, validated, and\ntested on a diverse, multi-institutional dataset of 611 cases. Internal testing\n(88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed\nSegResNet as the top performer with a median Dice similarity coefficient (DSC)\nof 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a\nmedian false negative volume (FNV) of 0 ml. On the unseen external test set\n(145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best\nmedian DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.\nWe assessed reproducibility of six lesion measures, calculated their prediction\nerrors, and examined DSC performance in relation to these lesion measures,\noffering insights into segmentation accuracy and clinical relevance.\nAdditionally, we introduced three lesion detection criteria, addressing the\nclinical need for identifying lesions, counting them, and segmenting based on\nmetabolic characteristics. We also performed expert intra-observer variability\nanalysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to\nassist in the development of more resilient segmentation algorithms. Finally,\nwe performed inter-observer agreement assessment underscoring the importance of\na standardized ground truth segmentation protocol involving multiple expert\nannotators. Code is available at:\nhttps://github.com/microsoft/lymphoma-segmentation-dnn\n","authors":["Shadab Ahamed","Yixi Xu","Claire Gowdy","Joo H. O","Ingrid Bloise","Don Wilson","Patrick Martineau","François Bénard","Fereshteh Yousefirizi","Rahul Dodhia","Juan M. Lavista","William B. Weeks","Carlos F. Uribe","Arman Rahmim"],"pdf_url":"https://arxiv.org/pdf/2311.09614v1.pdf","comment":"12 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.05116v2","updated":"2023-11-16T06:20:26Z","published":"2023-11-09T03:06:59Z","title":"Covering Number of Real Algebraic Varieties and Beyond: Improved Bounds\n  and Applications","summary":"  We prove an upper bound on the covering number of real algebraic varieties,\nimages of polynomial maps and semialgebraic sets. The bound remarkably improves\nthe best known general bound by Yomdin-Comte, and its proof is much more\nstraightforward. As a consequence, our result gives new bounds on the volume of\nthe tubular neighborhood of the image of a polynomial map and a semialgebraic\nset, where results for varieties by Lotz and Basu-Lerario are not directly\napplicable. We apply our theory to three main application domains. Firstly, we\nderive a near-optimal bound on the covering number of low rank CP tensors.\nSecondly, we prove a bound on the sketching dimension for (general) polynomial\noptimization problems. Lastly, we deduce generalization error bounds for deep\nneural networks with rational or ReLU activations, improving or matching the\nbest known results in the literature.\n","authors":["Yifan Zhang","Joe Kileel"],"pdf_url":"https://arxiv.org/pdf/2311.05116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09596v1","updated":"2023-11-16T06:09:14Z","published":"2023-11-16T06:09:14Z","title":"Generating Drug Repurposing Hypotheses through the Combination of\n  Disease-Specific Hypergraphs","summary":"  The drug development pipeline for a new compound can last 10-20 years and\ncost over 10 billion. Drug repurposing offers a more time- and cost-effective\nalternative. Computational approaches based on biomedical knowledge graph\nrepresentations have recently yielded new drug repurposing hypotheses. In this\nstudy, we present a novel, disease-specific hypergraph representation learning\ntechnique to derive contextual embeddings of biological pathways of various\nlengths but that all start at any given drug and all end at the disease of\ninterest. Further, we extend this method to multi-disease hypergraphs. To\ndetermine the repurposing potential of each of the 1,522 drugs, we derive\ndrug-specific distributions of cosine similarity values and ultimately consider\nthe median for ranking. Cosine similarity values are computed between (1) all\nbiological pathways starting at the considered drug and ending at the disease\nof interest and (2) all biological pathways starting at drugs currently\nprescribed against that disease and ending at the disease of interest. We\nillustrate our approach with Alzheimer's disease (AD) and two of its risk\nfactors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's\nrank across four hypergraph settings (single- or multi-disease): AD only, AD +\nHTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the\nidentification of two promising drugs whose repurposing potential was\nsignificantly higher in hypergraphs combining two diseases: dapagliflozin\n(antidiabetic; moved up, from top 32$\\%$ to top 7$\\%$, across all considered\ndrugs) and debrisoquine (antihypertensive; moved up, from top 76$\\%$ to top\n23$\\%$). Our approach serves as a hypothesis generation tool, to be paired with\na validation pipeline relying on laboratory experiments and semi-automated\nparsing of the biomedical literature.\n","authors":["Ayush Jain","Marie Laure-Charpignon","Irene Y. Chen","Anthony Philippakis","Ahmed Alaa"],"pdf_url":"https://arxiv.org/pdf/2311.09596v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 9 pages"},{"id":"http://arxiv.org/abs/2311.09591v1","updated":"2023-11-16T06:02:48Z","published":"2023-11-16T06:02:48Z","title":"Accelerating material discovery with a threshold-driven hybrid\n  acquisition policy-based Bayesian optimization","summary":"  Advancements in materials play a crucial role in technological progress.\nHowever, the process of discovering and developing materials with desired\nproperties is often impeded by substantial experimental costs, extensive\nresource utilization, and lengthy development periods. To address these\nchallenges, modern approaches often employ machine learning (ML) techniques\nsuch as Bayesian Optimization (BO), which streamline the search for optimal\nmaterials by iteratively selecting experiments that are most likely to yield\nbeneficial results. However, traditional BO methods, while beneficial, often\nstruggle with balancing the trade-off between exploration and exploitation,\nleading to sub-optimal performance in material discovery processes. This paper\nintroduces a novel Threshold-Driven UCB-EI Bayesian Optimization (TDUE-BO)\nmethod, which dynamically integrates the strengths of Upper Confidence Bound\n(UCB) and Expected Improvement (EI) acquisition functions to optimize the\nmaterial discovery process. Unlike the classical BO, our method focuses on\nefficiently navigating the high-dimensional material design space (MDS).\nTDUE-BO begins with an exploration-focused UCB approach, ensuring a\ncomprehensive initial sweep of the MDS. As the model gains confidence,\nindicated by reduced uncertainty, it transitions to the more exploitative EI\nmethod, focusing on promising areas identified earlier. The UCB-to-EI switching\npolicy dictated guided through continuous monitoring of the model uncertainty\nduring each step of sequential sampling results in navigating through the MDS\nmore efficiently while ensuring rapid convergence. The effectiveness of TDUE-BO\nis demonstrated through its application on three different material datasets,\nshowing significantly better approximation and optimization performance over\nthe EI and UCB-based BO methods in terms of the RMSE scores and convergence\nefficiency, respectively.\n","authors":["Ahmed Shoyeb Raihan","Hamed Khosravi","Srinjoy Das","Imtiaz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2311.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14670v2","updated":"2023-11-16T05:45:50Z","published":"2023-06-26T13:06:34Z","title":"Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition","summary":"  As the scale of machine learning models increases, trends such as scaling\nlaws anticipate consistent downstream improvements in predictive accuracy.\nHowever, these trends take the perspective of a single model-provider in\nisolation, while in reality providers often compete with each other for users.\nIn this work, we demonstrate that competition can fundamentally alter the\nbehavior of these scaling trends, even causing overall predictive accuracy\nacross users to be non-monotonic or decreasing with scale. We define a model of\ncompetition for classification tasks, and use data representations as a lens\nfor studying the impact of increases in scale. We find many settings where\nimproving data representation quality (as measured by Bayes risk) decreases the\noverall predictive accuracy across users (i.e., social welfare) for a\nmarketplace of competing model-providers. Our examples range from closed-form\nformulas in simple settings to simulations with pretrained representations on\nCIFAR-10. At a conceptual level, our work suggests that favorable scaling\ntrends for individual model-providers need not translate to downstream\nimprovements in social welfare in marketplaces with multiple model providers.\n","authors":["Meena Jagadeesan","Michael I. Jordan","Jacob Steinhardt","Nika Haghtalab"],"pdf_url":"https://arxiv.org/pdf/2306.14670v2.pdf","comment":"To appear at NeurIPS 2023; this is the full version"},{"id":"http://arxiv.org/abs/2311.00619v2","updated":"2023-11-16T05:31:27Z","published":"2023-11-01T16:14:34Z","title":"Loss Modeling for Multi-Annotator Datasets","summary":"  Accounting for the opinions of all annotators of a dataset is critical for\nfairness. However, when annotating large datasets, individual annotators will\nfrequently provide thousands of ratings which can lead to fatigue.\nAdditionally, these annotation processes can occur over multiple days which can\nlead to an inaccurate representation of an annotator's opinion over time. To\ncombat this, we propose to learn a more accurate representation of diverse\nopinions by utilizing multitask learning in conjunction with loss-based label\ncorrection. We show that using our novel formulation, we can cleanly separate\nagreeing and disagreeing annotations. Furthermore, we demonstrate that this\nmodification can improve prediction performance in a single or multi-annotator\nsetting. Lastly, we show that this method remains robust to additional label\nnoise that is applied to subjective data.\n","authors":["Uthman Jinadu","Jesse Annan","Shanshan Wen","Yi Ding"],"pdf_url":"https://arxiv.org/pdf/2311.00619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09578v1","updated":"2023-11-16T05:29:39Z","published":"2023-11-16T05:29:39Z","title":"Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying","summary":"  We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective\ntraining to further increase parameter efficiency of the Low-rank adaptation\n(LoRA) method. Our investigations include all feasible combinations parameter\ntraining/freezing in conjunction with weight tying to identify the optimal\nbalance between performance and the number of trainable parameters. Through\nexperiments covering a variety of tasks and two base language models, we\nprovide analysis revealing trade-offs between efficiency and performance. Our\nexperiments uncovered a particular Tied-LoRA configuration that stands out by\ndemonstrating comparable performance across several tasks while employing only\n13~\\% percent of parameters utilized by the standard LoRA method.\n","authors":["Adithya Renduchintala","Tugrul Konuk","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2311.09578v1.pdf","comment":"8 pages 4 figures"},{"id":"http://arxiv.org/abs/2311.09577v1","updated":"2023-11-16T05:23:53Z","published":"2023-11-16T05:23:53Z","title":"Group-Aware Interest Disentangled Dual-Training for Personalized\n  Recommendation","summary":"  Personalized recommender systems aim to predict users' preferences for items.\nIt has become an indispensable part of online services. Online social platforms\nenable users to form groups based on their common interests. The users' group\nparticipation on social platforms reveals their interests and can be utilized\nas side information to mitigate the data sparsity and cold-start problem in\nrecommender systems. Users join different groups out of different interests. In\nthis paper, we generate group representation from the user's interests and\npropose IGRec (Interest-based Group enhanced Recommendation) to utilize the\ngroup information accurately. It consists of four modules. (1) Interest\ndisentangler via self-gating that disentangles users' interests from their\ninitial embedding representation. (2) Interest aggregator that generates the\ninterest-based group representation by Gumbel-Softmax aggregation on the group\nmembers' interests. (3) Interest-based group aggregation that fuses user's\nrepresentation with the participated group representation. (4) A dual-trained\nrating prediction module to utilize both user-item and group-item interactions.\nWe conduct extensive experiments on three publicly available datasets. Results\nshow IGRec can effectively alleviate the data sparsity problem and enhance the\nrecommender system with interest-based group representation. Experiments on the\ngroup recommendation task further show the informativeness of interest-based\ngroup representation.\n","authors":["Xiaolong Liu","Liangwei Yang","Zhiwei Liu","Xiaohan Li","Mingdai Yang","Chen Wang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2311.09577v1.pdf","comment":"10 pages, 7 figures, 2023 IEEE International Conference on Big Data"},{"id":"http://arxiv.org/abs/2311.09574v1","updated":"2023-11-16T05:17:14Z","published":"2023-11-16T05:17:14Z","title":"LymphoML: An interpretable artificial intelligence-based method\n  identifies morphologic features that correlate with lymphoma subtype","summary":"  The accurate classification of lymphoma subtypes using hematoxylin and eosin\n(H&E)-stained tissue is complicated by the wide range of morphological features\nthese cancers can exhibit. We present LymphoML - an interpretable machine\nlearning method that identifies morphologic features that correlate with\nlymphoma subtypes. Our method applies steps to process H&E-stained tissue\nmicroarray cores, segment nuclei and cells, compute features encompassing\nmorphology, texture, and architecture, and train gradient-boosted models to\nmake diagnostic predictions. LymphoML's interpretable models, developed on a\nlimited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy\nto pathologists using whole-slide images and outperform black box deep-learning\non a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using\nSHapley Additive exPlanation (SHAP) analysis, we assess the impact of each\nfeature on model prediction and find that nuclear shape features are most\ndiscriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma\n(F1-score: 74.5%). Finally, we provide the first demonstration that a model\ncombining features from H&E-stained tissue with features from a standardized\npanel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a\n46-stain panel (86.1%).\n","authors":["Vivek Shankar","Xiaoli Yang","Vrishab Krishna","Brent Tan","Oscar Silva","Rebecca Rojansky","Andrew Ng","Fabiola Valvert","Edward Briercheck","David Weinstock","Yasodha Natkunam","Sebastian Fernandez-Pol","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2311.09574v1.pdf","comment":"To be published in Proceedings of the 3rd Machine Learning for Health\n  symposium, Proceedings of Machine Learning Research (PMLR)"},{"id":"http://arxiv.org/abs/2311.09566v1","updated":"2023-11-16T05:06:51Z","published":"2023-11-16T05:06:51Z","title":"A Knowledge Distillation Approach for Sepsis Outcome Prediction from\n  Multivariate Clinical Time Series","summary":"  Sepsis is a life-threatening condition triggered by an extreme infection\nresponse. Our objective is to forecast sepsis patient outcomes using their\nmedical history and treatments, while learning interpretable state\nrepresentations to assess patients' risks in developing various adverse\noutcomes. While neural networks excel in outcome prediction, their limited\ninterpretability remains a key issue. In this work, we use knowledge\ndistillation via constrained variational inference to distill the knowledge of\na powerful \"teacher\" neural network model with high predictive power to train a\n\"student\" latent variable model to learn interpretable hidden state\nrepresentations to achieve high predictive performance for sepsis outcome\nprediction. Using real-world data from the MIMIC-IV database, we trained an\nLSTM as the \"teacher\" model to predict mortality for sepsis patients, given\ninformation about their recent history of vital signs, lab values and\ntreatments. For our student model, we use an autoregressive hidden Markov model\n(AR-HMM) to learn interpretable hidden states from patients' clinical time\nseries, and use the posterior distribution of the learned state representations\nto predict various downstream outcomes, including hospital mortality, pulmonary\nedema, need for diuretics, dialysis, and mechanical ventilation. Our results\nshow that our approach successfully incorporates the constraint to achieve high\npredictive power similar to the teacher model, while maintaining the generative\nperformance.\n","authors":["Anna Wong","Shu Ge","Nassim Oufattole","Adam Dejl","Megan Su","Ardavan Saeedi","Li-wei H. Lehman"],"pdf_url":"https://arxiv.org/pdf/2311.09566v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 12 pages"},{"id":"http://arxiv.org/abs/2305.14456v2","updated":"2023-11-16T04:46:27Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  It is important that language models appropriately adapt to specific cultural\ncontexts. However, as we show in this paper, multilingual and Arabic\nmonolingual language models default to Western culture even when prompted in\nArabic and contextualized by an Arab cultural setting. To measure this Western\nbias, we introduce CAMeL, a dataset of naturally occurring Arabic prompts\nspanning eight diverse cultural aspects and an extensive list of 20,504\ncultural targets corresponding to Arab or Western culture. Using CAMeL, we show\nthat models favor Western targets and demonstrate cultural unfairness on\ndownstream tasks such as named entity recognition and sentiment analysis. Our\nanalyses of pretraining corpora also reveal that commonly used sources such as\nWikipedia may not be suited to build culturally aware models, underscoring the\nimportance of carefully curating pretraining data in constructing language\nmodels to serve a global population.\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09550v1","updated":"2023-11-16T04:11:19Z","published":"2023-11-16T04:11:19Z","title":"A Speed Odyssey for Deployable Quantization of LLMs","summary":"  The large language model era urges faster and less costly inference. Prior\nmodel compression works on LLMs tend to undertake a software-centric approach\nprimarily focused on the simulated quantization performance. By neglecting the\nfeasibility of deployment, these approaches are typically disabled in real\npractice. They used to drastically push down the quantization bit range for a\nreduced computation which might not be supported by the mainstream hardware, or\ninvolve sophisticated algorithms that introduce extra computation or memory\naccess overhead. We argue that pursuing a hardware-centric approach in the\nconstruction of quantization algorithms is crucial. In this regard, we are\ndriven to build our compression method on top of hardware awareness,\neliminating impractical algorithm choices while maximizing the benefit of\nhardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel\nimplementation called FastGEMM and a combined recipe of quantization\nstrategies. Extensive experiments manifest the superiority of our W4A8 method\nwhich brings the actual speed boosting up to \\textbf{4$\\times$} compared to\nHugging Face FP16 inference and \\textbf{2.23$\\times$} vs. the state-of-the-art\ninference engine TensorRT-LLM in FP16, and \\textbf{1.45$\\times$} vs.\nTensorRT-LLM in INT8, yet without substantially harming the performance.\n","authors":["Qingyuan Li","Ran Meng","Yiduo Li","Bo Zhang","Liang Li","Yifan Lu","Xiangxiang Chu","Yerui Sun","Yuchen Xie"],"pdf_url":"https://arxiv.org/pdf/2311.09550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09106v2","updated":"2023-11-16T03:51:30Z","published":"2022-08-19T00:55:05Z","title":"A Risk-Sensitive Approach to Policy Optimization","summary":"  Standard deep reinforcement learning (DRL) aims to maximize expected reward,\nconsidering collected experiences equally in formulating a policy. This differs\nfrom human decision-making, where gains and losses are valued differently and\noutlying outcomes are given increased consideration. It also fails to\ncapitalize on opportunities to improve safety and/or performance through the\nincorporation of distributional context. Several approaches to distributional\nDRL have been investigated, with one popular strategy being to evaluate the\nprojected distribution of returns for possible actions. We propose a more\ndirect approach whereby risk-sensitive objectives, specified in terms of the\ncumulative distribution function (CDF) of the distribution of full-episode\nrewards, are optimized. This approach allows for outcomes to be weighed based\non relative quality, can be used for both continuous and discrete action\nspaces, and may naturally be applied in both constrained and unconstrained\nsettings. We show how to compute an asymptotically consistent estimate of the\npolicy gradient for a broad class of risk-sensitive objectives via sampling,\nsubsequently incorporating variance reduction and regularization measures to\nfacilitate effective on-policy learning. We then demonstrate that the use of\nmoderately \"pessimistic\" risk profiles, which emphasize scenarios where the\nagent performs poorly, leads to enhanced exploration and a continual focus on\naddressing deficiencies. We test the approach using different risk profiles in\nsix OpenAI Safety Gym environments, comparing to state of the art on-policy\nmethods. Without cost constraints, we find that pessimistic risk profiles can\nbe used to reduce cost while improving total reward accumulation. With cost\nconstraints, they are seen to provide higher positive rewards than risk-neutral\napproaches at the prescribed allowable cost.\n","authors":["Jared Markowitz","Ryan W. Gardner","Ashley Llorens","Raman Arora","I-Jeng Wang"],"pdf_url":"https://arxiv.org/pdf/2208.09106v2.pdf","comment":"16 pages, 13 figures. AAAI 2023 (Special Track on Safe and Robust AI)"},{"id":"http://arxiv.org/abs/2311.09544v1","updated":"2023-11-16T03:47:48Z","published":"2023-11-16T03:47:48Z","title":"Scaling User Modeling: Large-scale Online User Representations for Ads\n  Personalization in Meta","summary":"  Effective user representations are pivotal in personalized advertising.\nHowever, stringent constraints on training throughput, serving latency, and\nmemory, often limit the complexity and input feature set of online ads ranking\nmodels. This challenge is magnified in extensive systems like Meta's, which\nencompass hundreds of models with diverse specifications, rendering the\ntailoring of user representation learning for each model impractical. To\naddress these challenges, we present Scaling User Modeling (SUM), a framework\nwidely deployed in Meta's ads ranking system, designed to facilitate efficient\nand scalable sharing of online user representation across hundreds of ads\nmodels. SUM leverages a few designated upstream user models to synthesize user\nembeddings from massive amounts of user features with advanced modeling\ntechniques. These embeddings then serve as inputs to downstream online ads\nranking models, promoting efficient representation sharing. To adapt to the\ndynamic nature of user features and ensure embedding freshness, we designed SUM\nOnline Asynchronous Platform (SOAP), a latency free online serving system\ncomplemented with model freshness and embedding stabilization, which enables\nfrequent user model updates and online inference of user embeddings upon each\nuser request. We share our hands-on deployment experiences for the SUM\nframework and validate its superiority through comprehensive experiments. To\ndate, SUM has been launched to hundreds of ads ranking models in Meta,\nprocessing hundreds of billions of user requests daily, yielding significant\nonline metric gains and infrastructure cost savings.\n","authors":["Wei Zhang","Dai Li","Chen Liang","Fang Zhou","Zhongke Zhang","Xuewei Wang","Ru Li","Yi Zhou","Yaning Huang","Dong Liang","Kai Wang","Zhangyuan Wang","Zhengxing Chen","Min Li","Fenggang Wu","Minghai Chen","Huayu Li","Yunnan Wu","Zhan Shu","Mindi Yuan","Sri Reddy"],"pdf_url":"https://arxiv.org/pdf/2311.09544v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.09528v1","updated":"2023-11-16T03:13:29Z","published":"2023-11-16T03:13:29Z","title":"HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM","summary":"  Existing open-source helpfulness preference datasets do not specify what\nmakes some responses more helpful and others less so. Models trained on these\ndatasets can incidentally learn to model dataset artifacts (e.g. preferring\nlonger but unhelpful responses only due to their length). To alleviate this\nproblem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated\nfor the various aspects that make responses helpful. Specifically, our\n37k-sample dataset has annotations for correctness, coherence, complexity, and\nverbosity in addition to overall helpfulness of responses. Training Llama 2 70B\nusing the HelpSteer dataset with SteerLM technique produces a model that scores\n7.54 on MT Bench, which is currently the highest score for open models that do\nnot require training data from more powerful models (e.g. GPT4). We release\nthis dataset with CC-BY-4.0 license at\nhttps://huggingface.co/datasets/nvidia/HelpSteer\n","authors":["Zhilin Wang","Yi Dong","Jiaqi Zeng","Virginia Adams","Makesh Narsimhan Sreedhar","Daniel Egert","Olivier Delalleau","Jane Polak Scowcroft","Neel Kant","Aidan Swope","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2311.09528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09514v1","updated":"2023-11-16T02:43:13Z","published":"2023-11-16T02:43:13Z","title":"Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based\n  Human Activity Recognition in Smart Homes","summary":"  There has been a resurgence of applications focused on Human Activity\nRecognition (HAR) in smart homes, especially in the field of ambient\nintelligence and assisted living technologies. However, such applications\npresent numerous significant challenges to any automated analysis system\noperating in the real world, such as variability, sparsity, and noise in sensor\nmeasurements. Although state-of-the-art HAR systems have made considerable\nstrides in addressing some of these challenges, they especially suffer from a\npractical limitation: they require successful pre-segmentation of continuous\nsensor data streams before automated recognition, i.e., they assume that an\noracle is present during deployment, which is capable of identifying time\nwindows of interest across discrete sensor events. To overcome this limitation,\nwe propose a novel graph-guided neural network approach that performs activity\nrecognition by learning explicit co-firing relationships between sensors. We\naccomplish this by learning a more expressive graph structure representing the\nsensor network in a smart home, in a data-driven manner. Our approach maps\ndiscrete input sensor measurements to a feature space through the application\nof attention mechanisms and hierarchical pooling of node embeddings. We\ndemonstrate the effectiveness of our proposed approach by conducting several\nexperiments on CASAS datasets, showing that the resulting graph-guided neural\nnetwork outperforms the state-of-the-art method for HAR in smart homes across\nmultiple datasets and by large margins. These results are promising because\nthey push HAR for smart homes closer to real-world applications.\n","authors":["Srivatsa P","Thomas Plötz"],"pdf_url":"https://arxiv.org/pdf/2311.09514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04822v2","updated":"2023-11-16T02:40:19Z","published":"2023-01-11T09:12:28Z","title":"Private estimation algorithms for stochastic block models and mixture\n  models","summary":"  We introduce general tools for designing efficient private estimation\nalgorithms, in the high-dimensional settings, whose statistical guarantees\nalmost match those of the best known non-private algorithms. To illustrate our\ntechniques, we consider two problems: recovery of stochastic block models and\nlearning mixtures of spherical Gaussians. For the former, we present the first\nefficient $(\\epsilon, \\delta)$-differentially private algorithm for both weak\nrecovery and exact recovery. Previously known algorithms achieving comparable\nguarantees required quasi-polynomial time. For the latter, we design an\n$(\\epsilon, \\delta)$-differentially private algorithm that recovers the centers\nof the $k$-mixture when the minimum separation is at least $\nO(k^{1/t}\\sqrt{t})$. For all choices of $t$, this algorithm requires sample\ncomplexity $n\\geq k^{O(1)}d^{O(t)}$ and time complexity $(nd)^{O(t)}$. Prior\nwork required minimum separation at least $O(\\sqrt{k})$ as well as an explicit\nupper bound on the Euclidean norm of the centers.\n","authors":["Hongjie Chen","Vincent Cohen-Addad","Tommaso d'Orsi","Alessandro Epasto","Jacob Imola","David Steurer","Stefan Tiegel"],"pdf_url":"https://arxiv.org/pdf/2301.04822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10265v4","updated":"2023-11-16T02:37:34Z","published":"2022-07-21T02:21:03Z","title":"FOCUS: Fairness via Agent-Awareness for Federated Learning on\n  Heterogeneous Data","summary":"  Federated learning (FL) allows agents to jointly train a global model without\nsharing their local data. However, due to the heterogeneous nature of local\ndata, it is challenging to optimize or even define fairness of the trained\nglobal model for the agents. For instance, existing work usually considers\naccuracy equity as fairness for different agents in FL, which is limited,\nespecially under the heterogeneous setting, since it is intuitively \"unfair\" to\nenforce agents with high-quality data to achieve similar accuracy to those who\ncontribute low-quality data, which may discourage the agents from participating\nin FL. In this work, we propose a formal FL fairness definition, fairness via\nagent-awareness (FAA), which takes different contributions of heterogeneous\nagents into account. Under FAA, the performance of agents with high-quality\ndata will not be sacrificed just due to the existence of large amounts of\nagents with low-quality data. In addition, we propose a fair FL training\nalgorithm based on agent clustering (FOCUS) to achieve fairness in FL measured\nby FAA. Theoretically, we prove the convergence and optimality of FOCUS under\nmild conditions for linear and general convex loss functions with bounded\nsmoothness. We also prove that FOCUS always achieves higher fairness in terms\nof FAA compared with standard FedAvg under both linear and general convex loss\nfunctions. Empirically, we show that on four FL datasets, including synthetic\ndata, images, and texts, FOCUS achieves significantly higher fairness in terms\nof FAA while maintaining competitive prediction accuracy compared with FedAvg\nand state-of-the-art fair FL algorithms.\n","authors":["Wenda Chu","Chulin Xie","Boxin Wang","Linyi Li","Lang Yin","Arash Nourian","Han Zhao","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2207.10265v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09511v1","updated":"2023-11-16T02:32:26Z","published":"2023-11-16T02:32:26Z","title":"Identifying Systems with Symmetries using Equivariant Autoregressive\n  Reservoir Computers","summary":"  The investigation reported in this document focuses on identifying systems\nwith symmetries using equivariant autoregressive reservoir computers. General\nresults in structured matrix approximation theory are presented, exploring a\ntwo-fold approach. Firstly, a comprehensive examination of generic\nsymmetry-preserving nonlinear time delay embedding is conducted. This involves\nanalyzing time series data sampled from an equivariant system under study.\nSecondly, sparse least-squares methods are applied to discern approximate\nrepresentations of the output coupling matrices. These matrices play a pivotal\nrole in determining the nonlinear autoregressive representation of an\nequivariant system. The structural characteristics of these matrices are\ndictated by the set of symmetries inherent in the system. The document outlines\nprototypical algorithms derived from the described techniques, offering insight\ninto their practical applications. Emphasis is placed on their effectiveness in\nthe identification and predictive simulation of equivariant nonlinear systems,\nregardless of whether such systems exhibit chaotic behavior.\n","authors":["Fredy Vides","Idelfonso B. R. Nogueira","Lendy Banegas","Evelyn Flores"],"pdf_url":"https://arxiv.org/pdf/2311.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09506v1","updated":"2023-11-16T02:06:23Z","published":"2023-11-16T02:06:23Z","title":"Investigating the Impact of Weight Sharing Decisions on Knowledge\n  Transfer in Continual Learning","summary":"  Continual Learning (CL) has generated attention as a method of avoiding\nCatastrophic Forgetting (CF) in the sequential training of neural networks,\nimproving network efficiency and adaptability to different tasks. Additionally,\nCL serves as an ideal setting for studying network behavior and Forward\nKnowledge Transfer (FKT) between tasks. Pruning methods for CL train\nsubnetworks to handle the sequential tasks which allows us to take a structured\napproach to investigating FKT. Sharing prior subnetworks' weights leverages\npast knowledge for the current task through FKT. Understanding which weights to\nshare is important as sharing all weights can yield sub-optimal accuracy. This\npaper investigates how different sharing decisions affect the FKT between\ntasks. Through this lens we demonstrate how task complexity and similarity\ninfluence the optimal weight sharing decisions, giving insights into the\nrelationships between tasks and helping inform decision making in similar CL\nmethods. We implement three sequential datasets designed to emphasize variation\nin task complexity and similarity, reporting results for both ResNet-18 and\nVGG-16. By sharing in accordance with the decisions supported by our findings,\nwe show that we can improve task accuracy compared to other sharing decisions.\n","authors":["Josh Andle","Ali Payani","Salimeh Yasaei-Sekeh"],"pdf_url":"https://arxiv.org/pdf/2311.09506v1.pdf","comment":"5 Figures, 4 Tables, 2 Algorithms"},{"id":"http://arxiv.org/abs/2311.09505v1","updated":"2023-11-16T02:05:15Z","published":"2023-11-16T02:05:15Z","title":"SegMix: A Simple Structure-Aware Data Augmentation Method","summary":"  Interpolation-based Data Augmentation (DA) methods (Mixup) linearly\ninterpolate the inputs and labels of two or more training examples. Mixup has\nmore recently been adapted to the field of Natural Language Processing (NLP),\nmainly for sequence labeling tasks. However, such a simple adoption yields\nmixed or unstable improvements over the baseline models. We argue that the\ndirect-adoption methods do not account for structures in NLP tasks. To this\nend, we propose SegMix, a collection of interpolation-based DA algorithms that\ncan adapt to task-specific structures. SegMix poses fewer constraints on data\nstructures, is robust to various hyperparameter settings, applies to more task\nsettings, and adds little computational overhead. In the algorithm's core, we\napply interpolation methods on task-specific meaningful segments, in contrast\nto applying them on sequences as in prior work. We find SegMix to be a flexible\nframework that combines rule-based DA methods with interpolation-based methods,\ncreating interesting mixtures of DA techniques. We show that SegMix\nconsistently improves performance over strong baseline models in Named Entity\nRecognition (NER) and Relation Extraction (RE) tasks, especially under\ndata-scarce settings. Furthermore, this method is easy to implement and adds\nnegligible training overhead.\n","authors":["Yuxin Pei","Pushkar Bhuse","Zhengzhong Liu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2311.09505v1.pdf","comment":"Upload of a work done in 2022"},{"id":"http://arxiv.org/abs/2311.09498v1","updated":"2023-11-16T01:50:54Z","published":"2023-11-16T01:50:54Z","title":"Network Wide Evacuation Traffic Prediction in a Rapidly Intensifying\n  Hurricane from Traffic Detectors and Facebook Movement Data: A Deep Learning\n  Approach","summary":"  Traffic prediction during hurricane evacuation is essential for optimizing\nthe use of transportation infrastructures. It can reduce evacuation time by\nproviding information on future congestion in advance. However, evacuation\ntraffic prediction can be challenging as evacuation traffic patterns is\nsignificantly different than regular period traffic. A data-driven traffic\nprediction model is developed in this study by utilizing traffic detector and\nFacebook movement data during Hurricane Ian, a rapidly intensifying hurricane.\nWe select 766 traffic detectors from Florida's 4 major interstates to collect\ntraffic features. Additionally, we use Facebook movement data collected during\nHurricane Ian's evacuation period. The deep-learning model is first trained on\nregular period (May-August 2022) data to understand regular traffic patterns\nand then Hurricane Ian's evacuation period data is used as test data. The model\nachieves 95% accuracy (RMSE = 356) during regular period, but it underperforms\nwith 55% accuracy (RMSE = 1084) during the evacuation period. Then, a transfer\nlearning approach is adopted where a pretrained model is used with additional\nevacuation related features to predict evacuation period traffic. After\ntransfer learning, the model achieves 89% accuracy (RMSE = 514). Adding\nFacebook movement data further reduces model's RMSE value to 393 and increases\naccuracy to 93%. The proposed model is capable to forecast traffic up to\n6-hours in advance. Evacuation traffic management officials can use the\ndeveloped traffic prediction model to anticipate future traffic congestion in\nadvance and take proactive measures to reduce delays during evacuation.\n","authors":["Md Mobasshir Rashid","Rezaur Rahman","Samiul Hasan"],"pdf_url":"https://arxiv.org/pdf/2311.09498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12047v3","updated":"2023-11-16T01:35:24Z","published":"2023-06-21T06:30:56Z","title":"Residual-Based Error Corrector Operator to Enhance Accuracy and\n  Reliability of Neural Operator Surrogates of Nonlinear Variational\n  Boundary-Value Problems","summary":"  This work focuses on developing methods for approximating the solution\noperators of a class of parametric partial differential equations via neural\noperators. Neural operators have several challenges, including the issue of\ngenerating appropriate training data, cost-accuracy trade-offs, and nontrivial\nhyperparameter tuning. The unpredictability of the accuracy of neural operators\nimpacts their applications in downstream problems of inference, optimization,\nand control. A framework based on the linear variational problem that gives the\ncorrection to the prediction furnished by neural operators is considered based\non earlier work in JCP 486 (2023) 112104. The operator, called Residual-based\nError Corrector Operator or simply Corrector Operator, associated with the\ncorrector problem is analyzed further. Numerical results involving a nonlinear\nreaction-diffusion model in two dimensions with PCANet-type neural operators\nshow almost two orders of increase in the accuracy of approximations when\nneural operators are corrected using the correction scheme. Further, topology\noptimization involving a nonlinear reaction-diffusion model is considered to\nhighlight the limitations of neural operators and the efficacy of the\ncorrection scheme. Optimizers with neural operator surrogates are seen to make\nsignificant errors (as high as 80 percent). However, the errors are much lower\n(below 7 percent) when neural operators are corrected.\n","authors":["Prashant K. Jha"],"pdf_url":"https://arxiv.org/pdf/2306.12047v3.pdf","comment":"36 pages, 14 figures, 3 tables"},{"id":"http://arxiv.org/abs/2311.09491v1","updated":"2023-11-16T01:22:22Z","published":"2023-11-16T01:22:22Z","title":"Spatial Bayesian Neural Networks","summary":"  Statistical models for spatial processes play a central role in statistical\nanalyses of spatial data. Yet, it is the simple, interpretable, and well\nunderstood models that are routinely employed even though, as is revealed\nthrough prior and posterior predictive checks, these can poorly characterise\nthe spatial heterogeneity in the underlying process of interest. Here, we\npropose a new, flexible class of spatial-process models, which we refer to as\nspatial Bayesian neural networks (SBNNs). An SBNN leverages the\nrepresentational capacity of a Bayesian neural network; it is tailored to a\nspatial setting by incorporating a spatial \"embedding layer\" into the network\nand, possibly, spatially-varying network parameters. An SBNN is calibrated by\nmatching its finite-dimensional distribution at locations on a fine gridding of\nspace to that of a target process of interest. That process could be easy to\nsimulate from or we have many realisations from it. We propose several variants\nof SBNNs, most of which are able to match the finite-dimensional distribution\nof the target process at the selected grid better than conventional BNNs of\nsimilar complexity. We also show that a single SBNN can be used to represent a\nvariety of spatial processes often used in practice, such as Gaussian processes\nand lognormal processes. We briefly discuss the tools that could be used to\nmake inference with SBNNs, and we conclude with a discussion of their\nadvantages and limitations.\n","authors":["Andrew Zammit-Mangion","Michael D. Kaminski","Ba-Hien Tran","Maurizio Filippone","Noel Cressie"],"pdf_url":"https://arxiv.org/pdf/2311.09491v1.pdf","comment":"29 pages, 18 figures"},{"id":"http://arxiv.org/abs/2311.09483v1","updated":"2023-11-16T01:00:04Z","published":"2023-11-16T01:00:04Z","title":"Adaptive Interventions with User-Defined Goals for Health Behavior\n  Change","summary":"  Physical inactivity remains a major public health concern, having\nassociations with adverse health outcomes such as cardiovascular disease and\ntype-2 diabetes. Mobile health applications present a promising avenue for\nlow-cost, scalable physical activity promotion, yet often suffer from small\neffect sizes and low adherence rates, particularly in comparison to human\ncoaching. Goal-setting is a critical component of health coaching that has been\nunderutilized in adaptive algorithms for mobile health interventions. This\npaper introduces a modification to the Thompson sampling algorithm that places\nemphasis on individualized goal-setting by optimizing personalized reward\nfunctions. As a step towards supporting goal-setting, this paper offers a\nbalanced approach that can leverage shared structure while optimizing\nindividual preferences and goals. We prove that our modification incurs only a\nconstant penalty on the cumulative regret while preserving the sample\ncomplexity benefits of data sharing. In a physical activity simulator, we\ndemonstrate that our algorithm achieves substantial improvements in cumulative\nregret compared to baselines that do not share data or do not optimize for\nindividualized rewards.\n","authors":["Aishwarya Mandyam","Matthew Joerke","Barbara E. Engelhardt","Emma Brunskill"],"pdf_url":"https://arxiv.org/pdf/2311.09483v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 5 pages"},{"id":"http://arxiv.org/abs/2311.09480v1","updated":"2023-11-16T00:50:37Z","published":"2023-11-16T00:50:37Z","title":"Show Your Work with Confidence: Confidence Bands for Tuning Curves","summary":"  The choice of hyperparameters greatly impacts performance in natural language\nprocessing. Often, it is hard to tell if a method is better than another or\njust better tuned. Tuning curves fix this ambiguity by accounting for tuning\neffort. Specifically, they plot validation performance as a function of the\nnumber of hyperparameter choices tried so far. While several estimators exist\nfor these curves, it is common to use point estimates, which we show fail\nsilently and give contradictory results when given too little data.\n  Beyond point estimates, confidence bands are necessary to rigorously\nestablish the relationship between different approaches. We present the first\nmethod to construct valid confidence bands for tuning curves. The bands are\nexact, simultaneous, and distribution-free, thus they provide a robust basis\nfor comparing methods.\n  Empirical analysis shows that while bootstrap confidence bands, which serve\nas a baseline, fail to approximate their target confidence, ours achieve it\nexactly. We validate our design with ablations, analyze the effect of sample\nsize, and provide guidance on comparing models with our method. To promote\nconfident comparisons in future work, we release a library implementing the\nmethod at https://github.com/nalourie/opda .\n","authors":["Nicholas Lourie","Kyunghyun Cho","He He"],"pdf_url":"https://arxiv.org/pdf/2311.09480v1.pdf","comment":"15 pages, 15 figures"},{"id":"http://arxiv.org/abs/2311.01655v2","updated":"2023-11-16T00:22:27Z","published":"2023-11-03T01:12:35Z","title":"Detecting Spurious Correlations via Robust Visual Concepts in Real and\n  AI-Generated Image Classification","summary":"  Often machine learning models tend to automatically learn associations\npresent in the training data without questioning their validity or\nappropriateness. This undesirable property is the root cause of the\nmanifestation of spurious correlations, which render models unreliable and\nprone to failure in the presence of distribution shifts. Research shows that\nmost methods attempting to remedy spurious correlations are only effective for\na model's known spurious associations. Current spurious correlation detection\nalgorithms either rely on extensive human annotations or are too restrictive in\ntheir formulation. Moreover, they rely on strict definitions of visual\nartifacts that may not apply to data produced by generative models, as they are\nknown to hallucinate contents that do not conform to standard specifications.\nIn this work, we introduce a general-purpose method that efficiently detects\npotential spurious correlations, and requires significantly less human\ninterference in comparison to the prior art. Additionally, the proposed method\nprovides intuitive explanations while eliminating the need for pixel-level\nannotations. We demonstrate the proposed method's tolerance to the peculiarity\nof AI-generated images, which is a considerably challenging task, one where\nmost of the existing methods fall short. Consequently, our method is also\nsuitable for detecting spurious correlations that may propagate to downstream\napplications originating from generative models.\n","authors":["Preetam Prabhu Srikar Dammu","Chirag Shah"],"pdf_url":"https://arxiv.org/pdf/2311.01655v2.pdf","comment":"Paper accepted at 37th Conference on Neural Information Processing\n  Systems (NeurIPS 2023), XAIA Workshop"},{"id":"http://arxiv.org/abs/2311.09466v1","updated":"2023-11-16T00:13:00Z","published":"2023-11-16T00:13:00Z","title":"Soft Matching Distance: A metric on neural representations that captures\n  single-neuron tuning","summary":"  Common measures of neural representational (dis)similarity are designed to be\ninsensitive to rotations and reflections of the neural activation space.\nMotivated by the premise that the tuning of individual units may be important,\nthere has been recent interest in developing stricter notions of\nrepresentational (dis)similarity that require neurons to be individually\nmatched across networks. When two networks have the same size (i.e. same number\nof neurons), a distance metric can be formulated by optimizing over neuron\nindex permutations to maximize tuning curve alignment. However, it is not clear\nhow to generalize this metric to measure distances between networks with\ndifferent sizes. Here, we leverage a connection to optimal transport theory to\nderive a natural generalization based on \"soft\" permutations. The resulting\nmetric is symmetric, satisfies the triangle inequality, and can be interpreted\nas a Wasserstein distance between two empirical distributions. Further, our\nproposed metric avoids counter-intuitive outcomes suffered by alternative\napproaches, and captures complementary geometric insights into neural\nrepresentations that are entirely missed by rotation-invariant metrics.\n","authors":["Meenakshi Khosla","Alex H. Williams"],"pdf_url":"https://arxiv.org/pdf/2311.09466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02738v2","updated":"2023-11-16T23:25:25Z","published":"2023-11-05T19:04:25Z","title":"Scenario Diffusion: Controllable Driving Scenario Generation With\n  Diffusion","summary":"  Automated creation of synthetic traffic scenarios is a key part of validating\nthe safety of autonomous vehicles (AVs). In this paper, we propose Scenario\nDiffusion, a novel diffusion-based architecture for generating traffic\nscenarios that enables controllable scenario generation. We combine latent\ndiffusion, object detection and trajectory regression to generate distributions\nof synthetic agent poses, orientations and trajectories simultaneously. To\nprovide additional control over the generated scenario, this distribution is\nconditioned on a map and sets of tokens describing the desired scenario. We\nshow that our approach has sufficient expressive capacity to model diverse\ntraffic patterns and generalizes to different geographical regions.\n","authors":["Ethan Pronovost","Meghana Reddy Ganesina","Noureldin Hendy","Zeyu Wang","Andres Morales","Kai Wang","Nicholas Roy"],"pdf_url":"https://arxiv.org/pdf/2311.02738v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.06668v2","updated":"2023-11-16T22:39:00Z","published":"2023-11-11T21:19:44Z","title":"In-context Vectors: Making In Context Learning More Effective and\n  Controllable Through Latent Space Steering","summary":"  Large language models (LLMs) demonstrate emergent in-context learning\ncapabilities, where they adapt to new tasks based on example demonstrations.\nHowever, in-context learning has seen limited effectiveness in many settings,\nis difficult to quantitatively control and takes up context window space. To\novercome these limitations, we propose an alternative approach that recasts\nin-context learning as in-context vectors (ICV). Using ICV has two steps. We\nfirst use a forward pass on demonstration examples to create the in-context\nvector from the latent embedding of the LLM. This vector captures essential\ninformation about the intended task. On a new query, instead of adding\ndemonstrations to the prompt, we shift the latent states of the LLM using the\nICV. The ICV approach has several benefits: 1) it enables the LLM to more\neffectively follow the demonstration examples; 2) it's easy to control by\nadjusting the magnitude of the ICV; 3) it reduces the length of the prompt by\nremoving the in-context demonstrations; 4) ICV is computationally much more\nefficient than fine-tuning. We demonstrate that ICV achieves better performance\ncompared to standard in-context learning and fine-tuning on diverse tasks\nincluding safety, style transfer, role-playing and formatting. Moreover, we\nshow that we can flexibly teach LLM to simultaneously follow different types of\ninstructions by simple vector arithmetics on the corresponding ICVs.\n","authors":["Sheng Liu","Lei Xing","James Zou"],"pdf_url":"https://arxiv.org/pdf/2311.06668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10224v1","updated":"2023-11-16T22:31:05Z","published":"2023-11-16T22:31:05Z","title":"CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular\n  Segmentation of Enhanced TOF-MRA Images","summary":"  Due to the lack of automated methods, to diagnose cerebrovascular disease,\ntime-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually,\nmaking it time-consuming. The commonly used encoder-decoder architectures for\ncerebrovascular segmentation utilize redundant features, eventually leading to\nthe extraction of low-level features multiple times. Additionally,\nconvolutional neural networks (CNNs) suffer from performance degradation when\nthe batch size is small, and deeper networks experience the vanishing gradient\nproblem. Methods: In this paper, we attempt to solve these limitations and\npropose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet,\nfor precise extraction of brain vessel images. We proposed a sequence of\npreprocessing techniques followed by deeply supervised UNet to improve the\naccuracy of segmentation of the brain vessels leading to a stroke. To combine\nthe low and high semantics, we applied the attention mechanism. This mechanism\nfocuses on relevant associations and neglects irrelevant anatomical\ninformation. Furthermore, the inclusion of deep supervision incorporates\ndifferent levels of features that prove to be beneficial for network\nconvergence. Results: We demonstrate the efficiency of the proposed method by\ncross-validating with an unlabeled dataset, which was further labeled by us. We\nbelieve that the novelty of this algorithm lies in its ability to perform well\non both labeled and unlabeled data with image processing-based enhancement. The\nresults indicate that our method performed better than the existing\nstate-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method\nwill help in accurate segmentation of cerebrovascular structure leading to\nstroke\n","authors":["Syed Farhan Abbas","Nguyen Thanh Duc","Yoonguu Song","Kyungwon Kim","Boreom Lee"],"pdf_url":"https://arxiv.org/pdf/2311.10224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10223v1","updated":"2023-11-16T22:28:38Z","published":"2023-11-16T22:28:38Z","title":"Asymptotically Fair Participation in Machine Learning Models: an Optimal\n  Control Perspective","summary":"  The performance of state-of-the-art machine learning models often\ndeteriorates when testing on demographics that are under-represented in the\ntraining dataset. This problem has predominately been studied in a supervised\nlearning setting where the data distribution is static. However, real-world\napplications often involve distribution shifts caused by the deployed models.\nFor instance, the performance disparity against monitory users can lead to a\nhigh customer churn rate, thus the available data provided by active users are\nskewed due to the lack of minority users. This feedback effect further\nexacerbates the disparity among different demographic groups in future steps.\nTo address this issue, we propose asymptotically fair participation as a\ncondition to maintain long-term model performance over all demographic groups.\nIn this work, we aim to address the problem of achieving asymptotically fair\nparticipation via optimal control formulation. Moreover, we design a surrogate\nretention system based on existing literature on evolutionary population\ndynamics to approximate the dynamics of distribution shifts on active user\ncounts, from which the objective of achieving asymptotically fair participation\nis formulated as an optimal control problem, and the control variables are\nconsidered as the model parameters. We apply an efficient implementation of\nPontryagin's maximum principle to estimate the optimal control solution. To\nevaluate the effectiveness of the proposed method, we design a generic\nsimulation environment that simulates the population dynamics of the feedback\neffect between user retention and model performance. When we deploy the\nresulting models to the simulation environment, the optimal control solution\naccounts for long-term planning and leads to superior performance compared with\nexisting baseline methods.\n","authors":["Zhuotong Chen","Qianxiao Li","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.10223v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2311.10207v1","updated":"2023-11-16T21:43:05Z","published":"2023-11-16T21:43:05Z","title":"Stella Nera: Achieving 161 TOp/s/W with Multiplier-free DNN Acceleration\n  based on Approximate Matrix Multiplication","summary":"  From classical HPC to deep learning, MatMul is at the heart of today's\ncomputing. The recent Maddness method approximates MatMul without the need for\nmultiplication by using a hash-based version of product quantization (PQ)\nindexing into a look-up table (LUT). Stella Nera is the first Maddness\naccelerator and it achieves 15x higher area efficiency (GMAC/s/mm^2) and more\nthan 25x higher energy efficiency (TMAC/s/W) than direct MatMul accelerators\nimplemented in the same technology. The hash function is a decision tree, which\nallows for an efficient hardware implementation as the multiply-accumulate\noperations are replaced by decision tree passes and LUT lookups. The entire\nMaddness MatMul can be broken down into parts that allow an effective\nimplementation with small computing units and memories, allowing it to reach\nextreme efficiency while remaining generically applicable for MatMul tasks. In\na commercial 14nm technology and scaled to 3nm, we achieve an energy efficiency\nof 161 TOp/s/W@0.55V with a Top-1 accuracy on CIFAR-10 of more than 92.5% using\nResNet9.\n","authors":["Jannis Schönleber","Lukas Cavigelli","Renzo Andri","Matteo Perotti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2311.10207v1.pdf","comment":"6 pages, 7 figures, preprint under review"},{"id":"http://arxiv.org/abs/2311.10206v1","updated":"2023-11-16T21:39:54Z","published":"2023-11-16T21:39:54Z","title":"Bayes in the age of intelligent machines","summary":"  The success of methods based on artificial neural networks in creating\nintelligent machines seems like it might pose a challenge to explanations of\nhuman cognition in terms of Bayesian inference. We argue that this is not the\ncase, and that in fact these systems offer new opportunities for Bayesian\nmodeling. Specifically, we argue that Bayesian models of cognition and\nartificial neural networks lie at different levels of analysis and are\ncomplementary modeling approaches, together offering a way to understand human\ncognition that spans these levels. We also argue that the same perspective can\nbe applied to intelligent machines, where a Bayesian approach may be uniquely\nvaluable in understanding the behavior of large, opaque artificial neural\nnetworks that are trained on proprietary data.\n","authors":["Thomas L. Griffiths","Jian-Qiao Zhu","Erin Grant","R. Thomas McCoy"],"pdf_url":"https://arxiv.org/pdf/2311.10206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03488v2","updated":"2023-11-16T21:31:59Z","published":"2023-11-06T19:52:55Z","title":"Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems","summary":"  While recommender systems have become an integral component of the Web\nexperience, their heavy reliance on user data raises privacy and security\nconcerns. Substituting user data with synthetic data can address these\nconcerns, but accurately replicating these real-world datasets has been a\nnotoriously challenging problem. Recent advancements in generative AI have\ndemonstrated the impressive capabilities of diffusion models in generating\nrealistic data across various domains. In this work we introduce a Score-based\nDiffusion Recommendation Module (SDRM), which captures the intricate patterns\nof real-world datasets required for training highly accurate recommender\nsystems. SDRM allows for the generation of synthetic data that can replace\nexisting datasets to preserve user privacy, or augment existing datasets to\naddress excessive data sparsity. Our method outperforms competing baselines\nsuch as generative adversarial networks, variational autoencoders, and recently\nproposed diffusion models in synthesizing various datasets to replace or\naugment the original data by an average improvement of 4.30% in Recall@$k$ and\n4.65% in NDCG@$k$.\n","authors":["Derek Lilienthal","Paul Mello","Magdalini Eirinaki","Stas Tiomkin"],"pdf_url":"https://arxiv.org/pdf/2311.03488v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.14789v2","updated":"2023-11-16T21:27:10Z","published":"2023-08-28T18:00:00Z","title":"Scattering with Neural Operators","summary":"  Recent advances in machine learning establish the ability of certain\nneural-network architectures called neural operators to approximate maps\nbetween function spaces. Motivated by a prospect of employing them in\nfundamental physics, we examine applications to scattering processes in quantum\nmechanics. We use an iterated variant of Fourier neural operators to learn the\nphysics of Schr\\\"odinger operators, which map from the space of initial wave\nfunctions and potentials to the final wave functions. These deep operator\nlearning ideas are put to test in two concrete problems: a neural operator\npredicting the time evolution of a wave packet scattering off a central\npotential in $1+1$ dimensions, and the double-slit experiment in $2+1$\ndimensions. At inference, neural operators can become orders of magnitude more\nefficient compared to traditional finite-difference solvers.\n","authors":["Sebastian Mizera"],"pdf_url":"https://arxiv.org/pdf/2308.14789v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2311.10203v1","updated":"2023-11-16T21:22:47Z","published":"2023-11-16T21:22:47Z","title":"Adaptive Optimization Algorithms for Machine Learning","summary":"  Machine learning assumes a pivotal role in our data-driven world. The\nincreasing scale of models and datasets necessitates quick and reliable\nalgorithms for model training. This dissertation investigates adaptivity in\nmachine learning optimizers. The ensuing chapters are dedicated to various\nfacets of adaptivity, including: 1. personalization and user-specific models\nvia personalized loss, 2. provable post-training model adaptations via\nmeta-learning, 3. learning unknown hyperparameters in real time via\nhyperparameter variance reduction, 4. fast O(1/k^2) global convergence of\nsecond-order methods via stepsized Newton method regardless of the\ninitialization and choice basis, 5. fast and scalable second-order methods via\nlow-dimensional updates. This thesis contributes novel insights, introduces new\nalgorithms with improved convergence guarantees, and improves analyses of\npopular practical algorithms.\n","authors":["Slavomír Hanzely"],"pdf_url":"https://arxiv.org/pdf/2311.10203v1.pdf","comment":"Dissertation thesis"},{"id":"http://arxiv.org/abs/2010.05995v2","updated":"2023-11-16T21:08:05Z","published":"2020-10-12T19:47:09Z","title":"A Skew-Sensitive Evaluation Framework for Imbalanced Data Classification","summary":"  Class distribution skews in imbalanced datasets may lead to models with\nprediction bias towards majority classes, making fair assessment of classifiers\na challenging task. Metrics such as Balanced Accuracy are commonly used to\nevaluate a classifier's prediction performance under such scenarios. However,\nthese metrics fall short when classes vary in importance. In this paper, we\npropose a simple and general-purpose evaluation framework for imbalanced data\nclassification that is sensitive to arbitrary skews in class cardinalities and\nimportances. Experiments with several state-of-the-art classifiers tested on\nreal-world datasets from three different domains show the effectiveness of our\nframework - not only in evaluating and ranking classifiers, but also training\nthem.\n","authors":["Min Du","Nesime Tatbul","Brian Rivers","Akhilesh Kumar Gupta","Lucas Hu","Wei Wang","Ryan Marcus","Shengtian Zhou","Insup Lee","Justin Gottschlich"],"pdf_url":"https://arxiv.org/pdf/2010.05995v2.pdf","comment":"17 pages, Data-centric Machine Learning Research (DMLR) Workshop at\n  ICML 2023"},{"id":"http://arxiv.org/abs/2303.17491v3","updated":"2023-11-16T20:15:14Z","published":"2023-03-30T16:01:52Z","title":"Language Models can Solve Computer Tasks","summary":"  Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent Recursively Criticizes\nand Improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. We compare multiple LLMs and find that RCI with the\nInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful\nof demonstrations per task rather than tens of thousands, and without a\ntask-specific reward function. Furthermore, we demonstrate RCI prompting's\neffectiveness in enhancing LLMs' reasoning abilities on a suite of natural\nlanguage reasoning tasks, outperforming chain of thought (CoT) prompting with\nexternal feedback. We find that RCI combined with CoT performs better than\neither separately. Our code can be found here:\nhttps://github.com/posgnu/rci-agent.\n","authors":["Geunwoo Kim","Pierre Baldi","Stephen McAleer"],"pdf_url":"https://arxiv.org/pdf/2303.17491v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10177v1","updated":"2023-11-16T20:09:47Z","published":"2023-11-16T20:09:47Z","title":"Towards Improving Robustness Against Common Corruptions using Mixture of\n  Class Specific Experts","summary":"  Neural networks have demonstrated significant accuracy across various\ndomains, yet their vulnerability to subtle input alterations remains a\npersistent challenge. Conventional methods like data augmentation, while\neffective to some extent, fall short in addressing unforeseen corruptions,\nlimiting the adaptability of neural networks in real-world scenarios. In\nresponse, this paper introduces a novel paradigm known as the Mixture of\nClass-Specific Expert Architecture. The approach involves disentangling feature\nlearning for individual classes, offering a nuanced enhancement in scalability\nand overall performance. By training dedicated network segments for each class\nand subsequently aggregating their outputs, the proposed architecture aims to\nmitigate vulnerabilities associated with common neural network structures. The\nstudy underscores the importance of comprehensive evaluation methodologies,\nadvocating for the incorporation of benchmarks like the common corruptions\nbenchmark. This inclusion provides nuanced insights into the vulnerabilities of\nneural networks, especially concerning their generalization capabilities and\nrobustness to unforeseen distortions. The research aligns with the broader\nobjective of advancing the development of highly robust learning systems\ncapable of nuanced reasoning across diverse and challenging real-world\nscenarios. Through this contribution, the paper aims to foster a deeper\nunderstanding of neural network limitations and proposes a practical approach\nto enhance their resilience in the face of evolving and unpredictable\nconditions.\n","authors":["Shashank Kotyan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2311.10177v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.07928;\n  text overlap with arXiv:1903.12261 by other authors"},{"id":"http://arxiv.org/abs/2311.10170v1","updated":"2023-11-16T19:53:35Z","published":"2023-11-16T19:53:35Z","title":"Improving Unimodal Inference with Multimodal Transformers","summary":"  This paper proposes an approach for improving performance of unimodal models\nwith multimodal training. Our approach involves a multi-branch architecture\nthat incorporates unimodal models with a multimodal transformer-based branch.\nBy co-training these branches, the stronger multimodal branch can transfer its\nknowledge to the weaker unimodal branches through a multi-task objective,\nthereby improving the performance of the resulting unimodal models. We evaluate\nour approach on tasks of dynamic hand gesture recognition based on RGB and\nDepth, audiovisual emotion recognition based on speech and facial video, and\naudio-video-text based sentiment analysis. Our approach outperforms the\nconventionally trained unimodal counterparts. Interestingly, we also observe\nthat optimization of the unimodal branches improves the multimodal branch,\ncompared to a similar multimodal model trained from scratch.\n","authors":["Kateryna Chumachenko","Alexandros Iosifidis","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2311.10170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01607v3","updated":"2023-11-16T19:44:01Z","published":"2022-05-03T16:38:13Z","title":"Modeling and Correcting Bias in Sequential Evaluation","summary":"  We consider the problem of sequential evaluation, in which an evaluator\nobserves candidates in a sequence and assigns scores to these candidates in an\nonline, irrevocable fashion. Motivated by the psychology literature that has\nstudied sequential bias in such settings -- namely, dependencies between the\nevaluation outcome and the order in which the candidates appear -- we propose a\nnatural model for the evaluator's rating process that captures the lack of\ncalibration inherent to such a task. We conduct crowdsourcing experiments to\ndemonstrate various facets of our model. We then proceed to study how to\ncorrect sequential bias under our model by posing this as a statistical\ninference problem. We propose a near-linear time, online algorithm for this\ntask and prove guarantees in terms of two canonical ranking metrics. We also\nprove that our algorithm is information theoretically optimal, by establishing\nmatching lower bounds in both metrics. Finally, we perform a host of numerical\nexperiments to show that our algorithm often outperforms the de facto method of\nusing the rankings induced by the reported scores, both in simulation and on\nthe crowdsourcing data that we collected.\n","authors":["Jingyan Wang","Ashwin Pananjady"],"pdf_url":"https://arxiv.org/pdf/2205.01607v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10162v1","updated":"2023-11-16T19:34:18Z","published":"2023-11-16T19:34:18Z","title":"K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without\n  Noise","summary":"  Deep learning-based MRI reconstruction models have achieved superior\nperformance these days. Most recently, diffusion models have shown remarkable\nperformance in image generation, in-painting, super-resolution, image editing\nand more. As a generalized diffusion model, cold diffusion further broadens the\nscope and considers models built around arbitrary image transformations such as\nblurring, down-sampling, etc. In this paper, we propose a k-space cold\ndiffusion model that performs image degradation and restoration in k-space\nwithout the need for Gaussian noise. We provide comparisons with multiple deep\nlearning-based MRI reconstruction models and perform tests on a well-known\nlarge open-source MRI dataset. Our results show that this novel way of\nperforming degradation can generate high-quality reconstruction images for\naccelerated MRI.\n","authors":["Guoyao Shen","Mengyu Li","Chad W. Farris","Stephan Anderson","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.10162v1.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2311.10156v1","updated":"2023-11-16T19:24:20Z","published":"2023-11-16T19:24:20Z","title":"Algebraic Topological Networks via the Persistent Local Homology Sheaf","summary":"  In this work, we introduce a novel approach based on algebraic topology to\nenhance graph convolution and attention modules by incorporating local\ntopological properties of the data. To do so, we consider the framework of\nsheaf neural networks, which has been previously leveraged to incorporate\nadditional structure into graph neural networks' features and construct more\nexpressive, non-isotropic messages. Specifically, given an input simplicial\ncomplex (e.g. generated by the cliques of a graph or the neighbors in a point\ncloud), we construct its local homology sheaf, which assigns to each node the\nvector space of its local homology. The intermediate features of our networks\nlive in these vector spaces and we leverage the associated sheaf Laplacian to\nconstruct more complex linear messages between them. Moreover, we extend this\napproach by considering the persistent version of local homology associated\nwith a weighted simplicial complex (e.g., built from pairwise distances of\nnodes embeddings). This i) solves the problem of the lack of a natural choice\nof basis for the local homology vector spaces and ii) makes the sheaf itself\ndifferentiable, which enables our models to directly optimize the topology of\ntheir intermediate features.\n","authors":["Gabriele Cesa","Arash Behboodi"],"pdf_url":"https://arxiv.org/pdf/2311.10156v1.pdf","comment":"Symmetry and Geometry in Neural Representations - NeurReps Workshop @\n  NeurIPS 2023"},{"id":"http://arxiv.org/abs/2302.11961v2","updated":"2023-11-16T19:14:56Z","published":"2023-02-23T12:17:36Z","title":"Sharp Calibrated Gaussian Processes","summary":"  While Gaussian processes are a mainstay for various engineering and\nscientific applications, the uncertainty estimates don't satisfy frequentist\nguarantees and can be miscalibrated in practice. State-of-the-art approaches\nfor designing calibrated models rely on inflating the Gaussian process\nposterior variance, which yields confidence intervals that are potentially too\ncoarse. To remedy this, we present a calibration approach that generates\npredictive quantiles using a computation inspired by the vanilla Gaussian\nprocess posterior variance but using a different set of hyperparameters chosen\nto satisfy an empirical calibration constraint. This results in a calibration\napproach that is considerably more flexible than existing approaches, which we\noptimize to yield tight predictive quantiles. Our approach is shown to yield a\ncalibrated model under reasonable assumptions. Furthermore, it outperforms\nexisting approaches in sharpness when employed for calibrated regression.\n","authors":["Alexandre Capone","Geoff Pleiss","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2302.11961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.12737v3","updated":"2023-11-16T19:09:50Z","published":"2023-04-25T11:27:27Z","title":"NPRL: Nightly Profile Representation Learning for Early Sepsis Onset\n  Prediction in ICU Trauma Patients","summary":"  Sepsis is a syndrome that develops in the body in response to the presence of\nan infection. Characterized by severe organ dysfunction, sepsis is one of the\nleading causes of mortality in Intensive Care Units (ICUs) worldwide. These\ncomplications can be reduced through early application of antibiotics. Hence,\nthe ability to anticipate the onset of sepsis early is crucial to the survival\nand well-being of patients. Current machine learning algorithms deployed inside\nmedical infrastructures have demonstrated poor performance and are insufficient\nfor anticipating sepsis onset early. Recently, deep learning methodologies have\nbeen proposed to predict sepsis, but some fail to capture the time of onset\n(e.g., classifying patients' entire visits as developing sepsis or not) and\nothers are unrealistic for deployment in clinical settings (e.g., creating\ntraining instances using a fixed time to onset, where the time of onset needs\nto be known apriori). In this paper, we first propose a novel but realistic\nprediction framework that predicts each morning whether sepsis onset will occur\nwithin the next 24 hours using the most recent data collected the previous\nnight, when patient-provider ratios are higher due to cross-coverage resulting\nin limited observation to each patient. However, as we increase the prediction\nrate into daily, the number of negative instances will increase, while that of\npositive instances remain the same. This causes a severe class imbalance\nproblem making it hard to capture these rare sepsis cases. To address this, we\npropose a nightly profile representation learning (NPRL) approach. We prove\nthat NPRL can theoretically alleviate the rare event problem and our empirical\nstudy using data from a level-1 trauma center demonstrates the effectiveness of\nour proposal.\n","authors":["Tucker Stewart","Katherine Stern","Grant O'Keefe","Ankur Teredesai","Juhua Hu"],"pdf_url":"https://arxiv.org/pdf/2304.12737v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.06978v2","updated":"2023-11-16T17:59:20Z","published":"2023-09-13T14:13:08Z","title":"Differentiable JPEG: The Devil is in the Details","summary":"  JPEG remains one of the most widespread lossy image coding methods. However,\nthe non-differentiable nature of JPEG restricts the application in deep\nlearning pipelines. Several differentiable approximations of JPEG have recently\nbeen proposed to address this issue. This paper conducts a comprehensive review\nof existing diff. JPEG approaches and identifies critical details that have\nbeen missed by previous methods. To this end, we propose a novel diff. JPEG\napproach, overcoming previous limitations. Our approach is differentiable\nw.r.t. the input image, the JPEG quality, the quantization tables, and the\ncolor conversion parameters. We evaluate the forward and backward performance\nof our diff. JPEG approach against existing methods. Additionally, extensive\nablations are performed to evaluate crucial design choices. Our proposed diff.\nJPEG resembles the (non-diff.) reference implementation best, significantly\nsurpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. For\nstrong compression rates, we can even improve PSNR by $9.51$dB. Strong\nadversarial attack results are yielded by our diff. JPEG, demonstrating the\neffective gradient approximation. Our code is available at\nhttps://github.com/necla-ml/Diff-JPEG.\n","authors":["Christoph Reich","Biplob Debnath","Deep Patel","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2309.06978v2.pdf","comment":"Accepted at WACV 2024. Project page:\n  https://christophreich1996.github.io/differentiable_jpeg/"},{"id":"http://arxiv.org/abs/2311.09939v1","updated":"2023-11-16T14:43:45Z","published":"2023-11-16T14:43:45Z","title":"RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection","summary":"  Online misinformation is often multimodal in nature, i.e., it is caused by\nmisleading associations between texts and accompanying images. To support the\nfact-checking process, researchers have been recently developing automatic\nmultimodal methods that gather and analyze external information, evidence,\nrelated to the image-text pairs under examination. However, prior works assumed\nall collected evidence to be relevant. In this study, we introduce a \"Relevant\nEvidence Detection\" (RED) module to discern whether each piece of evidence is\nrelevant, to support or refute the claim. Specifically, we develop the\n\"Relevant Evidence Detection Directed Transformer\" (RED-DOT) and explore\nmultiple architectural variants (e.g., single or dual-stage) and mechanisms\n(e.g., \"guided attention\"). Extensive ablation and comparative experiments\ndemonstrate that RED-DOT achieves significant improvements over the\nstate-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our\nevidence re-ranking and element-wise modality fusion led to RED-DOT achieving\ncompetitive and even improved performance on NewsCLIPings+, without the need\nfor numerous evidence or multiple backbone encoders. Finally, our qualitative\nanalysis demonstrates that the proposed \"guided attention\" module has the\npotential to enhance the architecture's interpretability. We release our code\nat: https://github.com/stevejpapad/relevant-evidence-detection\n","authors":["Stefanos-Iordanis Papadopoulos","Christos Koutlis","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2311.09939v1.pdf","comment":null}]},"2023-11-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.10702v1","updated":"2023-11-17T18:45:45Z","published":"2023-11-17T18:45:45Z","title":"Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2","summary":"  Since the release of T\\\"ULU [Wang et al., 2023b], open resources for\ninstruction tuning have developed quickly, from better base models to new\nfinetuning techniques. We test and incorporate a number of these advances into\nT\\\"ULU, resulting in T\\\"ULU 2, a suite of improved T\\\"ULU models for advancing\nthe understanding and best practices of adapting pretrained language models to\ndownstream tasks and user preferences. Concretely, we release: (1)\nT\\\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2)\nT\\\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\\\"ULU 2+DPO, T\\\"ULU\n2 models trained with direct preference optimization (DPO), including the\nlargest DPO-trained model to date (T\\\"ULU 2+DPO 70B); (4) CODE T\\\"ULU 2, CODE\nLLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its\ninstruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple\nperspectives shows that the T\\\"ULU 2 suite achieves state-of-the-art\nperformance among open models and matches or exceeds the performance of\nGPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data,\ntraining and evaluation code to facilitate future open efforts on adapting\nlarge language models.\n","authors":["Hamish Ivison","Yizhong Wang","Valentina Pyatkin","Nathan Lambert","Matthew Peters","Pradeep Dasigi","Joel Jang","David Wadden","Noah A. Smith","Iz Beltagy","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2311.10702v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2308.06595v3","updated":"2023-11-17T18:39:46Z","published":"2023-08-12T15:27:51Z","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following\n  Inspired by Real-World Use","summary":"  We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for\nevaluation of instruction-following vision-language models for real-world use.\nOur starting point is curating 70 'instruction families' that we envision\ninstruction tuned vision-language models should be able to address. Extending\nbeyond evaluations like VQAv2 and COCO, tasks range from basic recognition to\ngame playing and creative generation. Following curation, our dataset comprises\n592 test queries, each with a human-authored instruction-conditioned caption.\nThese descriptions surface instruction-specific factors, e.g., for an\ninstruction asking about the accessibility of a storefront for wheelchair\nusers, the instruction-conditioned caption describes ramps/potential obstacles.\nThese descriptions enable 1) collecting human-verified reference outputs for\neach instance; and 2) automatic evaluation of candidate multimodal generations\nusing a text-only LLM, aligning with human judgment. We quantify quality gaps\nbetween models and references using both human and automatic evaluations; e.g.,\nthe top-performing instruction-following model wins against the GPT-4 reference\nin just 27% of the comparison. VisIT-Bench is dynamic to participate,\npractitioners simply submit their model's response on the project website;\nData, code and leaderboard is available at visit-bench.github.io.\n","authors":["Yonatan Bitton","Hritik Bansal","Jack Hessel","Rulin Shao","Wanrong Zhu","Anas Awadalla","Josh Gardner","Rohan Taori","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2308.06595v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10697v1","updated":"2023-11-17T18:32:17Z","published":"2023-11-17T18:32:17Z","title":"PEFT-MedAware: Large Language Model for Medical Awareness","summary":"  Chat models are capable of answering a wide range of questions, however, the\naccuracy of their responses is highly uncertain. In this research, we propose a\nspecialized PEFT-MedAware model where we utilize parameter-efficient\nfine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized\nMedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of\nits trainable parameters to enhance computational efficiency. The paper adopts\ndata preprocessing and PEFT to optimize model performance, complemented by a\nBitsAndBytesConfig for efficient transformer training. The resulting model was\ncapable of outperforming other LLMs in medical question-answering tasks in\nspecific domains with greater accuracy utilizing limited computational\nresources making it suitable for deployment in resource-constrained\nenvironments. We propose further improvements through expanded datasets, larger\nmodels, and feedback mechanisms for sustained medical relevancy. Our work\nhighlights the efficiency gains and specialized capabilities of PEFT in medical\nAI, outpacing standard models in precision without extensive resource demands.\nThe proposed model and data are released for research purposes only.\n","authors":["Keivalya Pandya"],"pdf_url":"https://arxiv.org/pdf/2311.10697v1.pdf","comment":"7 pages, 1 figure, submitted to the Artificial Intelligence in\n  Medicine Journal"},{"id":"http://arxiv.org/abs/2305.14659v2","updated":"2023-11-17T17:31:52Z","published":"2023-05-24T02:53:22Z","title":"InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration\n  in Improving the Performance of Information Extraction","summary":"  Learning template based information extraction from documents is a crucial\nyet difficult task. Prior template-based IE approaches assume foreknowledge of\nthe domain templates; however, real-world IE do not have pre-defined schemas\nand it is a figure-out-as you go phenomena. To quickly bootstrap templates in a\nreal-world setting, we need to induce template slots from documents with zero\nor minimal supervision. Since the purpose of question answering intersect with\nthe goal of information extraction, we use automatic question generation to\ninduce template slots from the documents and investigate how a tiny amount of a\nproxy human-supervision on-the-fly (termed as InteractiveIE) can further boost\nthe performance. Extensive experiments on biomedical and legal documents, where\nobtaining training data is expensive, reveal encouraging trends of performance\nimprovement using InteractiveIE over AI-only baseline.\n","authors":["Ishani Mondal","Michelle Yuan","Anandhavelu N","Aparna Garimella","Francis Ferraro","Andrew Blair-Stanek","Benjamin Van Durme","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2305.14659v2.pdf","comment":"Version 2"},{"id":"http://arxiv.org/abs/2311.10642v1","updated":"2023-11-17T16:58:52Z","published":"2023-11-17T16:58:52Z","title":"Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as\n  an Alternative to Attention Layers in Transformers","summary":"  This work presents an analysis of the effectiveness of using standard shallow\nfeed-forward networks to mimic the behavior of the attention mechanism in the\noriginal Transformer model, a state-of-the-art architecture for\nsequence-to-sequence tasks. We substitute key elements of the attention\nmechanism in the Transformer with simple feed-forward networks, trained using\nthe original components via knowledge distillation. Our experiments, conducted\non the IWSLT2017 dataset, reveal the capacity of these \"attentionless\nTransformers\" to rival the performance of the original architecture. Through\nrigorous ablation studies, and experimenting with various replacement network\ntypes and sizes, we offer insights that support the viability of our approach.\nThis not only sheds light on the adaptability of shallow feed-forward networks\nin emulating attention mechanisms but also underscores their potential to\nstreamline complex architectures for sequence-to-sequence tasks.\n","authors":["Vukasin Bozic","Danilo Dordervic","Daniele Coppola","Joseph Thommes"],"pdf_url":"https://arxiv.org/pdf/2311.10642v1.pdf","comment":"Accepted at AAAI24(https://aaai.org/aaai-conference/)"},{"id":"http://arxiv.org/abs/2203.08436v2","updated":"2023-11-17T16:46:38Z","published":"2022-03-16T07:13:52Z","title":"Don't Say What You Don't Know: Improving the Consistency of Abstractive\n  Summarization by Constraining Beam Search","summary":"  Abstractive summarization systems today produce fluent and relevant output,\nbut often \"hallucinate\" statements not supported by the source text. We analyze\nthe connection between hallucinations and training data, and find evidence that\nmodels hallucinate because they train on target summaries that are unsupported\nby the source. Based on our findings, we present PINOCCHIO, a new decoding\nmethod that improves the consistency of a transformer-based abstractive\nsummarizer by constraining beam search to avoid hallucinations. Given the model\nstates and outputs at a given step, PINOCCHIO detects likely model\nhallucinations based on various measures of attribution to the source text.\nPINOCCHIO backtracks to find more consistent output, and can opt to produce no\nsummary at all when no consistent generation can be found. In experiments, we\nfind that PINOCCHIO improves the consistency of generation (in terms of F1) by\nan average of~67% on two abstractive summarization datasets.\n","authors":["Daniel King","Zejiang Shen","Nishant Subramani","Daniel S. Weld","Iz Beltagy","Doug Downey"],"pdf_url":"https://arxiv.org/pdf/2203.08436v2.pdf","comment":"16 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2311.10614v1","updated":"2023-11-17T16:09:10Z","published":"2023-11-17T16:09:10Z","title":"A Self-enhancement Approach for Domain-specific Chatbot Training via\n  Knowledge Mining and Digest","summary":"  Large Language Models (LLMs), despite their great power in language\ngeneration, often encounter challenges when dealing with intricate and\nknowledge-demanding queries in specific domains. This paper introduces a novel\napproach to enhance LLMs by effectively extracting the relevant knowledge from\ndomain-specific textual sources, and the adaptive training of a chatbot with\ndomain-specific inquiries. Our two-step approach starts from training a\nknowledge miner, namely LLMiner, which autonomously extracts Question-Answer\npairs from relevant documents through a chain-of-thought reasoning process.\nSubsequently, we blend the mined QA pairs with a conversational dataset to\nfine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise\nand conversational capabilities. We also developed a new evaluation benchmark\nwhich comprises four domain-specific text corpora and associated human-crafted\nQA pairs for testing. Our model shows remarkable performance improvement over\ngenerally aligned LLM and surpasses domain-adapted models directly fine-tuned\non domain corpus. In particular, LLMiner achieves this with minimal human\nintervention, requiring only 600 seed instances, thereby providing a pathway\ntowards self-improvement of LLMs through model-synthesized training data.\n","authors":["Ruohong Zhang","Luyu Gao","Chen Zheng","Zhen Fan","Guokun Lai","Zheng Zhang","Fangzhou Ai","Yiming Yang","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2311.10614v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.10596v1","updated":"2023-11-17T15:49:11Z","published":"2023-11-17T15:49:11Z","title":"Hashing it Out: Predicting Unhealthy Conversations on Twitter","summary":"  Personal attacks in the context of social media conversations often lead to\nfast-paced derailment, leading to even more harmful exchanges being made.\nState-of-the-art systems for the detection of such conversational derailment\noften make use of deep learning approaches for prediction purposes. In this\npaper, we show that an Attention-based BERT architecture, pre-trained on a\nlarge Twitter corpus and fine-tuned on our task, is efficient and effective in\nmaking such predictions. This model shows clear advantages in performance to\nthe existing LSTM model we use as a baseline. Additionally, we show that this\nimpressive performance can be attained through fine-tuning on a relatively\nsmall, novel dataset, particularly after mitigating overfitting issues through\nsynthetic oversampling techniques. By introducing the first transformer based\nmodel for forecasting conversational events on Twitter, this work lays the\nfoundation for a practical tool to encourage better interactions on one of the\nmost ubiquitous social media platforms.\n","authors":["Steven Leung","Filippos Papapolyzos"],"pdf_url":"https://arxiv.org/pdf/2311.10596v1.pdf","comment":"7 pages, 3 figures, academic"},{"id":"http://arxiv.org/abs/2309.15991v2","updated":"2023-11-17T15:47:35Z","published":"2023-09-27T20:12:41Z","title":"Targeted Image Data Augmentation Increases Basic Skills Captioning\n  Robustness","summary":"  Artificial neural networks typically struggle in generalizing to\nout-of-context examples. One reason for this limitation is caused by having\ndatasets that incorporate only partial information regarding the potential\ncorrelational structure of the world. In this work, we propose TIDA (Targeted\nImage-editing Data Augmentation), a targeted data augmentation method focused\non improving models' human-like abilities (e.g., gender recognition) by filling\nthe correlational structure gap using a text-to-image generative model. More\nspecifically, TIDA identifies specific skills in captions describing images\n(e.g., the presence of a specific gender in the image), changes the caption\n(e.g., \"woman\" to \"man\"), and then uses a text-to-image model to edit the image\nin order to match the novel caption (e.g., uniquely changing a woman to a man\nwhile maintaining the context identical). Based on the Flickr30K benchmark, we\nshow that, compared with the original data set, a TIDA-enhanced dataset related\nto gender, color, and counting abilities induces better performance in several\nimage captioning metrics. Furthermore, on top of relying on the classical BLEU\nmetric, we conduct a fine-grained analysis of the improvements of our models\nagainst the baseline in different ways. We compared text-to-image generative\nmodels and found different behaviors of the image captioning models in terms of\nencoding visual encoding and textual decoding.\n","authors":["Valentin Barriere","Felipe del Rio","Andres Carvallo De Ferari","Carlos Aspillaga","Eugenio Herrera-Berg","Cristian Buc Calderon"],"pdf_url":"https://arxiv.org/pdf/2309.15991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10587v1","updated":"2023-11-17T15:37:18Z","published":"2023-11-17T15:37:18Z","title":"Countering Misinformation via Emotional Response Generation","summary":"  The proliferation of misinformation on social media platforms (SMPs) poses a\nsignificant danger to public health, social cohesion and ultimately democracy.\nPrevious research has shown how social correction can be an effective way to\ncurb misinformation, by engaging directly in a constructive dialogue with users\nwho spread -- often in good faith -- misleading messages. Although professional\nfact-checkers are crucial to debunking viral claims, they usually do not engage\nin conversations on social media. Thereby, significant effort has been made to\nautomate the use of fact-checker material in social correction; however, no\nprevious work has tried to integrate it with the style and pragmatics that are\ncommonly employed in social media communication. To fill this gap, we present\nVerMouth, the first large-scale dataset comprising roughly 12 thousand\nclaim-response pairs (linked to debunking articles), accounting for both\nSMP-style and basic emotions, two factors which have a significant role in\nmisinformation credibility and spreading. To collect this dataset we used a\ntechnique based on an author-reviewer pipeline, which efficiently combines LLMs\nand human annotators to obtain high-quality data. We also provide comprehensive\nexperiments showing how models trained on our proposed dataset have significant\nimprovements in terms of output quality and generalization capabilities.\n","authors":["Daniel Russo","Shane Peter Kaszefski-Yaschuk","Jacopo Staiano","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2311.10587v1.pdf","comment":"Accepted to EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2305.14937v2","updated":"2023-11-17T15:28:00Z","published":"2023-05-24T09:20:15Z","title":"A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking\n  Systems","summary":"  Existing evaluations of entity linking systems often say little about how the\nsystem is going to perform for a particular application. There are two\nfundamental reasons for this. One is that many evaluations only use aggregate\nmeasures (like precision, recall, and F1 score), without a detailed error\nanalysis or a closer look at the results. The other is that all of the widely\nused benchmarks have strong biases and artifacts, in particular: a strong focus\non named entities, an unclear or missing specification of what else counts as\nan entity mention, poor handling of ambiguities, and an over- or\nunderrepresentation of certain kinds of entities.\n  We provide a more meaningful and fair in-depth evaluation of a variety of\nexisting end-to-end entity linkers. We characterize their strengths and\nweaknesses and also report on reproducibility aspects. The detailed results of\nour evaluation can be inspected under\nhttps://elevant.cs.uni-freiburg.de/emnlp2023 . Our evaluation is based on\nseveral widely used benchmarks, which exhibit the problems mentioned above to\nvarious degrees, as well as on two new benchmarks, which address the problems\nmentioned above. The new benchmarks can be found under\nhttps://github.com/ad-freiburg/fair-entity-linking-benchmarks .\n","authors":["Hannah Bast","Matthias Hertel","Natalie Prange"],"pdf_url":"https://arxiv.org/pdf/2305.14937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04354v2","updated":"2023-11-17T15:15:17Z","published":"2023-11-07T21:27:17Z","title":"Uncovering Intermediate Variables in Transformers using Circuit Probing","summary":"  Neural network models have achieved high performance on a wide variety of\ncomplex tasks, but the algorithms that they implement are notoriously difficult\nto interpret. In order to understand these algorithms, it is often necessary to\nhypothesize intermediate variables involved in the network's computation. For\nexample, does a language model depend on particular syntactic properties when\ngenerating a sentence? However, existing analysis tools make it difficult to\ntest hypotheses of this type. We propose a new analysis technique -- circuit\nprobing -- that automatically uncovers low-level circuits that compute\nhypothesized intermediate variables. This enables causal analysis through\ntargeted ablation at the level of model parameters. We apply this method to\nmodels trained on simple arithmetic tasks, demonstrating its effectiveness at\n(1) deciphering the algorithms that models have learned, (2) revealing modular\nstructure within a model, and (3) tracking the development of circuits over\ntraining. We compare circuit probing to other methods across these three\nexperiments, and find it on par or more effective than existing analysis\nmethods. Finally, we demonstrate circuit probing on a real-world use case,\nuncovering circuits that are responsible for subject-verb agreement and\nreflexive anaphora in GPT2-Small and Medium.\n","authors":["Michael A. Lepori","Thomas Serre","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2311.04354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10541v1","updated":"2023-11-17T14:08:44Z","published":"2023-11-17T14:08:44Z","title":"Detection of Offensive and Threatening Online Content in a Low Resource\n  Language","summary":"  Hausa is a major Chadic language, spoken by over 100 million people in\nAfrica. However, from a computational linguistic perspective, it is considered\na low-resource language, with limited resources to support Natural Language\nProcessing (NLP) tasks. Online platforms often facilitate social interactions\nthat can lead to the use of offensive and threatening language, which can go\nundetected due to the lack of detection systems designed for Hausa. This study\naimed to address this issue by (1) conducting two user studies (n=308) to\ninvestigate cyberbullying-related issues, (2) collecting and annotating the\nfirst set of offensive and threatening datasets to support relevant downstream\ntasks in Hausa, (3) developing a detection system to flag offensive and\nthreatening content, and (4) evaluating the detection system and the efficacy\nof the Google-based translation engine in detecting offensive and threatening\nterms in Hausa. We found that offensive and threatening content is quite\ncommon, particularly when discussing religion and politics. Our detection\nsystem was able to detect more than 70% of offensive and threatening content,\nalthough many of these were mistranslated by Google's translation engine. We\nattribute this to the subtle relationship between offensive and threatening\ncontent and idiomatic expressions in the Hausa language. We recommend that\ndiverse stakeholders participate in understanding local conventions and\ndemographics in order to develop a more effective detection system. These\ninsights are essential for implementing targeted moderation strategies to\ncreate a safe and inclusive online environment.\n","authors":["Fatima Muhammad Adam","Abubakar Yakubu Zandam","Isa Inuwa-Dutse"],"pdf_url":"https://arxiv.org/pdf/2311.10541v1.pdf","comment":"25 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2311.10514v1","updated":"2023-11-17T13:35:10Z","published":"2023-11-17T13:35:10Z","title":"When a Language Question Is at Stake. A Revisited Approach to Label\n  Sensitive Content","summary":"  Many under-resourced languages require high-quality datasets for specific\ntasks such as offensive language detection, disinformation, or misinformation\nidentification. However, the intricacies of the content may have a detrimental\neffect on the annotators. The article aims to revisit an approach of\npseudo-labeling sensitive data on the example of Ukrainian tweets covering the\nRussian-Ukrainian war. Nowadays, this acute topic is in the spotlight of\nvarious language manipulations that cause numerous disinformation and profanity\non social media platforms. The conducted experiment highlights three main\nstages of data annotation and underlines the main obstacles during machine\nannotation. Ultimately, we provide a fundamental statistical analysis of the\nobtained data, evaluation of models used for pseudo-labelling, and set further\nguidelines on how the scientists can leverage the corpus to execute more\nadvanced research and extend the existing data samples without annotators'\nengagement.\n","authors":["Stetsenko Daria"],"pdf_url":"https://arxiv.org/pdf/2311.10514v1.pdf","comment":"Ukrainian language, pseudo-labelling, dataset, offensive-language"},{"id":"http://arxiv.org/abs/2311.10505v1","updated":"2023-11-17T13:10:58Z","published":"2023-11-17T13:10:58Z","title":"CNL2ASP: converting controlled natural language sentences into ASP","summary":"  Answer Set Programming (ASP) is a popular declarative programming language\nfor solving hard combinatorial problems. Although ASP has gained widespread\nacceptance in academic and industrial contexts, there are certain user groups\nwho may find it more advantageous to employ a higher-level language that\nclosely resembles natural language when specifying ASP programs. In this paper,\nwe propose a novel tool, called CNL2ASP, for translating English sentences\nexpressed in a controlled natural language (CNL) form into ASP. In particular,\nwe first provide a definition of the type of sentences allowed by our CNL and\ntheir translation as ASP rules, and then exemplify the usage of the CNL for the\nspecification of both synthetic and real-world combinatorial problems. Finally,\nwe report the results of an experimental analysis conducted on the real-world\nproblems to compare the performance of automatically generated encodings with\nthe ones written by ASP practitioners, showing that our tool can obtain\nsatisfactory performance on these benchmarks. Under consideration in Theory and\nPractice of Logic Programming (TPLP).\n","authors":["Simone Caruso","Carmine Dodaro","Marco Maratea","Marco Mochi","Francesco Riccio"],"pdf_url":"https://arxiv.org/pdf/2311.10505v1.pdf","comment":"Under consideration in Theory and Practice of Logic Programming\n  (TPLP)"},{"id":"http://arxiv.org/abs/2201.05613v3","updated":"2023-11-17T13:01:01Z","published":"2022-01-14T16:04:09Z","title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet","summary":"  Pre-trained Transformers are challenging human performances in many NLP\ntasks. The massive datasets used for pre-training seem to be the key to their\nsuccess on existing tasks. In this paper, we explore how a range of pre-trained\nNatural Language Understanding models perform on definitely unseen sentences\nprovided by classification tasks over a DarkNet corpus. Surprisingly, results\nshow that syntactic and lexical neural networks perform on par with pre-trained\nTransformers even after fine-tuning. Only after what we call extreme domain\nadaptation, that is, retraining with the masked language model task on all the\nnovel corpus, pre-trained Transformers reach their standard high results. This\nsuggests that huge pre-training corpora may give Transformers unexpected help\nsince they are exposed to many of the possible sentences.\n","authors":["Leonardo Ranaldi","Aria Nourbakhsh","Arianna Patrizi","Elena Sofia Ruzzetti","Dario Onorati","Francesca Fallucchi","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2201.05613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10436v1","updated":"2023-11-17T10:14:45Z","published":"2023-11-17T10:14:45Z","title":"Sinhala-English Word Embedding Alignment: Introducing Datasets and\n  Benchmark for a Low Resource Language","summary":"  Since their inception, embeddings have become a primary ingredient in many\nflavours of Natural Language Processing (NLP) tasks supplanting earlier types\nof representation. Even though multilingual embeddings have been used for the\nincreasing number of multilingual tasks, due to the scarcity of parallel\ntraining data, low-resource languages such as Sinhala, tend to focus more on\nmonolingual embeddings. Then when it comes to the aforementioned multi-lingual\ntasks, it is challenging to utilize these monolingual embeddings given that\neven if the embedding spaces have a similar geometric arrangement due to an\nidentical training process, the embeddings of the languages considered are not\naligned. This is solved by the embedding alignment task. Even in this,\nhigh-resource language pairs are in the limelight while low-resource languages\nsuch as Sinhala which is in dire need of help seem to have fallen by the\nwayside. In this paper, we try to align Sinhala and English word embedding\nspaces based on available alignment techniques and introduce a benchmark for\nSinhala language embedding alignment. In addition to that, to facilitate the\nsupervised alignment, as an intermediate task, we also introduce\nSinhala-English alignment datasets. These datasets serve as our anchor datasets\nfor supervised word embedding alignment. Even though we do not obtain results\ncomparable to the high-resource languages such as French, German, or Chinese,\nwe believe our work lays the groundwork for more specialized alignment between\nEnglish and Sinhala embeddings.\n","authors":["Kasun Wickramasinghe","Nisansa de Silva"],"pdf_url":"https://arxiv.org/pdf/2311.10436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10431v1","updated":"2023-11-17T10:09:12Z","published":"2023-11-17T10:09:12Z","title":"Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human\n  Narrative Processing","summary":"  Understanding how humans process natural language has long been a vital\nresearch direction. The field of natural language processing (NLP) has recently\nexperienced a surge in the development of powerful language models. These\nmodels have proven to be invaluable tools for studying another complex system\nknown to process human language: the brain. Previous studies have demonstrated\nthat the features of language models can be mapped to fMRI brain activity. This\nraises the question: is there a commonality between information processing in\nlanguage models and the human brain? To estimate information flow patterns in a\nlanguage model, we examined the causal relationships between different layers.\nDrawing inspiration from the workspace framework for consciousness, we\nhypothesized that features integrating more information would more accurately\npredict higher hierarchical brain activity. To validate this hypothesis, we\nclassified language model features into two categories based on causal network\nmeasures: 'low in-degree' and 'high in-degree'. We subsequently compared the\nbrain prediction accuracy maps for these two groups. Our results reveal that\nthe difference in prediction accuracy follows a hierarchical pattern,\nconsistent with the cortical hierarchy map revealed by activity time constants.\nThis finding suggests a parallel between how language models and the human\nbrain process linguistic information.\n","authors":["Zhengqi He","Taro Toyoizumi"],"pdf_url":"https://arxiv.org/pdf/2311.10431v1.pdf","comment":"15 pages, 16 figures"},{"id":"http://arxiv.org/abs/2207.08522v2","updated":"2023-11-17T09:13:27Z","published":"2022-07-18T11:37:47Z","title":"Classifying COVID-19 vaccine narratives","summary":"  Vaccine hesitancy is widespread, despite the government's information\ncampaigns and the efforts of the World Health Organisation (WHO). Categorising\nthe topics within vaccine-related narratives is crucial to understand the\nconcerns expressed in discussions and identify the specific issues that\ncontribute to vaccine hesitancy. This paper addresses the need for monitoring\nand analysing vaccine narratives online by introducing a novel vaccine\nnarrative classification task, which categorises COVID-19 vaccine claims into\none of seven categories. Following a data augmentation approach, we first\nconstruct a novel dataset for this new classification task, focusing on the\nminority classes. We also make use of fact-checker annotated data. The paper\nalso presents a neural vaccine narrative classifier that achieves an accuracy\nof 84% under cross-validation. The classifier is publicly available for\nresearchers and journalists.\n","authors":["Yue Li","Carolina Scarton","Xingyi Song","Kalina Bontcheva"],"pdf_url":"https://arxiv.org/pdf/2207.08522v2.pdf","comment":"In Proceedings of the 14th International Conference on Recent\n  Advances in Natural Language Processing, 2023"},{"id":"http://arxiv.org/abs/2311.10395v1","updated":"2023-11-17T08:56:13Z","published":"2023-11-17T08:56:13Z","title":"Bias A-head? Analyzing Bias in Transformer-Based Language Model\n  Attention Heads","summary":"  Transformer-based pretrained large language models (PLM) such as BERT and GPT\nhave achieved remarkable success in NLP tasks. However, PLMs are prone to\nencoding stereotypical biases. Although a burgeoning literature has emerged on\nstereotypical bias mitigation in PLMs, such as work on debiasing gender and\nracial stereotyping, how such biases manifest and behave internally within PLMs\nremains largely unknown. Understanding the internal stereotyping mechanisms may\nallow better assessment of model fairness and guide the development of\neffective mitigation strategies. In this work, we focus on attention heads, a\nmajor component of the Transformer architecture, and propose a bias analysis\nframework to explore and identify a small set of biased heads that are found to\ncontribute to a PLM's stereotypical bias. We conduct extensive experiments to\nvalidate the existence of these biased heads and to better understand how they\nbehave. We investigate gender and racial bias in the English language in two\ntypes of Transformer-based PLMs: the encoder-based BERT model and the\ndecoder-based autoregressive GPT model. Overall, the results shed light on\nunderstanding the bias behavior in pretrained language models.\n","authors":["Yi Yang","Hanyu Duan","Ahmed Abbasi","John P. Lalor","Kar Yan Tam"],"pdf_url":"https://arxiv.org/pdf/2311.10395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10373v1","updated":"2023-11-17T07:56:01Z","published":"2023-11-17T07:56:01Z","title":"FOAL: Fine-grained Contrastive Learning for Cross-domain Aspect\n  Sentiment Triplet Extraction","summary":"  Aspect Sentiment Triplet Extraction (ASTE) has achieved promising results\nwhile relying on sufficient annotation data in a specific domain. However, it\nis infeasible to annotate data for each individual domain. We propose to\nexplore ASTE in the cross-domain setting, which transfers knowledge from a\nresource-rich source domain to a resource-poor target domain, thereby\nalleviating the reliance on labeled data in the target domain. To effectively\ntransfer the knowledge across domains and extract the sentiment triplets\naccurately, we propose a method named Fine-grained cOntrAstive Learning (FOAL)\nto reduce the domain discrepancy and preserve the discriminability of each\ncategory. Experiments on six transfer pairs show that FOAL achieves 6%\nperformance gains and reduces the domain discrepancy significantly compared\nwith strong baselines. Our code will be publicly available once accepted.\n","authors":["Ting Xu","Zhen Wu","Huiyun Yang","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2311.10373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10367v1","updated":"2023-11-17T07:40:46Z","published":"2023-11-17T07:40:46Z","title":"Exploring the Relationship between In-Context Learning and Instruction\n  Tuning","summary":"  In-Context Learning (ICL) and Instruction Tuning (IT) are two primary\nparadigms of adopting Large Language Models (LLMs) to downstream applications.\nHowever, they are significantly different. In ICL, a set of demonstrations are\nprovided at inference time but the LLM's parameters are not updated. In IT, a\nset of demonstrations are used to tune LLM's parameters in training time but no\ndemonstrations are used at inference time. Although a growing body of\nliterature has explored ICL and IT, studies on these topics have largely been\nconducted in isolation, leading to a disconnect between these two paradigms. In\nthis work, we explore the relationship between ICL and IT by examining how the\nhidden states of LLMs change in these two paradigms. Through carefully designed\nexperiments conducted with LLaMA-2 (7B and 13B), we find that ICL is implicit\nIT. In other words, ICL changes an LLM's hidden states as if the demonstrations\nwere used to instructionally tune the model. Furthermore, the convergence\nbetween ICL and IT is largely contingent upon several factors related to the\nprovided demonstrations. Overall, this work offers a unique perspective to\nexplore the connection between ICL and IT and sheds light on understanding the\nbehaviors of LLM.\n","authors":["Hanyu Duan","Yixuan Tang","Yi Yang","Ahmed Abbasi","Kar Yan Tam"],"pdf_url":"https://arxiv.org/pdf/2311.10367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18075v3","updated":"2023-11-17T06:55:45Z","published":"2023-10-27T11:43:46Z","title":"DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking","summary":"  Inspired by the dual-process theory of human cognition, we introduce DUMA, a\nnovel conversational agent framework that embodies a dual-mind mechanism\nthrough the utilization of two generative Large Language Models (LLMs)\ndedicated to fast and slow thinking respectively. The fast thinking model\nserves as the primary interface for external interactions and initial response\ngeneration, evaluating the necessity for engaging the slow thinking model based\non the complexity of the complete response. When invoked, the slow thinking\nmodel takes over the conversation, engaging in meticulous planning, reasoning,\nand tool utilization to provide a well-analyzed response. This dual-mind\nconfiguration allows for a seamless transition between intuitive responses and\ndeliberate problem-solving processes based on the situation. We have\nconstructed a conversational agent to handle online inquiries in the real\nestate industry. The experiment proves that our method balances effectiveness\nand efficiency, and has a significant improvement compared to the baseline.\n","authors":["Xiaoyu Tian","Liangyu Chen","Na Liu","Yaxuan Liu","Wei Zou","Kaijiang Chen","Ming Cui"],"pdf_url":"https://arxiv.org/pdf/2310.18075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00312v4","updated":"2023-11-17T06:32:49Z","published":"2023-09-01T07:53:28Z","title":"Insights Into the Nutritional Prevention of Macular Degeneration based\n  on a Comparative Topic Modeling Approach","summary":"  Topic modeling and text mining are subsets of Natural Language Processing\n(NLP) with relevance for conducting meta-analysis (MA) and systematic review\n(SR). For evidence synthesis, the above NLP methods are conventionally used for\ntopic-specific literature searches or extracting values from reports to\nautomate essential phases of SR and MA. Instead, this work proposes a\ncomparative topic modeling approach to analyze reports of contradictory results\non the same general research question. Specifically, the objective is to\nidentify topics exhibiting distinct associations with significant results for\nan outcome of interest by ranking them according to their proportional\noccurrence in (and consistency of distribution across) reports of significant\neffects. The proposed method was tested on broad-scope studies addressing\nwhether supplemental nutritional compounds significantly benefit macular\ndegeneration (MD). Four of these were further supported in terms of\neffectiveness upon conducting a follow-up literature search for validation\n(omega-3 fatty acids, copper, zeaxanthin, and nitrates). The two not supported\nby the follow-up literature search (niacin and molybdenum) also had scores in\nthe lowest range under the proposed scoring system, suggesting that the\nproposed methods score for a given topic may be a viable proxy for its degree\nof association with the outcome of interest and can be helpful in the search\nfor potentially causal relationships. These results underpin the proposed\nmethods potential to add specificity in understanding effects from broad-scope\nreports, elucidate topics of interest for future research, and guide evidence\nsynthesis in a systematic and scalable way. All of this is accomplished while\nyielding valuable insights into the prevention of MD.\n","authors":["Lucas Cassiel Jacaruso"],"pdf_url":"https://arxiv.org/pdf/2309.00312v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10344v1","updated":"2023-11-17T06:13:02Z","published":"2023-11-17T06:13:02Z","title":"Complementary Advantages of ChatGPTs and Human Readers in Reasoning:\n  Evidence from English Text Reading Comprehension","summary":"  ChatGPT has shown its great power in text processing, including its reasoning\nability from text reading. However, there has not been any direct comparison\nbetween human readers and ChatGPT in reasoning ability related to text reading.\nThis study was undertaken to investigate how ChatGPTs (i.e., ChatGPT and\nChatGPT Plus) and Chinese senior school students as ESL learners exhibited\ntheir reasoning ability from English narrative texts. Additionally, we compared\nthe two ChatGPTs in the reasoning performances when commands were updated\nelaborately. The whole study was composed of three reasoning tests: Test 1 for\ncommonsense inference, Test 2 for emotional inference, and Test 3 for causal\ninference. The results showed that in Test 1, the students outdid the two\nChatGPT versions in local-culture-related inferences but performed worse than\nthe chatbots in daily-life inferences. In Test 2, ChatGPT Plus excelled whereas\nChatGPT lagged behind in accuracy. In association with both accuracy and\nfrequency of correct responses, the students were inferior to the two chatbots.\nCompared with ChatGPTs' better performance in positive emotions, the students\nshowed their superiority in inferring negative emotions. In Test 3, the\nstudents demonstrated better logical analysis, outdoing both chatbots. In\nupdating command condition, ChatGPT Plus displayed good causal reasoning\nability while ChatGPT kept unchanged. Our study reveals that human readers and\nChatGPTs have their respective advantages and disadvantages in drawing\ninferences from text reading comprehension, unlocking a complementary\nrelationship in text-based reasoning.\n","authors":["Tongquan Zhou","Yao Zhang","Siyi Cao","Yulu Li","Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2311.10344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15060v2","updated":"2023-11-17T04:20:01Z","published":"2023-05-24T11:49:52Z","title":"Who Wrote this Code? Watermarking for Code Generation","summary":"  With the remarkable generation performance of large language models, ethical\nand legal concerns about using them have been raised, such as plagiarism and\ncopyright issues. For such concerns, several approaches to watermark and detect\nLLM-generated text have been proposed very recently. However, we discover that\nthe previous methods fail to function appropriately with code generation tasks\nbecause of the syntactic and semantic characteristics of code. Based on\n\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,\nSelective WatErmarking via Entropy Thresholding (SWEET), that promotes \"green\"\ntokens only at the position with high entropy of the token distribution during\ngeneration, thereby preserving the correctness of the generated code. The\nwatermarked code is detected by the statistical test and Z-score based on the\nentropy information. Our experiments on HumanEval and MBPP show that SWEET\nsignificantly improves the Pareto Frontier between the code correctness and\nwatermark detection performance. We also show that notable post-hoc detection\nmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show that\nsetting a reasonable entropy threshold is not much of a challenge. Code is\navailable at https://github.com/hongcheki/sweet-watermark.\n","authors":["Taehyun Lee","Seokhee Hong","Jaewoo Ahn","Ilgee Hong","Hwaran Lee","Sangdoo Yun","Jamin Shin","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2305.15060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09861v2","updated":"2023-11-17T03:17:05Z","published":"2023-11-16T12:43:18Z","title":"PsyBench: a balanced and in-depth Psychological Chinese Evaluation\n  Benchmark for Foundation Models","summary":"  As Large Language Models (LLMs) are becoming prevalent in various fields,\nthere is an urgent need for improved NLP benchmarks that encompass all the\nnecessary knowledge of individual discipline. Many contemporary benchmarks for\nfoundational models emphasize a broad range of subjects but often fall short in\npresenting all the critical subjects and encompassing necessary professional\nknowledge of them. This shortfall has led to skewed results, given that LLMs\nexhibit varying performance across different subjects and knowledge areas. To\naddress this issue, we present psybench, the first comprehensive Chinese\nevaluation suite that covers all the necessary knowledge required for graduate\nentrance exams. psybench offers a deep evaluation of a model's strengths and\nweaknesses in psychology through multiple-choice questions. Our findings show\nsignificant differences in performance across different sections of a subject,\nhighlighting the risk of skewed results when the knowledge in test sets is not\nbalanced. Notably, only the ChatGPT model reaches an average accuracy above\n$70\\%$, indicating that there is still plenty of room for improvement. We\nexpect that psybench will help to conduct thorough evaluations of base models'\nstrengths and weaknesses and assist in practical application in the field of\npsychology.\n","authors":["Junlei Zhang","Hongliang He","Nirui Song","Shuyuan He","Shuai Zhang","Huachuan Qiu","Anqi Li","Lizhi Ma","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2311.09861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11248v2","updated":"2023-11-17T02:51:39Z","published":"2023-10-17T13:18:01Z","title":"CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code\n  Completion","summary":"  Code completion models have made significant progress in recent years, yet\ncurrent popular evaluation datasets, such as HumanEval and MBPP, predominantly\nfocus on code completion tasks within a single file. This over-simplified\nsetting falls short of representing the real-world software development\nscenario where repositories span multiple files with numerous cross-file\ndependencies, and accessing and understanding cross-file context is often\nrequired to complete the code correctly.\n  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual\ncode completion benchmark that necessitates an in-depth cross-file contextual\nunderstanding to complete the code accurately. CrossCodeEval is built on a\ndiverse set of real-world, open-sourced, permissively-licensed repositories in\nfour popular programming languages: Python, Java, TypeScript, and C#. To create\nexamples that strictly require cross-file context for accurate completion, we\npropose a straightforward yet efficient static-analysis-based approach to\npinpoint the use of cross-file context within the current file.\n  Extensive experiments on state-of-the-art code language models like CodeGen\nand StarCoder demonstrate that CrossCodeEval is extremely challenging when the\nrelevant cross-file context is absent, and we see clear improvements when\nadding these context into the prompt. However, despite such improvements, the\npinnacle of performance remains notably unattained even with the\nhighest-performing model, indicating that CrossCodeEval is also capable of\nassessing model's capability in leveraging extensive context to make better\ncode completion. Finally, we benchmarked various methods in retrieving\ncross-file context, and show that CrossCodeEval can also be used to measure the\ncapability of code retrievers.\n","authors":["Yangruibo Ding","Zijian Wang","Wasi Uddin Ahmad","Hantian Ding","Ming Tan","Nihal Jain","Murali Krishna Ramanathan","Ramesh Nallapati","Parminder Bhatia","Dan Roth","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2310.11248v2.pdf","comment":"To appear at NeurIPS 2023 (Datasets and Benchmarks Track)"},{"id":"http://arxiv.org/abs/2304.03512v3","updated":"2023-11-17T02:08:14Z","published":"2023-04-07T07:13:35Z","title":"Hierarchical Catalogue Generation for Literature Review: A Benchmark","summary":"  Scientific literature review generation aims to extract and organize\nimportant information from an abundant collection of reference papers and\nproduces corresponding reviews while lacking a clear and logical hierarchy. We\nobserve that a high-quality catalogue-guided generation process can effectively\nalleviate this problem. Therefore, we present an atomic and challenging task\nnamed Hierarchical Catalogue Generation for Literature Review as the first step\nfor review generation, which aims to produce a hierarchical catalogue of a\nreview paper given various references. We construct a novel English\nHierarchical Catalogues of Literature Reviews Dataset with 7.6k literature\nreview catalogues and 389k reference papers. To accurately assess the model\nperformance, we design two evaluation metrics for informativeness and\nsimilarity to ground truth from semantics and structure.Our extensive analyses\nverify the high quality of our dataset and the effectiveness of our evaluation\nmetrics. We further benchmark diverse experiments on state-of-the-art\nsummarization models like BART and large language models like ChatGPT to\nevaluate their capabilities. We further discuss potential directions for this\ntask to motivate future research.\n","authors":["Kun Zhu","Xiaocheng Feng","Xiachong Feng","Yingsheng Wu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2304.03512v3.pdf","comment":"EMNLP 2023 findings"},{"id":"http://arxiv.org/abs/2303.17807v2","updated":"2023-11-17T01:49:57Z","published":"2023-03-31T05:43:21Z","title":"GPT-4 can pass the Korean National Licensing Examination for Korean\n  Medicine Doctors","summary":"  Traditional Korean medicine (TKM) emphasizes individualized diagnosis and\ntreatment. This uniqueness makes AI modeling difficult due to limited data and\nimplicit processes. Large language models (LLMs) have demonstrated impressive\nmedical inference, even without advanced training in medical texts. This study\nassessed the capabilities of GPT-4 in TKM, using the Korean National Licensing\nExamination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The\nK-NLEKMD, administered by a national organization, encompasses 12 major\nsubjects in TKM. We optimized prompts with Chinese-term annotation, English\ntranslation for questions and instruction, exam-optimized instruction, and\nself-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,\nsurpassing both the examination's average pass mark of 60% and the 40% minimum\nfor each subject. The gradual introduction of language-related prompts and\nprompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.\nGPT-4 showed low accuracy in subjects including public health &\nmedicine-related law, internal medicine (2) which are localized in Korea and\nTKM. The model's accuracy was lower for questions requiring TKM-specialized\nknowledge. It exhibited higher accuracy in diagnosis-based and recall-based\nquestions than in intervention-based questions. A positive correlation was\nobserved between the consistency and accuracy of GPT-4's responses. This study\nunveils both the potential and challenges of applying LLMs to TKM. These\nfindings underline the potential of LLMs like GPT-4 in culturally adapted\nmedicine, especially TKM, for tasks such as clinical assistance, medical\neducation, and research. But they also point towards the necessity for the\ndevelopment of methods to mitigate cultural bias inherent in large language\nmodels and validate their efficacy in real-world clinical settings.\n","authors":["Dongyeop Jang","Tae-Rim Yun","Choong-Yeol Lee","Young-Kyu Kwon","Chang-Eop Kim"],"pdf_url":"https://arxiv.org/pdf/2303.17807v2.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.10271v1","updated":"2023-11-17T01:33:05Z","published":"2023-11-17T01:33:05Z","title":"Prompt Pool based Class-Incremental Continual Learning for Dialog State\n  Tracking","summary":"  Continual learning is crucial for dialog state tracking (DST) in dialog\nsystems, since requirements from users for new functionalities are often\nencountered. However, most of existing continual learning methods for DST\nrequire task identities during testing, which is a severe limit in real-world\napplications. In this paper, we aim to address continual learning of DST in the\nclass-incremental scenario (namely the task identity is unknown in testing).\nInspired by the recently emerging prompt tuning method that performs well on\ndialog systems, we propose to use the prompt pool method, where we maintain a\npool of key-value paired prompts and select prompts from the pool according to\nthe distance between the dialog history and the prompt keys. The proposed\nmethod can automatically identify tasks and select appropriate prompts during\ntesting. We conduct experiments on Schema-Guided Dialog dataset (SGD) and\nanother dataset collected from a real-world dialog application. Experiment\nresults show that the prompt pool method achieves much higher joint goal\naccuracy than the baseline. After combining with a rehearsal buffer, the model\nperformance can be further improved.\n","authors":["Hong Liu","Yucheng Cai","Yuan Zhou","Zhijian Ou","Yi Huang","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2311.10271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10267v1","updated":"2023-11-17T01:27:01Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v1.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2311.10266v1","updated":"2023-11-17T01:20:08Z","published":"2023-11-17T01:20:08Z","title":"Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2","summary":"  The training of large language models (LLMs) on extensive, unfiltered corpora\nsourced from the internet is a common and advantageous practice. Consequently,\nLLMs have learned and inadvertently reproduced various types of biases,\nincluding violent, offensive, and toxic language. However, recent research\nshows that generative pretrained transformer (GPT) language models can\nrecognize their own biases and detect toxicity in generated content, a process\nreferred to as self-diagnosis. In response, researchers have developed a\ndecoding algorithm that allows LLMs to self-debias, or reduce their likelihood\nof generating harmful text. This study investigates the efficacy of the\ndiagnosing-debiasing approach in mitigating two additional types of biases:\ninsults and political bias. These biases are often used interchangeably in\ndiscourse, despite exhibiting potentially dissimilar semantic and syntactic\nproperties. We aim to contribute to the ongoing effort of investigating the\nethical and social implications of human-AI interaction.\n","authors":["Ambri Ma","Arnav Kumar","Brett Zeligson"],"pdf_url":"https://arxiv.org/pdf/2311.10266v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2311.06233v2","updated":"2023-11-17T00:20:44Z","published":"2023-11-10T18:48:58Z","title":"Data Contamination Quiz: A Tool to Detect and Estimate Contamination in\n  Large Language Models","summary":"  We propose the Data Contamination Quiz, a simple and effective approach to\ndetect data contamination in large language models (LLMs) and estimate the\namount of it. Specifically, we frame data contamination detection as a series\nof multiple-choice questions. We devise a quiz format wherein three perturbed\nversions of each dataset instance are created. These changes only include\nword-level perturbations, replacing words with their contextual synonyms,\nensuring both the semantic and sentence structure remain exactly the same as\nthe original instance. Together with the original instance, these perturbed\nversions constitute the choices in the quiz. Given that the only distinguishing\nsignal among these choices is the exact wording, an LLM, when tasked with\nidentifying the original instance from the choices, opts for the original if it\nhas memorized it in its pre-training phase--a trait intrinsic to LLMs. A\ndataset partition is then marked as contaminated if the LLM's performance on\nthe quiz surpasses what random chance suggests. Our evaluation spans seven\ndatasets and their respective splits (train and test/validation) on two\nstate-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the\npre-training data, our results suggest that our approach not only enhances the\ndetection of data contamination but also provides an accurate estimation of its\nextent, even when the contamination signal is weak.\n","authors":["Shahriar Golchin","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2311.06233v2.pdf","comment":"v1.1 preprint"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2311.10709v1","updated":"2023-11-17T18:59:04Z","published":"2023-11-17T18:59:04Z","title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image\n  Conditioning","summary":"  We present Emu Video, a text-to-video generation model that factorizes the\ngeneration into two steps: first generating an image conditioned on the text,\nand then generating a video conditioned on the text and the generated image. We\nidentify critical design decisions--adjusted noise schedules for diffusion, and\nmulti-stage training--that enable us to directly generate high quality and high\nresolution videos, without requiring a deep cascade of models as in prior work.\nIn human evaluations, our generated videos are strongly preferred in quality\ncompared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's\nPYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial\nsolutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing\napproach naturally lends itself to animating images based on a user's text\nprompt, where our generations are preferred 96% over prior work.\n","authors":["Rohit Girdhar","Mannat Singh","Andrew Brown","Quentin Duval","Samaneh Azadi","Sai Saketh Rambhatla","Akbar Shah","Xi Yin","Devi Parikh","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2311.10709v1.pdf","comment":"Project page: https://emu-video.metademolab.com"},{"id":"http://arxiv.org/abs/2311.10708v1","updated":"2023-11-17T18:58:16Z","published":"2023-11-17T18:58:16Z","title":"SelfEval: Leveraging the discriminative nature of generative models for\n  evaluation","summary":"  In this work, we show that text-to-image generative models can be 'inverted'\nto assess their own text-image understanding capabilities in a completely\nautomated manner.\n  Our method, called SelfEval, uses the generative model to compute the\nlikelihood of real images given text prompts, making the generative model\ndirectly applicable to discriminative tasks.\n  Using SelfEval, we repurpose standard datasets created for evaluating\nmultimodal text-image discriminative models to evaluate generative models in a\nfine-grained manner: assessing their performance on attribute binding, color\nrecognition, counting, shape recognition, spatial understanding.\n  To the best of our knowledge SelfEval is the first automated metric to show a\nhigh degree of agreement for measuring text-faithfulness with the gold-standard\nhuman evaluations across multiple models and benchmarks.\n  Moreover, SelfEval enables us to evaluate generative models on challenging\ntasks such as Winoground image-score where they demonstrate competitive\nperformance to discriminative models.\n  We also show severe drawbacks of standard automated metrics such as\nCLIP-score to measure text faithfulness on benchmarks such as DrawBench, and\nhow SelfEval sidesteps these issues.\n  We hope SelfEval enables easy and reliable automated evaluation for diffusion\nmodels.\n","authors":["Sai Saketh Rambhatla","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2311.10708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10707v1","updated":"2023-11-17T18:57:40Z","published":"2023-11-17T18:57:40Z","title":"Multimodal Representation Learning by Alternating Unimodal Adaptation","summary":"  Multimodal learning, which integrates data from diverse sensory modes, plays\na pivotal role in artificial intelligence. However, existing multimodal\nlearning methods often struggle with challenges where some modalities appear\nmore dominant than others during multimodal learning, resulting in suboptimal\nperformance. To address this challenge, we propose MLA (Multimodal Learning\nwith Alternating Unimodal Adaptation). MLA reframes the conventional joint\nmultimodal learning process by transforming it into an alternating unimodal\nlearning process, thereby minimizing interference between modalities.\nSimultaneously, it captures cross-modal interactions through a shared head,\nwhich undergoes continuous optimization across different modalities. This\noptimization process is controlled by a gradient modification mechanism to\nprevent the shared head from losing previously acquired information. During the\ninference phase, MLA utilizes a test-time uncertainty-based model fusion\nmechanism to integrate multimodal information. Extensive experiments are\nconducted on five diverse datasets, encompassing scenarios with complete\nmodalities and scenarios with missing modalities. These experiments demonstrate\nthe superiority of MLA over competing prior approaches.\n","authors":["Xiaohui Zhang","Jaehong Yoon","Mohit Bansal","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2311.10707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08003v2","updated":"2023-11-17T18:47:42Z","published":"2023-07-16T11:10:35Z","title":"SHAMSUL: Systematic Holistic Analysis to investigate Medical\n  Significance Utilizing Local interpretability methods in deep learning for\n  chest radiography pathology prediction","summary":"  The interpretability of deep neural networks has become a subject of great\ninterest within the medical and healthcare domain. This attention stems from\nconcerns regarding transparency, legal and ethical considerations, and the\nmedical significance of predictions generated by these deep neural networks in\nclinical decision support systems. To address this matter, our study delves\ninto the application of four well-established interpretability methods: Local\nInterpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations\n(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise\nRelevance Propagation (LRP). Leveraging the approach of transfer learning with\na multi-label-multi-class chest radiography dataset, we aim to interpret\npredictions pertaining to specific pathology classes. Our analysis encompasses\nboth single-label and multi-label predictions, providing a comprehensive and\nunbiased assessment through quantitative and qualitative investigations, which\nare compared against human expert annotation. Notably, Grad-CAM demonstrates\nthe most favorable performance in quantitative evaluation, while the LIME\nheatmap score segmentation visualization exhibits the highest level of medical\nsignificance. Our research underscores both the outcomes and the challenges\nfaced in the holistic approach adopted for assessing these interpretability\nmethods and suggests that a multimodal-based approach, incorporating diverse\nsources of information beyond chest radiography images, could offer additional\ninsights for enhancing interpretability in the medical domain.\n","authors":["Mahbub Ul Alam","Jaakko Hollmén","Jón Rúnar Baldvinsson","Rahim Rahmani"],"pdf_url":"https://arxiv.org/pdf/2307.08003v2.pdf","comment":"This version contains extensive modifications compared to the\n  previous version. The published version of this article can be obtained using\n  the following link: https://doi.org/10.5617/nmi.10471 Code Repository:\n  https://github.com/anondo1969/SHAMSUL"},{"id":"http://arxiv.org/abs/2311.10701v1","updated":"2023-11-17T18:45:00Z","published":"2023-11-17T18:45:00Z","title":"SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet\n  Variational Autoencoder for Hyperspectral Pixel Unmixing","summary":"  The Hyperspectral Unxming problem is to find the pure spectral signal of the\nunderlying materials (endmembers) and their proportions (abundances). The\nproposed method builds upon the recently proposed method, Latent Dirichlet\nVariational Autoencoder (LDVAE). It assumes that abundances can be encoded as\nDirichlet Distributions while mixed pixels and endmembers are represented by\nMultivariate Normal Distributions. However, LDVAE does not leverage spatial\ninformation present in an HSI; we propose an Isotropic CNN encoder with spatial\nattention to solve the hyperspectral unmixing problem. We evaluated our model\non Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model\nalso leverages the transfer learning paradigm for Cuprite Dataset, where we\ntrain the model on synthetic data and evaluate it on real-world data. We are\nable to observe the improvement in the results for the endmember extraction and\nabundance estimation by incorporating the spatial information. Code can be\nfound at https://github.com/faisalqureshi/cnn-ldvae\n","authors":["Soham Chitnis","Kiran Mantripragada","Faisal Z. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2311.10701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10699v1","updated":"2023-11-17T18:43:32Z","published":"2023-11-17T18:43:32Z","title":"Using linear initialisation to improve speed of convergence and\n  fully-trained error in Autoencoders","summary":"  Good weight initialisation is an important step in successful training of\nArtificial Neural Networks. Over time a number of improvements have been\nproposed to this process. In this paper we introduce a novel weight\ninitialisation technique called the Straddled Matrix Initialiser. This\ninitialisation technique is motivated by our assumption that major,\nglobal-scale relationships in data are linear with only smaller effects\nrequiring complex non-linearities. Combination of Straddled Matrix and ReLU\nactivation function initialises a Neural Network as a de facto linear model,\nwhich we postulate should be a better starting point for optimisation given our\nassumptions. We test this by training autoencoders on three datasets using\nStraddled Matrix and seven other state-of-the-art weight initialisation\ntechniques. In all our experiments the Straddeled Matrix Initialiser clearly\noutperforms all other methods.\n","authors":["Marcel Marais","Mate Hartstein","George Cevora"],"pdf_url":"https://arxiv.org/pdf/2311.10699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06595v3","updated":"2023-11-17T18:39:46Z","published":"2023-08-12T15:27:51Z","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following\n  Inspired by Real-World Use","summary":"  We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for\nevaluation of instruction-following vision-language models for real-world use.\nOur starting point is curating 70 'instruction families' that we envision\ninstruction tuned vision-language models should be able to address. Extending\nbeyond evaluations like VQAv2 and COCO, tasks range from basic recognition to\ngame playing and creative generation. Following curation, our dataset comprises\n592 test queries, each with a human-authored instruction-conditioned caption.\nThese descriptions surface instruction-specific factors, e.g., for an\ninstruction asking about the accessibility of a storefront for wheelchair\nusers, the instruction-conditioned caption describes ramps/potential obstacles.\nThese descriptions enable 1) collecting human-verified reference outputs for\neach instance; and 2) automatic evaluation of candidate multimodal generations\nusing a text-only LLM, aligning with human judgment. We quantify quality gaps\nbetween models and references using both human and automatic evaluations; e.g.,\nthe top-performing instruction-following model wins against the GPT-4 reference\nin just 27% of the comparison. VisIT-Bench is dynamic to participate,\npractitioners simply submit their model's response on the project website;\nData, code and leaderboard is available at visit-bench.github.io.\n","authors":["Yonatan Bitton","Hritik Bansal","Jack Hessel","Rulin Shao","Wanrong Zhu","Anas Awadalla","Josh Gardner","Rohan Taori","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2308.06595v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10696v1","updated":"2023-11-17T18:28:32Z","published":"2023-11-17T18:28:32Z","title":"Versatile Medical Image Segmentation Learned from Multi-Source Datasets\n  via Model Self-Disambiguation","summary":"  A versatile medical image segmentation model applicable to imaging data\ncollected with diverse equipment and protocols can facilitate model deployment\nand maintenance. However, building such a model typically requires a large,\ndiverse, and fully annotated dataset, which is rarely available due to the\nlabor-intensive and costly data curation. In this study, we develop a\ncost-efficient method by harnessing readily available data with partially or\neven sparsely annotated segmentation labels. We devise strategies for model\nself-disambiguation, prior knowledge incorporation, and imbalance mitigation to\naddress challenges associated with inconsistently labeled data from various\nsources, including label ambiguity and imbalances across modalities, datasets,\nand segmentation labels. Experimental results on a multi-modal dataset compiled\nfrom eight different sources for abdominal organ segmentation have demonstrated\nour method's effectiveness and superior performance over alternative\nstate-of-the-art methods, highlighting its potential for optimizing the use of\nexisting annotated data and reducing the annotation efforts for new data to\nfurther enhance model capability.\n","authors":["Xiaoyang Chen","Hao Zheng","Yuemeng Li","Yuncong Ma","Liang Ma","Hongming Li","Yong Fan"],"pdf_url":"https://arxiv.org/pdf/2311.10696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10651v1","updated":"2023-11-17T17:13:14Z","published":"2023-11-17T17:13:14Z","title":"3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual\n  Transformer Learning","summary":"  Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.\n","authors":["Iyyakutti Iyappan Ganapathi","Fayaz Ali","Sajid Javed","Syed Sadaf Ali","Naoufel Werghi"],"pdf_url":"https://arxiv.org/pdf/2311.10651v1.pdf","comment":"This paper is accepted in 3DV-2024"},{"id":"http://arxiv.org/abs/2311.00690v2","updated":"2023-11-17T17:09:56Z","published":"2023-11-01T17:45:52Z","title":"What User Behaviors Make the Differences During the Process of Visual\n  Analytics?","summary":"  The understanding of visual analytics process can benefit visualization\nresearchers from multiple aspects, including improving visual designs and\ndeveloping advanced interaction functions. However, the log files of user\nbehaviors are still hard to analyze due to the complexity of sensemaking and\nour lack of knowledge on the related user behaviors. This work presents a study\non a comprehensive data collection of user behaviors, and our analysis approach\nwith time-series classification methods. We have chosen a classical\nvisualization application, Covid-19 data analysis, with common analysis tasks\ncovering geo-spatial, time-series and multi-attributes. Our user study collects\nuser behaviors on a diverse set of visualization tasks with two comparable\nsystems, desktop and immersive visualizations. We summarize the classification\nresults with three time-series machine learning algorithms at two scales, and\nexplore the influences of behavior features. Our results reveal that user\nbehaviors can be distinguished during the process of visual analytics and there\nis a potentially strong association between the physical behaviors of users and\nthe visualization tasks they perform. We also demonstrate the usage of our\nmodels by interpreting open sessions of visual analytics, which provides an\nautomatic way to study sensemaking without tedious manual annotations.\n","authors":["Shahin Doroudian","Zekun Wu","Aidong Lu"],"pdf_url":"https://arxiv.org/pdf/2311.00690v2.pdf","comment":"The authors have decided to withdraw the paper due to identified\n  critical errors. These errors were deemed substantial enough to compromise\n  the integrity and reliability of the research findings presented in the\n  paper. As a result, the authors have chosen to retract the paper to maintain\n  academic standards and transparency in the dissemination of scientific\n  knowledge"},{"id":"http://arxiv.org/abs/2311.10648v1","updated":"2023-11-17T17:06:59Z","published":"2023-11-17T17:06:59Z","title":"Self-trained Panoptic Segmentation","summary":"  Panoptic segmentation is an important computer vision task which combines\nsemantic and instance segmentation. It plays a crucial role in domains of\nmedical image analysis, self-driving vehicles, and robotics by providing a\ncomprehensive understanding of visual environments. Traditionally, deep\nlearning panoptic segmentation models have relied on dense and accurately\nannotated training data, which is expensive and time consuming to obtain.\nRecent advancements in self-supervised learning approaches have shown great\npotential in leveraging synthetic and unlabelled data to generate pseudo-labels\nusing self-training to improve the performance of instance and semantic\nsegmentation models. The three available methods for self-supervised panoptic\nsegmentation use proposal-based transformer architectures which are\ncomputationally expensive, complicated and engineered for specific tasks. The\naim of this work is to develop a framework to perform embedding-based\nself-supervised panoptic segmentation using self-training in a\nsynthetic-to-real domain adaptation problem setting.\n","authors":["Shourya Verma"],"pdf_url":"https://arxiv.org/pdf/2311.10648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10617v1","updated":"2023-11-17T16:14:11Z","published":"2023-11-17T16:14:11Z","title":"Astronomical Images Quality Assessment with Automated Machine Learning","summary":"  Electronically Assisted Astronomy consists in capturing deep sky images with\na digital camera coupled to a telescope to display views of celestial objects\nthat would have been invisible through direct observation. This practice\ngenerates a large quantity of data, which may then be enhanced with dedicated\nimage editing software after observation sessions. In this study, we show how\nImage Quality Assessment can be useful for automatically rating astronomical\nimages, and we also develop a dedicated model by using Automated Machine\nLearning.\n","authors":["Olivier Parisot","Pierrick Bruneau","Patrik Hitzelberger"],"pdf_url":"https://arxiv.org/pdf/2311.10617v1.pdf","comment":"8 pages, accepted at DATA2024"},{"id":"http://arxiv.org/abs/2311.10605v1","updated":"2023-11-17T16:01:06Z","published":"2023-11-17T16:01:06Z","title":"CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification","summary":"  Person re-identification (re-ID) is a challenging task that aims to learn\ndiscriminative features for person retrieval. In person re-ID, Jaccard distance\nis a widely used distance metric, especially in re-ranking and clustering\nscenarios. However, we discover that camera variation has a significant\nnegative impact on the reliability of Jaccard distance. In particular, Jaccard\ndistance calculates the distance based on the overlap of relevant neighbors.\nDue to camera variation, intra-camera samples dominate the relevant neighbors,\nwhich reduces the reliability of the neighbors by introducing intra-camera\nnegative samples and excluding inter-camera positive samples. To overcome this\nproblem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that\nleverages camera information to enhance the reliability of Jaccard distance.\nSpecifically, we introduce camera-aware k-reciprocal nearest neighbors (CKRNNs)\nto find k-reciprocal nearest neighbors on the intra-camera and inter-camera\nranking lists, which improves the reliability of relevant neighbors and\nguarantees the contribution of inter-camera samples in the overlap. Moreover,\nwe propose a camera-aware local query expansion (CLQE) to exploit camera\nvariation as a strong constraint to mine reliable samples in relevant neighbors\nand assign these samples higher weights in overlap to further improve the\nreliability. Our CA-Jaccard distance is simple yet effective and can serve as a\ngeneral distance metric for person re-ID methods with high reliability and low\ncomputational cost. Extensive experiments demonstrate the effectiveness of our\nmethod.\n","authors":["Yiyu Chen","Zheyi Fan","Zhaoru Chen","Yixuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.10605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10601v1","updated":"2023-11-17T15:57:32Z","published":"2023-11-17T15:57:32Z","title":"Multimodal Indoor Localization Using Crowdsourced Radio Maps","summary":"  Indoor Positioning Systems (IPS) traditionally rely on odometry and building\ninfrastructures like WiFi, often supplemented by building floor plans for\nincreased accuracy. However, the limitation of floor plans in terms of\navailability and timeliness of updates challenges their wide applicability. In\ncontrast, the proliferation of smartphones and WiFi-enabled robots has made\ncrowdsourced radio maps - databases pairing locations with their corresponding\nReceived Signal Strengths (RSS) - increasingly accessible. These radio maps not\nonly provide WiFi fingerprint-location pairs but encode movement regularities\nakin to the constraints imposed by floor plans. This work investigates the\npossibility of leveraging these radio maps as a substitute for floor plans in\nmultimodal IPS. We introduce a new framework to address the challenges of radio\nmap inaccuracies and sparse coverage. Our proposed system integrates an\nuncertainty-aware neural network model for WiFi localization and a bespoken\nBayesian fusion technique for optimal fusion. Extensive evaluations on multiple\nreal-world sites indicate a significant performance enhancement, with results\nshowing ~ 25% improvement over the best baseline\n","authors":["Zhaoguang Yi","Xiangyu Wen","Qiyue Xia","Peize Li","Francisco Zampella","Firas Alsehly","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2311.10601v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.18849v2","updated":"2023-11-17T15:53:50Z","published":"2023-10-28T23:38:30Z","title":"Deep Learning-based Compressed Domain Multimedia for Man and Machine: A\n  Taxonomy and Application to Point Cloud Classification","summary":"  In the current golden age of multimedia, human visualization is no longer the\nsingle main target, with the final consumer often being a machine which\nperforms some processing or computer vision tasks. In both cases, deep learning\nplays a undamental role in extracting features from the multimedia\nrepresentation data, usually producing a compressed representation referred to\nas latent representation. The increasing development and adoption of deep\nlearning-based solutions in a wide area of multimedia applications have opened\nan exciting new vision where a common compressed multimedia representation is\nused for both man and machine. The main benefits of this vision are two-fold:\ni) improved performance for the computer vision tasks, since the effects of\ncoding artifacts are mitigated; and ii) reduced computational complexity, since\nprior decoding is not required. This paper proposes the first taxonomy for\ndesigning compressed domain computer vision solutions driven by the\narchitecture and weights compatibility with an available spatio-temporal\ncomputer vision processor. The potential of the proposed taxonomy is\ndemonstrated for the specific case of point cloud classification by designing\nnovel compressed domain processors using the JPEG Pleno Point Cloud Coding\nstandard under development and adaptations of the PointGrid classifier.\nExperimental results show that the designed compressed domain point cloud\nclassification solutions can significantly outperform the spatial-temporal\ndomain classification benchmarks when applied to the decompressed data,\ncontaining coding artifacts, and even surpass their performance when applied to\nthe original uncompressed data.\n","authors":["Abdelrahman Seleem","André F. R. Guarda","Nuno M. M. Rodrigues","Fernando Pereira"],"pdf_url":"https://arxiv.org/pdf/2310.18849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15991v2","updated":"2023-11-17T15:47:35Z","published":"2023-09-27T20:12:41Z","title":"Targeted Image Data Augmentation Increases Basic Skills Captioning\n  Robustness","summary":"  Artificial neural networks typically struggle in generalizing to\nout-of-context examples. One reason for this limitation is caused by having\ndatasets that incorporate only partial information regarding the potential\ncorrelational structure of the world. In this work, we propose TIDA (Targeted\nImage-editing Data Augmentation), a targeted data augmentation method focused\non improving models' human-like abilities (e.g., gender recognition) by filling\nthe correlational structure gap using a text-to-image generative model. More\nspecifically, TIDA identifies specific skills in captions describing images\n(e.g., the presence of a specific gender in the image), changes the caption\n(e.g., \"woman\" to \"man\"), and then uses a text-to-image model to edit the image\nin order to match the novel caption (e.g., uniquely changing a woman to a man\nwhile maintaining the context identical). Based on the Flickr30K benchmark, we\nshow that, compared with the original data set, a TIDA-enhanced dataset related\nto gender, color, and counting abilities induces better performance in several\nimage captioning metrics. Furthermore, on top of relying on the classical BLEU\nmetric, we conduct a fine-grained analysis of the improvements of our models\nagainst the baseline in different ways. We compared text-to-image generative\nmodels and found different behaviors of the image captioning models in terms of\nencoding visual encoding and textual decoding.\n","authors":["Valentin Barriere","Felipe del Rio","Andres Carvallo De Ferari","Carlos Aspillaga","Eugenio Herrera-Berg","Cristian Buc Calderon"],"pdf_url":"https://arxiv.org/pdf/2309.15991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10592v1","updated":"2023-11-17T15:46:50Z","published":"2023-11-17T15:46:50Z","title":"Détection d'objets célestes dans des images astronomiques par IA\n  explicable","summary":"  Amateur and professional astronomers can easily capture a large number of\ndeep sky images with recent smart telescopes. However, afterwards verification\nis still required to check whether the celestial objects targeted are actually\nvisible in the images produced. Depending on the magnitude of the targets, the\nobservation conditions and the time during which the data is captured, it is\npossible that only stars are present in the images. In this study, we propose\nan approach based on explainable Artificial Intelligence to automatically\ndetect the presence and position of captured objects. -- --\n  Gr\\^ace \\`a l'apport des t\\'elescopes automatis\\'es grand public, les\nastronomes amateurs et professionnels peuvent capturer facilement une grande\nquantit\\'e d'images du ciel profond (comme par exemple les galaxies,\nn\\'ebuleuses, ou amas globulaires). N\\'eanmoins, une v\\'erification reste\nn\\'ecessaire \\`a post\\'eriori pour v\\'erifier si les objets c\\'elestes vis\\'es\nsont effectivement visibles dans les images produites: cela d\\'epend notamment\nde la magnitude des cibles, des conditions d'observation mais aussi de la\ndur\\'ee pendant laquelle les donn\\'ees sont captur\\'ees. Dans cette \\'etude,\nnous proposons une approche bas\\'ee sur l'IA explicable pour d\\'etecter\nautomatiquement la pr\\'esence et la position des objets captur\\'es.\n","authors":["Olivier Parisot","Mahmoud Jaziri"],"pdf_url":"https://arxiv.org/pdf/2311.10592v1.pdf","comment":"9 pages, in French, accepted in short version for EGC2024 (24\\`eme\n  conf\\'erence francophone sur l'Extraction et la Gestion des Connaissances)"},{"id":"http://arxiv.org/abs/2311.10591v1","updated":"2023-11-17T15:46:09Z","published":"2023-11-17T15:46:09Z","title":"FOCAL: A Cost-Aware Video Dataset for Active Learning","summary":"  In this paper, we introduce the FOCAL (Ford-OLIVES Collaboration on Active\nLearning) dataset which enables the study of the impact of annotation-cost\nwithin a video active learning setting. Annotation-cost refers to the time it\ntakes an annotator to label and quality-assure a given video sequence. A\npractical motivation for active learning research is to minimize\nannotation-cost by selectively labeling informative samples that will maximize\nperformance within a given budget constraint. However, previous work in video\nactive learning lacks real-time annotation labels for accurately assessing cost\nminimization and instead operates under the assumption that annotation-cost\nscales linearly with the amount of data to annotate. This assumption does not\ntake into account a variety of real-world confounding factors that contribute\nto a nonlinear cost such as the effect of an assistive labeling tool and the\nvariety of interactions within a scene such as occluded objects, weather, and\nmotion of objects. FOCAL addresses this discrepancy by providing real\nannotation-cost labels for 126 video sequences across 69 unique city scenes\nwith a variety of weather, lighting, and seasonal conditions. We also introduce\na set of conformal active learning algorithms that take advantage of the\nsequential structure of video data in order to achieve a better trade-off\nbetween annotation-cost and performance while also reducing floating point\noperations (FLOPS) overhead by at least 77.67%. We show how these approaches\nbetter reflect how annotations on videos are done in practice through a\nsequence selection framework. We further demonstrate the advantage of these\napproaches by introducing two performance-cost metrics and show that the best\nconformal active learning method is cheaper than the best traditional active\nlearning method by 113 hours.\n","authors":["Kiran Kokilepersaud","Yash-Yee Logan","Ryan Benkert","Chen Zhou","Mohit Prabhushankar","Ghassan AlRegib","Enrique Corona","Kunjan Singh","Mostafa Parchami"],"pdf_url":"https://arxiv.org/pdf/2311.10591v1.pdf","comment":"This paper was accepted as a main conference paper at the IEEE\n  International Conference on Big Data"},{"id":"http://arxiv.org/abs/2311.10582v1","updated":"2023-11-17T15:32:21Z","published":"2023-11-17T15:32:21Z","title":"Human motion trajectory prediction using the Social Force Model for\n  real-time and low computational cost applications","summary":"  Human motion trajectory prediction is a very important functionality for\nhuman-robot collaboration, specifically in accompanying, guiding, or\napproaching tasks, but also in social robotics, self-driving vehicles, or\nsecurity systems. In this paper, a novel trajectory prediction model, Social\nForce Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a\nGenerative Adversarial Network (GAN) and Social Force Model (SFM) to generate\ndifferent plausible people trajectories reducing collisions in a scene.\nFurthermore, a Conditional Variational Autoencoder (CVAE) module is added to\nemphasize the destination learning. We show that our method is more accurate in\nmaking predictions in UCY or BIWI datasets than most of the current\nstate-of-the-art models and also reduces collisions in comparison to other\napproaches. Through real-life experiments, we demonstrate that the model can be\nused in real-time without GPU's to perform good quality predictions with a low\ncomputational cost.\n","authors":["Oscar Gil","Alberto Sanfeliu"],"pdf_url":"https://arxiv.org/pdf/2311.10582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10572v1","updated":"2023-11-17T15:14:40Z","published":"2023-11-17T15:14:40Z","title":"SSB: Simple but Strong Baseline for Boosting Performance of Open-Set\n  Semi-Supervised Learning","summary":"  Semi-supervised learning (SSL) methods effectively leverage unlabeled data to\nimprove model generalization. However, SSL models often underperform in\nopen-set scenarios, where unlabeled data contain outliers from novel categories\nthat do not appear in the labeled set. In this paper, we study the challenging\nand realistic open-set SSL setting, where the goal is to both correctly\nclassify inliers and to detect outliers. Intuitively, the inlier classifier\nshould be trained on inlier data only. However, we find that inlier\nclassification performance can be largely improved by incorporating\nhigh-confidence pseudo-labeled data, regardless of whether they are inliers or\noutliers. Also, we propose to utilize non-linear transformations to separate\nthe features used for inlier classification and outlier detection in the\nmulti-task learning framework, preventing adverse effects between them.\nAdditionally, we introduce pseudo-negative mining, which further boosts outlier\ndetection performance. The three ingredients lead to what we call Simple but\nStrong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves\nboth inlier classification and outlier detection performance, outperforming\nexisting methods by a large margin. Our code will be released at\nhttps://github.com/YUE-FAN/SSB.\n","authors":["Yue Fan","Anna Kukleva","Dengxin Dai","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2311.10572v1.pdf","comment":"Paper accepted in ICCV 2023"},{"id":"http://arxiv.org/abs/2311.10568v1","updated":"2023-11-17T15:08:15Z","published":"2023-11-17T15:08:15Z","title":"Phase Guided Light Field for Spatial-Depth High Resolution 3D Imaging","summary":"  On 3D imaging, light field cameras typically are of single shot, and however,\nthey heavily suffer from low spatial resolution and depth accuracy. In this\npaper, by employing an optical projector to project a group of single\nhigh-frequency phase-shifted sinusoid patterns, we propose a phase guided light\nfield algorithm to significantly improve both the spatial and depth resolutions\nfor off-the-shelf light field cameras. First, for correcting the axial\naberrations caused by the main lens of our light field camera, we propose a\ndeformed cone model to calibrate our structured light field system. Second,\nover wrapped phases computed from patterned images, we propose a stereo\nmatching algorithm, i.e. phase guided sum of absolute difference, to robustly\nobtain the correspondence for each pair of neighbored two lenslets. Finally, by\nintroducing a virtual camera according to the basic geometrical optics of light\nfield imaging, we propose a reorganization strategy to reconstruct 3D point\nclouds with spatial-depth high resolution. Experimental results show that,\ncompared with the state-of-the-art active light field methods, the proposed\nreconstructs 3D point clouds with a spatial resolution of 1280$\\times$720 with\nfactors 10$\\times$ increased, while maintaining the same high depth resolution\nand needing merely a single group of high-frequency patterns.\n","authors":["Geyou Zhang","Ce Zhu","Kai Liu","Yipeng Liu"],"pdf_url":"https://arxiv.org/pdf/2311.10568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10549v1","updated":"2023-11-17T14:24:12Z","published":"2023-11-17T14:24:12Z","title":"Archtree: on-the-fly tree-structured exploration for latency-aware\n  pruning of deep neural networks","summary":"  Deep neural networks (DNNs) have become ubiquitous in addressing a number of\nproblems, particularly in computer vision. However, DNN inference is\ncomputationally intensive, which can be prohibitive e.g. when considering edge\ndevices. To solve this problem, a popular solution is DNN pruning, and more so\nstructured pruning, where coherent computational blocks (e.g. channels for\nconvolutional networks) are removed: as an exhaustive search of the space of\npruned sub-models is intractable in practice, channels are typically removed\niteratively based on an importance estimation heuristic. Recently, promising\nlatency-aware pruning methods were proposed, where channels are removed until\nthe network reaches a target budget of wall-clock latency pre-emptively\nestimated on specific hardware. In this paper, we present Archtree, a novel\nmethod for latency-driven structured pruning of DNNs. Archtree explores\nmultiple candidate pruned sub-models in parallel in a tree-like fashion,\nallowing for a better exploration of the search space. Furthermore, it involves\non-the-fly latency estimation on the target hardware, accounting for closer\nlatencies as compared to the specified budget. Empirical results on several DNN\narchitectures and target hardware show that Archtree better preserves the\noriginal model accuracy while better fitting the latency budget as compared to\nexisting state-of-the-art methods.\n","authors":["Rémi Ouazan Reboul","Edouard Yvinec","Arnaud Dapogny","Kevin Bailly"],"pdf_url":"https://arxiv.org/pdf/2311.10549v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.10543v1","updated":"2023-11-17T14:10:55Z","published":"2023-11-17T14:10:55Z","title":"Joint covariance property under geometric image transformations for\n  spatio-temporal receptive fields according to generalized Gaussian model for\n  receptive fields","summary":"  The influence of natural image transformations on receptive field responses\nis crucial for modelling visual operations in computer vision and biological\nvision. In this regard, covariance properties with respect to geometric image\ntransformations in the earliest layers of the visual hierarchy are essential\nfor expressing robust image operations and for formulating invariant visual\noperations at higher levels. This paper defines and proves a joint covariance\nproperty under compositions of spatial scaling transformations, spatial affine\ntransformations, Galilean transformations and temporal scaling transformations,\nwhich makes it possible to characterize how different types of image\ntransformations interact with each other. Specifically, the derived relations\nshow the receptive field parameters need to be transformed, in order to match\nthe output from spatio-temporal receptive fields with the underlying\nspatio-temporal image transformations.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2311.10543v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2311.09178v2","updated":"2023-11-17T14:02:35Z","published":"2023-11-15T18:15:30Z","title":"RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution","summary":"  Recently, video super resolution (VSR) has become a very impactful task in\nthe area of Computer Vision due to its various applications. In this paper, we\npropose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for\nVSR in an attempt to generate temporally coherent solutions while preserving\nspatial details. RBPGAN integrates two state-of-the-art models to get the best\nin both worlds without compromising the accuracy of produced video. The\ngenerator of the model is inspired by RBPN system, while the discriminator is\ninspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal\nconsistency over time. Our contribution together results in a model that\noutperforms earlier work in terms of temporally consistent details, as we will\ndemonstrate qualitatively and quantitatively using different datasets.\n","authors":["Israa Fahmy","Marwah Sulaiman","Zahraa Shehabeldin","Mohammed Barakat","Dareen Hussein","Mohammed El-Naggar","Hesham Eraqi","Moustafa Youssef"],"pdf_url":"https://arxiv.org/pdf/2311.09178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10529v1","updated":"2023-11-17T13:49:00Z","published":"2023-11-17T13:49:00Z","title":"Segment Anything Model with Uncertainty Rectification for Auto-Prompting\n  Medical Image Segmentation","summary":"  The introduction of the Segment Anything Model (SAM) has marked a significant\nadvancement in prompt-driven image segmentation. However, SAM's application to\nmedical image segmentation requires manual prompting of target structures to\nobtain acceptable performance, which is still labor-intensive. Despite attempts\nof auto-prompting to turn SAM into a fully automatic manner, it still exhibits\nsubpar performance and lacks of reliability in the field of medical imaging. In\nthis paper, we propose UR-SAM, an uncertainty rectified SAM framework to\nenhance the robustness and reliability for auto-prompting medical image\nsegmentation. Our method incorporates a prompt augmentation module to estimate\nthe distribution of predictions and generate uncertainty maps, and an\nuncertainty-based rectification module to further enhance the performance of\nSAM. Extensive experiments on two public 3D medical datasets covering the\nsegmentation of 35 organs demonstrate that without supplementary training or\nfine-tuning, our method further improves the segmentation performance with up\nto 10.7 % and 13.8 % in dice similarity coefficient, demonstrating efficiency\nand broad capabilities for medical image segmentation without manual prompting.\n","authors":["Yichi Zhang","Shiyao Hu","Chen Jiang","Yuan Cheng","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2311.10529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10523v1","updated":"2023-11-17T13:44:51Z","published":"2023-11-17T13:44:51Z","title":"Removing Adverse Volumetric Effects From Trained Neural Radiance Fields","summary":"  While the use of neural radiance fields (NeRFs) in different challenging\nsettings has been explored, only very recently have there been any\ncontributions that focus on the use of NeRF in foggy environments. We argue\nthat the traditional NeRF models are able to replicate scenes filled with fog\nand propose a method to remove the fog when synthesizing novel views. By\ncalculating the global contrast of a scene, we can estimate a density threshold\nthat, when applied, removes all visible fog. This makes it possible to use NeRF\nas a way of rendering clear views of objects of interest located in fog-filled\nenvironments. Additionally, to benchmark performance on such scenes, we\nintroduce a new dataset that expands some of the original synthetic NeRF scenes\nthrough the addition of fog and natural environments. The code, dataset, and\nvideo results can be found on our project page: https://vegardskui.com/fognerf/\n","authors":["Andreas L. Teigen","Mauhing Yip","Victor P. Hamran","Vegard Skui","Annette Stahl","Rudolf Mester"],"pdf_url":"https://arxiv.org/pdf/2311.10523v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2311.10522v1","updated":"2023-11-17T13:43:43Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10517v1","updated":"2023-11-17T13:40:10Z","published":"2023-11-17T13:40:10Z","title":"Mind the map! Accounting for existing map information when estimating\n  online HDMaps from sensor data","summary":"  Online High Definition Map (HDMap) estimation from sensors offers a low-cost\nalternative to manually acquired HDMaps. As such, it promises to lighten costs\nfor already HDMap-reliant Autonomous Driving systems, and potentially even\nspread their use to new systems. In this paper, we propose to improve online\nHDMap estimation by accounting for already existing maps. We identify 3\nreasonable types of useful existing maps (minimalist, noisy, and outdated). We\nalso introduce MapEX, a novel online HDMap estimation framework that accounts\nfor existing maps. MapEX achieves this by encoding map elements into query\ntokens and by refining the matching algorithm used to train classic query based\nmap estimation models. We demonstrate that MapEX brings significant\nimprovements on the nuScenes dataset. For instance, MapEX - given noisy maps -\nimproves by 38% over the MapTRv2 detector it is based on and by 16% over the\ncurrent SOTA.\n","authors":["Rémy Sun","Li Yang","Diane Lingrand","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2311.10517v1.pdf","comment":"12 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2311.10513v1","updated":"2023-11-17T13:34:58Z","published":"2023-11-17T13:34:58Z","title":"A Framework of Landsat-8 Band Selection based on UMDA for Deforestation\n  Detection","summary":"  The conservation of tropical forests is a current subject of social and\necological relevance due to their crucial role in the global ecosystem.\nUnfortunately, millions of hectares are deforested and degraded each year.\nTherefore, government or private initiatives are needed for monitoring tropical\nforests. In this sense, this work proposes a novel framework, which uses of\ndistribution estimation algorithm (UMDA) to select spectral bands from\nLandsat-8 that yield a better representation of deforestation areas to guide a\nsemantic segmentation architecture called DeepLabv3+. In performed experiments,\nit was possible to find several compositions that reach balanced accuracy\nsuperior to 90% in segment classification tasks. Furthermore, the best\ncomposition (651) found by UMDA algorithm fed the DeepLabv3+ architecture and\nsurpassed in efficiency and effectiveness all compositions compared in this\nwork.\n","authors":["Eduardo B. Neto","Paulo R. C. Pedro","Alvaro Fazenda","Fabio A. Faria"],"pdf_url":"https://arxiv.org/pdf/2311.10513v1.pdf","comment":"in Portuguese language. Best Paper Award at the Workshop of\n  Undergraduate Works (WUW), SIBGRAPI 2023"},{"id":"http://arxiv.org/abs/2306.08984v3","updated":"2023-11-17T13:14:58Z","published":"2023-06-15T09:25:04Z","title":"Tree Variational Autoencoders","summary":"  We propose Tree Variational Autoencoder (TreeVAE), a new generative\nhierarchical clustering model that learns a flexible tree-based posterior\ndistribution over latent variables. TreeVAE hierarchically divides samples\naccording to their intrinsic characteristics, shedding light on hidden\nstructures in the data. It adapts its architecture to discover the optimal tree\nfor encoding dependencies between latent variables. The proposed tree-based\ngenerative architecture enables lightweight conditional inference and improves\ngenerative performance by utilizing specialized leaf decoders. We show that\nTreeVAE uncovers underlying clusters in the data and finds meaningful\nhierarchical relations between the different groups on a variety of datasets,\nincluding real-world imaging data. We present empirically that TreeVAE provides\na more competitive log-likelihood lower bound than the sequential counterparts.\nFinally, due to its generative nature, TreeVAE is able to generate new samples\nfrom the discovered clusters via conditional sampling.\n","authors":["Laura Manduchi","Moritz Vandenhirtz","Alain Ryser","Julia Vogt"],"pdf_url":"https://arxiv.org/pdf/2306.08984v3.pdf","comment":"Accepted as Spotlight to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2303.01913v2","updated":"2023-11-17T12:47:26Z","published":"2023-03-03T13:27:00Z","title":"Bespoke: A Block-Level Neural Network Optimization Framework for\n  Low-Cost Deployment","summary":"  As deep learning models become popular, there is a lot of need for deploying\nthem to diverse device environments. Because it is costly to develop and\noptimize a neural network for every single environment, there is a line of\nresearch to search neural networks for multiple target environments\nefficiently. However, existing works for such a situation still suffer from\nrequiring many GPUs and expensive costs. Motivated by this, we propose a novel\nneural network optimization framework named Bespoke for low-cost deployment.\nOur framework searches for a lightweight model by replacing parts of an\noriginal model with randomly selected alternatives, each of which comes from a\npretrained neural network or the original model. In the practical sense,\nBespoke has two significant merits. One is that it requires near zero cost for\ndesigning the search space of neural networks. The other merit is that it\nexploits the sub-networks of public pretrained neural networks, so the total\ncost is minimal compared to the existing works. We conduct experiments\nexploring Bespoke's the merits, and the results show that it finds efficient\nmodels for multiple targets with meager cost.\n","authors":["Jong-Ryul Lee","Yong-Hyuk Moon"],"pdf_url":"https://arxiv.org/pdf/2303.01913v2.pdf","comment":"This is the extended version of our AAAI-2023 paper\n  (https://ojs.aaai.org/index.php/AAAI/article/view/26020)"},{"id":"http://arxiv.org/abs/2311.10492v1","updated":"2023-11-17T12:45:30Z","published":"2023-11-17T12:45:30Z","title":"A Relay System for Semantic Image Transmission based on Shared Feature\n  Extraction and Hyperprior Entropy Compression","summary":"  Nowadays, the need for high-quality image reconstruction and restoration is\nmore and more urgent. However, most image transmission systems may suffer from\nimage quality degradation or transmission interruption in the face of\ninterference such as channel noise and link fading. To solve this problem, a\nrelay communication network for semantic image transmission based on shared\nfeature extraction and hyperprior entropy compression (HEC) is proposed, where\nthe shared feature extraction technology based on Pearson correlation is\nproposed to eliminate partial shared feature of extracted semantic latent\nfeature. In addition, the HEC technology is used to resist the effect of\nchannel noise and link fading and carried out respectively at the source node\nand the relay node. Experimental results demonstrate that compared with other\nrecent research methods, the proposed system has lower transmission overhead\nand higher semantic image transmission performance. Particularly, under the\nsame conditions, the multi-scale structural similarity (MS-SSIM) of this system\nis superior to the comparison method by approximately 0.2.\n","authors":["Wannian An","Zhicheng Bao","Haotai Liang","Chen Dong"," Xiaodong"],"pdf_url":"https://arxiv.org/pdf/2311.10492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07945v4","updated":"2023-11-17T12:43:46Z","published":"2023-03-14T14:35:59Z","title":"Edit-A-Video: Single Video Editing with Object-Aware Consistency","summary":"  Despite the fact that text-to-video (TTV) model has recently achieved\nremarkable success, there have been few approaches on TTV for its extension to\nvideo editing. Motivated by approaches on TTV models adapting from\ndiffusion-based text-to-image (TTI) models, we suggest the video editing\nframework given only a pretrained TTI model and a single <text, video> pair,\nwhich we term Edit-A-Video. The framework consists of two stages: (1) inflating\nthe 2D model into the 3D model by appending temporal modules and tuning on the\nsource video (2) inverting the source video into the noise and editing with\ntarget text prompt and attention map injection. Each stage enables the temporal\nmodeling and preservation of semantic attributes of the source video. One of\nthe key challenges for video editing include a background inconsistency\nproblem, where the regions not included for the edit suffer from undesirable\nand inconsistent temporal alterations. To mitigate this issue, we also\nintroduce a novel mask blending method, termed as sparse-causal blending (SC\nBlending). We improve previous mask blending methods to reflect the temporal\nconsistency so that the area where the editing is applied exhibits smooth\ntransition while also achieving spatio-temporal consistency of the unedited\nregions. We present extensive experimental results over various types of text\nand videos, and demonstrate the superiority of the proposed method compared to\nbaselines in terms of background consistency, text alignment, and video editing\nquality.\n","authors":["Chaehun Shin","Heeseung Kim","Che Hyun Lee","Sang-gil Lee","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.07945v4.pdf","comment":"ACML 2023 Best Paper Award"},{"id":"http://arxiv.org/abs/2303.17245v3","updated":"2023-11-17T12:43:09Z","published":"2023-03-30T09:22:17Z","title":"Investigating and Mitigating the Side Effects of Noisy Views for\n  Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios","summary":"  Multi-view clustering (MVC) aims at exploring category structures among\nmulti-view data in self-supervised manners. Multiple views provide more\ninformation than single views and thus existing MVC methods can achieve\nsatisfactory performance. However, their performance might seriously degenerate\nwhen the views are noisy in practical multi-view scenarios. In this paper, we\nfirst formally investigate the drawback of noisy views and then propose a\ntheoretically grounded deep MVC method (namely MVCAN) to address this issue.\nSpecifically, we propose a novel MVC objective that enables un-shared\nparameters and inconsistent clustering predictions across multiple views to\nreduce the side effects of noisy views. Furthermore, a two-level multi-view\niterative optimization is designed to generate robust learning targets for\nrefining individual views' representation learning. Theoretical analysis\nreveals that MVCAN works by achieving the multi-view consistency,\ncomplementarity, and noise robustness. Finally, experiments on extensive public\ndatasets demonstrate that MVCAN outperforms state-of-the-art methods and is\nrobust against the existence of noisy views.\n","authors":["Jie Xu","Yazhou Ren","Xiaolong Wang","Lei Feng","Zheng Zhang","Gang Niu","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17245v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10476v1","updated":"2023-11-17T12:15:40Z","published":"2023-11-17T12:15:40Z","title":"FRCSyn Challenge at WACV 2024:Face Recognition Challenge in the Era of\n  Synthetic Data","summary":"  Despite the widespread adoption of face recognition technology around the\nworld, and its remarkable performance on current benchmarks, there are still\nseveral challenges that must be covered in more detail. This paper offers an\noverview of the Face Recognition Challenge in the Era of Synthetic Data\n(FRCSyn) organized at WACV 2024. This is the first international challenge\naiming to explore the use of synthetic data in face recognition to address\nexisting limitations in the technology. Specifically, the FRCSyn Challenge\ntargets concerns related to data privacy issues, demographic biases,\ngeneralization to unseen scenarios, and performance limitations in challenging\nscenarios, including significant age disparities between enrollment and\ntesting, pose variations, and occlusions. The results achieved in the FRCSyn\nChallenge, together with the proposed benchmark, contribute significantly to\nthe application of synthetic data to improve face recognition technology.\n","authors":["Pietro Melzi","Ruben Tolosana","Ruben Vera-Rodriguez","Minchul Kim","Christian Rathgeb","Xiaoming Liu","Ivan DeAndres-Tame","Aythami Morales","Julian Fierrez","Javier Ortega-Garcia","Weisong Zhao","Xiangyu Zhu","Zheyu Yan","Xiao-Yu Zhang","Jinlin Wu","Zhen Lei","Suvidha Tripathi","Mahak Kothari","Md Haider Zama","Debayan Deb","Bernardo Biesseck","Pedro Vidal","Roger Granada","Guilherme Fickel","Gustavo Führ","David Menotti","Alexander Unnervik","Anjith George","Christophe Ecabert","Hatef Otroshi Shahreza","Parsa Rahimi","Sébastien Marcel","Ioannis Sarridis","Christos Koutlis","Georgia Baltsou","Symeon Papadopoulos","Christos Diou","Nicolò Di Domenico","Guido Borghi","Lorenzo Pellegrini","Enrique Mas-Candela","Ángela Sánchez-Pérez","Andrea Atzori","Fadi Boutros","Naser Damer","Gianni Fenu","Mirko Marras"],"pdf_url":"https://arxiv.org/pdf/2311.10476v1.pdf","comment":"10 pages, 1 figure, WACV 2024 Workshops"},{"id":"http://arxiv.org/abs/2311.10472v1","updated":"2023-11-17T11:56:53Z","published":"2023-11-17T11:56:53Z","title":"End-to-end autoencoding architecture for the simultaneous generation of\n  medical images and corresponding segmentation masks","summary":"  Despite the increasing use of deep learning in medical image segmentation,\nacquiring sufficient training data remains a challenge in the medical field. In\nresponse, data augmentation techniques have been proposed; however, the\ngeneration of diverse and realistic medical images and their corresponding\nmasks remains a difficult task, especially when working with insufficient\ntraining sets. To address these limitations, we present an end-to-end\narchitecture based on the Hamiltonian Variational Autoencoder (HVAE). This\napproach yields an improved posterior distribution approximation compared to\ntraditional Variational Autoencoders (VAE), resulting in higher image\ngeneration quality. Our method outperforms generative adversarial architectures\nunder data-scarce conditions, showcasing enhancements in image quality and\nprecise tumor mask synthesis. We conduct experiments on two publicly available\ndatasets, MICCAI's Brain Tumor Segmentation Challenge (BRATS), and Head and\nNeck Tumor Segmentation Challenge (HECKTOR), demonstrating the effectiveness of\nour method on different medical imaging modalities.\n","authors":["Aghiles Kebaili","Jérôme Lapuyade-Lahorgue","Pierre Vera","Su Ruan"],"pdf_url":"https://arxiv.org/pdf/2311.10472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10463v1","updated":"2023-11-17T11:34:01Z","published":"2023-11-17T11:34:01Z","title":"Correlation-Distance Graph Learning for Treatment Response Prediction\n  from rs-fMRI","summary":"  Resting-state fMRI (rs-fMRI) functional connectivity (FC) analysis provides\nvaluable insights into the relationships between different brain regions and\ntheir potential implications for neurological or psychiatric disorders.\nHowever, specific design efforts to predict treatment response from rs-fMRI\nremain limited due to difficulties in understanding the current brain state and\nthe underlying mechanisms driving the observed patterns, which limited the\nclinical application of rs-fMRI. To overcome that, we propose a graph learning\nframework that captures comprehensive features by integrating both correlation\nand distance-based similarity measures under a contrastive loss. This approach\nresults in a more expressive framework that captures brain dynamic features at\ndifferent scales and enables more accurate prediction of treatment response.\nOur experiments on the chronic pain and depersonalization disorder datasets\ndemonstrate that our proposed method outperforms current methods in different\nscenarios. To the best of our knowledge, we are the first to explore the\nintegration of distance-based and correlation-based neural similarity into\ngraph learning for treatment response prediction.\n","authors":["Xiatian Zhang","Sisi Zheng","Hubert P. H. Shum","Haozheng Zhang","Nan Song","Mingkang Song","Hongxiao Jia"],"pdf_url":"https://arxiv.org/pdf/2311.10463v1.pdf","comment":"Proceedings of the 2023 International Conference on Neural\n  Information Processing (ICONIP)"},{"id":"http://arxiv.org/abs/2307.07482v2","updated":"2023-11-17T11:30:33Z","published":"2023-07-14T17:06:49Z","title":"Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based\n  Tumor Classification","summary":"  Whole slide image (WSI) assessment is a challenging and crucial step in\ncancer diagnosis and treatment planning. WSIs require high magnifications to\nfacilitate sub-cellular analysis. Precise annotations for patch- or even\npixel-level classifications in the context of gigapixel WSIs are tedious to\nacquire and require domain experts. Coarse-grained labels, on the other hand,\nare easily accessible, which makes WSI classification an ideal use case for\nmultiple instance learning (MIL). In our work, we propose a novel\nembedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both the\nembedding and aggregation steps. Since all-purpose visual feature\nrepresentations are not yet available, embedding models are currently limited\nin terms of generalizability. With our work, we explore the potential of\ndynamic meta-embedding based on cutting-edge self-supervised pre-trained models\nin the context of MIL. Moreover, we propose a new MIL architecture capable of\ncombining MIL-attention with correlated self-attention. The Dual-Query\nPerceiver design of our approach allows us to leverage the concept of\nself-distillation and to combine the advantages of a small model in the context\nof a low data regime with the rich feature representation of a larger model. We\ndemonstrate the superior performance of our approach on three histopathological\ndatasets, where we show improvement of up to 10% over state-of-the-art\napproaches.\n","authors":["Simon Holdenried-Krafft","Peter Somers","Ivonne A. Montes-Majarro","Diana Silimon","Cristina Tarín","Falko Fend","Hendrik P. A. Lensch"],"pdf_url":"https://arxiv.org/pdf/2307.07482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10448v1","updated":"2023-11-17T11:03:13Z","published":"2023-11-17T11:03:13Z","title":"DeepClean: Machine Unlearning on the Cheap by Resetting Privacy\n  Sensitive Weights using the Fisher Diagonal","summary":"  Machine learning models trained on sensitive or private data can\ninadvertently memorize and leak that information. Machine unlearning seeks to\nretroactively remove such details from model weights to protect privacy. We\ncontribute a lightweight unlearning algorithm that leverages the Fisher\nInformation Matrix (FIM) for selective forgetting. Prior work in this area\nrequires full retraining or large matrix inversions, which are computationally\nexpensive. Our key insight is that the diagonal elements of the FIM, which\nmeasure the sensitivity of log-likelihood to changes in weights, contain\nsufficient information for effective forgetting. Specifically, we compute the\nFIM diagonal over two subsets -- the data to retain and forget -- for all\ntrainable weights. This diagonal representation approximates the complete FIM\nwhile dramatically reducing computation. We then use it to selectively update\nweights to maximize forgetting of the sensitive subset while minimizing impact\non the retained subset. Experiments show that our algorithm can successfully\nforget any randomly selected subsets of training data across neural network\narchitectures. By leveraging the FIM diagonal, our approach provides an\ninterpretable, lightweight, and efficient solution for machine unlearning with\npractical privacy benefits.\n","authors":["Jiaeli Shi","Najah Ghalyan","Kostis Gourgoulias","John Buford","Sean Moran"],"pdf_url":"https://arxiv.org/pdf/2311.10448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10437v1","updated":"2023-11-17T10:26:26Z","published":"2023-11-17T10:26:26Z","title":"DUA-DA: Distillation-based Unbiased Alignment for Domain Adaptive Object\n  Detection","summary":"  Though feature-alignment based Domain Adaptive Object Detection (DAOD) have\nachieved remarkable progress, they ignore the source bias issue, i.e. the\naligned features are more favorable towards the source domain, leading to a\nsub-optimal adaptation. Furthermore, the presence of domain shift between the\nsource and target domains exacerbates the problem of inconsistent\nclassification and localization in general detection pipelines. To overcome\nthese challenges, we propose a novel Distillation-based Unbiased Alignment\n(DUA) framework for DAOD, which can distill the source features towards a more\nbalanced position via a pre-trained teacher model during the training process,\nalleviating the problem of source bias effectively. In addition, we design a\nTarget-Relevant Object Localization Network (TROLN), which can mine\ntarget-related knowledge to produce two classification-free metrics (IoU and\ncenterness). Accordingly, we implement a Domain-aware Consistency Enhancing\n(DCE) strategy that utilizes these two metrics to further refine classification\nconfidences, achieving a harmonization between classification and localization\nin cross-domain scenarios. Extensive experiments have been conducted to\nmanifest the effectiveness of this method, which consistently improves the\nstrong baseline by large margins, outperforming existing alignment-based works.\n","authors":["Yongchao Feng","Shiwei Li","Yingjie Gao","Ziyue Huang","Yanan Zhang","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2311.10437v1.pdf","comment":"10pages,5 figures"},{"id":"http://arxiv.org/abs/2311.10430v1","updated":"2023-11-17T10:05:10Z","published":"2023-11-17T10:05:10Z","title":"Deep Residual CNN for Multi-Class Chest Infection Diagnosis","summary":"  The advent of deep learning has significantly propelled the capabilities of\nautomated medical image diagnosis, providing valuable tools and resources in\nthe realm of healthcare and medical diagnostics. This research delves into the\ndevelopment and evaluation of a Deep Residual Convolutional Neural Network\n(CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray\nimages. The implemented model, trained and validated on a dataset amalgamated\nfrom diverse sources, demonstrated a robust overall accuracy of 93%. However,\nnuanced disparities in performance across different classes, particularly\nFibrosis, underscored the complexity and challenges inherent in automated\nmedical image diagnosis. The insights derived pave the way for future research,\nfocusing on enhancing the model's proficiency in classifying conditions that\npresent more subtle and nuanced visual features in the images, as well as\noptimizing and refining the model architecture and training process. This paper\nprovides a comprehensive exploration into the development, implementation, and\nevaluation of the model, offering insights and directions for future research\nand development in the field.\n","authors":["Ryan Donghan Kwon","Dohyun Lim","Yoonha Lee","Seung Won Lee"],"pdf_url":"https://arxiv.org/pdf/2311.10430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03574v5","updated":"2023-11-17T10:01:56Z","published":"2022-02-04T12:30:49Z","title":"Structured Prediction Problem Archive","summary":"  Structured prediction problems are one of the fundamental tools in machine\nlearning. In order to facilitate algorithm development for their numerical\nsolution, we collect in one place a large number of datasets in easy to read\nformats for a diverse set of problem classes. We provide archival links to\ndatasets, description of the considered problems and problem formats, and a\nshort summary of problem characteristics including size, number of instances\netc. For reference we also give a non-exhaustive selection of algorithms\nproposed in the literature for their solution. We hope that this central\nrepository will make benchmarking and comparison to established works easier.\nWe welcome submission of interesting new datasets and algorithms for inclusion\nin our archive.\n","authors":["Paul Swoboda","Bjoern Andres","Andrea Hornakova","Florian Bernard","Jannik Irmai","Paul Roetzer","Bogdan Savchynskyy","David Stein","Ahmed Abbas"],"pdf_url":"https://arxiv.org/pdf/2202.03574v5.pdf","comment":"Added multicast instances from Andres group"},{"id":"http://arxiv.org/abs/2303.11573v2","updated":"2023-11-17T09:33:22Z","published":"2023-03-21T03:41:57Z","title":"BigSmall: Efficient Multi-Task Learning for Disparate Spatial and\n  Temporal Physiological Measurements","summary":"  Understanding of human visual perception has historically inspired the design\nof computer vision architectures. As an example, perception occurs at different\nscales both spatially and temporally, suggesting that the extraction of salient\nvisual information may be made more effective by paying attention to specific\nfeatures at varying scales. Visual changes in the body due to physiological\nprocesses also occur at different scales and with modality-specific\ncharacteristic properties. Inspired by this, we present BigSmall, an efficient\narchitecture for physiological and behavioral measurement. We present the first\njoint camera-based facial action, cardiac, and pulmonary measurement model. We\npropose a multi-branch network with wrapping temporal shift modules that yields\nboth accuracy and efficiency gains. We observe that fusing low-level features\nleads to suboptimal performance, but that fusing high level features enables\nefficiency gains with negligible loss in accuracy. Experimental results\ndemonstrate that BigSmall significantly reduces the computational costs.\nFurthermore, compared to existing task-specific models, BigSmall achieves\ncomparable or better results on multiple physiological measurement tasks\nsimultaneously with a unified model.\n","authors":["Girish Narayanswamy","Yujia Liu","Yuzhe Yang","Chengqian Ma","Xin Liu","Daniel McDuff","Shwetak Patel"],"pdf_url":"https://arxiv.org/pdf/2303.11573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10408v1","updated":"2023-11-17T09:24:04Z","published":"2023-11-17T09:24:04Z","title":"Deep Learning based CNN Model for Classification and Detection of\n  Individuals Wearing Face Mask","summary":"  In response to the global COVID-19 pandemic, there has been a critical demand\nfor protective measures, with face masks emerging as a primary safeguard. The\napproach involves a two-fold strategy: first, recognizing the presence of a\nface by detecting faces, and second, identifying masks on those faces. This\nproject utilizes deep learning to create a model that can detect face masks in\nreal-time streaming video as well as images. Face detection, a facet of object\ndetection, finds applications in diverse fields such as security, biometrics,\nand law enforcement. Various detector systems worldwide have been developed and\nimplemented, with convolutional neural networks chosen for their superior\nperformance accuracy and speed in object detection. Experimental results attest\nto the model's excellent accuracy on test data. The primary focus of this\nresearch is to enhance security, particularly in sensitive areas. The research\npaper proposes a rapid image pre-processing method with masks centred on faces.\nEmploying feature extraction and Convolutional Neural Network, the system\nclassifies and detects individuals wearing masks. The research unfolds in three\nstages: image pre-processing, image cropping, and image classification,\ncollectively contributing to the identification of masked faces. Continuous\nsurveillance through webcams or CCTV cameras ensures constant monitoring,\ntriggering a security alert if a person is detected without a mask.\n","authors":["R. Chinnaiyan","Iyyappan M","Al Raiyan Shariff A","Kondaveeti Sai","Mallikarjunaiah B M","P Bharath"],"pdf_url":"https://arxiv.org/pdf/2311.10408v1.pdf","comment":"8 Pages , 6 figures , 1 Table"},{"id":"http://arxiv.org/abs/2311.10399v1","updated":"2023-11-17T09:00:44Z","published":"2023-11-17T09:00:44Z","title":"Optimized Deep Learning Models for AUV Seabed Image Analysis","summary":"  Using autonomous underwater vehicles, or AUVs, has completely changed how we\ngather data from the ocean floor. AUV innovation has advanced significantly,\nespecially in the analysis of images, due to the increasing need for accurate\nand efficient seafloor mapping. This blog post provides a detailed summary and\ncomparison of the most current advancements in AUV seafloor image processing.\nWe will go into the realm of undersea technology, covering everything through\ncomputer and algorithmic advancements to advances in sensors and cameras. After\nreading this page through to the end, you will have a solid understanding of\nthe most up-to-date techniques and tools for using AUVs to process seabed\nphotos and how they could further our comprehension of the ocean floor\n","authors":["Rajesh Sharma R","Akey Sungheetha","Chinnaiyan R"],"pdf_url":"https://arxiv.org/pdf/2311.10399v1.pdf","comment":"6 pages , 4 figures"},{"id":"http://arxiv.org/abs/2207.08387v2","updated":"2023-11-17T08:50:15Z","published":"2022-07-18T05:38:37Z","title":"A Semantic-aware Attention and Visual Shielding Network for\n  Cloth-changing Person Re-identification","summary":"  Cloth-changing person reidentification (ReID) is a newly emerging research\ntopic that aims to retrieve pedestrians whose clothes are changed. Since the\nhuman appearance with different clothes exhibits large variations, it is very\ndifficult for existing approaches to extract discriminative and robust feature\nrepresentations. Current works mainly focus on body shape or contour sketches,\nbut the human semantic information and the potential consistency of pedestrian\nfeatures before and after changing clothes are not fully explored or are\nignored. To solve these issues, in this work, a novel semantic-aware attention\nand visual shielding network for cloth-changing person ReID (abbreviated as\nSAVS) is proposed where the key idea is to shield clues related to the\nappearance of clothes and only focus on visual semantic information that is not\nsensitive to view/posture changes. Specifically, a visual semantic encoder is\nfirst employed to locate the human body and clothing regions based on human\nsemantic segmentation information. Then, a human semantic attention module\n(HSA) is proposed to highlight the human semantic information and reweight the\nvisual feature map. In addition, a visual clothes shielding module (VCS) is\nalso designed to extract a more robust feature representation for the\ncloth-changing task by covering the clothing regions and focusing the model on\nthe visual semantic information unrelated to the clothes. Most importantly,\nthese two modules are jointly explored in an end-to-end unified framework.\nExtensive experiments demonstrate that the proposed method can significantly\noutperform state-of-the-art methods, and more robust features can be extracted\nfor cloth-changing persons. Compared with FSAM (published in CVPR 2021), this\nmethod can achieve improvements of 32.7% (16.5%) and 14.9% (-) on the LTCC and\nPRCC datasets in terms of mAP (rank-1), respectively.\n","authors":["Zan Gao","Hongwei Wei","Weili Guan","Jie Nie","Meng Wang","Shenyong Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08387v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2108.04527"},{"id":"http://arxiv.org/abs/2304.04400v2","updated":"2023-11-17T08:49:56Z","published":"2023-04-10T06:05:54Z","title":"Identity-Guided Collaborative Learning for Cloth-Changing Person\n  Reidentification","summary":"  Cloth-changing person reidentification (ReID) is a newly emerging research\ntopic that is aimed at addressing the issues of large feature variations due to\ncloth-changing and pedestrian view/pose changes. Although significant progress\nhas been achieved by introducing extra information (e.g., human contour\nsketching information, human body keypoints, and 3D human information),\ncloth-changing person ReID is still challenging due to impressionable\npedestrian representations. Moreover, human semantic information and pedestrian\nidentity information are not fully explored. To solve these issues, we propose\na novel identity-guided collaborative learning scheme (IGCL) for cloth-changing\nperson ReID, where the human semantic is fully utilized and the identity is\nunchangeable to guide collaborative learning. First, we design a novel clothing\nattention degradation stream to reasonably reduce the interference caused by\nclothing information where clothing attention and mid-level collaborative\nlearning are employed. Second, we propose a human semantic attention and body\njigsaw stream to highlight the human semantic information and simulate\ndifferent poses of the same identity. In this way, the extraction features not\nonly focus on human semantic information that is unrelated to the background\nbut also are suitable for pedestrian pose variations. Moreover, a pedestrian\nidentity enhancement stream is further proposed to enhance the identity\nimportance and extract more favorable identity robust features. Most\nimportantly, all these streams are jointly explored in an end-to-end unified\nframework, and the identity is utilized to guide the optimization. Extensive\nexperiments on five public clothing person ReID datasets demonstrate that the\nproposed IGCL significantly outperforms SOTA methods and that the extracted\nfeature is more robust, discriminative, and clothing-irrelevant.\n","authors":["Zan Gao","Shenxun Wei","Weili Guan","Lei Zhu","Meng Wang","Shenyong Chen"],"pdf_url":"https://arxiv.org/pdf/2304.04400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10389v1","updated":"2023-11-17T08:35:02Z","published":"2023-11-17T08:35:02Z","title":"Two-Factor Authentication Approach Based on Behavior Patterns for\n  Defeating Puppet Attacks","summary":"  Fingerprint traits are widely recognized for their unique qualities and\nsecurity benefits. Despite their extensive use, fingerprint features can be\nvulnerable to puppet attacks, where attackers manipulate a reluctant but\ngenuine user into completing the authentication process. Defending against such\nattacks is challenging due to the coexistence of a legitimate identity and an\nillegitimate intent. In this paper, we propose PUPGUARD, a solution designed to\nguard against puppet attacks. This method is based on user behavioral patterns,\nspecifically, the user needs to press the capture device twice successively\nwith different fingers during the authentication process. PUPGUARD leverages\nboth the image features of fingerprints and the timing characteristics of the\npressing intervals to establish two-factor authentication. More specifically,\nafter extracting image features and timing characteristics, and performing\nfeature selection on the image features, PUPGUARD fuses these two features into\na one-dimensional feature vector, and feeds it into a one-class classifier to\nobtain the classification result. This two-factor authentication method\nemphasizes dynamic behavioral patterns during the authentication process,\nthereby enhancing security against puppet attacks. To assess PUPGUARD's\neffectiveness, we conducted experiments on datasets collected from 31 subjects,\nincluding image features and timing characteristics. Our experimental results\ndemonstrate that PUPGUARD achieves an impressive accuracy rate of 97.87% and a\nremarkably low false positive rate (FPR) of 1.89%. Furthermore, we conducted\ncomparative experiments to validate the superiority of combining image features\nand timing characteristics within PUPGUARD for enhancing resistance against\npuppet attacks.\n","authors":["Wenhao Wang","Guyue Li","Zhiming Chu","Haobo Li","Daniele Faccio"],"pdf_url":"https://arxiv.org/pdf/2311.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02058v2","updated":"2023-11-17T08:26:16Z","published":"2023-11-03T17:38:35Z","title":"LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery","summary":"  We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.\n","authors":["Weikang Wan","Yifeng Zhu","Rutav Shah","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.02058v2.pdf","comment":"Submitted to ICRA 2024"},{"id":"http://arxiv.org/abs/2306.04226v2","updated":"2023-11-17T08:23:05Z","published":"2023-06-07T08:05:46Z","title":"Normalization Layers Are All That Sharpness-Aware Minimization Needs","summary":"  Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima\nand has been shown to enhance generalization performance in various settings.\nIn this work we show that perturbing only the affine normalization parameters\n(typically comprising 0.1% of the total parameters) in the adversarial step of\nSAM can outperform perturbing all of the parameters.This finding generalizes to\ndifferent SAM variants and both ResNet (Batch Normalization) and Vision\nTransformer (Layer Normalization) architectures. We consider alternative sparse\nperturbation approaches and find that these do not achieve similar performance\nenhancement at such extreme sparsity levels, showing that this behaviour is\nunique to the normalization layers. Although our findings reaffirm the\neffectiveness of SAM in improving generalization performance, they cast doubt\non whether this is solely caused by reduced sharpness.\n","authors":["Maximilian Mueller","Tiffany Vlaar","David Rolnick","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2306.04226v2.pdf","comment":"camera ready version"},{"id":"http://arxiv.org/abs/2311.10382v1","updated":"2023-11-17T08:17:49Z","published":"2023-11-17T08:17:49Z","title":"Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking","summary":"  Multi-Object Tracking (MOT) remains a vital component of intelligent video\nanalysis, which aims to locate targets and maintain a consistent identity for\neach target throughout a video sequence. Existing works usually learn a\ndiscriminative feature representation, such as motion and appearance, to\nassociate the detections across frames, which are easily affected by mutual\nocclusion and background clutter in practice. In this paper, we propose a\nsimple yet effective two-stage feature learning paradigm to jointly learn\nsingle-shot and multi-shot features for different targets, so as to achieve\nrobust data association in the tracking process. For the detections without\nbeing associated, we design a novel single-shot feature learning module to\nextract discriminative features of each detection, which can efficiently\nassociate targets between adjacent frames. For the tracklets being lost several\nframes, we design a novel multi-shot feature learning module to extract\ndiscriminative features of each tracklet, which can accurately refind these\nlost targets after a long period. Once equipped with a simple data association\nlogic, the resulting VisualTracker can perform robust MOT based on the\nsingle-shot and multi-shot feature representations. Extensive experimental\nresults demonstrate that our method has achieved significant improvements on\nMOT17 and MOT20 datasets while reaching state-of-the-art performance on\nDanceTrack dataset.\n","authors":["Yizhe Li","Sanping Zhou","Zheng Qin","Le Wang","Jinjun Wang","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2311.10382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10380v1","updated":"2023-11-17T08:14:24Z","published":"2023-11-17T08:14:24Z","title":"MSE-Nets: Multi-annotated Semi-supervised Ensemble Networks for\n  Improving Segmentation of Medical Image with Ambiguous Boundaries","summary":"  Medical image segmentation annotations exhibit variations among experts due\nto the ambiguous boundaries of segmented objects and backgrounds in medical\nimages. Although using multiple annotations for each image in the\nfully-supervised has been extensively studied for training deep models,\nobtaining a large amount of multi-annotated data is challenging due to the\nsubstantial time and manpower costs required for segmentation annotations,\nresulting in most images lacking any annotations. To address this, we propose\nMulti-annotated Semi-supervised Ensemble Networks (MSE-Nets) for learning\nsegmentation from limited multi-annotated and abundant unannotated data.\nSpecifically, we introduce the Network Pairwise Consistency Enhancement (NPCE)\nmodule and Multi-Network Pseudo Supervised (MNPS) module to enhance MSE-Nets\nfor the segmentation task by considering two major factors: (1) to optimize the\nutilization of all accessible multi-annotated data, the NPCE separates\n(dis)agreement annotations of multi-annotated data at the pixel level and\nhandles agreement and disagreement annotations in different ways, (2) to\nmitigate the introduction of imprecise pseudo-labels, the MNPS extends the\ntraining data by leveraging consistent pseudo-labels from unannotated data.\nFinally, we improve confidence calibration by averaging the predictions of base\nnetworks. Experiments on the ISIC dataset show that we reduced the demand for\nmulti-annotated data by 97.75\\% and narrowed the gap with the best\nfully-supervised baseline to just a Jaccard index of 4\\%. Furthermore, compared\nto other semi-supervised methods that rely only on a single annotation or a\ncombined fusion approach, the comprehensive experimental results on ISIC and\nRIGA datasets demonstrate the superior performance of our proposed method in\nmedical image segmentation with ambiguous boundaries.\n","authors":["Shuai Wang","Tengjin Weng","Jingyi Wang","Yang Shen","Zhidong Zhao","Yixiu Liu","Pengfei Jiao","Zhiming Cheng","Yaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2311.10380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11239v2","updated":"2023-11-17T07:57:57Z","published":"2023-08-22T07:27:09Z","title":"LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and\n  Bootstrapped Self-training","summary":"  Learning object segmentation in image and video datasets without human\nsupervision is a challenging problem. Humans easily identify moving salient\nobjects in videos using the gestalt principle of common fate, which suggests\nthat what moves together belongs together. Building upon this idea, we propose\na self-supervised object discovery approach that leverages motion and\nappearance information to produce high-quality object segmentation masks.\nSpecifically, we redesign the traditional graph cut on images to include motion\ninformation in a linear combination with appearance information to produce edge\nweights. Remarkably, this step produces object segmentation masks comparable to\nthe current state-of-the-art on multiple benchmarks. To further improve\nperformance, we bootstrap a segmentation network trained on these preliminary\nmasks as pseudo-ground truths to learn from its own outputs via self-training.\nWe demonstrate the effectiveness of our approach, named LOCATE, on multiple\nstandard video object segmentation, image saliency detection, and object\nsegmentation benchmarks, achieving results on par with and, in many cases\nsurpassing state-of-the-art methods. We also demonstrate the transferability of\nour approach to novel domains through a qualitative study on in-the-wild\nimages. Additionally, we present extensive ablation analysis to support our\ndesign choices and highlight the contribution of each component of our proposed\nmethod.\n","authors":["Silky Singh","Shripad Deshmukh","Mausoom Sarkar","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2308.11239v2.pdf","comment":"Accepted to British Machine Vision Conference (BMVC) 2023"},{"id":"http://arxiv.org/abs/2311.10366v1","updated":"2023-11-17T07:39:42Z","published":"2023-11-17T07:39:42Z","title":"Breaking Temporal Consistency: Generating Video Universal Adversarial\n  Perturbations Using Image Models","summary":"  As video analysis using deep learning models becomes more widespread, the\nvulnerability of such models to adversarial attacks is becoming a pressing\nconcern. In particular, Universal Adversarial Perturbation (UAP) poses a\nsignificant threat, as a single perturbation can mislead deep learning models\non entire datasets. We propose a novel video UAP using image data and image\nmodel. This enables us to take advantage of the rich image data and image\nmodel-based studies available for video applications. However, there is a\nchallenge that image models are limited in their ability to analyze the\ntemporal aspects of videos, which is crucial for a successful video attack. To\naddress this challenge, we introduce the Breaking Temporal Consistency (BTC)\nmethod, which is the first attempt to incorporate temporal information into\nvideo attacks using image models. We aim to generate adversarial videos that\nhave opposite patterns to the original. Specifically, BTC-UAP minimizes the\nfeature similarity between neighboring frames in videos. Our approach is simple\nbut effective at attacking unseen video models. Additionally, it is applicable\nto videos of varying lengths and invariant to temporal shifts. Our approach\nsurpasses existing methods in terms of effectiveness on various datasets,\nincluding ImageNet, UCF-101, and Kinetics-400.\n","authors":["Hee-Seon Kim","Minji Son","Minbeom Kim","Myung-Joon Kwon","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2311.10366v1.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2311.10365v1","updated":"2023-11-17T07:37:41Z","published":"2023-11-17T07:37:41Z","title":"Dates Fruit Disease Recognition using Machine Learning","summary":"  Many countries such as Saudi Arabia, Morocco and Tunisia are among the top\nexporters and consumers of palm date fruits. Date fruit production plays a\nmajor role in the economies of the date fruit exporting countries. Date fruits\nare susceptible to disease just like any fruit and early detection and\nintervention can end up saving the produce. However, with the vast farming\nlands, it is nearly impossible for farmers to observe date trees on a frequent\nbasis for early disease detection. In addition, even with human observation the\nprocess is prone to human error and increases the date fruit cost. With the\nrecent advances in computer vision, machine learning, drone technology, and\nother technologies; an integrated solution can be proposed for the automatic\ndetection of date fruit disease. In this paper, a hybrid features based method\nwith the standard classifiers is proposed based on the extraction of L*a*b\ncolor features, statistical features, and Discrete Wavelet Transform (DWT)\ntexture features for the early detection and classification of date fruit\ndisease. A dataset was developed for this work consisting of 871 images divided\ninto the following classes; Healthy date, Initial stage of disease,\nMalnourished date, and Parasite infected. The extracted features were input to\ncommon classifiers such as the Random Forest (RF), Multilayer Perceptron (MLP),\nNa\\\"ive Bayes (NB), and Fuzzy Decision Trees (FDT). The highest average\naccuracy was achieved when combining the L*a*b, Statistical, and DWT Features.\n","authors":["Ghassen Ben Brahim","Jaafar Alghazo","Ghazanfar Latif","Khalid Alnujaidi"],"pdf_url":"https://arxiv.org/pdf/2311.10365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10361v1","updated":"2023-11-17T07:30:00Z","published":"2023-11-17T07:30:00Z","title":"Video-based Sequential Bayesian Homography Estimation for Soccer Field\n  Registration","summary":"  A novel Bayesian framework is proposed, which explicitly relates the\nhomography of one video frame to the next through an affine transformation\nwhile explicitly modelling keypoint uncertainty. The literature has previously\nused differential homography between subsequent frames, but not in a Bayesian\nsetting. In cases where Bayesian methods have been applied, camera motion is\nnot adequately modelled, and keypoints are treated as deterministic. The\nproposed method, Bayesian Homography Inference from Tracked Keypoints (BHITK),\nemploys a two-stage Kalman filter and significantly improves existing methods.\nExisting keypoint detection methods may be easily augmented with BHITK. It\nenables less sophisticated and less computationally expensive methods to\noutperform the state-of-the-art approaches in most homography evaluation\nmetrics. Furthermore, the homography annotations of the WorldCup and\nTS-WorldCup datasets have been refined using a custom homography annotation\ntool released for public use. The refined datasets are consolidated and\nreleased as the consolidated and refined WorldCup (CARWC) dataset.\n","authors":["Paul J. Claasen","J. P. de Villiers"],"pdf_url":"https://arxiv.org/pdf/2311.10361v1.pdf","comment":"Submitted to Expert Systems with Applications and currently under\n  review"},{"id":"http://arxiv.org/abs/2310.15105v4","updated":"2023-11-17T07:27:34Z","published":"2023-10-23T17:12:01Z","title":"FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained\n  Models in Few-Shot Learning","summary":"  Due to the limited availability of data, existing few-shot learning methods\ntrained from scratch fail to achieve satisfactory performance. In contrast,\nlarge-scale pre-trained models such as CLIP demonstrate remarkable few-shot and\nzero-shot capabilities. To enhance the performance of pre-trained models for\ndownstream tasks, fine-tuning the model on downstream data is frequently\nnecessary. However, fine-tuning the pre-trained model leads to a decrease in\nits generalizability in the presence of distribution shift, while the limited\nnumber of samples in few-shot learning makes the model highly susceptible to\noverfitting. Consequently, existing methods for fine-tuning few-shot learning\nprimarily focus on fine-tuning the model's classification head or introducing\nadditional structure. In this paper, we introduce a fine-tuning approach termed\nFeature Discrimination Alignment (FD-Align). Our method aims to bolster the\nmodel's generalizability by preserving the consistency of spurious features\nacross the fine-tuning process. Extensive experimental results validate the\nefficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model\ncan seamlessly integrate with existing methods, leading to performance\nimprovements. Our code can be found in https://github.com/skingorz/FD-Align.\n","authors":["Kun Song","Huimin Ma","Bochao Zou","Huishuai Zhang","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2310.15105v4.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2306.06048v2","updated":"2023-11-17T07:22:04Z","published":"2023-06-09T17:16:50Z","title":"How Does Fine-Tuning Impact Out-of-Distribution Detection for\n  Vision-Language Models?","summary":"  Recent large vision-language models such as CLIP have shown remarkable\nout-of-distribution (OOD) detection and generalization performance. However,\ntheir zero-shot in-distribution (ID) accuracy is often limited for downstream\ndatasets. Recent CLIP-based fine-tuning methods such as prompt learning have\ndemonstrated significant improvements in ID classification and OOD\ngeneralization where OOD labels are available. Nonetheless, it remains unclear\nwhether the model is reliable to semantic shifts without OOD labels. In this\npaper, we aim to bridge the gap and present a comprehensive study to understand\nhow fine-tuning impact OOD detection for few-shot downstream tasks. By framing\nOOD detection as multi-modal concept matching, we establish a connection\nbetween fine-tuning methods and various OOD scores. Our results suggest that a\nproper choice of OOD scores is essential for CLIP-based fine-tuning. In\nparticular, the maximum concept matching (MCM) score provides a promising\nsolution consistently. We also show that prompt learning demonstrates the\nstate-of-the-art OOD detection performance over the zero-shot counterpart.\n","authors":["Yifei Ming","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2306.06048v2.pdf","comment":"Accepted to IJCV 2023"},{"id":"http://arxiv.org/abs/2311.10356v1","updated":"2023-11-17T07:06:21Z","published":"2023-11-17T07:06:21Z","title":"Garment Recovery with Shape and Deformation Priors","summary":"  While modeling people wearing tight-fitting clothing has made great strides\nin recent years, loose-fitting clothing remains a challenge. We propose a\nmethod that delivers realistic garment models from real-world images,\nregardless of garment shape or deformation. To this end, we introduce a fitting\napproach that utilizes shape and deformation priors learned from synthetic data\nto accurately capture garment shapes and deformations, including large ones.\nNot only does our approach recover the garment geometry accurately, it also\nyields models that can be directly used by downstream applications such as\nanimation and simulation.\n","authors":["Ren Li","Corentin Dumery","Benoît Guillard","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2311.10356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02256v2","updated":"2023-11-17T06:58:21Z","published":"2023-11-03T22:13:58Z","title":"Image Recognition of Oil Leakage Area Based on Logical Semantic\n  Discrimination","summary":"  Implementing precise detection of oil leaks in peak load equipment through\nimage analysis can significantly enhance inspection quality and ensure the\nsystem's safety and reliability. However, challenges such as varying shapes of\noil-stained regions, background noise, and fluctuating lighting conditions\ncomplicate the detection process. To address this, the integration of logical\nrule-based discrimination into image recognition has been proposed. This\napproach involves recognizing the spatial relationships among objects to\nsemantically segment images of oil spills using a Mask RCNN network. The\nprocess begins with histogram equalization to enhance the original image,\nfollowed by the use of Mask RCNN to identify the preliminary positions and\noutlines of oil tanks, the ground, and areas of potential oil contamination.\nSubsequent to this identification, the spatial relationships between these\nobjects are analyzed. Logical rules are then applied to ascertain whether the\nsuspected areas are indeed oil spills. This method's effectiveness has been\nconfirmed by testing on images captured from peak power equipment in the field.\nThe results indicate that this approach can adeptly tackle the challenges in\nidentifying oil-contaminated areas, showing a substantial improvement in\naccuracy compared to existing methods.\n","authors":["Weiying Lin","Che Liu","Xin Zhang","Zhen Wei","Sizhe Li","Xun Ma"],"pdf_url":"https://arxiv.org/pdf/2311.02256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10349v1","updated":"2023-11-17T06:36:43Z","published":"2023-11-17T06:36:43Z","title":"Pseudo Label-Guided Data Fusion and Output Consistency for\n  Semi-Supervised Medical Image Segmentation","summary":"  Supervised learning algorithms based on Convolutional Neural Networks have\nbecome the benchmark for medical image segmentation tasks, but their\neffectiveness heavily relies on a large amount of labeled data. However,\nannotating medical image datasets is a laborious and time-consuming process.\nInspired by semi-supervised algorithms that use both labeled and unlabeled data\nfor training, we propose the PLGDF framework, which builds upon the mean\nteacher network for segmenting medical images with less annotation. We propose\na novel pseudo-label utilization scheme, which combines labeled and unlabeled\ndata to augment the dataset effectively. Additionally, we enforce the\nconsistency between different scales in the decoder module of the segmentation\nnetwork and propose a loss function suitable for evaluating the consistency.\nMoreover, we incorporate a sharpening operation on the predicted results,\nfurther enhancing the accuracy of the segmentation.\n  Extensive experiments on three publicly available datasets demonstrate that\nthe PLGDF framework can largely improve performance by incorporating the\nunlabeled data. Meanwhile, our framework yields superior performance compared\nto six state-of-the-art semi-supervised learning methods. The codes of this\nstudy are available at https://github.com/ortonwang/PLGDF.\n","authors":["Tao Wang","Yuanbin Chen","Xinlin Zhang","Yuanbo Zhou","Junlin Lan","Bizhe Bai","Tao Tan","Min Du","Qinquan Gao","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2311.10349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10343v1","updated":"2023-11-17T06:07:54Z","published":"2023-11-17T06:07:54Z","title":"Enhancing Student Engagement in Online Learning through Facial\n  Expression Analysis and Complex Emotion Recognition using Deep Learning","summary":"  In response to the COVID-19 pandemic, traditional physical classrooms have\ntransitioned to online environments, necessitating effective strategies to\nensure sustained student engagement. A significant challenge in online teaching\nis the absence of real-time feedback from teachers on students learning\nprogress. This paper introduces a novel approach employing deep learning\ntechniques based on facial expressions to assess students engagement levels\nduring online learning sessions. Human emotions cannot be adequately conveyed\nby a student using only the basic emotions, including anger, disgust, fear,\njoy, sadness, surprise, and neutrality. To address this challenge, proposed a\ngeneration of four complex emotions such as confusion, satisfaction,\ndisappointment, and frustration by combining the basic emotions. These complex\nemotions are often experienced simultaneously by students during the learning\nsession. To depict these emotions dynamically,utilized a continuous stream of\nimage frames instead of discrete images. The proposed work utilized a\nConvolutional Neural Network (CNN) model to categorize the fundamental\nemotional states of learners accurately. The proposed CNN model demonstrates\nstrong performance, achieving a 95% accuracy in precise categorization of\nlearner emotions.\n","authors":["Rekha R Nair","Tina Babu","Pavithra K"],"pdf_url":"https://arxiv.org/pdf/2311.10343v1.pdf","comment":"Face emotion recognition work"},{"id":"http://arxiv.org/abs/2311.10339v1","updated":"2023-11-17T05:49:50Z","published":"2023-11-17T05:49:50Z","title":"A2XP: Towards Private Domain Generalization","summary":"  Deep Neural Networks (DNNs) have become pivotal in various fields, especially\nin computer vision, outperforming previous methodologies. A critical challenge\nin their deployment is the bias inherent in data across different domains, such\nas image style, and environmental conditions, leading to domain gaps. This\nnecessitates techniques for learning general representations from biased\ntraining data, known as domain generalization. This paper presents Attend to\neXpert Prompts (A2XP), a novel approach for domain generalization that\npreserves the privacy and integrity of the network architecture. A2XP consists\nof two phases: Expert Adaptation and Domain Generalization. In the first phase,\nprompts for each source domain are optimized to guide the model towards the\noptimal direction. In the second phase, two embedder networks are trained to\neffectively amalgamate these expert prompts, aiming for an optimal output. Our\nextensive experiments demonstrate that A2XP achieves state-of-the-art results\nover existing non-private domain generalization methods. The experimental\nresults validate that the proposed approach not only tackles the domain\ngeneralization challenge in DNNs but also offers a privacy-preserving,\nefficient solution to the broader field of computer vision.\n","authors":["Geunhyeok Yu","Hyoseok Hwang"],"pdf_url":"https://arxiv.org/pdf/2311.10339v1.pdf","comment":"10 pages (8 pages except for references), 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2311.10336v1","updated":"2023-11-17T05:41:23Z","published":"2023-11-17T05:41:23Z","title":"Cooperative Perception with Learning-Based V2V communications","summary":"  Cooperative perception has been widely used in autonomous driving to\nalleviate the inherent limitation of single automated vehicle perception. To\nenable cooperation, vehicle-to-vehicle (V2V) communication plays an\nindispensable role. This work analyzes the performance of cooperative\nperception accounting for communications channel impairments. Different fusion\nmethods and channel impairments are evaluated. A new late fusion scheme is\nproposed to leverage the robustness of intermediate features. In order to\ncompress the data size incurred by cooperation, a convolution neural\nnetwork-based autoencoder is adopted. Numerical results demonstrate that\nintermediate fusion is more robust to channel impairments than early fusion and\nlate fusion, when the SNR is greater than 0 dB. Also, the proposed fusion\nscheme outperforms the conventional late fusion using detection outputs, and\nautoencoder provides a good compromise between detection accuracy and bandwidth\nusage.\n","authors":["Chenguang Liu","Yunfei Chen","Jianjun Chen","Ryan Payton","Michael Riley","Shuang-Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2311.10336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05370v2","updated":"2023-11-17T05:38:45Z","published":"2023-05-09T12:05:14Z","title":"MSVQ: Self-Supervised Learning with Multiple Sample Views and Queues","summary":"  Self-supervised methods based on contrastive learning have achieved great\nsuccess in unsupervised visual representation learning. However, most methods\nunder this framework suffer from the problem of false negative samples.\nInspired by the mean shift for self-supervised learning, we propose a new\nsimple framework, namely Multiple Sample Views and Queues (MSVQ). We jointly\nconstruct three soft labels on-the-fly by utilizing two complementary and\nsymmetric approaches: multiple augmented positive views and two momentum\nencoders that generate various semantic features for negative samples. Two\nteacher networks perform similarity relationship calculations with negative\nsamples and then transfer this knowledge to the student network. Let the\nstudent network mimic the similarity relationships between the samples, thus\ngiving the student network a more flexible ability to identify false negative\nsamples in the dataset. The classification results on four benchmark image\ndatasets demonstrate the high effectiveness and efficiency of our approach\ncompared to some classical methods. Source code and pretrained models are\navailable \\href{https://github.com/pc-cp/MSVQ}{here}.\n","authors":["Chen Peng","Xianzhong Long","Yun Li"],"pdf_url":"https://arxiv.org/pdf/2305.05370v2.pdf","comment":"Accepted in KBS(Knowledge-Based Systems)"},{"id":"http://arxiv.org/abs/2311.10331v1","updated":"2023-11-17T05:23:57Z","published":"2023-11-17T05:23:57Z","title":"Leveraging Multimodal Fusion for Enhanced Diagnosis of Multiple Retinal\n  Diseases in Ultra-wide OCTA","summary":"  Ultra-wide optical coherence tomography angiography (UW-OCTA) is an emerging\nimaging technique that offers significant advantages over traditional OCTA by\nproviding an exceptionally wide scanning range of up to 24 x 20 $mm^{2}$,\ncovering both the anterior and posterior regions of the retina. However, the\ncurrently accessible UW-OCTA datasets suffer from limited comprehensive\nhierarchical information and corresponding disease annotations. To address this\nlimitation, we have curated the pioneering M3OCTA dataset, which is the first\nmultimodal (i.e., multilayer), multi-disease, and widest field-of-view UW-OCTA\ndataset. Furthermore, the effective utilization of multi-layer ultra-wide\nocular vasculature information from UW-OCTA remains underdeveloped. To tackle\nthis challenge, we propose the first cross-modal fusion framework that\nleverages multi-modal information for diagnosing multiple diseases. Through\nextensive experiments conducted on our openly available M3OCTA dataset, we\ndemonstrate the effectiveness and superior performance of our method, both in\nfixed and varying modalities settings. The construction of the M3OCTA dataset,\nthe first multimodal OCTA dataset encompassing multiple diseases, aims to\nadvance research in the ophthalmic image analysis community.\n","authors":["Hao Wei","Peilun Shi","Guitao Bai","Minqing Zhang","Shuangle Li","Wu Yuan"],"pdf_url":"https://arxiv.org/pdf/2311.10331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00022v2","updated":"2023-11-17T05:22:38Z","published":"2023-09-28T18:04:43Z","title":"CtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image\n  Understanding","summary":"  Learning representations through self-supervision on unlabeled data has\nproven highly effective for understanding diverse images. However, remote\nsensing images often have complex and densely populated scenes with multiple\nland objects and no clear foreground objects. This intrinsic property generates\nhigh object density, resulting in false positive pairs or missing contextual\ninformation in self-supervised learning. To address these problems, we propose\na context-enhanced masked image modeling method (CtxMIM), a simple yet\nefficient MIM-based self-supervised learning for remote sensing image\nunderstanding. CtxMIM formulates original image patches as a reconstructive\ntemplate and employs a Siamese framework to operate on two sets of image\npatches. A context-enhanced generative branch is introduced to provide\ncontextual information through context consistency constraints in the\nreconstruction. With the simple and elegant design, CtxMIM encourages the\npre-training model to learn object-level or pixel-level features on a\nlarge-scale dataset without specific temporal or geographical constraints.\nFinally, extensive experiments show that features learned by CtxMIM outperform\nfully supervised and state-of-the-art self-supervised learning methods on\nvarious downstream tasks, including land cover classification, semantic\nsegmentation, object detection, and instance segmentation. These results\ndemonstrate that CtxMIM learns impressive remote sensing representations with\nhigh generalization and transferability. Code and data will be made public\navailable.\n","authors":["Mingming Zhang","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.00022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10329v1","updated":"2023-11-17T05:03:53Z","published":"2023-11-17T05:03:53Z","title":"High-fidelity Person-centric Subject-to-Image Synthesis","summary":"  Current subject-driven image generation methods encounter significant\nchallenges in person-centric image generation. The reason is that they learn\nthe semantic scene and person generation by fine-tuning a common pre-trained\ndiffusion, which involves an irreconcilable training imbalance. Precisely, to\ngenerate realistic persons, they need to sufficiently tune the pre-trained\nmodel, which inevitably causes the model to forget the rich semantic scene\nprior and makes scene generation over-fit to the training data. Moreover, even\nwith sufficient fine-tuning, these methods can still not generate high-fidelity\npersons since joint learning of the scene and person generation also lead to\nquality compromise. In this paper, we propose Face-diffuser, an effective\ncollaborative generation pipeline to eliminate the above training imbalance and\nquality compromise. Specifically, we first develop two specialized pre-trained\ndiffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented\nDiffusion Model (SDM), for scene and person generation, respectively. The\nsampling process is divided into three sequential stages, i.e., semantic scene\nconstruction, subject-scene fusion, and subject enhancement. The first and last\nstages are performed by TDM and SDM respectively. The subject-scene fusion\nstage, that is the collaboration achieved through a novel and highly effective\nmechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on\nour key observation that there exists a robust link between classifier-free\nguidance responses and the saliency of generated images. In each time step, SNF\nleverages the unique strengths of each model and allows for the spatial\nblending of predicted noises from both models automatically in a saliency-aware\nmanner. Extensive experiments confirm the impressive effectiveness and\nrobustness of the Face-diffuser.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10328v1","updated":"2023-11-17T04:59:08Z","published":"2023-11-17T04:59:08Z","title":"TransONet: Automatic Segmentation of Vasculature in Computed Tomographic\n  Angiograms Using Deep Learning","summary":"  Pathological alterations in the human vascular system underlie many chronic\ndiseases, such as atherosclerosis and aneurysms. However, manually analyzing\ndiagnostic images of the vascular system, such as computed tomographic\nangiograms (CTAs) is a time-consuming and tedious process. To address this\nissue, we propose a deep learning model to segment the vascular system in CTA\nimages of patients undergoing surgery for peripheral arterial disease (PAD).\nOur study focused on accurately segmenting the vascular system (1) from the\ndescending thoracic aorta to the iliac bifurcation and (2) from the descending\nthoracic aorta to the knees in CTA images using deep learning techniques. Our\napproach achieved average Dice accuracies of 93.5% and 80.64% in test dataset\nfor (1) and (2), respectively, highlighting its high accuracy and potential\nclinical utility. These findings demonstrate the use of deep learning\ntechniques as a valuable tool for medical professionals to analyze the health\nof the vascular system efficiently and accurately. Please visit the GitHub page\nfor this paper at https://github.com/pip-alireza/TransOnet.\n","authors":["Alireza Bagheri Rajeoni","Breanna Pederson","Ali Firooz","Hamed Abdollahi","Andrew K. Smith","Daniel G. Clair","Susan M. Lessner","Homayoun Valafar"],"pdf_url":"https://arxiv.org/pdf/2311.10328v1.pdf","comment":"Accepted for the 2023 International Conference on Computational\n  Science and Computational Intelligence (CSCI), Las Vegas, USA"},{"id":"http://arxiv.org/abs/2303.10076v5","updated":"2023-11-17T04:25:19Z","published":"2023-03-17T15:57:14Z","title":"A Simple Framework for 3D Occupancy Estimation in Autonomous Driving","summary":"  The task of estimating 3D occupancy from surrounding-view images is an\nexciting development in the field of autonomous driving, following the success\nof Bird's Eye View (BEV) perception. This task provides crucial 3D attributes\nof the driving environment, enhancing the overall understanding and perception\nof the surrounding space. In this work, we present a simple framework for 3D\noccupancy estimation, which is a CNN-based framework designed to reveal several\nkey factors for 3D occupancy estimation, such as network design, optimization,\nand evaluation. In addition, we explore the relationship between 3D occupancy\nestimation and other related tasks, such as monocular depth estimation and 3D\nreconstruction, which could advance the study of 3D perception in autonomous\ndriving. For evaluation, we propose a simple sampling strategy to define the\nmetric for occupancy evaluation, which is flexible for current public datasets.\nMoreover, we establish the benchmark in terms of the depth estimation metric,\nwhere we compare our proposed method with monocular depth estimation methods on\nthe DDAD and Nuscenes datasets and achieve competitive performance. The\nrelevant code will be updated in https://github.com/GANWANSHUI/SimpleOccupancy.\n","authors":["Wanshui Gan","Ningkai Mo","Hongbin Xu","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2303.10076v5.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.11719v2","updated":"2023-11-17T04:17:34Z","published":"2023-06-20T17:53:00Z","title":"Diffusion with Forward Models: Solving Stochastic Inverse Problems\n  Without Direct Supervision","summary":"  Denoising diffusion models are a powerful type of generative models used to\ncapture complex distributions of real-world signals. However, their\napplicability is limited to scenarios where training samples are readily\navailable, which is not always the case in real-world applications. For\nexample, in inverse graphics, the goal is to generate samples from a\ndistribution of 3D scenes that align with a given image, but ground-truth 3D\nscenes are unavailable and only 2D images are accessible. To address this\nlimitation, we propose a novel class of denoising diffusion probabilistic\nmodels that learn to sample from distributions of signals that are never\ndirectly observed. Instead, these signals are measured indirectly through a\nknown differentiable forward model, which produces partial observations of the\nunknown signal. Our approach involves integrating the forward model directly\ninto the denoising process. This integration effectively connects the\ngenerative modeling of observations with the generative modeling of the\nunderlying signals, allowing for end-to-end training of a conditional\ngenerative model over signals. During inference, our approach enables sampling\nfrom the distribution of underlying signals that are consistent with a given\npartial observation. We demonstrate the effectiveness of our method on three\nchallenging computer vision tasks. For instance, in the context of inverse\ngraphics, our model enables direct sampling from the distribution of 3D scenes\nthat align with a single 2D input image.\n","authors":["Ayush Tewari","Tianwei Yin","George Cazenavette","Semon Rezchikov","Joshua B. Tenenbaum","Frédo Durand","William T. Freeman","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2306.11719v2.pdf","comment":"Project page: https://diffusion-with-forward-models.github.io/"},{"id":"http://arxiv.org/abs/2311.10320v1","updated":"2023-11-17T04:06:20Z","published":"2023-11-17T04:06:20Z","title":"Learning transformer-based heterogeneously salient graph representation\n  for multimodal fusion classification of hyperspectral image and LiDAR data","summary":"  Data collected by different modalities can provide a wealth of complementary\ninformation, such as hyperspectral image (HSI) to offer rich spectral-spatial\nproperties, synthetic aperture radar (SAR) to provide structural information\nabout the Earth's surface, and light detection and ranging (LiDAR) to cover\naltitude information about ground elevation. Therefore, a natural idea is to\ncombine multimodal images for refined and accurate land-cover interpretation.\nAlthough many efforts have been attempted to achieve multi-source remote\nsensing image classification, there are still three issues as follows: 1)\nindiscriminate feature representation without sufficiently considering modal\nheterogeneity, 2) abundant features and complex computations associated with\nmodeling long-range dependencies, and 3) overfitting phenomenon caused by\nsparsely labeled samples. To overcome the above barriers, a transformer-based\nheterogeneously salient graph representation (THSGR) approach is proposed in\nthis paper. First, a multimodal heterogeneous graph encoder is presented to\nencode distinctively non-Euclidean structural features from heterogeneous data.\nThen, a self-attention-free multi-convolutional modulator is designed for\neffective and efficient long-term dependency modeling. Finally, a mean forward\nis put forward in order to avoid overfitting. Based on the above structures,\nthe proposed model is able to break through modal gaps to obtain differentiated\ngraph representation with competitive time cost, even for a small fraction of\ntraining samples. Experiments and analyses on three benchmark datasets with\nvarious state-of-the-art (SOTA) methods show the performance of the proposed\napproach.\n","authors":["Jiaqi Yang","Bo Du","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.10320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10319v1","updated":"2023-11-17T04:04:29Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment and research are limited by supervised\nlearning techniques that rely on large amounts of annotated data, an expensive\ntask requiring many hours of clinical specialists' time. In this paper, we\npropose using self-supervised and semi-supervised learning. These techniques\nperform an auxiliary task that is label-free, scaling up machine-supervision is\neasier compared with fully-supervised techniques. This paper proposes S4MI\n(Self-Supervision and Semi-Supervision for Medical Imaging), our pipeline to\nleverage advances in self and semi-supervision learning. We benchmark them on\nthree medical imaging datasets to analyze their efficacy for classification and\nsegmentation. This advancement in self-supervised learning with 10% annotation\nperformed better than 100% annotation for the classification of most datasets.\nThe semi-supervised approach yielded favorable outcomes for segmentation,\noutperforming the fully-supervised approach by using 50% fewer labels in all\nthree datasets.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v1.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2311.10318v1","updated":"2023-11-17T04:04:11Z","published":"2023-11-17T04:04:11Z","title":"Nonparametric Teaching for Multiple Learners","summary":"  We study the problem of teaching multiple learners simultaneously in the\nnonparametric iterative teaching setting, where the teacher iteratively\nprovides examples to the learner for accelerating the acquisition of a target\nconcept. This problem is motivated by the gap between current single-learner\nteaching setting and the real-world scenario of human instruction where a\nteacher typically imparts knowledge to multiple students. Under the new problem\nformulation, we introduce a novel framework -- Multi-learner Nonparametric\nTeaching (MINT). In MINT, the teacher aims to instruct multiple learners, with\neach learner focusing on learning a scalar-valued target model. To achieve\nthis, we frame the problem as teaching a vector-valued target model and extend\nthe target model space from a scalar-valued reproducing kernel Hilbert space\nused in single-learner scenarios to a vector-valued space. Furthermore, we\ndemonstrate that MINT offers significant teaching speed-up over repeated\nsingle-learner teaching, particularly when the multiple learners can\ncommunicate with each other. Lastly, we conduct extensive experiments to\nvalidate the practicality and efficiency of MINT.\n","authors":["Chen Zhang","Xiaofeng Cao","Weiyang Liu","Ivor Tsang","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2311.10318v1.pdf","comment":"NeurIPS 2023 (31 pages, 20 figures)"},{"id":"http://arxiv.org/abs/2311.10306v1","updated":"2023-11-17T03:33:09Z","published":"2023-11-17T03:33:09Z","title":"MPSeg : Multi-Phase strategy for coronary artery Segmentation","summary":"  Accurate segmentation of coronary arteries is a pivotal process in assessing\ncardiovascular diseases. However, the intricate structure of the cardiovascular\nsystem presents significant challenges for automatic segmentation, especially\nwhen utilizing methodologies like the SYNTAX Score, which relies extensively on\ndetailed structural information for precise risk stratification. To address\nthese difficulties and cater to this need, we present MPSeg, an innovative\nmulti-phase strategy designed for coronary artery segmentation. Our approach\nspecifically accommodates these structural complexities and adheres to the\nprinciples of the SYNTAX Score. Initially, our method segregates vessels into\ntwo categories based on their unique morphological characteristics: Left\nCoronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble\nmodels are then deployed for each category to execute the challenging\nsegmentation task. Due to LCA's higher complexity over RCA, a refinement model\nis utilized to scrutinize and correct initial class predictions on segmented\nareas. Notably, our approach demonstrated exceptional effectiveness when\nevaluated in the Automatic Region-based Coronary Artery Disease diagnostics\nusing x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm\nchallenge at MICCAI 2023.\n","authors":["Jonghoe Ku","Yong-Hee Lee","Junsup Shin","In Kyu Lee","Hyun-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2311.10306v1.pdf","comment":"MICCAI 2023 Conference ARCADE Challenge"},{"id":"http://arxiv.org/abs/2311.10305v1","updated":"2023-11-17T03:32:11Z","published":"2023-11-17T03:32:11Z","title":"Semi-supervised ViT knowledge distillation network with style transfer\n  normalization for colorectal liver metastases survival prediction","summary":"  Colorectal liver metastases (CLM) significantly impact colon cancer patients,\ninfluencing survival based on systemic chemotherapy response. Traditional\nmethods like tumor grading scores (e.g., tumor regression grade - TRG) for\nprognosis suffer from subjectivity, time constraints, and expertise demands.\nCurrent machine learning approaches often focus on radiological data, yet the\nrelevance of histological images for survival predictions, capturing intricate\ntumor microenvironment characteristics, is gaining recognition. To address\nthese limitations, we propose an end-to-end approach for automated prognosis\nprediction using histology slides stained with H&E and HPS. We first employ a\nGenerative Adversarial Network (GAN) for slide normalization to reduce staining\nvariations and improve the overall quality of the images that are used as input\nto our prediction pipeline. We propose a semi-supervised model to perform\ntissue classification from sparse annotations, producing feature maps. We use\nan attention-based approach that weighs the importance of different slide\nregions in producing the final classification results. We exploit the extracted\nfeatures for the metastatic nodules and surrounding tissue to train a prognosis\nmodel. In parallel, we train a vision Transformer (ViT) in a knowledge\ndistillation framework to replicate and enhance the performance of the\nprognosis prediction. In our evaluation on a clinical dataset of 258 patients,\nour approach demonstrates superior performance with c-indexes of 0.804 (0.014)\nfor OS and 0.733 (0.014) for TTR. Achieving 86.9% to 90.3% accuracy in\npredicting TRG dichotomization and 78.5% to 82.1% accuracy for the 3-class TRG\nclassification task, our approach outperforms comparative methods. Our proposed\npipeline can provide automated prognosis for pathologists and oncologists, and\ncan greatly promote precision medicine progress in managing CLM patients.\n","authors":["Mohamed El Amine Elforaici","Emmanuel Montagnon","Francisco Perdigon Romero","William Trung Le","Feryel Azzi","Dominique Trudel","Bich Nguyen","Simon Turcotte","An Tang","Samuel Kadoury"],"pdf_url":"https://arxiv.org/pdf/2311.10305v1.pdf","comment":"16 pages, 7 figures and 7 tables. Submitted to Medical Journal\n  Analysis (MedIA) journal"},{"id":"http://arxiv.org/abs/2311.10296v1","updated":"2023-11-17T03:01:37Z","published":"2023-11-17T03:01:37Z","title":"BiHRNet: A Binary high-resolution network for Human Pose Estimation","summary":"  Human Pose Estimation (HPE) plays a crucial role in computer vision\napplications. However, it is difficult to deploy state-of-the-art models on\nresouce-limited devices due to the high computational costs of the networks. In\nthis work, a binary human pose estimator named BiHRNet(Binary HRNet) is\nproposed, whose weights and activations are expressed as $\\pm$1. BiHRNet\nretains the keypoint extraction ability of HRNet, while using fewer computing\nresources by adapting binary neural network (BNN). In order to reduce the\naccuracy drop caused by network binarization, two categories of techniques are\nproposed in this work. For optimizing the training process for binary pose\nestimator, we propose a new loss function combining KL divergence loss with\nAWing loss, which makes the binary network obtain more comprehensive output\ndistribution from its real-valued counterpart to reduce information loss caused\nby binarization. For designing more binarization-friendly structures, we\npropose a new information reconstruction bottleneck called IR Bottleneck to\nretain more information in the initial stage of the network. In addition, we\nalso propose a multi-scale basic block called MS-Block for information\nretention. Our work has less computation cost with few precision drop.\nExperimental results demonstrate that BiHRNet achieves a PCKh of 87.9 on the\nMPII dataset, which outperforms all binary pose estimation networks. On the\nchallenging of COCO dataset, the proposed method enables the binary neural\nnetwork to achieve 70.8 mAP, which is better than most tested lightweight\nfull-precision networks.\n","authors":["Zhicheng Zhang","Xueyao Sun","Yonghao Dang","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2311.10296v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.10293v1","updated":"2023-11-17T02:48:20Z","published":"2023-11-17T02:48:20Z","title":"Hierarchical Pruning of Deep Ensembles with Focal Diversity","summary":"  Deep neural network ensembles combine the wisdom of multiple deep neural\nnetworks to improve the generalizability and robustness over individual\nnetworks. It has gained increasing popularity to study deep ensemble techniques\nin the deep learning community. Some mission-critical applications utilize a\nlarge number of deep neural networks to form deep ensembles to achieve desired\naccuracy and resilience, which introduces high time and space costs for\nensemble execution. However, it still remains a critical challenge whether a\nsmall subset of the entire deep ensemble can achieve the same or better\ngeneralizability and how to effectively identify these small deep ensembles for\nimproving the space and time efficiency of ensemble execution. This paper\npresents a novel deep ensemble pruning approach, which can efficiently identify\nsmaller deep ensembles and provide higher ensemble accuracy than the entire\ndeep ensemble of a large number of member networks. Our hierarchical ensemble\npruning approach (HQ) leverages three novel ensemble pruning techniques. First,\nwe show that the focal diversity metrics can accurately capture the\ncomplementary capacity of the member networks of an ensemble, which can guide\nensemble pruning. Second, we design a focal diversity based hierarchical\npruning approach, which will iteratively find high quality deep ensembles with\nlow cost and high accuracy. Third, we develop a focal diversity consensus\nmethod to integrate multiple focal diversity metrics to refine ensemble pruning\nresults, where smaller deep ensembles can be effectively identified to offer\nhigh accuracy, high robustness and high efficiency. Evaluated using popular\nbenchmark datasets, we demonstrate that the proposed hierarchical ensemble\npruning approach can effectively identify high quality deep ensembles with\nbetter generalizability while being more time and space efficient in ensemble\ndecision making.\n","authors":["Yanzhao Wu","Ka-Ho Chow","Wenqi Wei","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2311.10293v1.pdf","comment":"To appear on ACM Transactions on Intelligent Systems and Technology"},{"id":"http://arxiv.org/abs/2308.14113v3","updated":"2023-11-17T02:37:13Z","published":"2023-08-27T14:07:57Z","title":"Semantic-aware Consistency Network for Cloth-changing Person\n  Re-Identification","summary":"  Cloth-changing Person Re-Identification (CC-ReID) is a challenging task that\naims to retrieve the target person across multiple surveillance cameras when\nclothing changes might happen. Despite recent progress in CC-ReID, existing\napproaches are still hindered by the interference of clothing variations since\nthey lack effective constraints to keep the model consistently focused on\nclothing-irrelevant regions. To address this issue, we present a Semantic-aware\nConsistency Network (SCNet) to learn identity-related semantic features by\nproposing effective consistency constraints. Specifically, we generate the\nblack-clothing image by erasing pixels in the clothing area, which explicitly\nmitigates the interference from clothing variations. In addition, to fully\nexploit the fine-grained identity information, a head-enhanced attention module\nis introduced, which learns soft attention maps by utilizing the proposed\npart-based matching loss to highlight head information. We further design a\nsemantic consistency loss to facilitate the learning of high-level\nidentity-related semantic features, forcing the model to focus on semantically\nconsistent cloth-irrelevant regions. By using the consistency constraint, our\nmodel does not require any extra auxiliary segmentation module to generate the\nblack-clothing image or locate the head region during the inference stage.\nExtensive experiments on four cloth-changing person Re-ID datasets (LTCC, PRCC,\nVc-Clothes, and DeepChange) demonstrate that our proposed SCNet makes\nsignificant improvements over prior state-of-the-art approaches. Our code is\navailable at: https://github.com/Gpn-star/SCNet.\n","authors":["Peini Guo","Hong Liu","Jianbing Wu","Guoquan Wang","Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2308.14113v3.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2311.05836v3","updated":"2023-11-17T02:35:52Z","published":"2023-11-10T02:47:15Z","title":"UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical\n  Neural Radiance Fields","summary":"  In the field of clinical medicine, computed tomography (CT) is an effective\nmedical imaging modality for the diagnosis of various pathologies. Compared\nwith X-ray images, CT images can provide more information, including\nmulti-planar slices and three-dimensional structures for clinical diagnosis.\nHowever, CT imaging requires patients to be exposed to large doses of ionizing\nradiation for a long time, which may cause irreversible physical harm. In this\npaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on\ngenerated radiation fields. The network can learn a continuous representation\nof CT projections from 2D X-ray images by obtaining the internal structure and\ndepth information and using adaptive loss weights to ensure the quality of the\ngenerated images. Our model is trained on publicly available knee and chest\ndatasets, and we show the results of CT projection rendering with a single\nX-ray and compare our method with other methods based on generated radiation\nfields.\n","authors":["Jing Hu","Qinrui Fan","Shu Hu","Siwei Lyu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10281v1","updated":"2023-11-17T02:01:19Z","published":"2023-11-17T02:01:19Z","title":"SSASS: Semi-Supervised Approach for Stenosis Segmentation","summary":"  Coronary artery stenosis is a critical health risk, and its precise\nidentification in Coronary Angiography (CAG) can significantly aid medical\npractitioners in accurately evaluating the severity of a patient's condition.\nThe complexity of coronary artery structures combined with the inherent noise\nin X-ray images poses a considerable challenge to this task. To tackle these\nobstacles, we introduce a semi-supervised approach for cardiovascular stenosis\nsegmentation. Our strategy begins with data augmentation, specifically tailored\nto replicate the structural characteristics of coronary arteries. We then apply\na pseudo-label-based semi-supervised learning technique that leverages the data\ngenerated through our augmentation process. Impressively, our approach\ndemonstrated an exceptional performance in the Automatic Region-based Coronary\nArtery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis\nDetection Algorithm challenge by utilizing a single model instead of relying on\nan ensemble of multiple models. This success emphasizes our method's capability\nand efficiency in providing an automated solution for accurately assessing\nstenosis severity from medical imaging data.\n","authors":["In Kyu Lee","Junsup Shin","Yong-Hee Lee","Jonghoe Ku","Hyun-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2311.10281v1.pdf","comment":"MICCAI 2023 Conference ARCADE Challenge"},{"id":"http://arxiv.org/abs/2311.10065v2","updated":"2023-11-17T01:59:10Z","published":"2023-11-16T18:02:10Z","title":"Visual Environment Assessment for Safe Autonomous Quadrotor Landing","summary":"  Autonomous identification and evaluation of safe landing zones are of\nparamount importance for ensuring the safety and effectiveness of aerial robots\nin the event of system failures, low battery, or the successful completion of\nspecific tasks. In this paper, we present a novel approach for detection and\nassessment of potential landing sites for safe quadrotor landing. Our solution\nefficiently integrates 2D and 3D environmental information, eliminating the\nneed for external aids such as GPS and computationally intensive elevation\nmaps. The proposed pipeline combines semantic data derived from a Neural\nNetwork (NN), to extract environmental features, with geometric data obtained\nfrom a disparity map, to extract critical geometric attributes such as slope,\nflatness, and roughness. We define several cost metrics based on these\nattributes to evaluate safety, stability, and suitability of regions in the\nenvironments and identify the most suitable landing area. Our approach runs in\nreal-time on quadrotors equipped with limited computational capabilities.\nExperimental results conducted in diverse environments demonstrate that the\nproposed method can effectively assess and identify suitable landing areas,\nenabling the safe and autonomous landing of a quadrotor.\n","authors":["Mattia Secchiero","Nishanth Bobbili","Yang Zhou","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2311.10065v2.pdf","comment":"7 pages, 5 figures, 1 table, submitted to IEEE International\n  Conference on Robotics and Automation (ICRA), 2024"},{"id":"http://arxiv.org/abs/2311.10278v1","updated":"2023-11-17T01:55:15Z","published":"2023-11-17T01:55:15Z","title":"Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint","summary":"  Human fingerprints serve as one unique and powerful characteristic for each\nperson, from which policemen can recognize the identity. Similar to humans,\nmany natural bodies and intrinsic mechanical qualities can also be uniquely\nidentified from surface characteristics. To measure the elasto-plastic\nproperties of one material, one formally sharp indenter is pushed into the\nmeasured body under constant force and retracted, leaving a unique residual\nimprint of the minute size from several micrometers to nanometers. However, one\ngreat challenge is how to map the optical image of this residual imprint into\nthe real wanted mechanical properties, i.e., the tensile force curve. In this\npaper, we propose a novel method to use multi-fidelity neural networks (MFNN)\nto solve this inverse problem. We first actively train the NN model via pure\nsimulation data, and then bridge the sim-to-real gap via transfer learning. The\nmost innovative part is that we use NN to dig out the unknown physics and also\nimplant the known physics into the transfer learning framework, thus highly\nimproving the model stability and decreasing the data requirement. This work\nserves as one great example of applying machine learning into the real\nexperimental research, especially under the constraints of data limitation and\nfidelity variance.\n","authors":["Yongchao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.10278v1.pdf","comment":"8 pages, 4 figures, NeurIPS 2023 Workshop on Adaptive Experimental\n  Design and Active Learning in the Real World"},{"id":"http://arxiv.org/abs/2311.10269v1","updated":"2023-11-17T01:29:16Z","published":"2023-11-17T01:29:16Z","title":"Interpretable pap smear cell representation for cervical cancer\n  screening","summary":"  Screening is critical for prevention and early detection of cervical cancer\nbut it is time-consuming and laborious. Supervised deep convolutional neural\nnetworks have been developed to automate pap smear screening and the results\nare promising. However, the interest in using only normal samples to train deep\nneural networks has increased owing to class imbalance problems and\nhigh-labeling costs that are both prevalent in healthcare. In this study, we\nintroduce a method to learn explainable deep cervical cell representations for\npap smear cytology images based on one class classification using variational\nautoencoders. Findings demonstrate that a score can be calculated for cell\nabnormality without training models with abnormal samples and localize\nabnormality to interpret our results with a novel metric based on absolute\ndifference in cross entropy in agglomerative clustering. The best model that\ndiscriminates squamous cell carcinoma (SCC) from normals gives 0.908 +- 0.003\narea under operating characteristic curve (AUC) and one that discriminates\nhigh-grade epithelial lesion (HSIL) 0.920 +- 0.002 AUC. Compared to other\nclustering methods, our method enhances the V-measure and yields higher\nhomogeneity scores, which more effectively isolate different abnormality\nregions, aiding in the interpretation of our results. Evaluation using in-house\nand additional open dataset show that our model can discriminate abnormality\nwithout the need of additional training of deep models.\n","authors":["Yu Ando","Nora Jee-Young Park and","Gun Oh Chong","Seokhwan Ko","Donghyeon Lee","Junghwan Cho","Hyungsoo Han"],"pdf_url":"https://arxiv.org/pdf/2311.10269v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.09574v2","updated":"2023-11-17T01:24:47Z","published":"2023-11-16T05:17:14Z","title":"LymphoML: An interpretable artificial intelligence-based method\n  identifies morphologic features that correlate with lymphoma subtype","summary":"  The accurate classification of lymphoma subtypes using hematoxylin and eosin\n(H&E)-stained tissue is complicated by the wide range of morphological features\nthese cancers can exhibit. We present LymphoML - an interpretable machine\nlearning method that identifies morphologic features that correlate with\nlymphoma subtypes. Our method applies steps to process H&E-stained tissue\nmicroarray cores, segment nuclei and cells, compute features encompassing\nmorphology, texture, and architecture, and train gradient-boosted models to\nmake diagnostic predictions. LymphoML's interpretable models, developed on a\nlimited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy\nto pathologists using whole-slide images and outperform black box deep-learning\non a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using\nSHapley Additive exPlanation (SHAP) analysis, we assess the impact of each\nfeature on model prediction and find that nuclear shape features are most\ndiscriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma\n(F1-score: 74.5%). Finally, we provide the first demonstration that a model\ncombining features from H&E-stained tissue with features from a standardized\npanel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a\n46-stain panel (86.1%).\n","authors":["Vivek Shankar","Xiaoli Yang","Vrishab Krishna","Brent Tan","Oscar Silva","Rebecca Rojansky","Andrew Ng","Fabiola Valvert","Edward Briercheck","David Weinstock","Yasodha Natkunam","Sebastian Fernandez-Pol","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2311.09574v2.pdf","comment":"To be published in Proceedings of the 3rd Machine Learning for Health\n  symposium, Proceedings of Machine Learning Research (PMLR)"},{"id":"http://arxiv.org/abs/2311.10261v1","updated":"2023-11-17T01:07:37Z","published":"2023-11-17T01:07:37Z","title":"Vision meets mmWave Radar: 3D Object Perception Benchmark for Autonomous\n  Driving","summary":"  Sensor fusion is crucial for an accurate and robust perception system on\nautonomous vehicles. Most existing datasets and perception solutions focus on\nfusing cameras and LiDAR. However, the collaboration between camera and radar\nis significantly under-exploited. The incorporation of rich semantic\ninformation from the camera, and reliable 3D information from the radar can\npotentially achieve an efficient, cheap, and portable solution for 3D object\nperception tasks. It can also be robust to different lighting or all-weather\ndriving scenarios due to the capability of mmWave radars. In this paper, we\nintroduce the CRUW3D dataset, including 66K synchronized and well-calibrated\ncamera, radar, and LiDAR frames in various driving scenarios. Unlike other\nlarge-scale autonomous driving datasets, our radar data is in the format of\nradio frequency (RF) tensors that contain not only 3D location information but\nalso spatio-temporal semantic information. This kind of radar format can enable\nmachine learning models to generate more reliable object perception results\nafter interacting and fusing the information or features between the camera and\nradar.\n","authors":["Yizhou Wang","Jen-Hao Cheng","Jui-Te Huang","Sheng-Yao Kuan","Qiqian Fu","Chiming Ni","Shengyu Hao","Gaoang Wang","Guanbin Xing","Hui Liu","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2311.10261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10251v1","updated":"2023-11-17T00:44:56Z","published":"2023-11-17T00:44:56Z","title":"UniMOS: A Universal Framework For Multi-Organ Segmentation Over\n  Label-Constrained Datasets","summary":"  Machine learning models for medical images can help physicians diagnose and\nmanage diseases. However, due to the fact that medical image annotation\nrequires a great deal of manpower and expertise, as well as the fact that\nclinical departments perform image annotation based on task orientation, there\nis the problem of having fewer medical image annotation data with more\nunlabeled data and having many datasets that annotate only a single organ. In\nthis paper, we present UniMOS, the first universal framework for achieving the\nutilization of fully and partially labeled images as well as unlabeled images.\nSpecifically, we construct a Multi-Organ Segmentation (MOS) module over\nfully/partially labeled data as the basenet and designed a new target adaptive\nloss. Furthermore, we incorporate a semi-supervised training module that\ncombines consistent regularization and pseudolabeling techniques on unlabeled\ndata, which significantly improves the segmentation of unlabeled data.\nExperiments show that the framework exhibits excellent performance in several\nmedical image segmentation tasks compared to other advanced methods, and also\nsignificantly improves data utilization and reduces annotation cost. Code and\nmodels are available at: https://github.com/lw8807001/UniMOS.\n","authors":["Can Li","Sheng Shao","Junyi Qu","Shuchao Pang","Mehmet A. Orgun"],"pdf_url":"https://arxiv.org/pdf/2311.10251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10245v1","updated":"2023-11-17T00:28:19Z","published":"2023-11-17T00:28:19Z","title":"Segment Anything in Defect Detection","summary":"  Defect detection plays a crucial role in infrared non-destructive testing\nsystems, offering non-contact, safe, and efficient inspection capabilities.\nHowever, challenges such as low resolution, high noise, and uneven heating in\ninfrared thermal images hinder comprehensive and accurate defect detection. In\nthis study, we propose DefectSAM, a novel approach for segmenting defects on\nhighly noisy thermal images based on the widely adopted model, Segment Anything\n(SAM)\\cite{kirillov2023segany}. Harnessing the power of a meticulously curated\ndataset generated through labor-intensive lab experiments and valuable prompts\nfrom experienced experts, DefectSAM surpasses existing state-of-the-art\nsegmentation algorithms and achieves significant improvements in defect\ndetection rates. Notably, DefectSAM excels in detecting weaker and smaller\ndefects on complex and irregular surfaces, reducing the occurrence of missed\ndetections and providing more accurate defect size estimations. Experimental\nstudies conducted on various materials have validated the effectiveness of our\nsolutions in defect detection, which hold significant potential to expedite the\nevolution of defect detection tools, enabling enhanced inspection capabilities\nand accuracy in defect identification.\n","authors":["Bozhen Hu","Bin Gao","Cheng Tan","Tongle Wu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2311.10245v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.10697v1","updated":"2023-11-17T18:32:17Z","published":"2023-11-17T18:32:17Z","title":"PEFT-MedAware: Large Language Model for Medical Awareness","summary":"  Chat models are capable of answering a wide range of questions, however, the\naccuracy of their responses is highly uncertain. In this research, we propose a\nspecialized PEFT-MedAware model where we utilize parameter-efficient\nfine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized\nMedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of\nits trainable parameters to enhance computational efficiency. The paper adopts\ndata preprocessing and PEFT to optimize model performance, complemented by a\nBitsAndBytesConfig for efficient transformer training. The resulting model was\ncapable of outperforming other LLMs in medical question-answering tasks in\nspecific domains with greater accuracy utilizing limited computational\nresources making it suitable for deployment in resource-constrained\nenvironments. We propose further improvements through expanded datasets, larger\nmodels, and feedback mechanisms for sustained medical relevancy. Our work\nhighlights the efficiency gains and specialized capabilities of PEFT in medical\nAI, outpacing standard models in precision without extensive resource demands.\nThe proposed model and data are released for research purposes only.\n","authors":["Keivalya Pandya"],"pdf_url":"https://arxiv.org/pdf/2311.10697v1.pdf","comment":"7 pages, 1 figure, submitted to the Artificial Intelligence in\n  Medicine Journal"},{"id":"http://arxiv.org/abs/2303.04689v2","updated":"2023-11-17T16:47:49Z","published":"2023-03-07T17:22:38Z","title":"A Privacy Preserving System for Movie Recommendations Using Federated\n  Learning","summary":"  Recommender systems have become ubiquitous in the past years. They solve the\ntyranny of choice problem faced by many users, and are utilized by many online\nbusinesses to drive engagement and sales. Besides other criticisms, like\ncreating filter bubbles within social networks, recommender systems are often\nreproved for collecting considerable amounts of personal data. However, to\npersonalize recommendations, personal information is fundamentally required. A\nrecent distributed learning scheme called federated learning has made it\npossible to learn from personal user data without its central collection.\nConsequently, we present a recommender system for movie recommendations, which\nprovides privacy and thus trustworthiness on multiple levels: First and\nforemost, it is trained using federated learning and thus, by its very nature,\nprivacy-preserving, while still enabling users to benefit from global insights.\nFurthermore, a novel federated learning scheme, called FedQ, is employed, which\nnot only addresses the problem of non-i.i.d.-ness and small local datasets, but\nalso prevents input data reconstruction attacks by aggregating client updates\nearly. Finally, to reduce the communication overhead, compression is applied,\nwhich significantly compresses the exchanged neural network parametrizations to\na fraction of their original size. We conjecture that this may also improve\ndata privacy through its lossy quantization stage.\n","authors":["David Neumann","Andreas Lutz","Karsten Müller","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2303.04689v2.pdf","comment":"Accepted by the ACM TORS Special Issue on Trustworthy Recommender\n  Systems"},{"id":"http://arxiv.org/abs/2311.10635v1","updated":"2023-11-17T16:42:57Z","published":"2023-11-17T16:42:57Z","title":"Ex2Vec: Characterizing Users and Items from the Mere Exposure Effect","summary":"  The traditional recommendation framework seeks to connect user and content,\nby finding the best match possible based on users past interaction. However, a\ngood content recommendation is not necessarily similar to what the user has\nchosen in the past. As humans, users naturally evolve, learn, forget, get\nbored, they change their perspective of the world and in consequence, of the\nrecommendable content. One well known mechanism that affects user interest is\nthe Mere Exposure Effect: when repeatedly exposed to stimuli, users' interest\ntends to rise with the initial exposures, reaching a peak, and gradually\ndecreasing thereafter, resulting in an inverted-U shape. Since previous\nresearch has shown that the magnitude of the effect depends on a number of\ninteresting factors such as stimulus complexity and familiarity, leveraging\nthis effect is a way to not only improve repeated recommendation but to gain a\nmore in-depth understanding of both users and stimuli. In this work we present\n(Mere) Exposure2Vec (Ex2Vec) our model that leverages the Mere Exposure Effect\nin repeat consumption to derive user and item characterization and track user\ninterest evolution. We validate our model through predicting future music\nconsumption based on repetition and discuss its implications for recommendation\nscenarios where repetition is common.\n","authors":["Bruno Sguerra","Viet-Anh Tran","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2311.10635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10567v1","updated":"2023-11-17T15:06:59Z","published":"2023-11-17T15:06:59Z","title":"Cross-Modal Search and Exploration of Greek Painted Pottery","summary":"  This paper focuses on digitally-supported research methods for an important\ngroup of cultural heritage objects, the Greek pottery, especially with figured\ndecoration. The design, development and application of new digital methods for\nsearching, comparing, and visually exploring these vases needs an\ninterdisciplinary approach to effectively analyse the various features of the\nvases, like shape, decoration, and manufacturing techniques, and relationships\nbetween the vases. We motivate the need and opportunities by a multimodal\nrepresentation of the objects, including 3D shape, material, and painting. We\nthen illustrate a range of innovative methods for these representations,\nincluding quantified surface and capacity comparison, material analysis, image\nflattening from 3D objects, retrieval and comparison of shapes and paintings,\nand multidimensional data visualization. We also discuss challenges and future\nwork in this area.\n","authors":["Elisabeth Trinkl","Stephan Karl","Stefan Lengauer","Reinhold Preiner","Tobias Schreck"],"pdf_url":"https://arxiv.org/pdf/2311.10567v1.pdf","comment":"14 pages, 10 figures, preprint for a book chapter, supplementary\n  video available at https://youtu.be/x_Xg0vy3nJY"},{"id":"http://arxiv.org/abs/2311.10501v1","updated":"2023-11-17T13:02:25Z","published":"2023-11-17T13:02:25Z","title":"Collaborative Word-based Pre-trained Item Representation for\n  Transferable Recommendation","summary":"  Item representation learning (IRL) plays an essential role in recommender\nsystems, especially for sequential recommendation. Traditional sequential\nrecommendation models usually utilize ID embeddings to represent items, which\nare not shared across different domains and lack the transferable ability.\nRecent studies use pre-trained language models (PLM) for item text embeddings\n(text-based IRL) that are universally applicable across domains. However, the\nexisting text-based IRL is unaware of the important collaborative filtering\n(CF) information. In this paper, we propose CoWPiRec, an approach of\nCollaborative Word-based Pre-trained item representation for Recommendation. To\neffectively incorporate CF information into text-based IRL, we convert the\nitem-level interaction data to a word graph containing word-level\ncollaborations. Subsequently, we design a novel pre-training task to align the\nword-level semantic- and CF-related item representation. Extensive experimental\nresults on multiple public datasets demonstrate that compared to\nstate-of-the-art transferable sequential recommenders, CoWPiRec achieves\nsignificantly better performances in both fine-tuning and zero-shot settings\nfor cross-scenario recommendation and effectively alleviates the cold-start\nissue. The code is available at: https://github.com/ysh-1998/CoWPiRec.\n","authors":["Shenghao Yang","Chenyang Wang","Yankai Liu","Kangping Xu","Weizhi Ma","Yiqun Liu","Min Zhang","Haitao Zeng","Junlan Feng","Chao Deng"],"pdf_url":"https://arxiv.org/pdf/2311.10501v1.pdf","comment":"Accepted by ICDM 2023"},{"id":"http://arxiv.org/abs/2208.06265v3","updated":"2023-11-17T12:38:19Z","published":"2022-08-10T08:28:46Z","title":"Trustworthy Recommender Systems","summary":"  Recommender systems (RSs) aim to help users to effectively retrieve items of\ntheir interests from a large catalogue. For a quite long period of time,\nresearchers and practitioners have been focusing on developing accurate RSs.\nRecent years have witnessed an increasing number of threats to RSs, coming from\nattacks, system and user generated noise, system bias. As a result, it has\nbecome clear that a strict focus on RS accuracy is limited and the research\nmust consider other important factors, e.g., trustworthiness. For end users, a\ntrustworthy RS (TRS) should not only be accurate, but also transparent,\nunbiased and fair as well as robust to noise or attacks. These observations\nactually led to a paradigm shift of the research on RSs: from accuracy-oriented\nRSs to TRSs. However, researchers lack a systematic overview and discussion of\nthe literature in this novel and fast developing field of TRSs. To this end, in\nthis paper, we provide an overview of TRSs, including a discussion of the\nmotivation and basic concepts of TRSs, a presentation of the challenges in\nbuilding TRSs, and a perspective on the future directions in this area. We also\nprovide a novel conceptual framework to support the construction of TRSs.\n","authors":["Shoujin Wang","Xiuzhen Zhang","Yan Wang","Huan Liu","Francesco Ricci"],"pdf_url":"https://arxiv.org/pdf/2208.06265v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10348v1","updated":"2023-11-17T06:35:38Z","published":"2023-11-17T06:35:38Z","title":"A Comparative Analysis of Retrievability and PageRank Measures","summary":"  The accessibility of documents within a collection holds a pivotal role in\nInformation Retrieval, signifying the ease of locating specific content in a\ncollection of documents. This accessibility can be achieved via two distinct\navenues. The first is through some retrieval model using a keyword or other\nfeature-based search, and the other is where a document can be navigated using\nlinks associated with them, if available. Metrics such as PageRank, Hub, and\nAuthority illuminate the pathways through which documents can be discovered\nwithin the network of content while the concept of Retrievability is used to\nquantify the ease with which a document can be found by a retrieval model. In\nthis paper, we compare these two perspectives, PageRank and retrievability, as\nthey quantify the importance and discoverability of content in a corpus.\nThrough empirical experimentation on benchmark datasets, we demonstrate a\nsubtle similarity between retrievability and PageRank particularly\ndistinguishable for larger datasets.\n","authors":["Aman Sinha","Priyanshu Raj Mall","Dwaipayan Roy"],"pdf_url":"https://arxiv.org/pdf/2311.10348v1.pdf","comment":"Accepted at FIRE 2023"},{"id":"http://arxiv.org/abs/2204.06522v2","updated":"2023-11-17T05:17:45Z","published":"2022-04-03T16:50:30Z","title":"Graph Enhanced BERT for Query Understanding","summary":"  Query understanding plays a key role in exploring users' search intents and\nfacilitating users to locate their most desired information. However, it is\ninherently challenging since it needs to capture semantic information from\nshort and ambiguous queries and often requires massive task-specific labeled\ndata. In recent years, pre-trained language models (PLMs) have advanced various\nnatural language processing tasks because they can extract general semantic\ninformation from large-scale corpora. Therefore, there are unprecedented\nopportunities to adopt PLMs for query understanding. However, there is a gap\nbetween the goal of query understanding and existing pre-training strategies --\nthe goal of query understanding is to boost search performance while existing\nstrategies rarely consider this goal. Thus, directly applying them to query\nunderstanding is sub-optimal. On the other hand, search logs contain user\nclicks between queries and urls that provide rich users' search behavioral\ninformation on queries beyond their content. Therefore, in this paper, we aim\nto fill this gap by exploring search logs. In particular, to incorporate search\nlogs into pre-training, we first construct a query graph where nodes are\nqueries and two queries are connected if they lead to clicks on the same urls.\nThen we propose a novel graph-enhanced pre-training framework, GE-BERT, which\ncan leverage both query content and the query graph. In other words, GE-BERT\ncan capture both the semantic information and the users' search behavioral\ninformation of queries. Extensive experiments on various query understanding\ntasks have demonstrated the effectiveness of the proposed framework.\n","authors":["Juanhui Li","Yao Ma","Wei Zeng","Suqi Cheng","Jiliang Tang","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2204.06522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10230v2","updated":"2023-11-17T04:00:09Z","published":"2023-07-15T11:49:43Z","title":"Prompt Tuning on Graph-augmented Low-resource Text Classification","summary":"  Text classification is a fundamental problem in information retrieval with\nmany real-world applications, such as predicting the topics of online articles\nand the categories of e-commerce product descriptions. However, low-resource\ntext classification, with no or few labeled samples, presents a serious concern\nfor supervised learning. Meanwhile, many text data are inherently grounded on a\nnetwork structure, such as a hyperlink/citation network for online articles,\nand a user-item purchase network for e-commerce products. These graph\nstructures capture rich semantic relationships, which can potentially augment\nlow-resource text classification. In this paper, we propose a novel model\ncalled Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource\ntext classification in a two-pronged approach. During pre-training, we propose\nthree graph interaction-based contrastive strategies to jointly pre-train a\ngraph-text model; during downstream classification, we explore handcrafted\ndiscrete prompts and continuous prompt tuning for the jointly pre-trained model\nto achieve zero- and few-shot classification, respectively. Besides, for\ngeneralizing continuous prompts to unseen classes, we propose conditional\nprompt tuning on graphs (G2P2$^*$). Extensive experiments on four real-world\ndatasets demonstrate the strength of G2P2 in zero- and few-shot low-resource\ntext classification tasks, and illustrate the advantage of G2P2$^*$ in dealing\nwith unseen classes.\n","authors":["Zhihao Wen","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2307.10230v2.pdf","comment":"26 pages, journal under review. arXiv admin note: substantial text\n  overlap with arXiv:2305.03324"},{"id":"http://arxiv.org/abs/2311.00423v4","updated":"2023-11-17T02:33:34Z","published":"2023-11-01T10:27:44Z","title":"LLMRec: Large Language Models with Graph Augmentation for Recommendation","summary":"  The problem of data sparsity has long been a challenge in recommendation\nsystems, and previous studies have attempted to address this issue by\nincorporating side information. However, this approach often introduces side\neffects such as noise, availability issues, and low data quality, which in turn\nhinder the accurate modeling of user preferences and adversely impact\nrecommendation performance. In light of the recent advancements in large\nlanguage models (LLMs), which possess extensive knowledge bases and strong\nreasoning capabilities, we propose a novel framework called LLMRec that\nenhances recommender systems by employing three simple yet effective LLM-based\ngraph augmentation strategies. Our approach leverages the rich content\navailable within online platforms (e.g., Netflix, MovieLens) to augment the\ninteraction graph in three ways: (i) reinforcing user-item interaction egde,\n(ii) enhancing the understanding of item node attributes, and (iii) conducting\nuser node profiling, intuitively from the natural language perspective. By\nemploying these strategies, we address the challenges posed by sparse implicit\nfeedback and low-quality side information in recommenders. Besides, to ensure\nthe quality of the augmentation, we develop a denoised data robustification\nmechanism that includes techniques of noisy implicit feedback pruning and\nMAE-based feature enhancement that help refine the augmented data and improve\nits reliability. Furthermore, we provide theoretical analysis to support the\neffectiveness of LLMRec and clarify the benefits of our method in facilitating\nmodel optimization. Experimental results on benchmark datasets demonstrate the\nsuperiority of our LLM-based augmentation approach over state-of-the-art\ntechniques. To ensure reproducibility, we have made our code and augmented data\npublicly available at: https://github.com/HKUDS/LLMRec.git\n","authors":["Wei Wei","Xubin Ren","Jiabin Tang","Qinyong Wang","Lixin Su","Suqi Cheng","Junfeng Wang","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2311.00423v4.pdf","comment":"WSDM 2024 Oral Presentation"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.10710v1","updated":"2023-11-17T18:59:35Z","published":"2023-11-17T18:59:35Z","title":"Machine learning phase transitions: Connections to the Fisher\n  information","summary":"  Despite the widespread use and success of machine-learning techniques for\ndetecting phase transitions from data, their working principle and fundamental\nlimits remain elusive. Here, we explain the inner workings and identify\npotential failure modes of these techniques by rooting popular machine-learning\nindicators of phase transitions in information-theoretic concepts. Using tools\nfrom information geometry, we prove that several machine-learning indicators of\nphase transitions approximate the square root of the system's (quantum) Fisher\ninformation from below -- a quantity that is known to indicate phase\ntransitions but is often difficult to compute from data. We numerically\ndemonstrate the quality of these bounds for phase transitions in classical and\nquantum systems.\n","authors":["Julian Arnold","Niels Lörch","Flemming Holtorf","Frank Schäfer"],"pdf_url":"https://arxiv.org/pdf/2311.10710v1.pdf","comment":"7+11 pages, 2+3 figures"},{"id":"http://arxiv.org/abs/2311.10709v1","updated":"2023-11-17T18:59:04Z","published":"2023-11-17T18:59:04Z","title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image\n  Conditioning","summary":"  We present Emu Video, a text-to-video generation model that factorizes the\ngeneration into two steps: first generating an image conditioned on the text,\nand then generating a video conditioned on the text and the generated image. We\nidentify critical design decisions--adjusted noise schedules for diffusion, and\nmulti-stage training--that enable us to directly generate high quality and high\nresolution videos, without requiring a deep cascade of models as in prior work.\nIn human evaluations, our generated videos are strongly preferred in quality\ncompared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's\nPYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial\nsolutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing\napproach naturally lends itself to animating images based on a user's text\nprompt, where our generations are preferred 96% over prior work.\n","authors":["Rohit Girdhar","Mannat Singh","Andrew Brown","Quentin Duval","Samaneh Azadi","Sai Saketh Rambhatla","Akbar Shah","Xi Yin","Devi Parikh","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2311.10709v1.pdf","comment":"Project page: https://emu-video.metademolab.com"},{"id":"http://arxiv.org/abs/2311.10708v1","updated":"2023-11-17T18:58:16Z","published":"2023-11-17T18:58:16Z","title":"SelfEval: Leveraging the discriminative nature of generative models for\n  evaluation","summary":"  In this work, we show that text-to-image generative models can be 'inverted'\nto assess their own text-image understanding capabilities in a completely\nautomated manner.\n  Our method, called SelfEval, uses the generative model to compute the\nlikelihood of real images given text prompts, making the generative model\ndirectly applicable to discriminative tasks.\n  Using SelfEval, we repurpose standard datasets created for evaluating\nmultimodal text-image discriminative models to evaluate generative models in a\nfine-grained manner: assessing their performance on attribute binding, color\nrecognition, counting, shape recognition, spatial understanding.\n  To the best of our knowledge SelfEval is the first automated metric to show a\nhigh degree of agreement for measuring text-faithfulness with the gold-standard\nhuman evaluations across multiple models and benchmarks.\n  Moreover, SelfEval enables us to evaluate generative models on challenging\ntasks such as Winoground image-score where they demonstrate competitive\nperformance to discriminative models.\n  We also show severe drawbacks of standard automated metrics such as\nCLIP-score to measure text faithfulness on benchmarks such as DrawBench, and\nhow SelfEval sidesteps these issues.\n  We hope SelfEval enables easy and reliable automated evaluation for diffusion\nmodels.\n","authors":["Sai Saketh Rambhatla","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2311.10708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10707v1","updated":"2023-11-17T18:57:40Z","published":"2023-11-17T18:57:40Z","title":"Multimodal Representation Learning by Alternating Unimodal Adaptation","summary":"  Multimodal learning, which integrates data from diverse sensory modes, plays\na pivotal role in artificial intelligence. However, existing multimodal\nlearning methods often struggle with challenges where some modalities appear\nmore dominant than others during multimodal learning, resulting in suboptimal\nperformance. To address this challenge, we propose MLA (Multimodal Learning\nwith Alternating Unimodal Adaptation). MLA reframes the conventional joint\nmultimodal learning process by transforming it into an alternating unimodal\nlearning process, thereby minimizing interference between modalities.\nSimultaneously, it captures cross-modal interactions through a shared head,\nwhich undergoes continuous optimization across different modalities. This\noptimization process is controlled by a gradient modification mechanism to\nprevent the shared head from losing previously acquired information. During the\ninference phase, MLA utilizes a test-time uncertainty-based model fusion\nmechanism to integrate multimodal information. Extensive experiments are\nconducted on five diverse datasets, encompassing scenarios with complete\nmodalities and scenarios with missing modalities. These experiments demonstrate\nthe superiority of MLA over competing prior approaches.\n","authors":["Xiaohui Zhang","Jaehong Yoon","Mohit Bansal","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2311.10707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10090v2","updated":"2023-11-17T18:49:04Z","published":"2023-11-16T18:58:43Z","title":"JaxMARL: Multi-Agent RL Environments in JAX","summary":"  Benchmarks play an important role in the development of machine learning\nalgorithms. For example, research in reinforcement learning (RL) has been\nheavily influenced by available environments and benchmarks. However, RL\nenvironments are traditionally run on the CPU, limiting their scalability with\ntypical academic compute. Recent advancements in JAX have enabled the wider use\nof hardware acceleration to overcome these computational hurdles, enabling\nmassively parallel RL training pipelines and environments. This is particularly\nuseful for multi-agent reinforcement learning (MARL) research. First of all,\nmultiple agents must be considered at each environment step, adding\ncomputational burden, and secondly, the sample complexity is increased due to\nnon-stationarity, decentralised partial observability, or other MARL\nchallenges. In this paper, we present JaxMARL, the first open-source code base\nthat combines ease-of-use with GPU enabled efficiency, and supports a large\nnumber of commonly used MARL environments as well as popular baseline\nalgorithms. When considering wall clock time, our experiments show that per-run\nour JAX-based training pipeline is up to 12500x faster than existing\napproaches. This enables efficient and thorough evaluations, with the potential\nto alleviate the evaluation crisis of the field. We also introduce and\nbenchmark SMAX, a vectorised, simplified version of the popular StarCraft\nMulti-Agent Challenge, which removes the need to run the StarCraft II game\nengine. This not only enables GPU acceleration, but also provides a more\nflexible MARL environment, unlocking the potential for self-play,\nmeta-learning, and other future applications in MARL. We provide code at\nhttps://github.com/flairox/jaxmarl.\n","authors":["Alexander Rutherford","Benjamin Ellis","Matteo Gallici","Jonathan Cook","Andrei Lupu","Gardar Ingvarsson","Timon Willi","Akbir Khan","Christian Schroeder de Witt","Alexandra Souly","Saptarashmi Bandyopadhyay","Mikayel Samvelyan","Minqi Jiang","Robert Tjarko Lange","Shimon Whiteson","Bruno Lacerda","Nick Hawes","Tim Rocktaschel","Chris Lu","Jakob Nicolaus Foerster"],"pdf_url":"https://arxiv.org/pdf/2311.10090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10701v1","updated":"2023-11-17T18:45:00Z","published":"2023-11-17T18:45:00Z","title":"SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet\n  Variational Autoencoder for Hyperspectral Pixel Unmixing","summary":"  The Hyperspectral Unxming problem is to find the pure spectral signal of the\nunderlying materials (endmembers) and their proportions (abundances). The\nproposed method builds upon the recently proposed method, Latent Dirichlet\nVariational Autoencoder (LDVAE). It assumes that abundances can be encoded as\nDirichlet Distributions while mixed pixels and endmembers are represented by\nMultivariate Normal Distributions. However, LDVAE does not leverage spatial\ninformation present in an HSI; we propose an Isotropic CNN encoder with spatial\nattention to solve the hyperspectral unmixing problem. We evaluated our model\non Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model\nalso leverages the transfer learning paradigm for Cuprite Dataset, where we\ntrain the model on synthetic data and evaluate it on real-world data. We are\nable to observe the improvement in the results for the endmember extraction and\nabundance estimation by incorporating the spatial information. Code can be\nfound at https://github.com/faisalqureshi/cnn-ldvae\n","authors":["Soham Chitnis","Kiran Mantripragada","Faisal Z. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2311.10701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10699v1","updated":"2023-11-17T18:43:32Z","published":"2023-11-17T18:43:32Z","title":"Using linear initialisation to improve speed of convergence and\n  fully-trained error in Autoencoders","summary":"  Good weight initialisation is an important step in successful training of\nArtificial Neural Networks. Over time a number of improvements have been\nproposed to this process. In this paper we introduce a novel weight\ninitialisation technique called the Straddled Matrix Initialiser. This\ninitialisation technique is motivated by our assumption that major,\nglobal-scale relationships in data are linear with only smaller effects\nrequiring complex non-linearities. Combination of Straddled Matrix and ReLU\nactivation function initialises a Neural Network as a de facto linear model,\nwhich we postulate should be a better starting point for optimisation given our\nassumptions. We test this by training autoencoders on three datasets using\nStraddled Matrix and seven other state-of-the-art weight initialisation\ntechniques. In all our experiments the Straddeled Matrix Initialiser clearly\noutperforms all other methods.\n","authors":["Marcel Marais","Mate Hartstein","George Cevora"],"pdf_url":"https://arxiv.org/pdf/2311.10699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10697v1","updated":"2023-11-17T18:32:17Z","published":"2023-11-17T18:32:17Z","title":"PEFT-MedAware: Large Language Model for Medical Awareness","summary":"  Chat models are capable of answering a wide range of questions, however, the\naccuracy of their responses is highly uncertain. In this research, we propose a\nspecialized PEFT-MedAware model where we utilize parameter-efficient\nfine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized\nMedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of\nits trainable parameters to enhance computational efficiency. The paper adopts\ndata preprocessing and PEFT to optimize model performance, complemented by a\nBitsAndBytesConfig for efficient transformer training. The resulting model was\ncapable of outperforming other LLMs in medical question-answering tasks in\nspecific domains with greater accuracy utilizing limited computational\nresources making it suitable for deployment in resource-constrained\nenvironments. We propose further improvements through expanded datasets, larger\nmodels, and feedback mechanisms for sustained medical relevancy. Our work\nhighlights the efficiency gains and specialized capabilities of PEFT in medical\nAI, outpacing standard models in precision without extensive resource demands.\nThe proposed model and data are released for research purposes only.\n","authors":["Keivalya Pandya"],"pdf_url":"https://arxiv.org/pdf/2311.10697v1.pdf","comment":"7 pages, 1 figure, submitted to the Artificial Intelligence in\n  Medicine Journal"},{"id":"http://arxiv.org/abs/2311.10680v1","updated":"2023-11-17T18:01:58Z","published":"2023-11-17T18:01:58Z","title":"Optimal Embedding Dimension for Sparse Subspace Embeddings","summary":"  A random $m\\times n$ matrix $S$ is an oblivious subspace embedding (OSE) with\nparameters $\\epsilon>0$, $\\delta\\in(0,1/3)$ and $d\\leq m\\leq n$, if for any\n$d$-dimensional subspace $W\\subseteq R^n$,\n  $P\\big(\\,\\forall_{x\\in W}\\ (1+\\epsilon)^{-1}\\|x\\|\\leq\\|Sx\\|\\leq\n(1+\\epsilon)\\|x\\|\\,\\big)\\geq 1-\\delta.$\n  It is known that the embedding dimension of an OSE must satisfy $m\\geq d$,\nand for any $\\theta > 0$, a Gaussian embedding matrix with $m\\geq (1+\\theta) d$\nis an OSE with $\\epsilon = O_\\theta(1)$. However, such optimal embedding\ndimension is not known for other embeddings. Of particular interest are sparse\nOSEs, having $s\\ll m$ non-zeros per column, with applications to problems such\nas least squares regression and low-rank approximation.\n  We show that, given any $\\theta > 0$, an $m\\times n$ random matrix $S$ with\n$m\\geq (1+\\theta)d$ consisting of randomly sparsified $\\pm1/\\sqrt s$ entries\nand having $s= O(\\log^4(d))$ non-zeros per column, is an oblivious subspace\nembedding with $\\epsilon = O_{\\theta}(1)$. Our result addresses the main open\nquestion posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse\nOSEs can achieve $m=O(d)$ embedding dimension, and it improves on\n$m=O(d\\log(d))$ shown by Cohen (SODA 2016). We use this to construct the first\noblivious subspace embedding with $O(d)$ embedding dimension that can be\napplied faster than current matrix multiplication time, and to obtain an\noptimal single-pass algorithm for least squares regression. We further extend\nour results to construct even sparser non-oblivious embeddings, leading to the\nfirst subspace embedding with low distortion $\\epsilon=o(1)$ and optimal\nembedding dimension $m=O(d/\\epsilon^2)$ that can be applied in current matrix\nmultiplication time.\n","authors":["Shabarish Chenakkod","Michał Dereziński","Xiaoyu Dong","Mark Rudelson"],"pdf_url":"https://arxiv.org/pdf/2311.10680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10678v1","updated":"2023-11-17T18:00:20Z","published":"2023-11-17T18:00:20Z","title":"Distilling and Retrieving Generalizable Knowledge for Robot Manipulation\n  via Language Corrections","summary":"  Today's robot policies exhibit subpar performance when faced with the\nchallenge of generalizing to novel environments. Human corrective feedback is a\ncrucial form of guidance to enable such generalization. However, adapting to\nand learning from online human corrections is a non-trivial endeavor: not only\ndo robots need to remember human feedback over time to retrieve the right\ninformation in new settings and reduce the intervention rate, but also they\nwould need to be able to respond to feedback that can be arbitrary corrections\nabout high-level human preferences to low-level adjustments to skill\nparameters. In this work, we present Distillation and Retrieval of Online\nCorrections (DROC), a large language model (LLM)-based system that can respond\nto arbitrary forms of language feedback, distill generalizable knowledge from\ncorrections, and retrieve relevant past experiences based on textual and visual\nsimilarity for improving performance in novel settings. DROC is able to respond\nto a sequence of online language corrections that address failures in both\nhigh-level task plans and low-level skill primitives. We demonstrate that DROC\neffectively distills the relevant information from the sequence of online\ncorrections in a knowledge base and retrieves that knowledge in settings with\nnew task or object instances. DROC outperforms other techniques that directly\ngenerate robot code via LLMs by using only half of the total number of\ncorrections needed in the first round and requires little to no corrections\nafter two iterations. We show further results, videos, prompts and code on\nhttps://sites.google.com/stanford.edu/droc .\n","authors":["Lihan Zha","Yuchen Cui","Li-Heng Lin","Minae Kwon","Montserrat Gonzalez Arenas","Andy Zeng","Fei Xia","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2311.10678v1.pdf","comment":"8 pages, 4 figures, videos and code links on website\n  https://sites.google.com/stanford.edu/droc"},{"id":"http://arxiv.org/abs/2311.10671v1","updated":"2023-11-17T17:43:11Z","published":"2023-11-17T17:43:11Z","title":"Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based\n  Inference","summary":"  We present multimodal neural posterior estimation (MultiNPE), a method to\nintegrate heterogeneous data from different sources in simulation-based\ninference with neural networks. Inspired by advances in attention-based deep\nfusion learning, it empowers researchers to analyze data from different domains\nand infer the parameters of complex mathematical models with increased\naccuracy. We formulate different multimodal fusion approaches for MultiNPE\n(early, late, and hybrid) and evaluate their performance in three challenging\nnumerical experiments. MultiNPE not only outperforms na\\\"ive baselines on a\nbenchmark model, but also achieves superior inference on representative\nscientific models from neuroscience and cardiology. In addition, we\nsystematically investigate the impact of partially missing data on the\ndifferent fusion strategies. Across our different experiments, late and hybrid\nfusion techniques emerge as the methods of choice for practical applications of\nmultimodal simulation-based inference.\n","authors":["Marvin Schmitt","Stefan T. Radev","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2311.10671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10665v1","updated":"2023-11-17T17:36:26Z","published":"2023-11-17T17:36:26Z","title":"Online Calibration of Deep Learning Sub-Models for Hybrid Numerical\n  Modeling Systems","summary":"  Artificial intelligence and deep learning are currently reshaping numerical\nsimulation frameworks by introducing new modeling capabilities. These\nframeworks are extensively investigated in the context of model correction and\nparameterization where they demonstrate great potential and often outperform\ntraditional physical models. Most of these efforts in defining hybrid dynamical\nsystems follow {offline} learning strategies in which the neural\nparameterization (called here sub-model) is trained to output an ideal\ncorrection. Yet, these hybrid models can face hard limitations when defining\nwhat should be a relevant sub-model response that would translate into a good\nforecasting performance. End-to-end learning schemes, also referred to as\nonline learning, could address such a shortcoming by allowing the deep learning\nsub-models to train on historical data. However, defining end-to-end training\nschemes for the calibration of neural sub-models in hybrid systems requires\nworking with an optimization problem that involves the solver of the physical\nequations. Online learning methodologies thus require the numerical model to be\ndifferentiable, which is not the case for most modeling systems. To overcome\nthis difficulty and bypass the differentiability challenge of physical models,\nwe present an efficient and practical online learning approach for hybrid\nsystems. The method, called EGA for Euler Gradient Approximation, assumes an\nadditive neural correction to the physical model, and an explicit Euler\napproximation of the gradients. We demonstrate that the EGA converges to the\nexact gradients in the limit of infinitely small time steps. Numerical\nexperiments are performed on various case studies, including prototypical\nocean-atmosphere dynamics. Results show significant improvements over offline\nlearning, highlighting the potential of end-to-end online learning for hybrid\nmodeling.\n","authors":["Said Ouala","Bertrand Chapron","Fabrice Collard","Lucile Gaultier","Ronan Fablet"],"pdf_url":"https://arxiv.org/pdf/2311.10665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10653v1","updated":"2023-11-17T17:14:42Z","published":"2023-11-17T17:14:42Z","title":"Learning Realistic Joint Space Boundaries for Range of Motion Analysis\n  of Healthy and Impaired Human Arms","summary":"  A realistic human kinematic model that satisfies anatomical constraints is\nessential for human-robot interaction, biomechanics and robot-assisted\nrehabilitation. Modeling realistic joint constraints, however, is challenging\nas human arm motion is constrained by joint limits, inter- and intra-joint\ndependencies, self-collisions, individual capabilities and muscular or\nneurological constraints which are difficult to represent. Hence, physicians\nand researchers have relied on simple box-constraints, ignoring important\nanatomical factors. In this paper, we propose a data-driven method to learn\nrealistic anatomically constrained upper-limb range of motion (RoM) boundaries\nfrom motion capture data. This is achieved by fitting a one-class support\nvector machine to a dataset of upper-limb joint space exploration motions with\nan efficient hyper-parameter tuning scheme. Our approach outperforms similar\nworks focused on valid RoM learning. Further, we propose an impairment index\n(II) metric that offers a quantitative assessment of capability/impairment when\ncomparing healthy and impaired arms. We validate the metric on healthy subjects\nphysically constrained to emulate hemiplegia and different disability levels as\nstroke patients.\n","authors":["Shafagh Keyvanian","Michelle J. Johnson","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2311.10653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00690v2","updated":"2023-11-17T17:09:56Z","published":"2023-11-01T17:45:52Z","title":"What User Behaviors Make the Differences During the Process of Visual\n  Analytics?","summary":"  The understanding of visual analytics process can benefit visualization\nresearchers from multiple aspects, including improving visual designs and\ndeveloping advanced interaction functions. However, the log files of user\nbehaviors are still hard to analyze due to the complexity of sensemaking and\nour lack of knowledge on the related user behaviors. This work presents a study\non a comprehensive data collection of user behaviors, and our analysis approach\nwith time-series classification methods. We have chosen a classical\nvisualization application, Covid-19 data analysis, with common analysis tasks\ncovering geo-spatial, time-series and multi-attributes. Our user study collects\nuser behaviors on a diverse set of visualization tasks with two comparable\nsystems, desktop and immersive visualizations. We summarize the classification\nresults with three time-series machine learning algorithms at two scales, and\nexplore the influences of behavior features. Our results reveal that user\nbehaviors can be distinguished during the process of visual analytics and there\nis a potentially strong association between the physical behaviors of users and\nthe visualization tasks they perform. We also demonstrate the usage of our\nmodels by interpreting open sessions of visual analytics, which provides an\nautomatic way to study sensemaking without tedious manual annotations.\n","authors":["Shahin Doroudian","Zekun Wu","Aidong Lu"],"pdf_url":"https://arxiv.org/pdf/2311.00690v2.pdf","comment":"The authors have decided to withdraw the paper due to identified\n  critical errors. These errors were deemed substantial enough to compromise\n  the integrity and reliability of the research findings presented in the\n  paper. As a result, the authors have chosen to retract the paper to maintain\n  academic standards and transparency in the dissemination of scientific\n  knowledge"},{"id":"http://arxiv.org/abs/2311.10648v1","updated":"2023-11-17T17:06:59Z","published":"2023-11-17T17:06:59Z","title":"Self-trained Panoptic Segmentation","summary":"  Panoptic segmentation is an important computer vision task which combines\nsemantic and instance segmentation. It plays a crucial role in domains of\nmedical image analysis, self-driving vehicles, and robotics by providing a\ncomprehensive understanding of visual environments. Traditionally, deep\nlearning panoptic segmentation models have relied on dense and accurately\nannotated training data, which is expensive and time consuming to obtain.\nRecent advancements in self-supervised learning approaches have shown great\npotential in leveraging synthetic and unlabelled data to generate pseudo-labels\nusing self-training to improve the performance of instance and semantic\nsegmentation models. The three available methods for self-supervised panoptic\nsegmentation use proposal-based transformer architectures which are\ncomputationally expensive, complicated and engineered for specific tasks. The\naim of this work is to develop a framework to perform embedding-based\nself-supervised panoptic segmentation using self-training in a\nsynthetic-to-real domain adaptation problem setting.\n","authors":["Shourya Verma"],"pdf_url":"https://arxiv.org/pdf/2311.10648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17290v2","updated":"2023-11-17T17:05:44Z","published":"2023-09-29T14:48:24Z","title":"In search of dispersed memories: Generative diffusion models are\n  associative memory networks","summary":"  Uncovering the mechanisms behind long-term memory is one of the most\nfascinating open problems in neuroscience and artificial intelligence.\nArtificial associative memory networks have been used to formalize important\naspects of biological memory. Generative diffusion models are a type of\ngenerative machine learning techniques that have shown great performance in\nmany tasks. Like associative memory systems, these networks define a dynamical\nsystem that converges to a set of target states. In this work we show that\ngenerative diffusion models can be interpreted as energy-based models and that,\nwhen trained on discrete patterns, their energy function is (asymptotically)\nidentical to that of modern Hopfield networks. This equivalence allows us to\ninterpret the supervised training of diffusion models as a synaptic learning\nprocess that encodes the associative dynamics of a modern Hopfield network in\nthe weight structure of a deep neural network. Leveraging this connection, we\nformulate a generalized framework for understanding the formation of long-term\nmemory, where creative generation and memory recall can be seen as parts of a\nunified continuum.\n","authors":["Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2309.17290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20654v3","updated":"2023-11-17T17:01:26Z","published":"2023-10-31T17:24:40Z","title":"Closed Drafting as a Case Study for First-Principle Interpretability,\n  Memory, and Generalizability in Deep Reinforcement Learning","summary":"  Closed drafting or \"pick and pass\" is a popular game mechanic where each\nround players select a card or other playable element from their hand and pass\nthe rest to the next player. In this paper, we establish first-principle\nmethods for studying the interpretability, generalizability, and memory of Deep\nQ-Network (DQN) models playing closed drafting games. In particular, we use a\npopular family of closed drafting games called \"Sushi Go Party\", in which we\nachieve state-of-the-art performance. We fit decision rules to interpret the\ndecision-making strategy of trained DRL agents by comparing them to the ranking\npreferences of different types of human players. As Sushi Go Party can be\nexpressed as a set of closely-related games based on the set of cards in play,\nwe quantify the generalizability of DRL models trained on various sets of\ncards, establishing a method to benchmark agent performance as a function of\nenvironment unfamiliarity. Using the explicitly calculable memory of other\nplayer's hands in closed drafting games, we create measures of the ability of\nDRL models to learn memory.\n","authors":["Ryan Rezai","Jason Wang"],"pdf_url":"https://arxiv.org/pdf/2310.20654v3.pdf","comment":"4 pages, 4 figures, equal contribution"},{"id":"http://arxiv.org/abs/2311.10642v1","updated":"2023-11-17T16:58:52Z","published":"2023-11-17T16:58:52Z","title":"Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as\n  an Alternative to Attention Layers in Transformers","summary":"  This work presents an analysis of the effectiveness of using standard shallow\nfeed-forward networks to mimic the behavior of the attention mechanism in the\noriginal Transformer model, a state-of-the-art architecture for\nsequence-to-sequence tasks. We substitute key elements of the attention\nmechanism in the Transformer with simple feed-forward networks, trained using\nthe original components via knowledge distillation. Our experiments, conducted\non the IWSLT2017 dataset, reveal the capacity of these \"attentionless\nTransformers\" to rival the performance of the original architecture. Through\nrigorous ablation studies, and experimenting with various replacement network\ntypes and sizes, we offer insights that support the viability of our approach.\nThis not only sheds light on the adaptability of shallow feed-forward networks\nin emulating attention mechanisms but also underscores their potential to\nstreamline complex architectures for sequence-to-sequence tasks.\n","authors":["Vukasin Bozic","Danilo Dordervic","Daniele Coppola","Joseph Thommes"],"pdf_url":"https://arxiv.org/pdf/2311.10642v1.pdf","comment":"Accepted at AAAI24(https://aaai.org/aaai-conference/)"},{"id":"http://arxiv.org/abs/2311.10640v1","updated":"2023-11-17T16:55:14Z","published":"2023-11-17T16:55:14Z","title":"Multi-delay arterial spin-labeled perfusion estimation with biophysics\n  simulation and deep learning","summary":"  Purpose: To develop biophysics-based method for estimating perfusion Q from\narterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net\n(QTMnet) was trained to estimate perfusion from 4D tracer propagation images.\nThe network was trained and tested on simulated 4D tracer concentration data\nbased on artificial vasculature structure generated by constrained constructive\noptimization (CCO) method. The trained network was further tested in a\nsynthetic brain ASL image based on vasculature network extracted from magnetic\nresonance (MR) angiography. The estimations from both trained network and a\nconventional kinetic model were compared in ASL images acquired from eight\nhealthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from\nconcentration data. Relative error of the synthetic brain ASL image was 7.04%\nfor perfusion Q, lower than the error using single-delay ASL model: 25.15% for\nQ, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet\nprovides accurate estimation on perfusion parameters and is a promising\napproach as a clinical ASL MRI image processing pipeline.\n","authors":["Renjiu Hu","Qihao Zhang","Pascal Spincemaille","Thanh D. Nguyen","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2311.10640v1.pdf","comment":"32 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.10638v1","updated":"2023-11-17T16:50:00Z","published":"2023-11-17T16:50:00Z","title":"Concept-free Causal Disentanglement with Variational Graph Auto-Encoder","summary":"  In disentangled representation learning, the goal is to achieve a compact\nrepresentation that consists of all interpretable generative factors in the\nobservational data. Learning disentangled representations for graphs becomes\nincreasingly important as graph data rapidly grows. Existing approaches often\nrely on Variational Auto-Encoder (VAE) or its causal structure learning-based\nrefinement, which suffer from sub-optimality in VAEs due to the independence\nfactor assumption and unavailability of concept labels, respectively. In this\npaper, we propose an unsupervised solution, dubbed concept-free causal\ndisentanglement, built on a theoretically provable tight upper bound\napproximating the optimal factor. This results in an SCM-like causal structure\nmodeling that directly learns concept structures from data. Based on this idea,\nwe propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal\ndisentanglement layer into Variational Graph Auto-Encoder. Furthermore, we\nprove concept consistency under our concept-free causal disentanglement\nframework, hence employing it to enhance the meta-learning framework, called\nconcept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive\nexperiments to demonstrate the superiority of the proposed models: CCVGAE and\nCC-Meta-Graph, reaching up to $29\\%$ and $11\\%$ absolute improvements over\nbaselines in terms of AUC, respectively.\n","authors":["Jingyun Feng","Lin Zhang","Lili Yang"],"pdf_url":"https://arxiv.org/pdf/2311.10638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04689v2","updated":"2023-11-17T16:47:49Z","published":"2023-03-07T17:22:38Z","title":"A Privacy Preserving System for Movie Recommendations Using Federated\n  Learning","summary":"  Recommender systems have become ubiquitous in the past years. They solve the\ntyranny of choice problem faced by many users, and are utilized by many online\nbusinesses to drive engagement and sales. Besides other criticisms, like\ncreating filter bubbles within social networks, recommender systems are often\nreproved for collecting considerable amounts of personal data. However, to\npersonalize recommendations, personal information is fundamentally required. A\nrecent distributed learning scheme called federated learning has made it\npossible to learn from personal user data without its central collection.\nConsequently, we present a recommender system for movie recommendations, which\nprovides privacy and thus trustworthiness on multiple levels: First and\nforemost, it is trained using federated learning and thus, by its very nature,\nprivacy-preserving, while still enabling users to benefit from global insights.\nFurthermore, a novel federated learning scheme, called FedQ, is employed, which\nnot only addresses the problem of non-i.i.d.-ness and small local datasets, but\nalso prevents input data reconstruction attacks by aggregating client updates\nearly. Finally, to reduce the communication overhead, compression is applied,\nwhich significantly compresses the exchanged neural network parametrizations to\na fraction of their original size. We conjecture that this may also improve\ndata privacy through its lossy quantization stage.\n","authors":["David Neumann","Andreas Lutz","Karsten Müller","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2303.04689v2.pdf","comment":"Accepted by the ACM TORS Special Issue on Trustworthy Recommender\n  Systems"},{"id":"http://arxiv.org/abs/2204.09603v3","updated":"2023-11-17T16:42:46Z","published":"2022-04-20T16:33:01Z","title":"Comparing Deep Reinforcement Learning Algorithms in Two-Echelon Supply\n  Chains","summary":"  In this study, we analyze and compare the performance of state-of-the-art\ndeep reinforcement learning algorithms for solving the supply chain inventory\nmanagement problem. This complex sequential decision-making problem consists of\ndetermining the optimal quantity of products to be produced and shipped across\ndifferent warehouses over a given time horizon. In particular, we present a\nmathematical formulation of a two-echelon supply chain environment with\nstochastic and seasonal demand, which allows managing an arbitrary number of\nwarehouses and product types. Through a rich set of numerical experiments, we\ncompare the performance of different deep reinforcement learning algorithms\nunder various supply chain structures, topologies, demands, capacities, and\ncosts. The results of the experimental plan indicate that deep reinforcement\nlearning algorithms outperform traditional inventory management strategies,\nsuch as the static (s, Q)-policy. Furthermore, this study provides detailed\ninsight into the design and development of an open-source software library that\nprovides a customizable environment for solving the supply chain inventory\nmanagement problem using a wide range of data-driven approaches.\n","authors":["Francesco Stranieri","Fabio Stella"],"pdf_url":"https://arxiv.org/pdf/2204.09603v3.pdf","comment":"The paper has been accepted for presentation and inclusion in the\n  proceedings of the AI for Manufacturing workshop (AI4M), co-located with the\n  ECML PKDD 2023 (European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases). For supplementary material and\n  source code, please visit https://github.com/frenkowski/SCIMAI-Gym"},{"id":"http://arxiv.org/abs/2311.10633v1","updated":"2023-11-17T16:41:35Z","published":"2023-11-17T16:41:35Z","title":"Predicting the Probability of Collision of a Satellite with Space\n  Debris: A Bayesian Machine Learning Approach","summary":"  Space is becoming more crowded in Low Earth Orbit due to increased space\nactivity. Such a dense space environment increases the risk of collisions\nbetween space objects endangering the whole space population. Therefore, the\nneed to consider collision avoidance as part of routine operations is evident\nto satellite operators. Current procedures rely on the analysis of multiple\ncollision warnings by human analysts. However, with the continuous growth of\nthe space population, this manual approach may become unfeasible, highlighting\nthe importance of automation in risk assessment. In 2019, ESA launched a\ncompetition to study the feasibility of applying machine learning in collision\nrisk estimation and released a dataset that contained sequences of Conjunction\nData Messages (CDMs) in support of real close encounters. The competition\nresults showed that the naive forecast and its variants are strong predictors\nfor this problem, which suggests that the CDMs may follow the Markov property.\nThe proposed work investigates this theory by benchmarking Hidden Markov Models\n(HMM) in predicting the risk of collision between two resident space objects by\nusing one feature of the entire dataset: the sequence of the probability in the\nCDMs. In addition, Bayesian statistics are used to infer a joint distribution\nfor the parameters of the models, which allows the development of robust and\nreliable probabilistic predictive models that can incorporate physical or prior\nknowledge about the problem within a rigorous theoretical framework and\nprovides prediction uncertainties that nicely reflect the accuracy of the\npredicted risk. This work shows that the implemented HMM outperforms the naive\nsolution in some metrics, which further adds to the idea that the collision\nwarnings may be Markovian and suggests that this is a powerful method to be\nfurther explored.\n","authors":["João Simões Catulo","Cláudia Soares","Marta Guimarães"],"pdf_url":"https://arxiv.org/pdf/2311.10633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01841v2","updated":"2023-11-17T16:37:44Z","published":"2023-05-03T00:56:12Z","title":"Inferential Moments of Uncertain Multivariable Systems","summary":"  This article expands the framework of Bayesian inference and provides direct\nprobabilistic methods for approaching inference tasks that are typically\nhandled with information theory. We treat Bayesian probability updating as a\nrandom process and uncover intrinsic quantitative features of joint probability\ndistributions called inferential moments. Inferential moments quantify shape\ninformation about how a prior distribution is expected to update in response to\nyet to be obtained information. Further, we quantify the unique probability\ndistribution whose statistical moments are the inferential moments in question.\nWe find a power series expansion of the mutual information in terms of\ninferential moments, which implies a connection between inferential theoretic\nlogic and elements of information theory. Of particular interest is the\ninferential deviation, which is the expected variation of the probability of\none variable in response to an inferential update of another. We explore two\napplications that analyze the inferential deviations of a Bayesian network to\nimprove decision-making. We implement simple greedy algorithms for exploring\nsensor tasking using inferential deviations that generally outperform similar\ngreedy mutual information algorithms in terms of root mean squared error\nbetween epistemic probability estimates and the ground truth probabilities they\nare estimating.\n","authors":["Kevin Vanslette"],"pdf_url":"https://arxiv.org/pdf/2305.01841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10436v3","updated":"2023-11-17T16:29:49Z","published":"2023-08-21T03:13:38Z","title":"Approximately Equivariant Graph Networks","summary":"  Graph neural networks (GNNs) are commonly described as being permutation\nequivariant with respect to node relabeling in the graph. This symmetry of GNNs\nis often compared to the translation equivariance of Euclidean convolution\nneural networks (CNNs). However, these two symmetries are fundamentally\ndifferent: The translation equivariance of CNNs corresponds to symmetries of\nthe fixed domain acting on the image signals (sometimes known as active\nsymmetries), whereas in GNNs any permutation acts on both the graph signals and\nthe graph domain (sometimes described as passive symmetries). In this work, we\nfocus on the active symmetries of GNNs, by considering a learning setting where\nsignals are supported on a fixed graph. In this case, the natural symmetries of\nGNNs are the automorphisms of the graph. Since real-world graphs tend to be\nasymmetric, we relax the notion of symmetries by formalizing approximate\nsymmetries via graph coarsening. We present a bias-variance formula that\nquantifies the tradeoff between the loss in expressivity and the gain in the\nregularity of the learned estimator, depending on the chosen symmetry group. To\nillustrate our approach, we conduct extensive experiments on image inpainting,\ntraffic flow prediction, and human pose estimation with different choices of\nsymmetries. We show theoretically and empirically that the best generalization\nperformance can be achieved by choosing a suitably larger group than the graph\nautomorphism, but smaller than the permutation group.\n","authors":["Ningyuan Huang","Ron Levie","Soledad Villar"],"pdf_url":"https://arxiv.org/pdf/2308.10436v3.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.08502v2","updated":"2023-11-17T16:08:52Z","published":"2023-11-14T19:49:09Z","title":"Variational Quantum Eigensolver with Constraints (VQEC): Solving\n  Constrained Optimization Problems via VQE","summary":"  Variational quantum approaches have shown great promise in finding\nnear-optimal solutions to computationally challenging tasks. Nonetheless,\nenforcing constraints in a disciplined fashion has been largely unexplored. To\naddress this gap, this work proposes a hybrid quantum-classical algorithmic\nparadigm termed VQEC that extends the celebrated VQE to handle optimization\nwith constraints. As with the standard VQE, the vector of optimization\nvariables is captured by the state of a variational quantum circuit (VQC). To\ndeal with constraints, VQEC optimizes a Lagrangian function classically over\nboth the VQC parameters as well as the dual variables associated with\nconstraints. To comply with the quantum setup, variables are updated via a\nperturbed primal-dual method leveraging the parameter shift rule. Among a wide\ngamut of potential applications, we showcase how VQEC can approximately solve\nquadratically-constrained binary optimization (QCBO) problems, find stochastic\nbinary policies satisfying quadratic constraints on the average and in\nprobability, and solve large-scale linear programs (LP) over the probability\nsimplex. Under an assumption on the error for the VQC to approximate an\narbitrary probability mass function (PMF), we provide bounds on the optimality\ngap attained by a VQC. Numerical tests on a quantum simulator investigate the\neffect of various parameters and corroborate that VQEC can generate\nhigh-quality solutions.\n","authors":["Thinh Viet Le","Vassilis Kekatos"],"pdf_url":"https://arxiv.org/pdf/2311.08502v2.pdf","comment":"22 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2311.10610v1","updated":"2023-11-17T16:04:31Z","published":"2023-11-17T16:04:31Z","title":"A Poincaré Inequality and Consistency Results for Signal Sampling on\n  Large Graphs","summary":"  Large-scale graph machine learning is challenging as the complexity of\nlearning models scales with the graph size. Subsampling the graph is a viable\nalternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.\nExisting graph sampling techniques require not only computing the spectra of\nlarge matrices but also repeating these computations when the graph changes,\ne.g., grows. In this paper, we introduce a signal sampling theory for a type of\ngraph limit -- the graphon. We prove a Poincar\\'e inequality for graphon\nsignals and show that complements of node subsets satisfying this inequality\nare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting\nconnections with spectral clustering and Gaussian elimination, we prove that\nsuch sampling sets are consistent in the sense that unique sampling sets on a\nconvergent graph sequence converge to unique sampling sets on the graphon. We\nthen propose a related graphon signal sampling algorithm for large graphs, and\ndemonstrate its good empirical performance on graph machine learning tasks.\n","authors":["Thien Le","Luana Ruiz","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2311.10610v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2311.10609v1","updated":"2023-11-17T16:04:27Z","published":"2023-11-17T16:04:27Z","title":"Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data\n  Fitted Networks","summary":"  Tabular classification has traditionally relied on supervised algorithms,\nwhich estimate the parameters of a prediction model using its training data.\nRecently, Prior-Data Fitted Networks (PFNs) such as TabPFN have successfully\nlearned to classify tabular data in-context: the model parameters are designed\nto classify new samples based on labelled training samples given after the\nmodel training. While such models show great promise, their applicability to\nreal-world data remains limited due to the computational scale needed. Here we\nstudy the following question: given a pre-trained PFN for tabular data, what is\nthe best way to summarize the labelled training samples before feeding them to\nthe model? We conduct an initial investigation of sketching and\nfeature-selection methods for TabPFN, and note certain key differences between\nit and conventionally fitted tabular models.\n","authors":["Benjamin Feuer","Chinmay Hegde","Niv Cohen"],"pdf_url":"https://arxiv.org/pdf/2311.10609v1.pdf","comment":"2nd Table Representation Learning Workshop: 37th Conference on Neural\n  Information Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2311.10607v1","updated":"2023-11-17T16:03:04Z","published":"2023-11-17T16:03:04Z","title":"Active Inference on the Edge: A Design Study","summary":"  Machine Learning (ML) is a common tool to interpret and predict the behavior\nof distributed computing systems, e.g., to optimize the task distribution\nbetween devices. As more and more data is created by Internet of Things (IoT)\ndevices, data processing and ML training are carried out by edge devices in\nclose proximity. To ensure Quality of Service (QoS) throughout these\noperations, systems are supervised and dynamically adapted with the help of ML.\nHowever, as long as ML models are not retrained, they fail to capture gradual\nshifts in the variable distribution, leading to an inaccurate view of the\nsystem state. Moreover, as the prediction accuracy decreases, the reporting\ndevice should actively resolve uncertainties to improve the model's precision.\nSuch a level of self-determination could be provided by Active Inference (ACI)\n-- a concept from neuroscience that describes how the brain constantly predicts\nand evaluates sensory information to decrease long-term surprise. We\nencompassed these concepts in a single action-perception cycle, which we\nimplemented for distributed agents in a smart manufacturing use case. As a\nresult, we showed how our ACI agent was able to quickly and traceably solve an\noptimization problem while fulfilling QoS requirements.\n","authors":["Boris Sedlak","Victor Casamayor Pujol","Praveen Kumar Donta","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2311.10607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10597v1","updated":"2023-11-17T15:49:56Z","published":"2023-11-17T15:49:56Z","title":"Designing Reconfigurable Intelligent Systems with Markov Blankets","summary":"  Compute Continuum (CC) systems comprise a vast number of devices distributed\nover computational tiers. Evaluating business requirements, i.e., Service Level\nObjectives (SLOs), requires collecting data from all those devices; if SLOs are\nviolated, devices must be reconfigured to ensure correct operation. If done\ncentrally, this dramatically increases the number of devices and variables that\nmust be considered, while creating an enormous communication overhead. To\naddress this, we (1) introduce a causality filter based on Markov blankets (MB)\nthat limits the number of variables that each device must track, (2) evaluate\nSLOs decentralized on a device basis, and (3) infer optimal device\nconfiguration for fulfilling SLOs. We evaluated our methodology by analyzing\nvideo stream transformations and providing device configurations that ensure\nthe Quality of Service (QoS). The devices thus perceived their environment and\nacted accordingly -- a form of decentralized intelligence.\n","authors":["Boris Sedlak","Victor Casamayor Pujol","Praveen Kumar Donta","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2311.10597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10590v1","updated":"2023-11-17T15:45:00Z","published":"2023-11-17T15:45:00Z","title":"EduGym: An Environment Suite for Reinforcement Learning Education","summary":"  Due to the empirical success of reinforcement learning, an increasing number\nof students study the subject. However, from our practical teaching experience,\nwe see students entering the field (bachelor, master and early PhD) often\nstruggle. On the one hand, textbooks and (online) lectures provide the\nfundamentals, but students find it hard to translate between equations and\ncode. On the other hand, public codebases do provide practical examples, but\nthe implemented algorithms tend to be complex, and the underlying test\nenvironments contain multiple reinforcement learning challenges at once.\nAlthough this is realistic from a research perspective, it often hinders\neducational conceptual understanding. To solve this issue we introduce EduGym,\na set of educational reinforcement learning environments and associated\ninteractive notebooks tailored for education. Each EduGym environment is\nspecifically designed to illustrate a certain aspect/challenge of reinforcement\nlearning (e.g., exploration, partial observability, stochasticity, etc.), while\nthe associated interactive notebook explains the challenge and its possible\nsolution approaches, connecting equations and code in a single document. An\nevaluation among RL students and researchers shows 86% of them think EduGym is\na useful tool for reinforcement learning education. All notebooks are available\nfrom https://sites.google.com/view/edu-gym/home, while the full software\npackage can be installed from https://github.com/RLG-Leiden/edugym.\n","authors":["Thomas M. Moerland","Matthias Müller-Brockhausen","Zhao Yang","Andrius Bernatavicius","Koen Ponse","Tom Kouwenhoven","Andreas Sauter","Michiel van der Meer","Bram Renting","Aske Plaat"],"pdf_url":"https://arxiv.org/pdf/2311.10590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16047v3","updated":"2023-11-17T15:41:33Z","published":"2023-03-28T15:25:46Z","title":"Exploring and Interacting with the Set of Good Sparse Generalized\n  Additive Models","summary":"  In real applications, interaction between machine learning models and domain\nexperts is critical; however, the classical machine learning paradigm that\nusually produces only a single model does not facilitate such interaction.\nApproximating and exploring the Rashomon set, i.e., the set of all near-optimal\nmodels, addresses this practical challenge by providing the user with a\nsearchable space containing a diverse set of models from which domain experts\ncan choose. We present algorithms to efficiently and accurately approximate the\nRashomon set of sparse, generalized additive models with ellipsoids for fixed\nsupport sets and use these ellipsoids to approximate Rashomon sets for many\ndifferent support sets. The approximated Rashomon set serves as a cornerstone\nto solve practical challenges such as (1) studying the variable importance for\nthe model class; (2) finding models under user-specified constraints\n(monotonicity, direct editing); and (3) investigating sudden changes in the\nshape functions. Experiments demonstrate the fidelity of the approximated\nRashomon set and its effectiveness in solving practical challenges.\n","authors":["Chudi Zhong","Zhi Chen","Jiachang Liu","Margo Seltzer","Cynthia Rudin"],"pdf_url":"https://arxiv.org/pdf/2303.16047v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.10580v1","updated":"2023-11-17T15:30:44Z","published":"2023-11-17T15:30:44Z","title":"Implicit Maximum a Posteriori Filtering via Adaptive Optimization","summary":"  Bayesian filtering approximates the true underlying behavior of a\ntime-varying system by inverting an explicit generative model to convert noisy\nmeasurements into state estimates. This process typically requires either\nstorage, inversion, and multiplication of large matrices or Monte Carlo\nestimation, neither of which are practical in high-dimensional state spaces\nsuch as the weight spaces of artificial neural networks. Here, we frame the\nstandard Bayesian filtering problem as optimization over a time-varying\nobjective. Instead of maintaining matrices for the filtering equations or\nsimulating particles, we specify an optimizer that defines the Bayesian filter\nimplicitly. In the linear-Gaussian setting, we show that every Kalman filter\nhas an equivalent formulation using K steps of gradient descent. In the\nnonlinear setting, our experiments demonstrate that our framework results in\nfilters that are effective, robust, and scalable to high-dimensional systems,\ncomparing well against the standard toolbox of Bayesian filtering solutions. We\nsuggest that it is easier to fine-tune an optimizer than it is to specify the\ncorrect filtering equations, making our framework an attractive option for\nhigh-dimensional filtering problems.\n","authors":["Gianluca M. Bencomo","Jake C. Snell","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2311.10580v1.pdf","comment":"Under review at ICLR 2024"},{"id":"http://arxiv.org/abs/2311.10579v1","updated":"2023-11-17T15:30:12Z","published":"2023-11-17T15:30:12Z","title":"Graph Neural Networks for Pressure Estimation in Water Distribution\n  Systems","summary":"  Pressure and flow estimation in Water Distribution Networks (WDN) allows\nwater management companies to optimize their control operations. For many\nyears, mathematical simulation tools have been the most common approach to\nreconstructing an estimate of the WDN hydraulics. However, pure physics-based\nsimulations involve several challenges, e.g. partially observable data, high\nuncertainty, and extensive manual configuration. Thus, data-driven approaches\nhave gained traction to overcome such limitations. In this work, we combine\nphysics-based modeling and Graph Neural Networks (GNN), a data-driven approach,\nto address the pressure estimation problem. First, we propose a new data\ngeneration method using a mathematical simulation but not considering temporal\npatterns and including some control parameters that remain untouched in\nprevious works; this contributes to a more diverse training data. Second, our\ntraining strategy relies on random sensor placement making our GNN-based\nestimation model robust to unexpected sensor location changes. Third, a\nrealistic evaluation protocol considers real temporal patterns and additionally\ninjects the uncertainties intrinsic to real-world scenarios. Finally, a\nmulti-graph pre-training strategy allows the model to be reused for pressure\nestimation in unseen target WDNs. Our GNN-based model estimates the pressure of\na large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of\n7%, surpassing the performance of previous studies. Likewise, it outperformed\nprevious approaches on other WDN benchmarks, showing a reduction of absolute\nerror up to approximately 52% in the best cases.\n","authors":["Huy Truong","Andrés Tello","Alexander Lazovik","Victoria Degeler"],"pdf_url":"https://arxiv.org/pdf/2311.10579v1.pdf","comment":"submitted to Water Resources Research. Huy Truong and Andr\\'es Tello\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2211.06302v3","updated":"2023-11-17T15:14:41Z","published":"2022-11-11T16:13:34Z","title":"GCondNet: A Novel Method for Improving Neural Networks on Small\n  High-Dimensional Tabular Data","summary":"  Neural network models often struggle with high-dimensional but small\nsample-size tabular datasets. One reason is that current weight initialisation\nmethods assume independence between weights, which can be problematic when\nthere are insufficient samples to estimate the model's parameters accurately.\nIn such small data scenarios, leveraging additional structures can improve the\nmodel's performance and training stability. To address this, we propose\nGCondNet, a general approach to enhance neural networks by leveraging implicit\nstructures present in tabular data. We create a graph between samples for each\ndata dimension, and utilise Graph Neural Networks (GNNs) for extracting this\nimplicit structure, and for conditioning the parameters of the first layer of\nan underlying predictor network. By creating many small graphs, GCondNet\nexploits the data's high-dimensionality, and thus improves the performance of\nan underlying predictor network. We demonstrate the effectiveness of our method\non 9 real-world datasets, where GCondNet outperforms 15 standard and\nstate-of-the-art methods. The results show that GCondNet is a versatile\nframework for injecting graph-regularisation into various types of neural\nnetworks, including MLPs and tabular Transformers.\n","authors":["Andrei Margeloiu","Nikola Simidjievski","Pietro Lio","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2211.06302v3.pdf","comment":"Accepted at the 2nd Table Representation Learning Workshop at NeurIPS\n  2023 [selected for oral presentation]"},{"id":"http://arxiv.org/abs/2311.10572v1","updated":"2023-11-17T15:14:40Z","published":"2023-11-17T15:14:40Z","title":"SSB: Simple but Strong Baseline for Boosting Performance of Open-Set\n  Semi-Supervised Learning","summary":"  Semi-supervised learning (SSL) methods effectively leverage unlabeled data to\nimprove model generalization. However, SSL models often underperform in\nopen-set scenarios, where unlabeled data contain outliers from novel categories\nthat do not appear in the labeled set. In this paper, we study the challenging\nand realistic open-set SSL setting, where the goal is to both correctly\nclassify inliers and to detect outliers. Intuitively, the inlier classifier\nshould be trained on inlier data only. However, we find that inlier\nclassification performance can be largely improved by incorporating\nhigh-confidence pseudo-labeled data, regardless of whether they are inliers or\noutliers. Also, we propose to utilize non-linear transformations to separate\nthe features used for inlier classification and outlier detection in the\nmulti-task learning framework, preventing adverse effects between them.\nAdditionally, we introduce pseudo-negative mining, which further boosts outlier\ndetection performance. The three ingredients lead to what we call Simple but\nStrong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves\nboth inlier classification and outlier detection performance, outperforming\nexisting methods by a large margin. Our code will be released at\nhttps://github.com/YUE-FAN/SSB.\n","authors":["Yue Fan","Anna Kukleva","Dengxin Dai","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2311.10572v1.pdf","comment":"Paper accepted in ICCV 2023"},{"id":"http://arxiv.org/abs/2311.10571v1","updated":"2023-11-17T15:10:35Z","published":"2023-11-17T15:10:35Z","title":"Direct Amortized Likelihood Ratio Estimation","summary":"  We introduce a new amortized likelihood ratio estimator for likelihood-free\nsimulation-based inference (SBI). Our estimator is simple to train and\nestimates the likelihood ratio using a single forward pass of the neural\nestimator. Our approach directly computes the likelihood ratio between two\ncompeting parameter sets which is different from the previous approach of\ncomparing two neural network output values. We refer to our model as the direct\nneural ratio estimator (DNRE). As part of introducing the DNRE, we derive a\ncorresponding Monte Carlo estimate of the posterior. We benchmark our new ratio\nestimator and compare to previous ratio estimators in the literature. We show\nthat our new ratio estimator often outperforms these previous approaches. As a\nfurther contribution, we introduce a new derivative estimator for likelihood\nratio estimators that enables us to compare likelihood-free Hamiltonian Monte\nCarlo (HMC) with random-walk Metropolis-Hastings (MH). We show that HMC is\nequally competitive, which has not been previously shown. Finally, we include a\nnovel real-world application of SBI by using our neural ratio estimator to\ndesign a quadcopter. Code is available at https://github.com/SRI-CSL/dnre.\n","authors":["Adam D. Cobb","Brian Matejek","Daniel Elenius","Anirban Roy","Susmit Jha"],"pdf_url":"https://arxiv.org/pdf/2311.10571v1.pdf","comment":"12 Pages, 10 Figures, GitHub: https://github.com/SRI-CSL/dnre"},{"id":"http://arxiv.org/abs/2309.07383v4","updated":"2023-11-17T15:04:12Z","published":"2023-09-14T02:02:08Z","title":"Rates of Convergence in Certain Native Spaces of Approximations used in\n  Reinforcement Learning","summary":"  This paper studies convergence rates for some value function approximations\nthat arise in a collection of reproducing kernel Hilbert spaces (RKHS)\n$H(\\Omega)$. By casting an optimal control problem in a specific class of\nnative spaces, strong rates of convergence are derived for the operator\nequation that enables offline approximations that appear in policy iteration.\nExplicit upper bounds on error in value function and controller approximations\nare derived in terms of power function $\\mathcal{P}_{H,N}$ for the space of\nfinite dimensional approximants $H_N$ in the native space $H(\\Omega)$. These\nbounds are geometric in nature and refine some well-known, now classical\nresults concerning convergence of approximations of value functions.\n","authors":["Ali Bouland","Shengyuan Niu","Sai Tej Paruchuri","Andrew Kurdila","John Burns","Eugenio Schuster"],"pdf_url":"https://arxiv.org/pdf/2309.07383v4.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2007.12882v4","updated":"2023-11-17T14:44:26Z","published":"2020-07-25T08:40:29Z","title":"A finite sample analysis of the benign overfitting phenomenon for ridge\n  function estimation","summary":"  Recent extensive numerical experiments in high scale machine learning have\nallowed to uncover a quite counterintuitive phase transition, as a function of\nthe ratio between the sample size and the number of parameters in the model. As\nthe number of parameters $p$ approaches the sample size $n$, the generalisation\nerror increases, but surprisingly, it starts decreasing again past the\nthreshold $p=n$. This phenomenon, brought to the theoretical community\nattention in \\cite{belkin2019reconciling}, has been thoroughly investigated\nlately, more specifically for simpler models than deep neural networks, such as\nthe linear model when the parameter is taken to be the minimum norm solution to\nthe least-squares problem, firstly in the asymptotic regime when $p$ and $n$\ntend to infinity, see e.g. \\cite{hastie2019surprises}, and recently in the\nfinite dimensional regime and more specifically for linear models\n\\cite{bartlett2020benign}, \\cite{tsigler2020benign},\n\\cite{lecue2022geometrical}. In the present paper, we propose a finite sample\nanalysis of non-linear models of \\textit{ridge} type, where we investigate the\n\\textit{overparametrised regime} of the double descent phenomenon for both the\n\\textit{estimation problem} and the \\textit{prediction} problem. Our results\nprovide a precise analysis of the distance of the best estimator from the true\nparameter as well as a generalisation bound which complements recent works of\n\\cite{bartlett2020benign} and \\cite{chinot2020benign}. Our analysis is based on\ntools closely related to the continuous Newton method\n\\cite{neuberger2007continuous} and a refined quantitative analysis of the\nperformance in prediction of the minimum $\\ell_2$-norm solution.\n","authors":["Emmanuel Caron","Stephane Chretien"],"pdf_url":"https://arxiv.org/pdf/2007.12882v4.pdf","comment":"New section on generalisation added"},{"id":"http://arxiv.org/abs/2311.10550v1","updated":"2023-11-17T14:32:43Z","published":"2023-11-17T14:32:43Z","title":"RONAALP: Reduced-Order Nonlinear Approximation with Active Learning\n  Procedure","summary":"  Many engineering applications rely on the evaluation of expensive, non-linear\nhigh-dimensional functions. In this paper, we propose the RONAALP algorithm\n(Reduced Order Nonlinear Approximation with Active Learning Procedure) to\nincrementally learn a fast and accurate reduced-order surrogate model of a\ntarget function on-the-fly as the application progresses. First, the\ncombination of nonlinear auto-encoder, community clustering and radial basis\nfunction networks allows to learn an efficient and compact surrogate model with\nlimited training data. Secondly, the active learning procedure overcome any\nextrapolation issue when evaluating the surrogate model outside of its initial\ntraining range during the online stage. This results in generalizable, fast and\naccurate reduced-order models of high-dimensional functions. The method is\ndemonstrated on three direct numerical simulations of hypersonic flows in\nchemical nonequilibrium. Accurate simulations of these flows rely on detailed\nthermochemical gas models that dramatically increase the cost of such\ncalculations. Using RONAALP to learn a reduced-order thermodynamic model\nsurrogate on-the-fly, the cost of such simulation was reduced by up to 75%\nwhile maintaining an error of less than 10% on relevant quantities of interest.\n","authors":["Clément Scherding","Georgios Rigas","Denis Sipp","Peter J Schmid","Taraneh Sayadi"],"pdf_url":"https://arxiv.org/pdf/2311.10550v1.pdf","comment":"38 pages, 16 figures"},{"id":"http://arxiv.org/abs/2311.10525v1","updated":"2023-11-17T13:45:31Z","published":"2023-11-17T13:45:31Z","title":"Utilizing VQ-VAE for End-to-End Health Indicator Generation in\n  Predicting Rolling Bearing RUL","summary":"  The prediction of the remaining useful life (RUL) of rolling bearings is a\npivotal issue in industrial production. A crucial approach to tackling this\nissue involves transforming vibration signals into health indicators (HI) to\naid model training. This paper presents an end-to-end HI construction method,\nvector quantised variational autoencoder (VQ-VAE), which addresses the need for\ndimensionality reduction of latent variables in traditional unsupervised\nlearning methods such as autoencoder. Moreover, concerning the inadequacy of\ntraditional statistical metrics in reflecting curve fluctuations accurately,\ntwo novel statistical metrics, mean absolute distance (MAD) and mean variance\n(MV), are introduced. These metrics accurately depict the fluctuation patterns\nin the curves, thereby indicating the model's accuracy in discerning similar\nfeatures. On the PMH2012 dataset, methods employing VQ-VAE for label\nconstruction achieved lower values for MAD and MV. Furthermore, the ASTCN\nprediction model trained with VQ-VAE labels demonstrated commendable\nperformance, attaining the lowest values for MAD and MV.\n","authors":["Junliang Wang","Qinghua Zhang","Guanhua Zhu","Guoxi Sun"],"pdf_url":"https://arxiv.org/pdf/2311.10525v1.pdf","comment":"17 figures"},{"id":"http://arxiv.org/abs/2311.10517v1","updated":"2023-11-17T13:40:10Z","published":"2023-11-17T13:40:10Z","title":"Mind the map! Accounting for existing map information when estimating\n  online HDMaps from sensor data","summary":"  Online High Definition Map (HDMap) estimation from sensors offers a low-cost\nalternative to manually acquired HDMaps. As such, it promises to lighten costs\nfor already HDMap-reliant Autonomous Driving systems, and potentially even\nspread their use to new systems. In this paper, we propose to improve online\nHDMap estimation by accounting for already existing maps. We identify 3\nreasonable types of useful existing maps (minimalist, noisy, and outdated). We\nalso introduce MapEX, a novel online HDMap estimation framework that accounts\nfor existing maps. MapEX achieves this by encoding map elements into query\ntokens and by refining the matching algorithm used to train classic query based\nmap estimation models. We demonstrate that MapEX brings significant\nimprovements on the nuScenes dataset. For instance, MapEX - given noisy maps -\nimproves by 38% over the MapTRv2 detector it is based on and by 16% over the\ncurrent SOTA.\n","authors":["Rémy Sun","Li Yang","Diane Lingrand","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2311.10517v1.pdf","comment":"12 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2311.10512v1","updated":"2023-11-17T13:31:19Z","published":"2023-11-17T13:31:19Z","title":"Causal Fairness-Guided Dataset Reweighting using Neural Networks","summary":"  The importance of achieving fairness in machine learning models cannot be\noverstated. Recent research has pointed out that fairness should be examined\nfrom a causal perspective, and several fairness notions based on the on Pearl's\ncausal framework have been proposed. In this paper, we construct a reweighting\nscheme of datasets to address causal fairness. Our approach aims at mitigating\nbias by considering the causal relationships among variables and incorporating\nthem into the reweighting process. The proposed method adopts two neural\nnetworks, whose structures are intentionally used to reflect the structures of\na causal graph and of an interventional graph. The two neural networks can\napproximate the causal model of the data, and the causal model of\ninterventions. Furthermore, reweighting guided by a discriminator is applied to\nachieve various fairness notions. Experiments on real-world datasets show that\nour method can achieve causal fairness on the data while remaining close to the\noriginal data for downstream tasks.\n","authors":["Xuan Zhao","Klaus Broelemann","Salvatore Ruggieri","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2311.10512v1.pdf","comment":"To be published in the proceedings of 2023 IEEE International\n  Conference on Big Data (IEEE BigData 2023)"},{"id":"http://arxiv.org/abs/2306.08984v3","updated":"2023-11-17T13:14:58Z","published":"2023-06-15T09:25:04Z","title":"Tree Variational Autoencoders","summary":"  We propose Tree Variational Autoencoder (TreeVAE), a new generative\nhierarchical clustering model that learns a flexible tree-based posterior\ndistribution over latent variables. TreeVAE hierarchically divides samples\naccording to their intrinsic characteristics, shedding light on hidden\nstructures in the data. It adapts its architecture to discover the optimal tree\nfor encoding dependencies between latent variables. The proposed tree-based\ngenerative architecture enables lightweight conditional inference and improves\ngenerative performance by utilizing specialized leaf decoders. We show that\nTreeVAE uncovers underlying clusters in the data and finds meaningful\nhierarchical relations between the different groups on a variety of datasets,\nincluding real-world imaging data. We present empirically that TreeVAE provides\na more competitive log-likelihood lower bound than the sequential counterparts.\nFinally, due to its generative nature, TreeVAE is able to generate new samples\nfrom the discovered clusters via conditional sampling.\n","authors":["Laura Manduchi","Moritz Vandenhirtz","Alain Ryser","Julia Vogt"],"pdf_url":"https://arxiv.org/pdf/2306.08984v3.pdf","comment":"Accepted as Spotlight to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.10500v1","updated":"2023-11-17T13:01:09Z","published":"2023-11-17T13:01:09Z","title":"From Principle to Practice: Vertical Data Minimization for Machine\n  Learning","summary":"  Aiming to train and deploy predictive models, organizations collect large\namounts of detailed client data, risking the exposure of private information in\nthe event of a breach. To mitigate this, policymakers increasingly demand\ncompliance with the data minimization (DM) principle, restricting data\ncollection to only that data which is relevant and necessary for the task.\nDespite regulatory pressure, the problem of deploying machine learning models\nthat obey DM has so far received little attention. In this work, we address\nthis challenge in a comprehensive manner. We propose a novel vertical DM (vDM)\nworkflow based on data generalization, which by design ensures that no\nfull-resolution client data is collected during training and deployment of\nmodels, benefiting client privacy by reducing the attack surface in case of a\nbreach. We formalize and study the corresponding problem of finding\ngeneralizations that both maximize data utility and minimize empirical privacy\nrisk, which we quantify by introducing a diverse set of policy-aligned\nadversarial scenarios. Finally, we propose a range of baseline vDM algorithms,\nas well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that\noutperforms all baselines across several settings. We plan to release our code\nas a publicly available library, helping advance the standardization of DM for\nmachine learning. Overall, we believe our work can help lay the foundation for\nfurther exploration and adoption of DM principles in real-world applications.\n","authors":["Robin Staab","Nikola Jovanović","Mislav Balunović","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2311.10500v1.pdf","comment":"Accepted at IEEE S&P 2024"},{"id":"http://arxiv.org/abs/2201.05613v3","updated":"2023-11-17T13:01:01Z","published":"2022-01-14T16:04:09Z","title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet","summary":"  Pre-trained Transformers are challenging human performances in many NLP\ntasks. The massive datasets used for pre-training seem to be the key to their\nsuccess on existing tasks. In this paper, we explore how a range of pre-trained\nNatural Language Understanding models perform on definitely unseen sentences\nprovided by classification tasks over a DarkNet corpus. Surprisingly, results\nshow that syntactic and lexical neural networks perform on par with pre-trained\nTransformers even after fine-tuning. Only after what we call extreme domain\nadaptation, that is, retraining with the masked language model task on all the\nnovel corpus, pre-trained Transformers reach their standard high results. This\nsuggests that huge pre-training corpora may give Transformers unexpected help\nsince they are exposed to many of the possible sentences.\n","authors":["Leonardo Ranaldi","Aria Nourbakhsh","Arianna Patrizi","Elena Sofia Ruzzetti","Dario Onorati","Francesca Fallucchi","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2201.05613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01913v2","updated":"2023-11-17T12:47:26Z","published":"2023-03-03T13:27:00Z","title":"Bespoke: A Block-Level Neural Network Optimization Framework for\n  Low-Cost Deployment","summary":"  As deep learning models become popular, there is a lot of need for deploying\nthem to diverse device environments. Because it is costly to develop and\noptimize a neural network for every single environment, there is a line of\nresearch to search neural networks for multiple target environments\nefficiently. However, existing works for such a situation still suffer from\nrequiring many GPUs and expensive costs. Motivated by this, we propose a novel\nneural network optimization framework named Bespoke for low-cost deployment.\nOur framework searches for a lightweight model by replacing parts of an\noriginal model with randomly selected alternatives, each of which comes from a\npretrained neural network or the original model. In the practical sense,\nBespoke has two significant merits. One is that it requires near zero cost for\ndesigning the search space of neural networks. The other merit is that it\nexploits the sub-networks of public pretrained neural networks, so the total\ncost is minimal compared to the existing works. We conduct experiments\nexploring Bespoke's the merits, and the results show that it finds efficient\nmodels for multiple targets with meager cost.\n","authors":["Jong-Ryul Lee","Yong-Hyuk Moon"],"pdf_url":"https://arxiv.org/pdf/2303.01913v2.pdf","comment":"This is the extended version of our AAAI-2023 paper\n  (https://ojs.aaai.org/index.php/AAAI/article/view/26020)"},{"id":"http://arxiv.org/abs/2303.17245v3","updated":"2023-11-17T12:43:09Z","published":"2023-03-30T09:22:17Z","title":"Investigating and Mitigating the Side Effects of Noisy Views for\n  Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios","summary":"  Multi-view clustering (MVC) aims at exploring category structures among\nmulti-view data in self-supervised manners. Multiple views provide more\ninformation than single views and thus existing MVC methods can achieve\nsatisfactory performance. However, their performance might seriously degenerate\nwhen the views are noisy in practical multi-view scenarios. In this paper, we\nfirst formally investigate the drawback of noisy views and then propose a\ntheoretically grounded deep MVC method (namely MVCAN) to address this issue.\nSpecifically, we propose a novel MVC objective that enables un-shared\nparameters and inconsistent clustering predictions across multiple views to\nreduce the side effects of noisy views. Furthermore, a two-level multi-view\niterative optimization is designed to generate robust learning targets for\nrefining individual views' representation learning. Theoretical analysis\nreveals that MVCAN works by achieving the multi-view consistency,\ncomplementarity, and noise robustness. Finally, experiments on extensive public\ndatasets demonstrate that MVCAN outperforms state-of-the-art methods and is\nrobust against the existence of noisy views.\n","authors":["Jie Xu","Yazhou Ren","Xiaolong Wang","Lei Feng","Zheng Zhang","Gang Niu","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17245v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10489v1","updated":"2023-11-17T12:41:07Z","published":"2023-11-17T12:41:07Z","title":"Handling Overlapping Asymmetric Datasets -- A Twice Penalized P-Spline\n  Approach","summary":"  Overlapping asymmetric datasets are common in data science and pose questions\nof how they can be incorporated together into a predictive analysis. In\nhealthcare datasets there is often a small amount of information that is\navailable for a larger number of patients such as an electronic health record,\nhowever a small number of patients may have had extensive further testing.\nCommon solutions such as missing imputation can often be unwise if the smaller\ncohort is significantly different in scale to the larger sample, therefore the\naim of this research is to develop a new method which can model the smaller\ncohort against a particular response, whilst considering the larger cohort\nalso. Motivated by non-parametric models, and specifically flexible smoothing\ntechniques via generalized additive models, we model a twice penalized P-Spline\napproximation method to firstly prevent over/under-fitting of the smaller\ncohort and secondly to consider the larger cohort. This second penalty is\ncreated through discrepancies in the marginal value of covariates that exist in\nboth the smaller and larger cohorts. Through data simulations, parameter\ntunings and model adaptations to consider a continuous and binary response, we\nfind our twice penalized approach offers an enhanced fit over a linear B-Spline\nand once penalized P-Spline approximation. Applying to a real-life dataset\nrelating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see\nan improved model fit performance of over 65%. Areas for future work within\nthis space include adapting our method to not require dimensionality reduction\nand also consider parametric modelling methods. However, to our knowledge this\nis the first work to propose additional marginal penalties in a flexible\nregression of which we can report a vastly improved model fit that is able to\nconsider asymmetric datasets, without the need for missing data imputation.\n","authors":["Matthew McTeer","Robin Henderson","Quentin M Anstee","Paolo Missier"],"pdf_url":"https://arxiv.org/pdf/2311.10489v1.pdf","comment":"52 pages, 17 figures, 8 tables, 34 references"},{"id":"http://arxiv.org/abs/2208.06265v3","updated":"2023-11-17T12:38:19Z","published":"2022-08-10T08:28:46Z","title":"Trustworthy Recommender Systems","summary":"  Recommender systems (RSs) aim to help users to effectively retrieve items of\ntheir interests from a large catalogue. For a quite long period of time,\nresearchers and practitioners have been focusing on developing accurate RSs.\nRecent years have witnessed an increasing number of threats to RSs, coming from\nattacks, system and user generated noise, system bias. As a result, it has\nbecome clear that a strict focus on RS accuracy is limited and the research\nmust consider other important factors, e.g., trustworthiness. For end users, a\ntrustworthy RS (TRS) should not only be accurate, but also transparent,\nunbiased and fair as well as robust to noise or attacks. These observations\nactually led to a paradigm shift of the research on RSs: from accuracy-oriented\nRSs to TRSs. However, researchers lack a systematic overview and discussion of\nthe literature in this novel and fast developing field of TRSs. To this end, in\nthis paper, we provide an overview of TRSs, including a discussion of the\nmotivation and basic concepts of TRSs, a presentation of the challenges in\nbuilding TRSs, and a perspective on the future directions in this area. We also\nprovide a novel conceptual framework to support the construction of TRSs.\n","authors":["Shoujin Wang","Xiuzhen Zhang","Yan Wang","Huan Liu","Francesco Ricci"],"pdf_url":"https://arxiv.org/pdf/2208.06265v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10471v1","updated":"2023-11-17T11:55:11Z","published":"2023-11-17T11:55:11Z","title":"Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model\n  Based on Human Mobility for Ubiquitous Urban Sensing","summary":"  User profiling and region analysis are two tasks of significant commercial\nvalue. However, in practical applications, modeling different features\ntypically involves four main steps: data preparation, data processing, model\nestablishment, evaluation, and optimization. This process is time-consuming and\nlabor-intensive. Repeating this workflow for each feature results in abundant\ndevelopment time for tasks and a reduced overall volume of task development.\nIndeed, human mobility data contains a wealth of information. Several\nsuccessful cases suggest that conducting in-depth analysis of population\nmovement data could potentially yield meaningful profiles about users and\nareas. Nonetheless, most related works have not thoroughly utilized the\nsemantic information within human mobility data and trained on a fixed number\nof the regions. To tap into the rich information within population movement,\nbased on the perspective that Regions Are Who walk them, we propose a large\nspatiotemporal model based on trajectories (RAW). It possesses the following\ncharacteristics: 1) Tailored for trajectory data, introducing a GPT-like\nstructure with a parameter count of up to 1B; 2) Introducing a spatiotemporal\nfine-tuning module, interpreting trajectories as collection of users to derive\narbitrary region embedding. This framework allows rapid task development based\non the large spatiotemporal model. We conducted extensive experiments to\nvalidate the effectiveness of our proposed large spatiotemporal model. It's\nevident that our proposed method, relying solely on human mobility data without\nadditional features, exhibits a certain level of relevance in user profiling\nand region analysis. Moreover, our model showcases promising predictive\ncapabilities in trajectory generation tasks based on the current state,\noffering the potential for further innovative work utilizing this large\nspatiotemporal model.\n","authors":["Ruixing Zhang","Liangzhe Han","Leilei Sun","Yunqi Liu","Jibin Wang","Weifeng Lv"],"pdf_url":"https://arxiv.org/pdf/2311.10471v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.08149v2","updated":"2023-11-17T11:51:18Z","published":"2023-11-14T13:25:41Z","title":"Modeling Complex Disease Trajectories using Deep Generative Models with\n  Semi-Supervised Latent Processes","summary":"  In this paper, we propose a deep generative time series approach using latent\ntemporal processes for modeling and holistically analyzing complex disease\ntrajectories. We aim to find meaningful temporal latent representations of an\nunderlying generative process that explain the observed disease trajectories in\nan interpretable and comprehensive way. To enhance the interpretability of\nthese latent temporal processes, we develop a semi-supervised approach for\ndisentangling the latent space using established medical concepts. By combining\nthe generative approach with medical knowledge, we leverage the ability to\ndiscover novel aspects of the disease while integrating medical concepts into\nthe model. We show that the learned temporal latent processes can be utilized\nfor further data analysis and clinical hypothesis testing, including finding\nsimilar patients and clustering the disease into new sub-types. Moreover, our\nmethod enables personalized online monitoring and prediction of multivariate\ntime series including uncertainty quantification. We demonstrate the\neffectiveness of our approach in modeling systemic sclerosis, showcasing the\npotential of our machine learning model to capture complex disease trajectories\nand acquire new medical knowledge.\n","authors":["Cécile Trottet","Manuel Schürch","Ahmed Allam","Imon Barua","Liubov Petelytska","Oliver Distler","Anna-Maria Hoffmann-Vold","Michael Krauthammer","the EUSTAR collaborators"],"pdf_url":"https://arxiv.org/pdf/2311.08149v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 23 pages"},{"id":"http://arxiv.org/abs/2311.10468v1","updated":"2023-11-17T11:48:10Z","published":"2023-11-17T11:48:10Z","title":"Using Cooperative Game Theory to Prune Neural Networks","summary":"  We show how solution concepts from cooperative game theory can be used to\ntackle the problem of pruning neural networks.\n  The ever-growing size of deep neural networks (DNNs) increases their\nperformance, but also their computational requirements. We introduce a method\ncalled Game Theory Assisted Pruning (GTAP), which reduces the neural network's\nsize while preserving its predictive accuracy. GTAP is based on eliminating\nneurons in the network based on an estimation of their joint impact on the\nprediction quality through game theoretic solutions. Specifically, we use a\npower index akin to the Shapley value or Banzhaf index, tailored using a\nprocedure similar to Dropout (commonly used to tackle overfitting problems in\nmachine learning).\n  Empirical evaluation of both feedforward networks and convolutional neural\nnetworks shows that this method outperforms existing approaches in the achieved\ntradeoff between the number of parameters and model accuracy.\n","authors":["Mauricio Diaz-Ortiz Jr","Benjamin Kempinski","Daphne Cornelisse","Yoram Bachrach","Tal Kachman"],"pdf_url":"https://arxiv.org/pdf/2311.10468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11247v2","updated":"2023-11-17T11:28:29Z","published":"2023-04-21T20:49:29Z","title":"Hybrid quantum physics-informed neural networks for simulating\n  computational fluid dynamics in complex shapes","summary":"  Finding the distribution of the velocities and pressures of a fluid (by\nsolving the Navier-Stokes equations) is a principal task in the chemical,\nenergy, and pharmaceutical industries, as well as in mechanical engineering and\nthe design of pipeline systems. With existing solvers, such as OpenFOAM and\nAnsys, simulations of fluid dynamics in intricate geometries are\ncomputationally expensive and require re-simulation whenever the geometric\nparameters or the initial and boundary conditions are altered. Physics-informed\nneural networks are a promising tool for simulating fluid flows in complex\ngeometries, as they can adapt to changes in the geometry and mesh definitions,\nallowing for generalization across different shapes. We present a hybrid\nquantum physics-informed neural network that simulates laminar fluid flows in\n3D Y-shaped mixers. Our approach combines the expressive power of a quantum\nmodel with the flexibility of a physics-informed neural network, resulting in a\n21% higher accuracy compared to a purely classical neural network. Our findings\nhighlight the potential of machine learning approaches, and in particular\nhybrid quantum physics-informed neural network, for complex shape optimization\ntasks in computational fluid dynamics. By improving the accuracy of fluid\nsimulations in complex geometries, our research using hybrid quantum models\ncontributes to the development of more efficient and reliable fluid dynamics\nsolvers.\n","authors":["Alexandr Sedykh","Maninadh Podapaka","Asel Sagingalieva","Karan Pinto","Markus Pflitsch","Alexey Melnikov"],"pdf_url":"https://arxiv.org/pdf/2304.11247v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.10456v1","updated":"2023-11-17T11:21:09Z","published":"2023-11-17T11:21:09Z","title":"Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs","summary":"  Microkinetics allows detailed modelling of chemical transformations occurring\nin many industrially relevant reactions. Traditional way of solving the\nmicrokinetics model for Fischer-Tropsch synthesis (FTS) becomes inefficient\nwhen it comes to more advanced real-time applications. In this work, we address\nthese challenges by using physics-informed neural networks(PINNs) for modelling\nFTS microkinetics. We propose a computationally efficient and accurate method,\nenabling the ultra-fast solution of the existing microkinetics models in\nrealistic process conditions. The proposed PINN model computes the fraction of\nvacant catalytic sites, a key quantity in FTS microkinetics, with median\nrelative error (MRE) of 0.03%, and the FTS product formation rates with MRE of\n0.1%. Compared to conventional equation solvers, the model achieves up to 1E+06\ntimes speed-up when running on GPUs, thus being fast enough for multi-scale and\nmulti-physics reactor modelling and enabling its applications in real-time\nprocess control and optimization.\n","authors":["Harshil Patel","Aniruddha Panda","Tymofii Nikolaienko","Stanislav Jaso","Alejandro Lopez","Kaushic Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2311.10456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14889v2","updated":"2023-11-17T11:18:51Z","published":"2023-03-27T02:55:56Z","title":"Model-Based Reinforcement Learning with Isolated Imaginations","summary":"  World models learn the consequences of actions in vision-based interactive\nsystems. However, in practical scenarios like autonomous driving,\nnoncontrollable dynamics that are independent or sparsely dependent on action\nsignals often exist, making it challenging to learn effective world models. To\naddress this issue, we propose Iso-Dream++, a model-based reinforcement\nlearning approach that has two main contributions. First, we optimize the\ninverse dynamics to encourage the world model to isolate controllable state\ntransitions from the mixed spatiotemporal variations of the environment.\nSecond, we perform policy optimization based on the decoupled latent\nimaginations, where we roll out noncontrollable states into the future and\nadaptively associate them with the current controllable state. This enables\nlong-horizon visuomotor control tasks to benefit from isolating mixed dynamics\nsources in the wild, such as self-driving cars that can anticipate the movement\nof other vehicles, thereby avoiding potential risks. On top of our previous\nwork, we further consider the sparse dependencies between controllable and\nnoncontrollable states, address the training collapse problem of state\ndecoupling, and validate our approach in transfer learning setups. Our\nempirical study demonstrates that Iso-Dream++ outperforms existing\nreinforcement learning models significantly on CARLA and DeepMind Control.\n","authors":["Minting Pan","Xiangming Zhu","Yitao Zheng","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.14889v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2205.13817"},{"id":"http://arxiv.org/abs/2311.01404v2","updated":"2023-11-17T11:06:52Z","published":"2023-11-02T17:17:03Z","title":"Normalizing flows as approximations of optimal transport maps via\n  linear-control neural ODEs","summary":"  The term \"Normalizing Flows\" is related to the task of constructing\ninvertible transport maps between probability measures by means of deep neural\nnetworks. In this paper, we consider the problem of recovering the\n$W_2$-optimal transport map $T$ between absolutely continuous measures\n$\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^n)$ as the flow of a linear-control neural\nODE. We first show that, under suitable assumptions on $\\mu,\\nu$ and on the\ncontrolled vector fields, the optimal transport map is contained in the\n$C^0_c$-closure of the flows generated by the system. Assuming that discrete\napproximations $\\mu_N,\\nu_N$ of the original measures $\\mu,\\nu$ are available,\nwe use a discrete optimal coupling $\\gamma_N$ to define an optimal control\nproblem. With a $\\Gamma$-convergence argument, we prove that its solutions\ncorrespond to flows that approximate the optimal transport map $T$. Finally,\ntaking advantage of the Pontryagin Maximum Principle, we propose an iterative\nnumerical scheme for the resolution of the optimal control problem, resulting\nin an algorithm for the practical computation of the approximated optimal\ntransport map.\n","authors":["Alessandro Scagliotti","Sara Farinelli"],"pdf_url":"https://arxiv.org/pdf/2311.01404v2.pdf","comment":"Correction of typos and new bibliographical references. 32 pages, 1\n  figure"},{"id":"http://arxiv.org/abs/2311.10448v1","updated":"2023-11-17T11:03:13Z","published":"2023-11-17T11:03:13Z","title":"DeepClean: Machine Unlearning on the Cheap by Resetting Privacy\n  Sensitive Weights using the Fisher Diagonal","summary":"  Machine learning models trained on sensitive or private data can\ninadvertently memorize and leak that information. Machine unlearning seeks to\nretroactively remove such details from model weights to protect privacy. We\ncontribute a lightweight unlearning algorithm that leverages the Fisher\nInformation Matrix (FIM) for selective forgetting. Prior work in this area\nrequires full retraining or large matrix inversions, which are computationally\nexpensive. Our key insight is that the diagonal elements of the FIM, which\nmeasure the sensitivity of log-likelihood to changes in weights, contain\nsufficient information for effective forgetting. Specifically, we compute the\nFIM diagonal over two subsets -- the data to retain and forget -- for all\ntrainable weights. This diagonal representation approximates the complete FIM\nwhile dramatically reducing computation. We then use it to selectively update\nweights to maximize forgetting of the sensitive subset while minimizing impact\non the retained subset. Experiments show that our algorithm can successfully\nforget any randomly selected subsets of training data across neural network\narchitectures. By leveraging the FIM diagonal, our approach provides an\ninterpretable, lightweight, and efficient solution for machine unlearning with\npractical privacy benefits.\n","authors":["Jiaeli Shi","Najah Ghalyan","Kostis Gourgoulias","John Buford","Sean Moran"],"pdf_url":"https://arxiv.org/pdf/2311.10448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10865v2","updated":"2023-11-17T10:48:43Z","published":"2023-07-20T13:34:11Z","title":"Addressing caveats of neural persistence with deep graph persistence","summary":"  Neural Persistence is a prominent measure for quantifying neural network\ncomplexity, proposed in the emerging field of topological data analysis in deep\nlearning. In this work, however, we find both theoretically and empirically\nthat the variance of network weights and spatial concentration of large weights\nare the main factors that impact neural persistence. Whilst this captures\nuseful information for linear classifiers, we find that no relevant spatial\nstructure is present in later layers of deep neural networks, making neural\npersistence roughly equivalent to the variance of weights. Additionally, the\nproposed averaging procedure across layers for deep neural networks does not\nconsider interaction between layers. Based on our analysis, we propose an\nextension of the filtration underlying neural persistence to the whole neural\nnetwork instead of single layers, which is equivalent to calculating neural\npersistence on one particular matrix. This yields our deep graph persistence\nmeasure, which implicitly incorporates persistent paths through the network and\nalleviates variance-related issues through standardisation. Code is available\nat https://github.com/ExplainableML/Deep-Graph-Persistence .\n","authors":["Leander Girrbach","Anders Christensen","Ole Winther","Zeynep Akata","A. Sophia Koepke"],"pdf_url":"https://arxiv.org/pdf/2307.10865v2.pdf","comment":"Transactions on Machine Learning Research (TMLR), 2023"},{"id":"http://arxiv.org/abs/2310.14858v2","updated":"2023-11-17T10:35:48Z","published":"2023-10-23T12:28:21Z","title":"Dynamically Weighted Federated k-Means","summary":"  Federated clustering, an integral aspect of federated machine learning,\nenables multiple data sources to collaboratively cluster their data,\nmaintaining decentralization and preserving privacy. In this paper, we\nintroduce a novel federated clustering algorithm named Dynamically Weighted\nFederated k-means (DWF k-means) based on Lloyd's method for k-means clustering,\nto address the challenges associated with distributed data sources and\nheterogeneous data. Our proposed algorithm combines the benefits of traditional\nclustering techniques with the privacy and scalability benefits offered by\nfederated learning. The algorithm facilitates collaborative clustering among\nmultiple data owners, allowing them to cluster their local data collectively\nwhile exchanging minimal information with the central coordinator. The\nalgorithm optimizes the clustering process by adaptively aggregating cluster\nassignments and centroids from each data source, thereby learning a global\nclustering solution that reflects the collective knowledge of the entire\nfederated network. We address the issue of empty clusters, which commonly\narises in the context of federated clustering. We conduct experiments on\nmultiple datasets and data distribution settings to evaluate the performance of\nour algorithm in terms of clustering score, accuracy, and v-measure. The\nresults demonstrate that our approach can match the performance of the\ncentralized classical k-means baseline, and outperform existing federated\nclustering methods like k-FED in realistic scenarios.\n","authors":["Patrick Holzer","Tania Jacob","Shubham Kavane"],"pdf_url":"https://arxiv.org/pdf/2310.14858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09790v2","updated":"2023-11-17T10:21:44Z","published":"2023-11-16T11:10:38Z","title":"Breaking Boundaries: Balancing Performance and Robustness in Deep\n  Wireless Traffic Forecasting","summary":"  Balancing the trade-off between accuracy and robustness is a long-standing\nchallenge in time series forecasting. While most of existing robust algorithms\nhave achieved certain suboptimal performance on clean data, sustaining the same\nperformance level in the presence of data perturbations remains extremely hard.\nIn this paper, we study a wide array of perturbation scenarios and propose\nnovel defense mechanisms against adversarial attacks using real-world telecom\ndata. We compare our strategy against two existing adversarial training\nalgorithms under a range of maximal allowed perturbations, defined using\n$\\ell_{\\infty}$-norm, $\\in [0.1,0.4]$. Our findings reveal that our hybrid\nstrategy, which is composed of a classifier to detect adversarial examples, a\ndenoiser to eliminate noise from the perturbed data samples, and a standard\nforecaster, achieves the best performance on both clean and perturbed data. Our\noptimal model can retain up to $92.02\\%$ the performance of the original\nforecasting model in terms of Mean Squared Error (MSE) on clean data, while\nbeing more robust than the standard adversarially trained models on perturbed\ndata. Its MSE is 2.71$\\times$ and 2.51$\\times$ lower than those of comparing\nmethods on normal and perturbed data, respectively. In addition, the components\nof our models can be trained in parallel, resulting in better computational\nefficiency. Our results indicate that we can optimally balance the trade-off\nbetween the performance and robustness of forecasting models by improving the\nclassifier and denoiser, even in the presence of sophisticated and destructive\npoisoning attacks.\n","authors":["Romain Ilbert","Thai V. Hoang","Zonghua Zhang","Themis Palpanas"],"pdf_url":"https://arxiv.org/pdf/2311.09790v2.pdf","comment":"Accepted for presentation at the ARTMAN workshop, part of the ACM\n  Conference on Computer and Communications Security (CCS), 2023"},{"id":"http://arxiv.org/abs/2311.10430v1","updated":"2023-11-17T10:05:10Z","published":"2023-11-17T10:05:10Z","title":"Deep Residual CNN for Multi-Class Chest Infection Diagnosis","summary":"  The advent of deep learning has significantly propelled the capabilities of\nautomated medical image diagnosis, providing valuable tools and resources in\nthe realm of healthcare and medical diagnostics. This research delves into the\ndevelopment and evaluation of a Deep Residual Convolutional Neural Network\n(CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray\nimages. The implemented model, trained and validated on a dataset amalgamated\nfrom diverse sources, demonstrated a robust overall accuracy of 93%. However,\nnuanced disparities in performance across different classes, particularly\nFibrosis, underscored the complexity and challenges inherent in automated\nmedical image diagnosis. The insights derived pave the way for future research,\nfocusing on enhancing the model's proficiency in classifying conditions that\npresent more subtle and nuanced visual features in the images, as well as\noptimizing and refining the model architecture and training process. This paper\nprovides a comprehensive exploration into the development, implementation, and\nevaluation of the model, offering insights and directions for future research\nand development in the field.\n","authors":["Ryan Donghan Kwon","Dohyun Lim","Yoonha Lee","Seung Won Lee"],"pdf_url":"https://arxiv.org/pdf/2311.10430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03574v5","updated":"2023-11-17T10:01:56Z","published":"2022-02-04T12:30:49Z","title":"Structured Prediction Problem Archive","summary":"  Structured prediction problems are one of the fundamental tools in machine\nlearning. In order to facilitate algorithm development for their numerical\nsolution, we collect in one place a large number of datasets in easy to read\nformats for a diverse set of problem classes. We provide archival links to\ndatasets, description of the considered problems and problem formats, and a\nshort summary of problem characteristics including size, number of instances\netc. For reference we also give a non-exhaustive selection of algorithms\nproposed in the literature for their solution. We hope that this central\nrepository will make benchmarking and comparison to established works easier.\nWe welcome submission of interesting new datasets and algorithms for inclusion\nin our archive.\n","authors":["Paul Swoboda","Bjoern Andres","Andrea Hornakova","Florian Bernard","Jannik Irmai","Paul Roetzer","Bogdan Savchynskyy","David Stein","Ahmed Abbas"],"pdf_url":"https://arxiv.org/pdf/2202.03574v5.pdf","comment":"Added multicast instances from Andres group"},{"id":"http://arxiv.org/abs/2311.10421v1","updated":"2023-11-17T09:54:35Z","published":"2023-11-17T09:54:35Z","title":"Maintenance Techniques for Anomaly Detection AIOps Solutions","summary":"  Anomaly detection techniques are essential in automating the monitoring of IT\nsystems and operations. These techniques imply that machine learning algorithms\nare trained on operational data corresponding to a specific period of time and\nthat they are continuously evaluated on newly emerging data. Operational data\nis constantly changing over time, which affects the performance of deployed\nanomaly detection models. Therefore, continuous model maintenance is required\nto preserve the performance of anomaly detectors over time. In this work, we\nanalyze two different anomaly detection model maintenance techniques in terms\nof the model update frequency, namely blind model retraining and informed model\nretraining. We further investigate the effects of updating the model by\nretraining it on all the available data (full-history approach) and on only the\nnewest data (sliding window approach). Moreover, we investigate whether a data\nchange monitoring tool is capable of determining when the anomaly detection\nmodel needs to be updated through retraining.\n","authors":["Lorena Poenaru-Olaru","Natalia Karpova","Luis Cruz","Jan Rellermeyer","Arie van Deursen"],"pdf_url":"https://arxiv.org/pdf/2311.10421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10418v1","updated":"2023-11-17T09:48:45Z","published":"2023-11-17T09:48:45Z","title":"DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines","summary":"  Multi-task model training has been adopted to enable a single deep neural\nnetwork model (often a large language model) to handle multiple tasks (e.g.,\nquestion answering and text summarization). Multi-task training commonly\nreceives input sequences of highly different lengths due to the diverse\ncontexts of different tasks. Padding (to the same sequence length) or packing\n(short examples into long sequences of the same length) is usually adopted to\nprepare input samples for model training, which is nonetheless not space or\ncomputation efficient. This paper proposes a dynamic micro-batching approach to\ntackle sequence length variation and enable efficient multi-task model\ntraining. We advocate pipeline-parallel training of the large model with\nvariable-length micro-batches, each of which potentially comprises a different\nnumber of samples. We optimize micro-batch construction using a dynamic\nprogramming-based approach, and handle micro-batch execution time variation\nthrough dynamic pipeline and communication scheduling, enabling highly\nefficient pipeline training. Extensive evaluation on the FLANv2 dataset\ndemonstrates up to 4.39x higher training throughput when training T5, and 3.25x\nwhen training GPT, as compared with packing-based baselines. DynaPipe's source\ncode is publicly available at\nhttps://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines.\n","authors":["Chenyu Jiang","Zhen Jia","Shuai Zheng","Yida Wang","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2311.10418v1.pdf","comment":"18 pages, 18 figures"},{"id":"http://arxiv.org/abs/2306.16058v3","updated":"2023-11-17T09:26:40Z","published":"2023-06-28T09:40:03Z","title":"DUET: 2D Structured and Approximately Equivariant Representations","summary":"  Multiview Self-Supervised Learning (MSSL) is based on learning invariances\nwith respect to a set of input transformations. However, invariance partially\nor totally removes transformation-related information from the representations,\nwhich might harm performance for specific downstream tasks that require such\ninformation. We propose 2D strUctured and EquivarianT representations (coined\nDUET), which are 2d representations organized in a matrix structure, and\nequivariant with respect to transformations acting on the input data. DUET\nrepresentations maintain information about an input transformation, while\nremaining semantically expressive. Compared to SimCLR (Chen et al., 2020)\n(unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured\nand equivariant), the structured and equivariant nature of DUET representations\nenables controlled generation with lower reconstruction error, while\ncontrollability is not possible with SimCLR or ESSL. DUET also achieves higher\naccuracy for several discriminative tasks, and improves transfer learning.\n","authors":["Xavier Suau","Federico Danieli","T. Anderson Keller","Arno Blaas","Chen Huang","Jason Ramapuram","Dan Busbridge","Luca Zappella"],"pdf_url":"https://arxiv.org/pdf/2306.16058v3.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2311.09930v2","updated":"2023-11-17T09:23:20Z","published":"2023-11-16T14:32:18Z","title":"A Framework for Monitoring and Retraining Language Models in Real-World\n  Applications","summary":"  In the Machine Learning (ML) model development lifecycle, training candidate\nmodels using an offline holdout dataset and identifying the best model for the\ngiven task is only the first step. After the deployment of the selected model,\ncontinuous model monitoring and model retraining is required in many real-world\napplications. There are multiple reasons for retraining, including data or\nconcept drift, which may be reflected on the model performance as monitored by\nan appropriate metric. Another motivation for retraining is the acquisition of\nincreasing amounts of data over time, which may be used to retrain and improve\nthe model performance even in the absence of drifts. We examine the impact of\nvarious retraining decision points on crucial factors, such as model\nperformance and resource utilization, in the context of Multilabel\nClassification models. We explain our key decision points and propose a\nreference framework for designing an effective model retraining strategy.\n","authors":["Jaykumar Kasundra","Claudia Schulz","Melicaalsadat Mirsafian","Stavroula Skylaki"],"pdf_url":"https://arxiv.org/pdf/2311.09930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10406v1","updated":"2023-11-17T09:15:43Z","published":"2023-11-17T09:15:43Z","title":"Decentralized Energy Marketplace via NFTs and AI-based Agents","summary":"  The paper introduces an advanced Decentralized Energy Marketplace (DEM)\nintegrating blockchain technology and artificial intelligence to manage energy\nexchanges among smart homes with energy storage systems. The proposed framework\nuses Non-Fungible Tokens (NFTs) to represent unique energy profiles in a\ntransparent and secure trading environment. Leveraging Federated Deep\nReinforcement Learning (FDRL), the system promotes collaborative and adaptive\nenergy management strategies, maintaining user privacy. A notable innovation is\nthe use of smart contracts, ensuring high efficiency and integrity in energy\ntransactions. Extensive evaluations demonstrate the system's scalability and\nthe effectiveness of the FDRL method in optimizing energy distribution. This\nresearch significantly contributes to developing sophisticated decentralized\nsmart grid infrastructures. Our approach broadens potential blockchain and AI\napplications in sustainable energy systems and addresses incentive alignment\nand transparency challenges in traditional energy trading mechanisms. The\nimplementation of this paper is publicly accessible at\n\\url{https://github.com/RasoulNik/DEM}.\n","authors":["Rasoul Nikbakht","Farhana Javed","Farhad Rezazadeh","Nikolaos Bartzoudis","Josep Mangues-Bafalluy"],"pdf_url":"https://arxiv.org/pdf/2311.10406v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2308.00273v2","updated":"2023-11-17T08:37:18Z","published":"2023-08-01T04:11:19Z","title":"Neural approximation of Wasserstein distance via a universal\n  architecture for symmetric and factorwise group invariant functions","summary":"  Learning distance functions between complex objects, such as the Wasserstein\ndistance to compare point sets, is a common goal in machine learning\napplications. However, functions on such complex objects (e.g., point sets and\ngraphs) are often required to be invariant to a wide variety of group actions\ne.g. permutation or rigid transformation. Therefore, continuous and symmetric\nproduct functions (such as distance functions) on such complex objects must\nalso be invariant to the product of such group actions. We call these functions\nsymmetric and factor-wise group invariant (or SFGI functions in short). In this\npaper, we first present a general neural network architecture for approximating\nSFGI functions. The main contribution of this paper combines this general\nneural network with a sketching idea to develop a specific and efficient neural\nnetwork which can approximate the $p$-th Wasserstein distance between point\nsets. Very importantly, the required model complexity is independent of the\nsizes of input point sets. On the theoretical front, to the best of our\nknowledge, this is the first result showing that there exists a neural network\nwith the capacity to approximate Wasserstein distance with bounded model\ncomplexity. Our work provides an interesting integration of sketching ideas for\ngeometric problems with universal approximation of symmetric functions. On the\nempirical front, we present a range of results showing that our newly proposed\nneural network architecture performs comparatively or better than other models\n(including a SOTA Siamese Autoencoder based approach). In particular, our\nneural network generalizes significantly better and trains much faster than the\nSOTA Siamese AE. Finally, this line of investigation could be useful in\nexploring effective neural network design for solving a broad range of\ngeometric optimization problems (e.g., $k$-means in a metric space).\n","authors":["Samantha Chen","Yusu Wang"],"pdf_url":"https://arxiv.org/pdf/2308.00273v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.10387v1","updated":"2023-11-17T08:30:41Z","published":"2023-11-17T08:30:41Z","title":"A Bridge between Dynamical Systems and Machine Learning: Engineered\n  Ordinary Differential Equations as Classification Algorithm (EODECA)","summary":"  In a world increasingly reliant on machine learning, the interpretability of\nthese models remains a substantial challenge, with many equating their\nfunctionality to an enigmatic black box. This study seeks to bridge machine\nlearning and dynamical systems. Recognizing the deep parallels between dense\nneural networks and dynamical systems, particularly in the light of\nnon-linearities and successive transformations, this manuscript introduces the\nEngineered Ordinary Differential Equations as Classification Algorithms\n(EODECAs). Uniquely designed as neural networks underpinned by continuous\nordinary differential equations, EODECAs aim to capitalize on the\nwell-established toolkit of dynamical systems. Unlike traditional deep learning\nmodels, which often suffer from opacity, EODECAs promise both high\nclassification performance and intrinsic interpretability. They are naturally\ninvertible, granting them an edge in understanding and transparency over their\ncounterparts. By bridging these domains, we hope to usher in a new era of\nmachine learning models where genuine comprehension of data processes\ncomplements predictive prowess.\n","authors":["Raffaele Marino","Lorenzo Giambagli","Lorenzo Chicchi","Lorenzo Buffoni","Duccio Fanelli"],"pdf_url":"https://arxiv.org/pdf/2311.10387v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2311.02058v2","updated":"2023-11-17T08:26:16Z","published":"2023-11-03T17:38:35Z","title":"LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery","summary":"  We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.\n","authors":["Weikang Wan","Yifeng Zhu","Rutav Shah","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.02058v2.pdf","comment":"Submitted to ICRA 2024"},{"id":"http://arxiv.org/abs/2311.10385v1","updated":"2023-11-17T08:23:17Z","published":"2023-11-17T08:23:17Z","title":"Delete My Account: Impact of Data Deletion on Machine Learning\n  Classifiers","summary":"  Users are more aware than ever of the importance of their own data, thanks to\nreports about security breaches and leaks of private, often sensitive data in\nrecent years. Additionally, the GDPR has been in effect in the European Union\nfor over three years and many people have encountered its effects in one way or\nanother. Consequently, more and more users are actively protecting their\npersonal data. One way to do this is to make of the right to erasure guaranteed\nin the GDPR, which has potential implications for a number of different fields,\nsuch as big data and machine learning.\n  Our paper presents an in-depth analysis about the impact of the use of the\nright to erasure on the performance of machine learning models on\nclassification tasks. We conduct various experiments utilising different\ndatasets as well as different machine learning algorithms to analyse a variety\nof deletion behaviour scenarios. Due to the lack of credible data on actual\nuser behaviour, we make reasonable assumptions for various deletion modes and\nbiases and provide insight into the effects of different plausible scenarios\nfor right to erasure usage on data quality of machine learning. Our results\nshow that the impact depends strongly on the amount of data deleted, the\nparticular characteristics of the dataset and the bias chosen for deletion and\nassumptions on user behaviour.\n","authors":["Tobias Dam","Maximilian Henzl","Lukas Daniel Klausner"],"pdf_url":"https://arxiv.org/pdf/2311.10385v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2311.09690v2","updated":"2023-11-17T08:23:11Z","published":"2023-11-16T09:05:52Z","title":"CDMPP: A Device-Model Agnostic Framework for Latency Prediction of\n  Tensor Programs","summary":"  Deep Neural Networks (DNNs) have shown excellent performance in a wide range\nof machine learning applications. Knowing the latency of running a DNN model or\ntensor program on a specific device is useful in various tasks, such as DNN\ngraph- or tensor-level optimization and device selection. Considering the large\nspace of DNN models and devices that impede direct profiling of all\ncombinations, recent efforts focus on building a predictor to model the\nperformance of DNN models on different devices. However, none of the existing\nattempts have achieved a cost model that can accurately predict the performance\nof various tensor programs while supporting both training and inference\naccelerators. We propose CDMPP, an efficient tensor program latency prediction\nframework for both cross-model and cross-device prediction. We design an\ninformative but efficient representation of tensor programs, called compact\nASTs, and a pre-order-based positional encoding method, to capture the internal\nstructure of tensor programs. We develop a domain-adaption-inspired method to\nlearn domain-invariant representations and devise a KMeans-based sampling\nalgorithm, for the predictor to learn from different domains (i.e., different\nDNN operators and devices). Our extensive experiments on a diverse range of DNN\nmodels and devices demonstrate that CDMPP significantly outperforms\nstate-of-the-art baselines with 14.03% and 10.85% prediction error for\ncross-model and cross-device prediction, respectively, and one order of\nmagnitude higher training efficiency. The implementation and the expanded\ndataset are available at https://github.com/joapolarbear/cdmpp.\n","authors":["Hanpeng Hu","Junwei Su","Juntao Zhao","Yanghua Peng","Yibo Zhu","Haibin Lin","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2311.09690v2.pdf","comment":"Accepted by EuroSys 2024"},{"id":"http://arxiv.org/abs/2306.04226v2","updated":"2023-11-17T08:23:05Z","published":"2023-06-07T08:05:46Z","title":"Normalization Layers Are All That Sharpness-Aware Minimization Needs","summary":"  Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima\nand has been shown to enhance generalization performance in various settings.\nIn this work we show that perturbing only the affine normalization parameters\n(typically comprising 0.1% of the total parameters) in the adversarial step of\nSAM can outperform perturbing all of the parameters.This finding generalizes to\ndifferent SAM variants and both ResNet (Batch Normalization) and Vision\nTransformer (Layer Normalization) architectures. We consider alternative sparse\nperturbation approaches and find that these do not achieve similar performance\nenhancement at such extreme sparsity levels, showing that this behaviour is\nunique to the normalization layers. Although our findings reaffirm the\neffectiveness of SAM in improving generalization performance, they cast doubt\non whether this is solely caused by reduced sharpness.\n","authors":["Maximilian Mueller","Tiffany Vlaar","David Rolnick","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2306.04226v2.pdf","comment":"camera ready version"},{"id":"http://arxiv.org/abs/2310.14710v2","updated":"2023-11-17T08:01:34Z","published":"2023-10-23T08:49:39Z","title":"Random Forest Kernel for High-Dimension Low Sample Size Classification","summary":"  High dimension, low sample size (HDLSS) problems are numerous among\nreal-world applications of machine learning. From medical images to text\nprocessing, traditional machine learning algorithms are usually unsuccessful in\nlearning the best possible concept from such data. In a previous work, we\nproposed a dissimilarity-based approach for multi-view classification, the\nRandom Forest Dissimilarity (RFD), that perfoms state-of-the-art results for\nsuch problems. In this work, we transpose the core principle of this approach\nto solving HDLSS classification problems, by using the RF similarity measure as\na learned precomputed SVM kernel (RFSVM). We show that such a learned\nsimilarity measure is particularly suited and accurate for this classification\ncontext. Experiments conducted on 40 public HDLSS classification datasets,\nsupported by rigorous statistical analyses, show that the RFSVM method\noutperforms existing methods for the majority of HDLSS problems and remains at\nthe same time very competitive for low or non-HDLSS problems.\n","authors":["Lucca Portes Cavalheiro","Simon Bernard","Jean Paul Barddal","Laurent Heutte"],"pdf_url":"https://arxiv.org/pdf/2310.14710v2.pdf","comment":"23 pages. To be published in statistics and computing (accepted\n  September 26, 2023)"},{"id":"http://arxiv.org/abs/2311.02818v2","updated":"2023-11-17T07:58:45Z","published":"2023-11-06T01:41:46Z","title":"Signal Processing Meets SGD: From Momentum to Filter","summary":"  In the field of deep learning, Stochastic Gradient Descent (SGD) and its\nmomentum-based variants are the predominant choices for optimization\nalgorithms. Despite all that, these momentum strategies, which accumulate\nhistorical gradients by using a fixed $\\beta$ hyperparameter to smooth the\noptimization processing, often neglect the potential impact of the variance of\nhistorical gradients on the current gradient estimation. In the gradient\nvariance during training, fluctuation indicates the objective function does not\nmeet the Lipschitz continuity condition at all time, which raises the\ntroublesome optimization problem. This paper aims to explore the potential\nbenefits of reducing the variance of historical gradients to make optimizer\nconverge to flat solutions. Moreover, we proposed a new optimization method\nbased on reducing the variance. We employed the Wiener filter theory to enhance\nthe first moment estimation of SGD, notably introducing an adaptive weight to\noptimizer. Specifically, the adaptive weight dynamically changes along with\ntemporal fluctuation of gradient variance during deep learning model training.\nExperimental results demonstrated our proposed adaptive weight optimizer, SGDF\n(Stochastic Gradient Descent With Filter), can achieve satisfactory performance\ncompared with state-of-the-art optimizers.\n","authors":["Zhipeng Yao","Guisong Chang","Jiaqi Zhang","Qi Zhang","Yu Zhang","Dazhou Li"],"pdf_url":"https://arxiv.org/pdf/2311.02818v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2010.07468 by other authors"},{"id":"http://arxiv.org/abs/2309.08835v3","updated":"2023-11-17T07:54:42Z","published":"2023-09-16T01:45:13Z","title":"Intelligent machines work in unstructured environments by differential\n  neuromorphic computing","summary":"  Efficient operation of intelligent machines in the real world requires\nmethods that allow them to understand and predict the uncertainties presented\nby the unstructured environments with good accuracy, scalability and\ngeneralization, similar to humans. Current methods rely on pretrained networks\ninstead of continuously learning from the dynamic signal properties of working\nenvironments and suffer inherent limitations, such as data-hungry procedures,\nand limited generalization capabilities. Herein, we present a memristor-based\ndifferential neuromorphic computing, perceptual signal processing and learning\nmethod for intelligent machines. The main features of environmental information\nsuch as amplification (>720%) and adaptation (<50%) of mechanical stimuli\nencoded in memristors, are extracted to obtain human-like processing in\nunstructured environments. The developed method takes advantage of the\nintrinsic multi-state property of memristors and exhibits good scalability and\ngeneralization, as confirmed by validation in two different application\nscenarios: object grasping and autonomous driving. In the former, a robot hand\nexperimentally realizes safe and stable grasping through fast learning (in ~1\nms) the unknown object features (e.g., sharp corner and smooth surface) with a\nsingle memristor. In the latter, the decision-making information of 10\nunstructured environments in autonomous driving (e.g., overtaking cars,\npedestrians) is accurately (94%) extracted with a 40*25 memristor array. By\nmimicking the intrinsic nature of human low-level perception mechanisms, the\nelectronic memristive neuromorphic circuit-based method, presented here shows\nthe potential for adapting to diverse sensing technologies and helping\nintelligent machines generate smart high-level decisions in the real world.\n","authors":["Shengbo Wang","Shuo Gao","Chenyu Tang","Edoardo Occhipinti","Cong Li","Shurui Wang","Jiaqi Wang","Hubin Zhao","Guohua Hu","Arokia Nathan","Ravinder Dahiya","Luigi Occhipinti"],"pdf_url":"https://arxiv.org/pdf/2309.08835v3.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.13860v2","updated":"2023-11-17T07:54:27Z","published":"2023-04-26T23:05:57Z","title":"Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators\n  and Promising Candidates","summary":"  Deep-learning inverse techniques have attracted significant attention in\nrecent years. Among them, the neural adjoint (NA) method, which employs a\nneural network surrogate simulator, has demonstrated impressive performance in\nthe design tasks of artificial electromagnetic materials (AEM). However, the\nimpact of the surrogate simulators' accuracy on the solutions in the NA method\nremains uncertain. Furthermore, achieving sufficient optimization becomes\nchallenging in this method when the surrogate simulator is large, and\ncomputational resources are limited. Additionally, the behavior under\nconstraints has not been studied, despite its importance from the engineering\nperspective. In this study, we investigated the impact of surrogate simulators'\naccuracy on the solutions and discovered that the more accurate the surrogate\nsimulator is, the better the solutions become. We then developed an extension\nof the NA method, named Neural Lagrangian (NeuLag) method, capable of\nefficiently optimizing a sufficient number of solution candidates. We then\ndemonstrated that the NeuLag method can find optimal solutions even when\nhandling sufficient candidates is difficult due to the use of a large and\naccurate surrogate simulator. The resimulation errors of the NeuLag method were\napproximately 1/50 compared to previous methods for three AEM tasks. Finally,\nwe performed optimization under constraint using NA and NeuLag, and confirmed\ntheir potential in optimization with soft or hard constraints. We believe our\nmethod holds potential in areas that require large and accurate surrogate\nsimulators.\n","authors":["Akihiro Fujii","Hideki Tsunashima","Yoshihiro Fukuhara","Koji Shimizu","Satoshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2304.13860v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.10370v1","updated":"2023-11-17T07:49:20Z","published":"2023-11-17T07:49:20Z","title":"Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly\n  Detection","summary":"  Graph anomaly detection plays a crucial role in identifying exceptional\ninstances in graph data that deviate significantly from the majority. It has\ngained substantial attention in various domains of information security,\nincluding network intrusion, financial fraud, and malicious comments, et al.\nExisting methods are primarily developed in an unsupervised manner due to the\nchallenge in obtaining labeled data. For lack of guidance from prior knowledge\nin unsupervised manner, the identified anomalies may prove to be data noise or\nindividual data instances. In real-world scenarios, a limited batch of labeled\nanomalies can be captured, making it crucial to investigate the few-shot\nproblem in graph anomaly detection. Taking advantage of this potential, we\npropose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot\nMessage-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a\nself-supervised contrastive learning strategy within and across views to\ncapture intrinsic and transferable structural representations. Furthermore, we\npropose the Deep-GNN message-enhanced reconstruction module, which extensively\nexploits the few-shot label information and enables long-range propagation to\ndisseminate supervision signals to deeper unlabeled nodes. This module in turn\nassists in the training of self-supervised contrastive learning. Comprehensive\nexperimental results on six real-world datasets demonstrate that FMGAD can\nachieve better performance than other state-of-the-art methods, regardless of\nartificially injected anomalies or domain-organic anomalies.\n","authors":["Fan Xu","Nan Wang","Xuezhi Wen","Meiqi Gao","Chaoqun Guo","Xibin Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.10370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10359v1","updated":"2023-11-17T07:25:18Z","published":"2023-11-17T07:25:18Z","title":"FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel\n  Identification","summary":"  Highly parallelized workloads like machine learning training, inferences and\ngeneral HPC tasks are greatly accelerated using GPU devices. In a cloud\ncomputing cluster, serving a GPU's computation power through multi-tasks\nsharing is highly demanded since there are always more task requests than the\nnumber of GPU available. Existing GPU sharing solutions focus on reducing\ntask-level waiting time or task-level switching costs when multiple jobs\ncompeting for a single GPU. Non-stopped computation requests come with\ndifferent priorities, having non-symmetric impact on QoS for sharing a GPU\ndevice. Existing work missed the kernel-level optimization opportunity brought\nby this setting. To address this problem, we present a novel kernel-level\nscheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT\nincorporates task-level priority information, fine-grained kernel\nidentification, and kernel measurement, allowing low priorities task's\nexecution during high priority task's inter-kernel idle time. Thereby, filling\nthe GPU's device runtime fully, and reduce overall GPU sharing impact to cloud\nservices. Across a set of ML models, the FIKIT based inference system\naccelerated high priority tasks by 1.33 to 14.87 times compared to the JCT in\nGPU sharing mode, and more than half of the cases are accelerated by more than\n3.5 times. Alternatively, under preemptive sharing, the low-priority tasks have\na comparable to default GPU sharing mode JCT, with a 0.84 to 1 times ratio. We\nfurther limit the kernel measurement and runtime fine-grained kernel scheduling\noverhead to less than 10%.\n","authors":["Wenqing Wu"],"pdf_url":"https://arxiv.org/pdf/2311.10359v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2306.06048v2","updated":"2023-11-17T07:22:04Z","published":"2023-06-09T17:16:50Z","title":"How Does Fine-Tuning Impact Out-of-Distribution Detection for\n  Vision-Language Models?","summary":"  Recent large vision-language models such as CLIP have shown remarkable\nout-of-distribution (OOD) detection and generalization performance. However,\ntheir zero-shot in-distribution (ID) accuracy is often limited for downstream\ndatasets. Recent CLIP-based fine-tuning methods such as prompt learning have\ndemonstrated significant improvements in ID classification and OOD\ngeneralization where OOD labels are available. Nonetheless, it remains unclear\nwhether the model is reliable to semantic shifts without OOD labels. In this\npaper, we aim to bridge the gap and present a comprehensive study to understand\nhow fine-tuning impact OOD detection for few-shot downstream tasks. By framing\nOOD detection as multi-modal concept matching, we establish a connection\nbetween fine-tuning methods and various OOD scores. Our results suggest that a\nproper choice of OOD scores is essential for CLIP-based fine-tuning. In\nparticular, the maximum concept matching (MCM) score provides a promising\nsolution consistently. We also show that prompt learning demonstrates the\nstate-of-the-art OOD detection performance over the zero-shot counterpart.\n","authors":["Yifei Ming","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2306.06048v2.pdf","comment":"Accepted to IJCV 2023"},{"id":"http://arxiv.org/abs/2311.10349v1","updated":"2023-11-17T06:36:43Z","published":"2023-11-17T06:36:43Z","title":"Pseudo Label-Guided Data Fusion and Output Consistency for\n  Semi-Supervised Medical Image Segmentation","summary":"  Supervised learning algorithms based on Convolutional Neural Networks have\nbecome the benchmark for medical image segmentation tasks, but their\neffectiveness heavily relies on a large amount of labeled data. However,\nannotating medical image datasets is a laborious and time-consuming process.\nInspired by semi-supervised algorithms that use both labeled and unlabeled data\nfor training, we propose the PLGDF framework, which builds upon the mean\nteacher network for segmenting medical images with less annotation. We propose\na novel pseudo-label utilization scheme, which combines labeled and unlabeled\ndata to augment the dataset effectively. Additionally, we enforce the\nconsistency between different scales in the decoder module of the segmentation\nnetwork and propose a loss function suitable for evaluating the consistency.\nMoreover, we incorporate a sharpening operation on the predicted results,\nfurther enhancing the accuracy of the segmentation.\n  Extensive experiments on three publicly available datasets demonstrate that\nthe PLGDF framework can largely improve performance by incorporating the\nunlabeled data. Meanwhile, our framework yields superior performance compared\nto six state-of-the-art semi-supervised learning methods. The codes of this\nstudy are available at https://github.com/ortonwang/PLGDF.\n","authors":["Tao Wang","Yuanbin Chen","Xinlin Zhang","Yuanbo Zhou","Junlin Lan","Bizhe Bai","Tao Tan","Min Du","Qinquan Gao","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2311.10349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10341v1","updated":"2023-11-17T06:03:56Z","published":"2023-11-17T06:03:56Z","title":"Federated Knowledge Graph Completion via Latent Embedding Sharing and\n  Tensor Factorization","summary":"  Knowledge graphs (KGs), which consist of triples, are inherently incomplete\nand always require completion procedure to predict missing triples. In\nreal-world scenarios, KGs are distributed across clients, complicating\ncompletion tasks due to privacy restrictions. Many frameworks have been\nproposed to address the issue of federated knowledge graph completion. However,\nthe existing frameworks, including FedE, FedR, and FEKG, have certain\nlimitations. = FedE poses a risk of information leakage, FedR's optimization\nefficacy diminishes when there is minimal overlap among relations, and FKGE\nsuffers from computational costs and mode collapse issues. To address these\nissues, we propose a novel method, i.e., Federated Latent Embedding Sharing\nTensor factorization (FLEST), which is a novel approach using federated tensor\nfactorization for KG completion. FLEST decompose the embedding matrix and\nenables sharing of latent dictionary embeddings to lower privacy risks.\nEmpirical results demonstrate FLEST's effectiveness and efficiency, offering a\nbalanced solution between performance and privacy. FLEST expands the\napplication of federated tensor factorization in KG completion tasks.\n","authors":["Maolin Wang","Dun Zeng","Zenglin Xu","Ruocheng Guo","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.10341v1.pdf","comment":"Accepted by ICDM 2023"},{"id":"http://arxiv.org/abs/2309.16397v2","updated":"2023-11-17T05:41:45Z","published":"2023-09-28T12:44:51Z","title":"Uncertainty-Aware Decision Transformer for Stochastic Driving\n  Environments","summary":"  Offline Reinforcement Learning (RL) has emerged as a promising framework for\nlearning policies without active interactions, making it especially appealing\nfor autonomous driving tasks. Recent successes of Transformers inspire casting\noffline RL as sequence modeling, which performs well in long-horizon tasks.\nHowever, they are overly optimistic in stochastic environments with incorrect\nassumptions that the same goal can be consistently achieved by identical\nactions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer\n(UNREST) for planning in stochastic driving environments without introducing\nadditional transition or complex generative models. Specifically, UNREST\nestimates state uncertainties by the conditional mutual information between\ntransitions and returns, and segments sequences accordingly. Discovering the\n`uncertainty accumulation' and `temporal locality' properties of driving\nenvironments, UNREST replaces the global returns in decision transformers with\nless uncertain truncated returns, to learn from true outcomes of agent actions\nrather than environment transitions. We also dynamically evaluate environmental\nuncertainty during inference for cautious planning. Extensive experimental\nresults demonstrate UNREST's superior performance in various driving scenarios\nand the power of our uncertainty estimation strategy.\n","authors":["Zenan Li","Fan Nie","Qiao Sun","Fang Da","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2309.16397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05433v2","updated":"2023-11-17T05:27:43Z","published":"2023-05-09T13:22:13Z","title":"Tomography of Quantum States from Structured Measurements via\n  quantum-aware transformer","summary":"  Quantum state tomography (QST) is the process of reconstructing the state of\na quantum system (mathematically described as a density matrix) through a\nseries of different measurements, which can be solved by learning a\nparameterized function to translate experimentally measured statistics into\nphysical density matrices. However, the specific structure of quantum\nmeasurements for characterizing a quantum state has been neglected in previous\nwork. In this paper, we explore the similarity between highly structured\nsentences in natural language and intrinsically structured measurements in QST.\nTo fully leverage the intrinsic quantum characteristics involved in QST, we\ndesign a quantum-aware transformer (QAT) model to capture the complex\nrelationship between measured frequencies and density matrices. In particular,\nwe query quantum operators in the architecture to facilitate informative\nrepresentations of quantum data and integrate the Bures distance into the loss\nfunction to evaluate quantum state fidelity, thereby enabling the\nreconstruction of quantum states from measured data with high fidelity.\nExtensive simulations and experiments (on IBM quantum computers) demonstrate\nthe superiority of the QAT in reconstructing quantum states with favorable\nrobustness against experimental noise.\n","authors":["Hailan Ma","Zhenhong Sun","Daoyi Dong","Chunlin Chen","Herschel Rabitz"],"pdf_url":"https://arxiv.org/pdf/2305.05433v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10328v1","updated":"2023-11-17T04:59:08Z","published":"2023-11-17T04:59:08Z","title":"TransONet: Automatic Segmentation of Vasculature in Computed Tomographic\n  Angiograms Using Deep Learning","summary":"  Pathological alterations in the human vascular system underlie many chronic\ndiseases, such as atherosclerosis and aneurysms. However, manually analyzing\ndiagnostic images of the vascular system, such as computed tomographic\nangiograms (CTAs) is a time-consuming and tedious process. To address this\nissue, we propose a deep learning model to segment the vascular system in CTA\nimages of patients undergoing surgery for peripheral arterial disease (PAD).\nOur study focused on accurately segmenting the vascular system (1) from the\ndescending thoracic aorta to the iliac bifurcation and (2) from the descending\nthoracic aorta to the knees in CTA images using deep learning techniques. Our\napproach achieved average Dice accuracies of 93.5% and 80.64% in test dataset\nfor (1) and (2), respectively, highlighting its high accuracy and potential\nclinical utility. These findings demonstrate the use of deep learning\ntechniques as a valuable tool for medical professionals to analyze the health\nof the vascular system efficiently and accurately. Please visit the GitHub page\nfor this paper at https://github.com/pip-alireza/TransOnet.\n","authors":["Alireza Bagheri Rajeoni","Breanna Pederson","Ali Firooz","Hamed Abdollahi","Andrew K. Smith","Daniel G. Clair","Susan M. Lessner","Homayoun Valafar"],"pdf_url":"https://arxiv.org/pdf/2311.10328v1.pdf","comment":"Accepted for the 2023 International Conference on Computational\n  Science and Computational Intelligence (CSCI), Las Vegas, USA"},{"id":"http://arxiv.org/abs/2311.10322v1","updated":"2023-11-17T04:24:52Z","published":"2023-11-17T04:24:52Z","title":"Clustering Techniques for Stable Linear Dynamical Systems with\n  applications to Hard Disk Drives","summary":"  In Robust Control and Data Driven Robust Control design methodologies,\nmultiple plant transfer functions or a family of transfer functions are\nconsidered and a common controller is designed such that all the plants that\nfall into this family are stabilized. Though the plants are stabilized, the\ncontroller might be sub-optimal for each of the plants when the variations in\nthe plants are large. This paper presents a way of clustering stable linear\ndynamical systems for the design of robust controllers within each of the\nclusters such that the controllers are optimal for each of the clusters. First\na k-medoids algorithm for hard clustering will be presented for stable Linear\nTime Invariant (LTI) systems and then a Gaussian Mixture Models (GMM)\nclustering for a special class of LTI systems, common for Hard Disk Drive\nplants, will be presented.\n","authors":["Nikhil Potu Surya Prakash","Joohwan Seo","Jongeun Choi","Roberto Horowitz"],"pdf_url":"https://arxiv.org/pdf/2311.10322v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.11719v2","updated":"2023-11-17T04:17:34Z","published":"2023-06-20T17:53:00Z","title":"Diffusion with Forward Models: Solving Stochastic Inverse Problems\n  Without Direct Supervision","summary":"  Denoising diffusion models are a powerful type of generative models used to\ncapture complex distributions of real-world signals. However, their\napplicability is limited to scenarios where training samples are readily\navailable, which is not always the case in real-world applications. For\nexample, in inverse graphics, the goal is to generate samples from a\ndistribution of 3D scenes that align with a given image, but ground-truth 3D\nscenes are unavailable and only 2D images are accessible. To address this\nlimitation, we propose a novel class of denoising diffusion probabilistic\nmodels that learn to sample from distributions of signals that are never\ndirectly observed. Instead, these signals are measured indirectly through a\nknown differentiable forward model, which produces partial observations of the\nunknown signal. Our approach involves integrating the forward model directly\ninto the denoising process. This integration effectively connects the\ngenerative modeling of observations with the generative modeling of the\nunderlying signals, allowing for end-to-end training of a conditional\ngenerative model over signals. During inference, our approach enables sampling\nfrom the distribution of underlying signals that are consistent with a given\npartial observation. We demonstrate the effectiveness of our method on three\nchallenging computer vision tasks. For instance, in the context of inverse\ngraphics, our model enables direct sampling from the distribution of 3D scenes\nthat align with a single 2D input image.\n","authors":["Ayush Tewari","Tianwei Yin","George Cazenavette","Semon Rezchikov","Joshua B. Tenenbaum","Frédo Durand","William T. Freeman","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2306.11719v2.pdf","comment":"Project page: https://diffusion-with-forward-models.github.io/"},{"id":"http://arxiv.org/abs/2311.10321v1","updated":"2023-11-17T04:15:27Z","published":"2023-11-17T04:15:27Z","title":"Towards Machine Learning-based Quantitative Hyperspectral Image Guidance\n  for Brain Tumor Resection","summary":"  Complete resection of malignant gliomas is hampered by the difficulty in\ndistinguishing tumor cells at the infiltration zone. Fluorescence guidance with\n5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work\ncharacterized five fluorophores' emission spectra in most human brain tumors.\nIn this paper, the effectiveness of these five spectra was explored for\ndifferent tumor and tissue classification tasks in 184 patients (891\nhyperspectral measurements) harboring low- (n=30) and high-grade gliomas\n(n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2),\nmiscellaneous (n=10) and metastases (n=8). Four machine learning models were\ntrained to classify tumor type, grade, glioma margins and IDH mutation. Using\nrandom forests and multi-layer perceptrons, the classifiers achieved average\ntest accuracies of 74-82%, 79%, 81%, and 93% respectively. All five fluorophore\nabundances varied between tumor margin types and tumor grades (p < 0.01). For\ntissue type, at least four of the five fluorophore abundances were found to be\nsignificantly different (p < 0.01) between all classes. These results\ndemonstrate the fluorophores' differing abundances in different tissue classes,\nas well as the value of the five fluorophores as potential optical biomarkers,\nopening new opportunities for intraoperative classification systems in\nfluorescence-guided neurosurgery.\n","authors":["David Black","Declan Byrne","Anna Walke","Sidong Liu","Antonio Di leva","Sadahiro Kaneko","Walter Stummer","Septimiu Salcudean","Eric Suero Molina"],"pdf_url":"https://arxiv.org/pdf/2311.10321v1.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.07958v3","updated":"2023-11-17T04:07:41Z","published":"2023-10-12T00:51:06Z","title":"Towards Causal Deep Learning for Vulnerability Detection","summary":"  Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2.\n","authors":["Md Mahbubur Rahman","Ira Ceka","Chengzhi Mao","Saikat Chakraborty","Baishakhi Ray","Wei Le"],"pdf_url":"https://arxiv.org/pdf/2310.07958v3.pdf","comment":"Accepted at ICSE 2024 (not camera-ready version)"},{"id":"http://arxiv.org/abs/2311.10318v1","updated":"2023-11-17T04:04:11Z","published":"2023-11-17T04:04:11Z","title":"Nonparametric Teaching for Multiple Learners","summary":"  We study the problem of teaching multiple learners simultaneously in the\nnonparametric iterative teaching setting, where the teacher iteratively\nprovides examples to the learner for accelerating the acquisition of a target\nconcept. This problem is motivated by the gap between current single-learner\nteaching setting and the real-world scenario of human instruction where a\nteacher typically imparts knowledge to multiple students. Under the new problem\nformulation, we introduce a novel framework -- Multi-learner Nonparametric\nTeaching (MINT). In MINT, the teacher aims to instruct multiple learners, with\neach learner focusing on learning a scalar-valued target model. To achieve\nthis, we frame the problem as teaching a vector-valued target model and extend\nthe target model space from a scalar-valued reproducing kernel Hilbert space\nused in single-learner scenarios to a vector-valued space. Furthermore, we\ndemonstrate that MINT offers significant teaching speed-up over repeated\nsingle-learner teaching, particularly when the multiple learners can\ncommunicate with each other. Lastly, we conduct extensive experiments to\nvalidate the practicality and efficiency of MINT.\n","authors":["Chen Zhang","Xiaofeng Cao","Weiyang Liu","Ivor Tsang","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2311.10318v1.pdf","comment":"NeurIPS 2023 (31 pages, 20 figures)"},{"id":"http://arxiv.org/abs/2301.12829v2","updated":"2023-11-17T04:01:17Z","published":"2023-01-27T13:12:21Z","title":"Identifying the Key Attributes in an Unlabeled Event Log for Automated\n  Process Discovery","summary":"  Process mining discovers and analyzes a process model from historical event\nlogs. The prior art methods use the key attributes of case-id, activity, and\ntimestamp hidden in an event log as clues to discover a process model. However,\na user needs to specify them manually, and this can be an exhaustive task. In\nthis paper, we propose a two-stage key attribute identification method to avoid\nsuch a manual investigation, and thus this is a step toward fully automated\nprocess discovery. One of the challenging tasks is how to avoid exhaustive\ncomputation due to combinatorial explosion. For this, we narrow down candidates\nfor each key attribute by using supervised machine learning in the first stage\nand identify the best combination of the key attributes by discovering process\nmodels and evaluating them in the second stage. Our computational complexity\ncan be reduced from $\\mathcal{O}(N^3)$ to $\\mathcal{O}(k^3)$ where $N$ and $k$\nare the numbers of columns and candidates we keep in the first stage,\nrespectively, and usually $k$ is much smaller than $N$. We evaluated our method\nwith 14 open datasets and showed that our method could identify the key\nattributes even with $k = 2$ for about 20 seconds for many datasets.\n","authors":["Kentaroh Toyoda","Rachel Gan Kai Ying","Allan NengSheng Zhang","Tan Puay Siew"],"pdf_url":"https://arxiv.org/pdf/2301.12829v2.pdf","comment":"IEEE Transactions on Services Computing (Early Access version)"},{"id":"http://arxiv.org/abs/2311.10316v1","updated":"2023-11-17T03:59:50Z","published":"2023-11-17T03:59:50Z","title":"Graph Sparsifications using Neural Network Assisted Monte Carlo Tree\n  Search","summary":"  Graph neural networks have been successful for machine learning, as well as\nfor combinatorial and graph problems such as the Subgraph Isomorphism Problem\nand the Traveling Salesman Problem. We describe an approach for computing graph\nsparsifiers by combining a graph neural network and Monte Carlo Tree Search. We\nfirst train a graph neural network that takes as input a partial solution and\nproposes a new node to be added as output. This neural network is then used in\na Monte Carlo search to compute a sparsifier. The proposed method consistently\noutperforms several standard approximation algorithms on different types of\ngraphs and often finds the optimal solution.\n","authors":["Alvin Chiu","Mithun Ghosh","Reyan Ahmed","Kwang-Sung Jun","Stephen Kobourov","Michael T. Goodrich"],"pdf_url":"https://arxiv.org/pdf/2311.10316v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2305.00535"},{"id":"http://arxiv.org/abs/2311.10315v1","updated":"2023-11-17T03:58:59Z","published":"2023-11-17T03:58:59Z","title":"Interpretable Modeling of Single-cell perturbation Responses to Novel\n  Drugs Using Cycle Consistence Learning","summary":"  Phenotype-based screening has attracted much attention for identifying\ncell-active compounds. Transcriptional and proteomic profiles of cell\npopulation or single cells are informative phenotypic measures of cellular\nresponses to perturbations. In this paper, we proposed a deep learning\nframework based on encoder-decoder architecture that maps the initial cellular\nstates to a latent space, in which we assume the effects of drug perturbation\non cellular states follow linear additivity. Next, we introduced the cycle\nconsistency constraints to enforce that initial cellular state subjected to\ndrug perturbations would produce the perturbed cellular responses, and,\nconversely, removal of drug perturbation from the perturbed cellular states\nwould restore the initial cellular states. The cycle consistency constraints\nand linear modeling in latent space enable to learn interpretable and\ntransferable drug perturbation representations, so that our model can predict\ncellular response to unseen drugs. We validated our model on three different\ntypes of datasets, including bulk transcriptional responses, bulk proteomic\nresponses, and single-cell transcriptional responses to drug perturbations. The\nexperimental results show that our model achieves better performance than\nexisting state-of-the-art methods.\n","authors":["Wei Huang","Aichun Zhu","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2311.10315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10309v1","updated":"2023-11-17T03:41:22Z","published":"2023-11-17T03:41:22Z","title":"Imagination-augmented Hierarchical Reinforcement Learning for Safe and\n  Interactive Autonomous Driving in Urban Environments","summary":"  Hierarchical reinforcement learning (HRL) has led to remarkable achievements\nin diverse fields. However, existing HRL algorithms still cannot be applied to\nreal-world navigation tasks. These tasks require an agent to perform\nsafety-aware behaviors and interact with surrounding objects in dynamic\nenvironments. In addition, an agent in these tasks should perform consistent\nand structured exploration as they are long-horizon and have complex structures\nwith diverse objects and task-specific rules. Designing HRL agents that can\nhandle these challenges in real-world navigation tasks is an open problem. In\nthis paper, we propose imagination-augmented HRL (IAHRL), a new and general\nnavigation algorithm that allows an agent to learn safe and interactive\nbehaviors in real-world navigation tasks. Our key idea is to train a\nhierarchical agent in which a high-level policy infers interactions by\ninterpreting behaviors imagined with low-level policies. Specifically, the\nhigh-level policy is designed with a permutation-invariant attention mechanism\nto determine which low-level policy generates the most interactive behavior,\nand the low-level policies are implemented with an optimization-based behavior\nplanner to generate safe and structured behaviors following task-specific\nrules. To evaluate our algorithm, we introduce five complex urban driving\ntasks, which are among the most challenging real-world navigation tasks. The\nexperimental results indicate that our hierarchical agent performs safety-aware\nbehaviors and properly interacts with surrounding vehicles, achieving higher\nsuccess rates and lower average episode steps than baselines in urban driving\ntasks.\n","authors":["Sang-Hyun Lee","Yoonjae Jung","Seung-Woo Seo"],"pdf_url":"https://arxiv.org/pdf/2311.10309v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2210.12723v2","updated":"2023-11-17T03:37:18Z","published":"2022-10-23T13:10:12Z","title":"A Faithful Deep Sensitivity Estimation for Accelerated Magnetic\n  Resonance Imaging","summary":"  Magnetic resonance imaging (MRI) is an essential diagnostic tool that suffers\nfrom prolonged scan time. To alleviate this limitation, advanced fast MRI\ntechnology attracts extensive research interests. Recent deep learning has\nshown its great potential in improving image quality and reconstruction speed.\nFaithful coil sensitivity estimation is vital for MRI reconstruction. However,\nmost deep learning methods still rely on pre-estimated sensitivity maps and\nignore their inaccuracy, resulting in the significant quality degradation of\nreconstructed images. In this work, we propose a Joint Deep Sensitivity\nestimation and Image reconstruction network, called JDSI. During the image\nartifacts removal, it gradually provides more faithful sensitivity maps with\nhigh-frequency information, leading to improved image reconstructions. To\nunderstand the behavior of the network, the mutual promotion of sensitivity\nestimation and image reconstruction is revealed through the visualization of\nnetwork intermediate results. Results on in vivo datasets and radiologist\nreader study demonstrate that, for both calibration-based and calibrationless\nreconstruction, the proposed JDSI achieves the state-of-the-art performance\nvisually and quantitatively, especially when the acceleration factor is high.\nAdditionally, JDSI owns nice robustness to patients and autocalibration\nsignals.\n","authors":["Zi Wang","Haoming Fang","Chen Qian","Boxuan Shi","Lijun Bao","Liuhong Zhu","Jianjun Zhou","Wenping Wei","Jianzhong Lin","Di Guo","Xiaobo Qu"],"pdf_url":"https://arxiv.org/pdf/2210.12723v2.pdf","comment":"11 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2311.10306v1","updated":"2023-11-17T03:33:09Z","published":"2023-11-17T03:33:09Z","title":"MPSeg : Multi-Phase strategy for coronary artery Segmentation","summary":"  Accurate segmentation of coronary arteries is a pivotal process in assessing\ncardiovascular diseases. However, the intricate structure of the cardiovascular\nsystem presents significant challenges for automatic segmentation, especially\nwhen utilizing methodologies like the SYNTAX Score, which relies extensively on\ndetailed structural information for precise risk stratification. To address\nthese difficulties and cater to this need, we present MPSeg, an innovative\nmulti-phase strategy designed for coronary artery segmentation. Our approach\nspecifically accommodates these structural complexities and adheres to the\nprinciples of the SYNTAX Score. Initially, our method segregates vessels into\ntwo categories based on their unique morphological characteristics: Left\nCoronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble\nmodels are then deployed for each category to execute the challenging\nsegmentation task. Due to LCA's higher complexity over RCA, a refinement model\nis utilized to scrutinize and correct initial class predictions on segmented\nareas. Notably, our approach demonstrated exceptional effectiveness when\nevaluated in the Automatic Region-based Coronary Artery Disease diagnostics\nusing x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm\nchallenge at MICCAI 2023.\n","authors":["Jonghoe Ku","Yong-Hee Lee","Junsup Shin","In Kyu Lee","Hyun-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2311.10306v1.pdf","comment":"MICCAI 2023 Conference ARCADE Challenge"},{"id":"http://arxiv.org/abs/2310.15479v2","updated":"2023-11-17T03:24:50Z","published":"2023-10-24T03:15:19Z","title":"AutoDiff: combining Auto-encoder and Diffusion model for tabular data\n  synthesizing","summary":"  Diffusion model has become a main paradigm for synthetic data generation in\nmany subfields of modern machine learning, including computer vision, language\nmodel, or speech synthesis. In this paper, we leverage the power of diffusion\nmodel for generating synthetic tabular data. The heterogeneous features in\ntabular data have been main obstacles in tabular data synthesis, and we tackle\nthis problem by employing the auto-encoder architecture. When compared with the\nstate-of-the-art tabular synthesizers, the resulting synthetic tables from our\nmodel show nice statistical fidelities to the real data, and perform well in\ndownstream tasks for machine learning utilities. We conducted the experiments\nover $15$ publicly available datasets. Notably, our model adeptly captures the\ncorrelations among features, which has been a long-standing challenge in\ntabular data synthesis. Our code is available at\nhttps://github.com/UCLA-Trustworthy-AI-Lab/AutoDiffusion.\n","authors":["Namjoon Suh","Xiaofeng Lin","Din-Yin Hsieh","Merhdad Honarkhah","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.15479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10300v1","updated":"2023-11-17T03:18:55Z","published":"2023-11-17T03:18:55Z","title":"Supervised structure learning","summary":"  This paper concerns structure learning or discovery of discrete generative\nmodels. It focuses on Bayesian model selection and the assimilation of training\ndata or content, with a special emphasis on the order in which data are\ningested. A key move - in the ensuing schemes - is to place priors on the\nselection of models, based upon expected free energy. In this setting, expected\nfree energy reduces to a constrained mutual information, where the constraints\ninherit from priors over outcomes (i.e., preferred outcomes). The resulting\nscheme is first used to perform image classification on the MNIST dataset to\nillustrate the basic idea, and then tested on a more challenging problem of\ndiscovering models with dynamics, using a simple sprite-based visual\ndisentanglement paradigm and the Tower of Hanoi (cf., blocks world) problem. In\nthese examples, generative models are constructed autodidactically to recover\n(i.e., disentangle) the factorial structure of latent states - and their\ncharacteristic paths or dynamics.\n","authors":["Karl J. Friston","Lancelot Da Costa","Alexander Tschantz","Alex Kiefer","Tommaso Salvatori","Victorita Neacsu","Magnus Koudahl","Conor Heins","Noor Sajid","Dimitrije Markovic","Thomas Parr","Tim Verbelen","Christopher L Buckley"],"pdf_url":"https://arxiv.org/pdf/2311.10300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09299v2","updated":"2023-11-17T03:00:26Z","published":"2023-10-07T09:09:19Z","title":"Digital Twin Accelerated Deep Reinforcement Learning for Online\n  Admission Control of Network Slicing","summary":"  The proliferation of diverse wireless services in 5G and beyond has led to\nthe emergence of network slicing technologies. Among these, admission control\nplays a crucial role in achieving service-oriented optimization goals through\nthe selective acceptance of service requests. Although deep reinforcement\nlearning (DRL) forms the foundation in many admission control approaches thanks\nto its effectiveness and flexibility, initial instability with excessive\nconvergence delay of DRL models hinders their deployment in real-world\nnetworks. We propose a digital twin (DT) accelerated DRL solution to address\nthis issue. Specifically, we first formulate the admission decision-making\nprocess as a semi-Markov decision process, which is subsequently simplified\ninto an equivalent discrete-time Markov decision process to facilitate the\nimplementation of DRL methods. A neural network-based DT is established with a\ncustomized output layer for queuing systems, trained through supervised\nlearning, and then employed to assist the training phase of the DRL model.\nExtensive simulations show that the DT-accelerated DRL improves resource\nutilization by over 40% compared to the directly trained state-of-the-art\ndueling deep Q-learning model. This improvement is achieved while preserving\nthe model's capability to optimize the long-term rewards of the admission\nprocess.\n","authors":["Zhenyu Tao","Wei Xu","Xiaohu You"],"pdf_url":"https://arxiv.org/pdf/2310.09299v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.11248v2","updated":"2023-11-17T02:51:39Z","published":"2023-10-17T13:18:01Z","title":"CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code\n  Completion","summary":"  Code completion models have made significant progress in recent years, yet\ncurrent popular evaluation datasets, such as HumanEval and MBPP, predominantly\nfocus on code completion tasks within a single file. This over-simplified\nsetting falls short of representing the real-world software development\nscenario where repositories span multiple files with numerous cross-file\ndependencies, and accessing and understanding cross-file context is often\nrequired to complete the code correctly.\n  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual\ncode completion benchmark that necessitates an in-depth cross-file contextual\nunderstanding to complete the code accurately. CrossCodeEval is built on a\ndiverse set of real-world, open-sourced, permissively-licensed repositories in\nfour popular programming languages: Python, Java, TypeScript, and C#. To create\nexamples that strictly require cross-file context for accurate completion, we\npropose a straightforward yet efficient static-analysis-based approach to\npinpoint the use of cross-file context within the current file.\n  Extensive experiments on state-of-the-art code language models like CodeGen\nand StarCoder demonstrate that CrossCodeEval is extremely challenging when the\nrelevant cross-file context is absent, and we see clear improvements when\nadding these context into the prompt. However, despite such improvements, the\npinnacle of performance remains notably unattained even with the\nhighest-performing model, indicating that CrossCodeEval is also capable of\nassessing model's capability in leveraging extensive context to make better\ncode completion. Finally, we benchmarked various methods in retrieving\ncross-file context, and show that CrossCodeEval can also be used to measure the\ncapability of code retrievers.\n","authors":["Yangruibo Ding","Zijian Wang","Wasi Uddin Ahmad","Hantian Ding","Ming Tan","Nihal Jain","Murali Krishna Ramanathan","Ramesh Nallapati","Parminder Bhatia","Dan Roth","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2310.11248v2.pdf","comment":"To appear at NeurIPS 2023 (Datasets and Benchmarks Track)"},{"id":"http://arxiv.org/abs/2311.10293v1","updated":"2023-11-17T02:48:20Z","published":"2023-11-17T02:48:20Z","title":"Hierarchical Pruning of Deep Ensembles with Focal Diversity","summary":"  Deep neural network ensembles combine the wisdom of multiple deep neural\nnetworks to improve the generalizability and robustness over individual\nnetworks. It has gained increasing popularity to study deep ensemble techniques\nin the deep learning community. Some mission-critical applications utilize a\nlarge number of deep neural networks to form deep ensembles to achieve desired\naccuracy and resilience, which introduces high time and space costs for\nensemble execution. However, it still remains a critical challenge whether a\nsmall subset of the entire deep ensemble can achieve the same or better\ngeneralizability and how to effectively identify these small deep ensembles for\nimproving the space and time efficiency of ensemble execution. This paper\npresents a novel deep ensemble pruning approach, which can efficiently identify\nsmaller deep ensembles and provide higher ensemble accuracy than the entire\ndeep ensemble of a large number of member networks. Our hierarchical ensemble\npruning approach (HQ) leverages three novel ensemble pruning techniques. First,\nwe show that the focal diversity metrics can accurately capture the\ncomplementary capacity of the member networks of an ensemble, which can guide\nensemble pruning. Second, we design a focal diversity based hierarchical\npruning approach, which will iteratively find high quality deep ensembles with\nlow cost and high accuracy. Third, we develop a focal diversity consensus\nmethod to integrate multiple focal diversity metrics to refine ensemble pruning\nresults, where smaller deep ensembles can be effectively identified to offer\nhigh accuracy, high robustness and high efficiency. Evaluated using popular\nbenchmark datasets, we demonstrate that the proposed hierarchical ensemble\npruning approach can effectively identify high quality deep ensembles with\nbetter generalizability while being more time and space efficient in ensemble\ndecision making.\n","authors":["Yanzhao Wu","Ka-Ho Chow","Wenqi Wei","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2311.10293v1.pdf","comment":"To appear on ACM Transactions on Intelligent Systems and Technology"},{"id":"http://arxiv.org/abs/2311.10291v1","updated":"2023-11-17T02:37:10Z","published":"2023-11-17T02:37:10Z","title":"Leveraging Function Space Aggregation for Federated Learning at Scale","summary":"  The federated learning paradigm has motivated the development of methods for\naggregating multiple client updates into a global server model, without sharing\nclient data. Many federated learning algorithms, including the canonical\nFederated Averaging (FedAvg), take a direct (possibly weighted) average of the\nclient parameter updates, motivated by results in distributed optimization. In\nthis work, we adopt a function space perspective and propose a new algorithm,\nFedFish, that aggregates local approximations to the functions learned by\nclients, using an estimate based on their Fisher information. We evaluate\nFedFish on realistic, large-scale cross-device benchmarks. While the\nperformance of FedAvg can suffer as client models drift further apart, we\ndemonstrate that FedFish is more robust to longer local training. Our\nevaluation across several settings in image and language benchmarks shows that\nFedFish outperforms FedAvg as local training epochs increase. Further, FedFish\nresults in global networks that are more amenable to efficient personalization\nvia local fine-tuning on the same or shifted data distributions. For instance,\nfederated pretraining on the C4 dataset, followed by few-shot personalization\non Stack Overflow, results in a 7% improvement in next-token prediction by\nFedFish over FedAvg.\n","authors":["Nikita Dhawan","Nicole Mitchell","Zachary Charles","Zachary Garrett","Gintare Karolina Dziugaite"],"pdf_url":"https://arxiv.org/pdf/2311.10291v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.05836v3","updated":"2023-11-17T02:35:52Z","published":"2023-11-10T02:47:15Z","title":"UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical\n  Neural Radiance Fields","summary":"  In the field of clinical medicine, computed tomography (CT) is an effective\nmedical imaging modality for the diagnosis of various pathologies. Compared\nwith X-ray images, CT images can provide more information, including\nmulti-planar slices and three-dimensional structures for clinical diagnosis.\nHowever, CT imaging requires patients to be exposed to large doses of ionizing\nradiation for a long time, which may cause irreversible physical harm. In this\npaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on\ngenerated radiation fields. The network can learn a continuous representation\nof CT projections from 2D X-ray images by obtaining the internal structure and\ndepth information and using adaptive loss weights to ensure the quality of the\ngenerated images. Our model is trained on publicly available knee and chest\ndatasets, and we show the results of CT projection rendering with a single\nX-ray and compare our method with other methods based on generated radiation\nfields.\n","authors":["Jing Hu","Qinrui Fan","Shu Hu","Siwei Lyu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.05439v3","updated":"2023-11-17T02:20:40Z","published":"2021-09-12T06:13:33Z","title":"Concave Utility Reinforcement Learning with Zero-Constraint Violations","summary":"  We consider the problem of tabular infinite horizon concave utility\nreinforcement learning (CURL) with convex constraints. For this, we propose a\nmodel-based learning algorithm that also achieves zero constraint violations.\nAssuming that the concave objective and the convex constraints have a solution\ninterior to the set of feasible occupation measures, we solve a tighter\noptimization problem to ensure that the constraints are never violated despite\nthe imprecise model knowledge and model stochasticity. We use Bellman\nerror-based analysis for tabular infinite-horizon setups which allows analyzing\nstochastic policies. Combining the Bellman error-based analysis and tighter\noptimization equation, for $T$ interactions with the environment, we obtain a\nhigh-probability regret guarantee for objective which grows as\n$\\Tilde{O}(1/\\sqrt{T})$, excluding other factors. The proposed method can be\napplied for optimistic algorithms to obtain high-probability regret bounds and\nalso be used for posterior sampling algorithms to obtain a loose Bayesian\nregret bounds but with significant improvement in computational complexity.\n","authors":["Mridul Agarwal","Qinbo Bai","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2109.05439v3.pdf","comment":"Transactions on Machine Learning Research, Dec 2022"},{"id":"http://arxiv.org/abs/2311.10278v1","updated":"2023-11-17T01:55:15Z","published":"2023-11-17T01:55:15Z","title":"Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint","summary":"  Human fingerprints serve as one unique and powerful characteristic for each\nperson, from which policemen can recognize the identity. Similar to humans,\nmany natural bodies and intrinsic mechanical qualities can also be uniquely\nidentified from surface characteristics. To measure the elasto-plastic\nproperties of one material, one formally sharp indenter is pushed into the\nmeasured body under constant force and retracted, leaving a unique residual\nimprint of the minute size from several micrometers to nanometers. However, one\ngreat challenge is how to map the optical image of this residual imprint into\nthe real wanted mechanical properties, i.e., the tensile force curve. In this\npaper, we propose a novel method to use multi-fidelity neural networks (MFNN)\nto solve this inverse problem. We first actively train the NN model via pure\nsimulation data, and then bridge the sim-to-real gap via transfer learning. The\nmost innovative part is that we use NN to dig out the unknown physics and also\nimplant the known physics into the transfer learning framework, thus highly\nimproving the model stability and decreasing the data requirement. This work\nserves as one great example of applying machine learning into the real\nexperimental research, especially under the constraints of data limitation and\nfidelity variance.\n","authors":["Yongchao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.10278v1.pdf","comment":"8 pages, 4 figures, NeurIPS 2023 Workshop on Adaptive Experimental\n  Design and Active Learning in the Real World"},{"id":"http://arxiv.org/abs/2303.17807v2","updated":"2023-11-17T01:49:57Z","published":"2023-03-31T05:43:21Z","title":"GPT-4 can pass the Korean National Licensing Examination for Korean\n  Medicine Doctors","summary":"  Traditional Korean medicine (TKM) emphasizes individualized diagnosis and\ntreatment. This uniqueness makes AI modeling difficult due to limited data and\nimplicit processes. Large language models (LLMs) have demonstrated impressive\nmedical inference, even without advanced training in medical texts. This study\nassessed the capabilities of GPT-4 in TKM, using the Korean National Licensing\nExamination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The\nK-NLEKMD, administered by a national organization, encompasses 12 major\nsubjects in TKM. We optimized prompts with Chinese-term annotation, English\ntranslation for questions and instruction, exam-optimized instruction, and\nself-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,\nsurpassing both the examination's average pass mark of 60% and the 40% minimum\nfor each subject. The gradual introduction of language-related prompts and\nprompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.\nGPT-4 showed low accuracy in subjects including public health &\nmedicine-related law, internal medicine (2) which are localized in Korea and\nTKM. The model's accuracy was lower for questions requiring TKM-specialized\nknowledge. It exhibited higher accuracy in diagnosis-based and recall-based\nquestions than in intervention-based questions. A positive correlation was\nobserved between the consistency and accuracy of GPT-4's responses. This study\nunveils both the potential and challenges of applying LLMs to TKM. These\nfindings underline the potential of LLMs like GPT-4 in culturally adapted\nmedicine, especially TKM, for tasks such as clinical assistance, medical\neducation, and research. But they also point towards the necessity for the\ndevelopment of methods to mitigate cultural bias inherent in large language\nmodels and validate their efficacy in real-world clinical settings.\n","authors":["Dongyeop Jang","Tae-Rim Yun","Choong-Yeol Lee","Young-Kyu Kwon","Chang-Eop Kim"],"pdf_url":"https://arxiv.org/pdf/2303.17807v2.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.10277v1","updated":"2023-11-17T01:48:07Z","published":"2023-11-17T01:48:07Z","title":"Sobol Sequence Optimization for Hardware-Efficient Vector Symbolic\n  Architectures","summary":"  Hyperdimensional computing (HDC) is an emerging computing paradigm with\nsignificant promise for efficient and robust learning. In HDC, objects are\nencoded with high-dimensional vector symbolic sequences called hypervectors.\nThe quality of hypervectors, defined by their distribution and independence,\ndirectly impacts the performance of HDC systems. Despite a large body of work\non the processing parts of HDC systems, little to no attention has been paid to\ndata encoding and the quality of hypervectors. Most prior studies have\ngenerated hypervectors using inherent random functions, such as MATLAB`s or\nPython`s random function. This work introduces an optimization technique for\ngenerating hypervectors by employing quasi-random sequences. These sequences\nhave recently demonstrated their effectiveness in achieving accurate and\nlow-discrepancy data encoding in stochastic computing systems. The study\noutlines the optimization steps for utilizing Sobol sequences to produce\nhigh-quality hypervectors in HDC systems. An optimization algorithm is proposed\nto select the most suitable Sobol sequences for generating minimally correlated\nhypervectors, particularly in applications related to symbol-oriented\narchitectures. The performance of the proposed technique is evaluated in\ncomparison to two traditional approaches of generating hypervectors based on\nlinear-feedback shift registers and MATLAB random function. The evaluation is\nconducted for two applications: (i) language and (ii) headline classification.\nOur experimental results demonstrate accuracy improvements of up to 10.79%,\ndepending on the vector size. Additionally, the proposed encoding hardware\nexhibits reduced energy consumption and a superior area-delay product.\n","authors":["Sercan Aygun","M. Hassan Najafi"],"pdf_url":"https://arxiv.org/pdf/2311.10277v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.10270v1","updated":"2023-11-17T01:30:43Z","published":"2023-11-17T01:30:43Z","title":"Multiscale Hodge Scattering Networks for Data Analysis","summary":"  We propose new scattering networks for signals measured on simplicial\ncomplexes, which we call \\emph{Multiscale Hodge Scattering Networks} (MHSNs).\nOur construction is based on multiscale basis dictionaries on simplicial\ncomplexes, i.e., the $\\kappa$-GHWT and $\\kappa$-HGLET, which we recently\ndeveloped for simplices of dimension $\\kappa \\in \\N$ in a given simplicial\ncomplex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT)\nand Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\\kappa$-GHWT and\nthe $\\kk$-HGLET both form redundant sets (i.e., dictionaries) of multiscale\nbasis vectors and the corresponding expansion coefficients of a given signal.\nOur MHSNs use a layered structure analogous to a convolutional neural network\n(CNN) to cascade the moments of the modulus of the dictionary coefficients. The\nresulting features are invariant to reordering of the simplices (i.e., node\npermutation of the underlying graphs). Importantly, the use of multiscale basis\ndictionaries in our MHSNs admits a natural pooling operation that is akin to\nlocal pooling in CNNs, and which may be performed either locally or per-scale.\nThese pooling operations are harder to define in both traditional scattering\nnetworks based on Morlet wavelets, and geometric scattering networks based on\nDiffusion Wavelets. As a result, we are able to extract a rich set of\ndescriptive yet robust features that can be used along with very simple machine\nlearning methods (i.e., logistic regression or support vector machines) to\nachieve high-accuracy classification systems with far fewer parameters to train\nthan most modern graph neural networks. Finally, we demonstrate the usefulness\nof our MHSNs in three distinct types of problems: signal classification, domain\n(i.e., graph/simplex) classification, and molecular dynamics prediction.\n","authors":["Naoki Saito","Stefan C. Schonsheck","Eugene Shvarts"],"pdf_url":"https://arxiv.org/pdf/2311.10270v1.pdf","comment":"20 Pages, Comments Welcome"},{"id":"http://arxiv.org/abs/2311.10267v1","updated":"2023-11-17T01:27:01Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v1.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2311.09574v2","updated":"2023-11-17T01:24:47Z","published":"2023-11-16T05:17:14Z","title":"LymphoML: An interpretable artificial intelligence-based method\n  identifies morphologic features that correlate with lymphoma subtype","summary":"  The accurate classification of lymphoma subtypes using hematoxylin and eosin\n(H&E)-stained tissue is complicated by the wide range of morphological features\nthese cancers can exhibit. We present LymphoML - an interpretable machine\nlearning method that identifies morphologic features that correlate with\nlymphoma subtypes. Our method applies steps to process H&E-stained tissue\nmicroarray cores, segment nuclei and cells, compute features encompassing\nmorphology, texture, and architecture, and train gradient-boosted models to\nmake diagnostic predictions. LymphoML's interpretable models, developed on a\nlimited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy\nto pathologists using whole-slide images and outperform black box deep-learning\non a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using\nSHapley Additive exPlanation (SHAP) analysis, we assess the impact of each\nfeature on model prediction and find that nuclear shape features are most\ndiscriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma\n(F1-score: 74.5%). Finally, we provide the first demonstration that a model\ncombining features from H&E-stained tissue with features from a standardized\npanel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a\n46-stain panel (86.1%).\n","authors":["Vivek Shankar","Xiaoli Yang","Vrishab Krishna","Brent Tan","Oscar Silva","Rebecca Rojansky","Andrew Ng","Fabiola Valvert","Edward Briercheck","David Weinstock","Yasodha Natkunam","Sebastian Fernandez-Pol","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2311.09574v2.pdf","comment":"To be published in Proceedings of the 3rd Machine Learning for Health\n  symposium, Proceedings of Machine Learning Research (PMLR)"},{"id":"http://arxiv.org/abs/2309.09969v2","updated":"2023-11-17T01:24:26Z","published":"2023-09-18T17:50:17Z","title":"Prompt a Robot to Walk with Large Language Models","summary":"  Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .\n","authors":["Yen-Jen Wang","Bike Zhang","Jianyu Chen","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2309.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10263v1","updated":"2023-11-17T01:14:24Z","published":"2023-11-17T01:14:24Z","title":"Stable Differentiable Causal Discovery","summary":"  Inferring causal relationships as directed acyclic graphs (DAGs) is an\nimportant but challenging problem. Differentiable Causal Discovery (DCD) is a\npromising approach to this problem, framing the search as a continuous\noptimization. But existing DCD methods are numerically unstable, with poor\nperformance beyond tens of variables. In this paper, we propose Stable\nDifferentiable Causal Discovery (SDCD), a new method that improves previous DCD\nmethods in two ways: (1) It employs an alternative constraint for acyclicity;\nthis constraint is more stable, both theoretically and empirically, and fast to\ncompute. (2) It uses a training procedure tailored for sparse causal graphs,\nwhich are common in real-world scenarios. We first derive SDCD and prove its\nstability and correctness. We then evaluate it with both observational and\ninterventional data and on both small-scale and large-scale settings. We find\nthat SDCD outperforms existing methods in both convergence speed and accuracy\nand can scale to thousands of variables.\n","authors":["Achille Nazaret","Justin Hong","Elham Azizi","David Blei"],"pdf_url":"https://arxiv.org/pdf/2311.10263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10255v1","updated":"2023-11-17T00:53:09Z","published":"2023-11-17T00:53:09Z","title":"FREE: The Foundational Semantic Recognition for Modeling Environmental\n  Ecosystems","summary":"  Modeling environmental ecosystems is critical for the sustainability of our\nplanet, but is extremely challenging due to the complex underlying processes\ndriven by interactions amongst a large number of physical variables. As many\nvariables are difficult to measure at large scales, existing works often\nutilize a combination of observable features and locally available measurements\nor modeled values as input to build models for a specific study region and time\nperiod. This raises a fundamental question in advancing the modeling of\nenvironmental ecosystems: how to build a general framework for modeling the\ncomplex relationships amongst various environmental data over space and time?\nIn this paper, we introduce a new framework, FREE, which maps available\nenvironmental data into a text space and then converts the traditional\npredictive modeling task in environmental science to the semantic recognition\nproblem. The proposed FREE framework leverages recent advances in Large\nLanguage Models (LLMs) to supplement the original input features with natural\nlanguage descriptions. This facilitates capturing the data semantics and also\nallows harnessing the irregularities of input features. When used for long-term\nprediction, FREE has the flexibility to incorporate newly collected\nobservations to enhance future prediction. The efficacy of FREE is evaluated in\nthe context of two societally important real-world applications, predicting\nstream water temperature in the Delaware River Basin and predicting annual corn\nyield in Illinois and Iowa. Beyond the superior predictive performance over\nmultiple baseline methods, FREE is shown to be more data- and\ncomputation-efficient as it can be pre-trained on simulated data generated by\nphysics-based models.\n","authors":["Shiyuan Luo","Juntong Ni","Shengyu Chen","Runlong Yu","Yiqun Xie","Licheng Liu","Zhenong Jin","Huaxiu Yao","Xiaowei Jia"],"pdf_url":"https://arxiv.org/pdf/2311.10255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10251v1","updated":"2023-11-17T00:44:56Z","published":"2023-11-17T00:44:56Z","title":"UniMOS: A Universal Framework For Multi-Organ Segmentation Over\n  Label-Constrained Datasets","summary":"  Machine learning models for medical images can help physicians diagnose and\nmanage diseases. However, due to the fact that medical image annotation\nrequires a great deal of manpower and expertise, as well as the fact that\nclinical departments perform image annotation based on task orientation, there\nis the problem of having fewer medical image annotation data with more\nunlabeled data and having many datasets that annotate only a single organ. In\nthis paper, we present UniMOS, the first universal framework for achieving the\nutilization of fully and partially labeled images as well as unlabeled images.\nSpecifically, we construct a Multi-Organ Segmentation (MOS) module over\nfully/partially labeled data as the basenet and designed a new target adaptive\nloss. Furthermore, we incorporate a semi-supervised training module that\ncombines consistent regularization and pseudolabeling techniques on unlabeled\ndata, which significantly improves the segmentation of unlabeled data.\nExperiments show that the framework exhibits excellent performance in several\nmedical image segmentation tasks compared to other advanced methods, and also\nsignificantly improves data utilization and reduces annotation cost. Code and\nmodels are available at: https://github.com/lw8807001/UniMOS.\n","authors":["Can Li","Sheng Shao","Junyi Qu","Shuchao Pang","Mehmet A. Orgun"],"pdf_url":"https://arxiv.org/pdf/2311.10251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10248v1","updated":"2023-11-17T00:39:59Z","published":"2023-11-17T00:39:59Z","title":"FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning\n  Framework","summary":"  Federated Learning (FL) enables collaborative machine learning model training\nacross multiple parties without sharing raw data. However, FL's distributed\nnature allows malicious clients to impact model training through Byzantine or\nbackdoor attacks, using erroneous model updates. Existing defenses measure the\ndeviation of each update from a 'ground-truth model update.' They often rely on\na benign root dataset on the server or use trimmed mean or median for clipping,\nboth methods having limitations.\n  We introduce FedTruth, a robust defense against model poisoning in FL.\nFedTruth doesn't assume specific data distributions nor requires a benign root\ndataset. It estimates a global model update with dynamic aggregation weights,\nconsidering contributions from all benign clients. Empirical studies\ndemonstrate FedTruth's efficacy in mitigating the impacts of poisoned updates\nfrom both Byzantine and backdoor attacks.\n","authors":["Sheldon C. Ebron Jr.","Kan Yang"],"pdf_url":"https://arxiv.org/pdf/2311.10248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10246v1","updated":"2023-11-17T00:35:38Z","published":"2023-11-17T00:35:38Z","title":"Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric\n  Learning","summary":"  Nonparametric learning is a fundamental concept in machine learning that aims\nto capture complex patterns and relationships in data without making strong\nassumptions about the underlying data distribution. Owing to simplicity and\nfamiliarity, one of the most well-known algorithms under this paradigm is the\n$k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine\nlearning in safety-critical applications, in this work, we shed new light on\nthe traditional nearest neighbors algorithm from the perspective of information\ntheory and propose a robust and interpretable framework for tasks such as\nclassification, regression, and anomaly detection using a single model. Instead\nof using a traditional distance measure which needs to be scaled and\ncontextualized, we use a novel formulation of \\textit{surprisal} (amount of\ninformation required to explain the difference between the observed and\nexpected result). Finally, we demonstrate this architecture's capability to\nperform at-par or above the state-of-the-art on classification, regression, and\nanomaly detection tasks using a single model with enhanced interpretability by\nproviding novel concepts for characterizing data and predictions.\n","authors":["Amartya Banerjee","Christopher J. Hazard","Jacob Beel","Cade Mack","Jack Xia","Michael Resnick","Will Goddin"],"pdf_url":"https://arxiv.org/pdf/2311.10246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06233v2","updated":"2023-11-17T00:20:44Z","published":"2023-11-10T18:48:58Z","title":"Data Contamination Quiz: A Tool to Detect and Estimate Contamination in\n  Large Language Models","summary":"  We propose the Data Contamination Quiz, a simple and effective approach to\ndetect data contamination in large language models (LLMs) and estimate the\namount of it. Specifically, we frame data contamination detection as a series\nof multiple-choice questions. We devise a quiz format wherein three perturbed\nversions of each dataset instance are created. These changes only include\nword-level perturbations, replacing words with their contextual synonyms,\nensuring both the semantic and sentence structure remain exactly the same as\nthe original instance. Together with the original instance, these perturbed\nversions constitute the choices in the quiz. Given that the only distinguishing\nsignal among these choices is the exact wording, an LLM, when tasked with\nidentifying the original instance from the choices, opts for the original if it\nhas memorized it in its pre-training phase--a trait intrinsic to LLMs. A\ndataset partition is then marked as contaminated if the LLM's performance on\nthe quiz surpasses what random chance suggests. Our evaluation spans seven\ndatasets and their respective splits (train and test/validation) on two\nstate-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the\npre-training data, our results suggest that our approach not only enhances the\ndetection of data contamination but also provides an accurate estimation of its\nextent, even when the contamination signal is weak.\n","authors":["Shahriar Golchin","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2311.06233v2.pdf","comment":"v1.1 preprint"},{"id":"http://arxiv.org/abs/2311.10242v1","updated":"2023-11-17T00:08:19Z","published":"2023-11-17T00:08:19Z","title":"Advancements in Generative AI: A Comprehensive Review of GANs, GPT,\n  Autoencoders, Diffusion Model, and Transformers","summary":"  The launch of ChatGPT has garnered global attention, marking a significant\nmilestone in the field of Generative Artificial Intelligence. While Generative\nAI has been in effect for the past decade, the introduction of ChatGPT has\nignited a new wave of research and innovation in the AI domain. This surge in\ninterest has led to the development and release of numerous cutting-edge tools,\nsuch as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,\namong others. These tools exhibit remarkable capabilities, encompassing tasks\nranging from text generation and music composition, image creation, video\nproduction, code generation, and even scientific work. They are built upon\nvarious state-of-the-art models, including Stable Diffusion, transformer models\nlike GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial\nnetworks. This advancement in Generative AI presents a wealth of exciting\nopportunities and, simultaneously, unprecedented challenges. Throughout this\npaper, we have explored these state-of-the-art models, the diverse array of\ntasks they can accomplish, the challenges they pose, and the promising future\nof Generative Artificial Intelligence.\n","authors":["Staphord Bengesi","Hoda El-Sayed","Md Kamruzzaman Sarker","Yao Houkpati","John Irungu","Timothy Oladunni"],"pdf_url":"https://arxiv.org/pdf/2311.10242v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2311.10709v1","updated":"2023-11-17T18:59:04Z","published":"2023-11-17T18:59:04Z","title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image\n  Conditioning","summary":"  We present Emu Video, a text-to-video generation model that factorizes the\ngeneration into two steps: first generating an image conditioned on the text,\nand then generating a video conditioned on the text and the generated image. We\nidentify critical design decisions--adjusted noise schedules for diffusion, and\nmulti-stage training--that enable us to directly generate high quality and high\nresolution videos, without requiring a deep cascade of models as in prior work.\nIn human evaluations, our generated videos are strongly preferred in quality\ncompared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's\nPYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial\nsolutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing\napproach naturally lends itself to animating images based on a user's text\nprompt, where our generations are preferred 96% over prior work.\n","authors":["Rohit Girdhar","Mannat Singh","Andrew Brown","Quentin Duval","Samaneh Azadi","Sai Saketh Rambhatla","Akbar Shah","Xi Yin","Devi Parikh","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2311.10709v1.pdf","comment":"Project page: https://emu-video.metademolab.com"},{"id":"http://arxiv.org/abs/2311.10645v1","updated":"2023-11-17T17:01:09Z","published":"2023-11-17T17:01:09Z","title":"User Dynamics-Aware Edge Caching and Computing for Mobile Virtual\n  Reality","summary":"  In this paper, we present a novel content caching and delivery approach for\nmobile virtual reality (VR) video streaming. The proposed approach aims to\nmaximize VR video streaming performance, i.e., minimizing video frame missing\nrate, by proactively caching popular VR video chunks and adaptively scheduling\ncomputing resources at an edge server based on user and network dynamics.\nFirst, we design a scalable content placement scheme for deciding which video\nchunks to cache at the edge server based on tradeoffs between computing and\ncaching resource consumption. Second, we propose a machine learning-assisted VR\nvideo delivery scheme, which allocates computing resources at the edge server\nto satisfy video delivery requests from multiple VR headsets. A Whittle\nindex-based method is adopted to reduce the video frame missing rate by\nidentifying network and user dynamics with low signaling overhead. Simulation\nresults demonstrate that the proposed approach can significantly improve VR\nvideo streaming performance over conventional caching and computing resource\nscheduling strategies.\n","authors":["Mushu Li","Jie Gao","Conghao Zhou","Xuemin Shen","Weihua Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.10645v1.pdf","comment":"38 pages, 13 figures, single column double spaced, published in IEEE\n  Journal of Selected Topics in Signal Processing"},{"id":"http://arxiv.org/abs/2311.10256v1","updated":"2023-11-17T00:56:55Z","published":"2023-11-17T00:56:55Z","title":"Exploring User Perceptions of Virtual Reality Scene Design in Metaverse\n  Learning Environments","summary":"  Metaverse learning environments allow for a seamless and intuitive transition\nbetween activities compared to Virtual Reality (VR) learning environments, due\nto their interconnected design. The design of VR scenes is important for\ncreating effective learning experiences in the Metaverse. However, there is\nlimited research on the impact of different design elements on user's learning\nexperiences in VR scenes. To address this, a study was conducted with 16\nparticipants who interacted with two VR scenes, each with varying design\nelements such as style, color, texture, object, and background, while watching\na short tutorial. Participant rankings of the scenes for learning were obtained\nusing a seven-point Likert scale, and the Mann-Whitney U test was used to\nvalidate differences in preference between the scenes. The results showed a\nsignificant difference in preference between the scenes. Further analysis using\nthe NASA TLX questionnaire was conducted to examine the impact of this\ndifference on cognitive load, and participant feedback was also considered. The\nstudy emphasizes the importance of careful VR scene design to improve the\nuser's learning experience.\n","authors":["Rahatara Ferdousi","Mohammed Faisal","Fedwa Laamarti","Chunsheng Yang","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2311.10256v1.pdf","comment":"6 pages,3 figures, accepted to present at IEEE 42nd International\n  Conference on Consumer Electronics"}]}}